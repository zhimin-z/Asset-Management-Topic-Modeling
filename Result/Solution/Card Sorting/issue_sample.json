[
    {
        "Challenge_adjusted_solved_time":280.5022222222,
        "Challenge_answer_count":6,
        "Challenge_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1589303399000,
        "Challenge_created_time":1588293591000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.2,
        "Challenge_reading_time":47.45,
        "Challenge_repo_contributor_count":326,
        "Challenge_repo_fork_count":1350,
        "Challenge_repo_issue_count":8555,
        "Challenge_repo_star_count":3062,
        "Challenge_repo_watch_count":103,
        "Challenge_score_count":0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":280.5022222222,
        "Challenge_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":390,
        "Platform":"Github",
        "Solution_body":"Thanks Marwan for trying out the component. \r\nI believe your script was buried inside your custom image, if so that custom image runs inside sagemaker which does not inherit the `use_aws_secret` values. So either you need to add permission to the role https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L10 or read it from AWS secret manager. \r\n\r\nWould you mind sharing your script or minimal reproducible code ?  @gautamkmr  - thank you for taking the time to respond to this issue\r\n\r\nPlease find below a very simplified version of the script I'd like to run but hopefully should be good enough to show where the issue is - please note the comments in the script\r\n```\r\nimport pathlib\r\nimport os\r\nimport boto3\r\nimport sys\r\n\r\n\r\ndef main():\r\n    try:\r\n        # Reading data that sagemaker has copied from s3\r\n        # works fine\r\n        prefix = pathlib.Path('\/opt\/ml\/')\r\n        input_path = prefix \/ 'input\/data\/train\/'\r\n\r\n        with open(input_path \/ 'test.txt', 'r') as f:\r\n            content = f.read()\r\n\r\n        assert 'hello world' in content\r\n\r\n        # the below portion is trying to read data from s3\r\n        # using boto3 but it fails\r\n        bucket_name = os.environ['AWS_BUCKET']\r\n        object_name = 'dummy_input\/test.txt'\r\n        file_name = 'test.txt'\r\n\r\n        s3 = boto3.client('s3')\r\n\r\n        # specifically the below line fails:\r\n        # botocore.exceptions.NoCredentialsError: Unable to locate credentials\r\n        s3.download_file(bucket_name, object_name, file_name)\r\n\r\n    except Exception:\r\n        sys.exit(255)\r\n```\r\n\r\nHere is the script for compiling the pipeline just in case you need it\r\n```\r\nimport kfp.compiler as compiler\r\nimport json\r\nimport os\r\nfrom kfp import components, dsl\r\nfrom kfp.aws import use_aws_secret\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"sm_kfp_example\",\r\n    description=\"sample sagemaker training job\"\r\n)\r\ndef sm_kfp_example():\r\n    bucket = os.environ.get('AWS_BUCKET')\r\n    role = os.environ.get('SAGEMAKER_ROLE_ARN')\r\n\r\n    train_channels = json.dumps([{\r\n        'ChannelName': 'train',\r\n        'DataSource': {\r\n            'S3DataSource': {\r\n                'S3Uri': f's3:\/\/{bucket}\/dummy_input\/',\r\n                'S3DataType': 'S3Prefix',\r\n                'S3DataDistributionType': 'FullyReplicated'\r\n            }\r\n        },\r\n        'ContentType': '',\r\n        'CompressionType': 'None',\r\n        'RecordWrapperType': 'None',\r\n        'InputMode': 'File'\r\n    }])\r\n\r\n    # use the sagemaker training operator defined by aws\r\n    # [a wrapper around Sagemaker CreateTrainingJob]\r\n    repo_path = 'https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines'\r\n    # commit hash of current version of kfp that we are using\r\n    commit = 'master'\r\n    suffix = 'components\/aws\/sagemaker\/train\/component.yaml'\r\n    path = f'{repo_path}\/{commit}\/{suffix}'\r\n\r\n    sagemaker_train_op = components.load_component_from_url(path)\r\n    output_path = f's3:\/\/{bucket}\/output'\r\n    account_id = os.environ.get('AWS_ACCOUNT_ID')\r\n    region = os.environ.get('AWS_REGION')\r\n    image = f'{account_id}.dkr.ecr.{region}.amazonaws.com\/sm_kfp_example:latest'\r\n\r\n    _ = sagemaker_train_op(\r\n        region=region,\r\n        endpoint_url='',\r\n        image=image,\r\n        training_input_mode='File',\r\n        hyperparameters='{}',\r\n        channels=train_channels,\r\n        instance_type='ml.m5.xlarge',\r\n        instance_count='1',\r\n        volume_size='20',\r\n        max_run_time='3600',\r\n        model_artifact_path=output_path,\r\n        output_encryption_key='',\r\n        network_isolation='True',\r\n        traffic_encryption='False',\r\n        spot_instance='False',\r\n        max_wait_time='3600',\r\n        checkpoint_config='{}',\r\n        role=role,\r\n    ).apply(\r\n        use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')\r\n    )\r\n\r\n\r\ndef compile(pipeline_func):\r\n    pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # compile the pipeline\r\n    compile(sm_kfp_example)\r\n```\r\n\r\nThe very strange thing is if I try to create the training job using the sagemaker python sdk (i.e. not using sagemaker's k8s training operator) - the script runs fine - i.e. the credentials are passed down to the container - below is the script in case you need it\r\n\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.estimator import Estimator\r\nimport os\r\n\r\naws_region = os.environ['AWS_REGION']\r\nalgorithm_name = \"sm_kfp_example\"\r\ns3_bucket = os.environ['AWS_BUCKET']\r\n\r\n# use the security token service to verify the account identity\r\nclient = boto3.client('sts')\r\naccount = client.get_caller_identity()['Account']\r\n\r\n# set the sagemaker role\r\nrole = os.environ['SAGEMAKER_ROLE_ARN']\r\n\r\n# create a boto_session\r\nboto_session = boto3.session.Session(\r\n    region_name=aws_region,\r\n)\r\n\r\n# get full training image url\r\ntraining_image = f\"{account}.dkr.ecr.{aws_region}.amazonaws.com\/{algorithm_name}:latest\"\r\n\r\n\r\n# specify location on s3_bucket to output the results\r\ns3_output_location = f's3:\/\/{s3_bucket}\/output'\r\n\r\n# create a sagemaker_session\r\nsagemaker_session = sagemaker.session.Session(boto_session=boto_session)\r\n\r\n# create an estimator\r\nestimator = Estimator(\r\n    training_image,\r\n    role,\r\n    train_instance_count=1,\r\n    train_instance_type='ml.m5.xlarge',\r\n    train_volume_size=10,  # 10 GB\r\n    train_max_run=600,  # 10 minutes = 600seconds\r\n    input_mode='File',\r\n    output_path=s3_output_location,\r\n    sagemaker_session=sagemaker_session,\r\n    hyperparameters={},\r\n    base_job_name=\"sagemaker-sample\",\r\n)\r\n\r\nestimator.fit(\r\n    inputs={\r\n        'train': f's3:\/\/{s3_bucket}\/dummy_input\/'\r\n    },\r\n    logs=True\r\n)\r\n```\r\n Note, to avoid opening other Sagemaker issues, I will list out some of the pain points I have faced trying to integrate sagemaker with Kubeflow here and let me know if there are any solutions to these - excuse me for not following protocol - but if these are deemed as valid issues -I would be glad to open up the relevant issues:\r\n\r\n- I can't seem to pass an image to sagemaker_training_op that is not hosted on ECR and if the image is hosted on ECR - it has to be in the same region as that specified for sagemaker_training_op ... \r\n\r\n- Currently, the sagemaker logs are being output to cloudwatch not to kubeflow (would be much easier if they can be forwarded to kubeflow)\r\n  There has been some undocumented change to the `load_component_*` functions. It used to return a `ContainerOp`, now it returns a `TaskSpec` instead.\r\n\r\nCurrently there are 2 possible workaround:\r\n\r\n1.  use the private func `_create_container_op_from_component_and_arguments` to generate ur containerop from taskspec\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\n\r\ncomponent_op = components.load_component_from_url(...)\r\ntaskspec = component_op(...)\r\ncontainerop = _create_container_op_from_component_and_arguments(\r\n  taskspec.component_ref.spec, \r\n  taskspec.arguments, \r\n  taskspec.component_ref\r\n)\r\n```\r\n\r\n2. overwrite the default `_default_container_task_constructor`\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\nimport kfp.components._components as _components\r\n\r\n_components._default_container_task_constructor = _create_container_op_from_component_and_arguments\r\n\r\n# now load_component will return a containerop\r\ncontainerop = components.load_component_from_url(...)\r\n```\r\n\r\nPS: @Ark-kun this is not the first time I seen this issue\/qns - what do u think? Hi @marwan116, I was able to reproduce the failure and the root cause is that the default value for `network_isolation` parameter is set to [False in python sdk](https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/bf48fb1219bd8ea22e78a913bfa091e544c57cc3\/src\/sagemaker\/estimator.py#L1112)  whereas in the pipeline definition you provided it is set to True, which is also the [default value](https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L73-L75) in training component\r\n\r\nCan you try set it to False and let us know if your issue has been resolved ?\r\n\r\n\r\n----\r\n\r\nHere are some clarifications based on posts on this thread: \r\n\r\n- The logs you posted in the issue initially [under whats happened section](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670#issue-610481941) (except the exception) is from the component pod and NOT from the training job itself. As you have already observed, for the training job logs you need to go to cloudwatch.\r\n  - the first log line which you see `INFO:botocore.credentials:Found credentials in environment variables.` is from boto session which is created by the component backend to call create_training_job API. It uses the credentials are from `aws-secret` that you would have created. These credentials are only used to invoke the job and are not passed to the instance in SageMaker\r\n\r\n- The SageMaker instance which runs in AWS assumes the credentials from the role ARN you provide in`SAGEMAKER_ROLE_ARN` not not from the secret\r\n\r\nLet us know if you have more questions.\r\n @surajkota  - thank you so much for taking the time to reproduce this - yes you are right it is because I had `network_isolation` set to `True` - (sorry I should have taken the time to understand what `network_isolation` does)\r\n\r\nAlso thank you for the clarifications!\r\n\r\nI saw @gautamkmr graciously took the time to open an issue concerning the logs - thank you @gautamkmr !\r\n\r\nI am closing this now as this particular issue is now resolved",
        "Solution_gpt_summary":"add permiss role read secret pass credenti imag train oper run set network isol privat function creat compon argument gener containerop taskspec overwrit default default task constructor return containerop load compon url log initi except compon pod train job train job log cloudwatch instanc run credenti role arn role arn secret",
        "Solution_link_count":5.0,
        "Solution_original_content":"marwan compon believ buri insid imag imag run insid inherit secret valu add permiss role http github com kubeflow pipelin blob master compon train compon yaml read secret share minim reproduc gautamkmr time simplifi version run hopefulli note comment import pathlib import import boto import sy read data copi prefix pathlib path opt input path prefix input data train open input path test txt read assert world portion read data boto bucket environ bucket object dummi input test txt file test txt boto client line botocor except nocredentialserror locat credenti download file bucket object file except sy exit compil pipelin import kfp compil compil import json import kfp import compon dsl kfp import secret dsl pipelin kfp descript sampl train job kfp bucket environ bucket role environ role arn train channel json dump channelnam train datasourc sdatasourc suri bucket dummi input sdatatyp sprefix sdatadistributiontyp fullyrepl contenttyp compressiontyp recordwrappertyp inputmod file train oper defin wrapper createtrainingjob repo path http raw githubusercont com kubeflow pipelin commit hash version kfp commit master suffix compon train compon yaml path repo path commit suffix train compon load compon url path output path bucket output account environ account region environ region imag account dkr ecr region amazonaw com kfp latest train region region endpoint url imag imag train input mode file hyperparamet channel train channel instanc type xlarg instanc count volum size run time model artifact path output path output encrypt kei network isol traffic encrypt spot instanc wait time checkpoint config role role appli secret secret access kei secret access kei compil pipelin func pipelin filenam pipelin func pipelin tar compil compil compil pipelin func pipelin filenam compil pipelin compil kfp creat train job sdk train oper run credenti pass import boto import estim import estim import region environ region algorithm kfp bucket environ bucket secur token servic verifi account ident client boto client st account client caller ident account set role role environ role arn creat boto session boto session boto session session region region train imag url train imag account dkr ecr region amazonaw com algorithm latest specifi locat bucket output output locat bucket output creat session session session session boto session boto session creat estim estim estim train imag role train instanc count train instanc type xlarg train volum size train run minut input mode file output path output locat session session hyperparamet base job sampl estim fit input train bucket dummi input log note avoid open list pain integr kubeflow excus protocol deem glad open relev pass imag train host ecr imag host ecr region specifi train log output cloudwatch kubeflow easier forward kubeflow undocu load compon function return containerop return taskspec workaround privat func creat compon argument gener containerop taskspec kfp dsl compon bridg import creat compon argument compon compon load compon url taskspec compon containerop creat compon argument taskspec compon ref spec taskspec argument taskspec compon ref overwrit default default task constructor kfp dsl compon bridg import creat compon argument import kfp compon compon compon compon default task constructor creat compon argument load compon return containerop containerop compon load compon url ark kun time qn marwan reproduc root default valu network isol paramet set sdk http github com sdk blob bffbbdeaeabfaeccc src estim pipelin definit set default valu http github com kubeflow pipelin blob master compon train compon yaml train compon set clarif base thread log initi what section http github com kubeflow pipelin except compon pod train job observ train job log cloudwatch log line botocor credenti credenti environ variabl boto session creat compon backend creat train job api credenti secret creat credenti invok job pass instanc instanc run credenti role arn role arn secret surajkota time reproduc network isol set sorri taken time network isol clarif gautamkmr gracious took time open log gautamkmr close",
        "Solution_preprocessed_content":"marwan compon believ buri insid imag imag run insid inherit valu add permiss role read secret share minim reproduc time simplifi version run hopefulli note comment compil pipelin creat train job sdk run credenti pass note avoid open list pain integr kubeflow excus protocol deem glad open relev pass imag host ecr imag host ecr region specifi log output cloudwatch kubeflow undocu function return return workaround privat func gener containerop taskspec overwrit default time reproduc root default valu paramet set pipelin definit set train compon set clarif base thread log initi compon pod train job observ train job log cloudwatch log line boto session creat compon backend api credenti creat credenti invok job pass instanc instanc run credenti role arn secret time reproduc set clarif gracious took time open log close",
        "Solution_readability":13.4,
        "Solution_reading_time":112.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":69.0,
        "Solution_word_count":954.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":120.5566666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551859000,
        "Challenge_created_time":1622117855000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":139,
        "Challenge_repo_star_count":357,
        "Challenge_repo_watch_count":9,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":120.5566666667,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"Fixed in #46 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":27.1641666667,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644307000,
        "Challenge_created_time":1599546516000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":1,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":27.1641666667,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":532,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_gpt_summary":"identifi logger commun server train step metric log epoch modifi codebas elimin overhead store respons client avoid logger commun server train loop plan rais",
        "Solution_link_count":0.0,
        "Solution_original_content":"contribut great tri increas row log interv trainer flag control log sent logger network singl linear layer probabl run epoch fast convinc awaelchli repli model contriv test realis perform mflow logger tensorboard logger inher client debug section properti logger time access client call commun server store respons dramat tensorboard logger sens send test pass happi review yeah link shortli",
        "Solution_preprocessed_content":"contribut great tri increas trainer flag control log sent logger network singl linear layer probabl run epoch fast convinc repli model contriv test realis perform mflow logger tensorboard logger inher client debug section properti logger time access call commun server store respons dramat tensorboard logger sens send test pass happi review yeah link shortli",
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":1,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_created_time":1601890192000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":212,
        "Platform":"Github",
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":305.2888888889,
        "Challenge_answer_count":9,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017877000,
        "Challenge_created_time":1642918837000,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":24,
        "Challenge_repo_issue_count":492,
        "Challenge_repo_star_count":255,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":305.2888888889,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_gpt_summary":"resum option set allow auto avoid log duplic step suppress warn messag overwrit valu gap",
        "Solution_link_count":2.0,
        "Solution_original_content":"checkpoint step train run crash step checkpoint step resum train start step warn log duplic step step log earlier step option resum run callback restor checkpoint step step crash resum option reread doc set resum allow auto http github com allenai tango pull doc http doc ref init auto preivou run crash automat resum start run allow set init uniqu run uniqu ident previou run automat resum run start run allow littl robust mayb run crash auto option resum auto step input resum step step step run step step complain step step document sai valu overwrit on warn messag clearli sai drop probabl suppress warn overwrit valu valu gap determin overwrit valu",
        "Solution_preprocessed_content":"checkpoint step train run crash step checkpoint step resum train start step warn log duplic step step log earlier step option resum run callback restor checkpoint step step crash option reread doc set allow auto auto preivou run crash automat resum start run allow set ident previou run automat resum run start run allow littl robust mayb run crash step input resum step step step step complain step step document sai valu overwrit on warn messag clearli sai drop probabl suppress warn overwrit valu valu gap overwrit valu",
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":376.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0304878049,
        "Challenge_watch_issue_ratio":0.012195122
    },
    {
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_created_time":1624893279000,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11,
        "Challenge_repo_fork_count":14,
        "Challenge_repo_issue_count":160,
        "Challenge_repo_star_count":28,
        "Challenge_repo_watch_count":11,
        "Challenge_score_count":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":22,
        "Platform":"Github",
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.06875,
        "Challenge_watch_issue_ratio":0.06875
    },
    {
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_created_time":1617867548000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171,
        "Challenge_repo_fork_count":1727,
        "Challenge_repo_issue_count":5102,
        "Challenge_repo_star_count":12323,
        "Challenge_repo_watch_count":282,
        "Challenge_score_count":0,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":12402.8375,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_gpt_summary":"gpunum useactivegpu start trial job run speed train process slow train soon set gpunum convert circular structur json nni",
        "Solution_link_count":0.0,
        "Solution_original_content":"yangmingwanli run start trial job start run sparksnail gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap sparksnail workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json sparksnail convert circular structur json nni",
        "Solution_preprocessed_content":"run start trial job start run gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json convert circular structur json nni",
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0335162681,
        "Challenge_watch_issue_ratio":0.0552724422
    },
    {
        "Challenge_adjusted_solved_time":229.9927777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449895000,
        "Challenge_created_time":1648621921000,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":94,
        "Challenge_repo_fork_count":1046,
        "Challenge_repo_issue_count":5133,
        "Challenge_repo_star_count":4495,
        "Challenge_repo_watch_count":116,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":229.9927777778,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":159,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0183128775,
        "Challenge_watch_issue_ratio":0.0225988701
    },
    {
        "Challenge_adjusted_solved_time":52.5838888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Challenge_closed_time":1624287268000,
        "Challenge_created_time":1624097966000,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":1,
        "Challenge_repo_fork_count":0,
        "Challenge_repo_issue_count":35,
        "Challenge_repo_star_count":3,
        "Challenge_repo_watch_count":3,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.5838888889,
        "Challenge_title":"Loading configs from wandb yields incorrect parameters",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0285714286,
        "Challenge_watch_issue_ratio":0.0857142857
    },
    {
        "Challenge_adjusted_solved_time":1225.01,
        "Challenge_answer_count":1,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830449000,
        "Challenge_created_time":1664420413000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":13,
        "Challenge_repo_issue_count":97,
        "Challenge_repo_star_count":37,
        "Challenge_repo_watch_count":2,
        "Challenge_score_count":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1225.01,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":222,
        "Platform":"Github",
        "Solution_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"miss data catalog yml iri data type panda csvdataset filepath data raw iri csv train type pickl pickledataset filepath data model input train pkl train type pickl pickledataset filepath data model input train pkl test type pickl pickledataset filepath data model input test pkl test type pickl pickledataset filepath data model input test pkl model type pickl pickledataset filepath data model model pkl predict type pickl pickledataset filepath data model output predict pkl http readthedoc stabl deploy airflow astronom html highlight astro airflow iri step reproduc version airflow run",
        "Solution_preprocessed_content":"miss data catalog step reproduc version airflow run",
        "Solution_readability":17.8,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":8,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_created_time":1581000215000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":795.3458333333,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":98,
        "Platform":"Github",
        "Solution_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Solution_gpt_summary":"item file whitelist doubl xgboostclassifi blacklist share runid import instal import pip instal train warn ignor potenti model local environ regardless local remot comput bias summari",
        "Solution_link_count":1.0,
        "Solution_original_content":"sorri gone ahead file item whitelist reach sabina xgboostclassifi blacklist doubl share runid improv msg waltzu run line jupyt notebook sai import sabina waltzu reproduc overcom doubl import instal import pip instal end cartacio jialiu sorri late repli proce close thread cartacio kick automl run local remot comput aml comput target http github com machinelearningnotebook add env local technic packag automl exercis jadhosn train simpli ignor warn potenti model add local environ regardless local remot comput",
        "Solution_preprocessed_content":"sorri gone ahead file item whitelist reach sabina xgboostclassifi blacklist doubl share runid improv msg run line jupyt notebook sai sabina reproduc overcom doubl import instal end sorri late repli proce close thread kick automl run local remot comput aml comput target add env technic packag automl exercis train simpli ignor warn potenti model add local environ regardless comput",
        "Solution_readability":6.3,
        "Solution_reading_time":19.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":275.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1488.5780555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Challenge_closed_time":1651060598000,
        "Challenge_created_time":1645701717000,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":148,
        "Challenge_repo_fork_count":956,
        "Challenge_repo_issue_count":3383,
        "Challenge_repo_star_count":6165,
        "Challenge_repo_watch_count":89,
        "Challenge_score_count":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1488.5780555556,
        "Challenge_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":83,
        "Platform":"Github",
        "Solution_body":"fixed by https:\/\/github.com\/deepset-ai\/haystack\/pull\/2337",
        "Solution_gpt_summary":"pull request github repositori haystack librari involv remov logger disabl function initi inferenc addit",
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com deepset haystack pull",
        "Solution_preprocessed_content":null,
        "Solution_readability":21.0,
        "Solution_reading_time":0.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0437481525,
        "Challenge_watch_issue_ratio":0.0263080106
    },
    {
        "Challenge_adjusted_solved_time":29.6355555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Challenge_closed_time":1635986654000,
        "Challenge_created_time":1635879966000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":22,
        "Challenge_repo_fork_count":115,
        "Challenge_repo_issue_count":411,
        "Challenge_repo_star_count":500,
        "Challenge_repo_watch_count":33,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":29.6355555556,
        "Challenge_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":103,
        "Platform":"Github",
        "Solution_body":"Resolved as of release 3.0.8",
        "Solution_gpt_summary":"misidentif endpoint cluster region config option auth mode region set releas amazonaw com",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.9,
        "Solution_reading_time":0.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":75.8813888889,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Challenge_closed_time":1650063297000,
        "Challenge_created_time":1649790124000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":20.9,
        "Challenge_reading_time":56.91,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":1,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":75.8813888889,
        "Challenge_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Facing the same issue but with W and B. see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12529 and https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12714 This was fixed and included in the 1.6.1 release. Could you try upgrading lightning? \r\n`pip install --upgrade pytorch-lightning` \ud83e\udd1f  @awaelchli not the same bug, but my colab link to reproduce the bug is now throwing another error.",
        "Solution_gpt_summary":"keyword argument agg kei func pytorch lightn cli upgrad latest version pytorch lightn pip instal upgrad pytorch lightn",
        "Solution_link_count":2.0,
        "Solution_original_content":"http github com pytorchlightn pytorch lightn http github com pytorchlightn pytorch lightn releas upgrad lightn pip instal upgrad pytorch lightn awaelchli colab link reproduc throw",
        "Solution_preprocessed_content":"releas upgrad lightn colab link reproduc throw",
        "Solution_readability":9.5,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":162.0608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1613510526000,
        "Challenge_created_time":1612927107000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":4,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":162.0608333333,
        "Challenge_title":"MlflowLogger fail when logging long parameters",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":"Dear @ducthienbui97,\n\nThanks for opening a PR.\n\nBest,\nT.C",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":0.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":121.7916666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531232000,
        "Challenge_created_time":1573092782000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":121.7916666667,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":936.1641666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1619815518000,
        "Challenge_created_time":1616445327000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.86,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":936.1641666667,
        "Challenge_title":"External MLFlow logging failures cause training job to fail",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":224,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":3,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_created_time":1548435846000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92,
        "Challenge_repo_fork_count":2591,
        "Challenge_repo_issue_count":1867,
        "Challenge_repo_star_count":14671,
        "Challenge_repo_watch_count":266,
        "Challenge_score_count":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Solution_gpt_summary":"updat notebook newer version spark remov extra index url sdk regular pypi preview version unsupport implement pull request github",
        "Solution_link_count":1.0,
        "Solution_original_content":"rastala pointer updat notebook newer version spark jreynold base http github com",
        "Solution_preprocessed_content":"pointer updat notebook newer version spark base",
        "Solution_readability":6.2,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":487.0688888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Challenge_closed_time":1592925645000,
        "Challenge_created_time":1591172197000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":19.8,
        "Challenge_reading_time":50.14,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":487.0688888889,
        "Challenge_title":"Hydra MLFlow Clash",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":248,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! > Check whether the instance if `dict` or `DictConfig` in the given line.\r\n\r\n@ssakhavi that sounds reasonable solution, mind sending a PR - fix and its test?",
        "Solution_gpt_summary":"instanc dict dictconfig line send pull request test",
        "Solution_link_count":0.0,
        "Solution_original_content":"contribut great instanc dict dictconfig line ssakhavi reason send test",
        "Solution_preprocessed_content":"contribut great instanc line reason send test",
        "Solution_readability":4.2,
        "Solution_reading_time":2.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":76.3483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Challenge_closed_time":1658150380000,
        "Challenge_created_time":1657875526000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.1,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":439,
        "Challenge_repo_fork_count":17206,
        "Challenge_repo_issue_count":20680,
        "Challenge_repo_star_count":76088,
        "Challenge_repo_watch_count":860,
        "Challenge_score_count":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":76.3483333333,
        "Challenge_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0212282398,
        "Challenge_watch_issue_ratio":0.0415860735
    },
    {
        "Challenge_adjusted_solved_time":326.7363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Challenge_closed_time":1571326528000,
        "Challenge_created_time":1570150277000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":6,
        "Challenge_repo_issue_count":1054,
        "Challenge_repo_star_count":11,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":326.7363888889,
        "Challenge_title":"Cannot run benchmark for sagemaker",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"3 tactics were used to address this issue (by @perdasilva): \r\nStop gap, watcher, error reporting in the status.\r\n@haohanchen-yagao - Please confirm and the close this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tactic address perdasilva stop gap watcher report statu haohanchen yagao close",
        "Solution_preprocessed_content":"tactic address stop gap watcher report statu close",
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0132827324,
        "Challenge_watch_issue_ratio":0.0075901328
    },
    {
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":8,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_created_time":1612300739000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":134,
        "Platform":"Github",
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_gpt_summary":"team fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev miss featur local pipelin eta releas date run pipelin local target",
        "Solution_link_count":2.0,
        "Solution_original_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note http doc com api core core workspac class core workspac comput target comput target section http doc com api core core runconfig runconfigur target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession lostmygithubaccount meghalv block alloc remot comput target local featur deliv lostmygithubaccount statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin data flow core variabl access pipelin run",
        "Solution_preprocessed_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note section target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession block alloc remot comput target local featur deliv statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin core variabl access pipelin run",
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":333.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":53.7558333333,
        "Challenge_answer_count":5,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345866000,
        "Challenge_created_time":1660152345000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5,
        "Challenge_repo_fork_count":8,
        "Challenge_repo_issue_count":39,
        "Challenge_repo_star_count":9,
        "Challenge_repo_watch_count":1,
        "Challenge_score_count":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":53.7558333333,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":261,
        "Platform":"Github",
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_gpt_summary":"burak endpoint url configur github repositori url http minio airflow svc cluster local vpn set deploi vpn set setup git execut",
        "Solution_link_count":4.0,
        "Solution_original_content":"mohittalel report share pod train pod log burak mohittalel misconfigur http github com commun chart tree bitnami postgresql bitnami minio sklearn train simpli endpoint url configur url http minio airflow svc cluster local configur burak burakinc log train trine endpoint local path cest pod warn util git util import git git execut probabl path git sha initi git execut cest pod git execut specifi cest pod path cest pod set git git execut cest pod explicitli set git refresh cest pod cest pod git rectifi cest pod cest pod initi warn silenc aggrav futur set cest pod git refresh environ variabl valu cest pod quiet silenc warn except cest pod warn warn print warn cest pod rais rais except cest pod cest pod cest pod export git refresh quiet cest pod cest pod elasticnet model alpha ratio cest pod rmse cest pod mae cest pod cest pod artifact uri bbcaaafebf artifact cest pod registri uri http airflow svc cluster local cest pod track uri http airflow svc cluster local cest pod regist model elasticnetwinemodel creat version model cest pod track model registri client wait model version finish creation model elasticnetwinemodel version cest pod creat version model elasticnetwinemodel cest kubernet pod delet pod eceefbfcb cest taskinst mark task dag task execut date start date end date cest local task job task exit return cest local task job downstream task schedul schedul log log edit port specifi improv burakinc setup vpn set deploi close mohittalel glad hesit open burak",
        "Solution_preprocessed_content":"report share pod train pod log burak misconfigur simpli configur url configur burak log train trine endpoint local path log log edit port specifi improv setup vpn set deploi close glad hesit open burak",
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":2,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_created_time":1635882064000,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56,
        "Challenge_repo_fork_count":208,
        "Challenge_repo_issue_count":1067,
        "Challenge_repo_star_count":1173,
        "Challenge_repo_watch_count":30,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":87,
        "Platform":"Github",
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_gpt_summary":"convers config paramet logger outsid logger gener term config input agre plain dictionari logger remov convers insid logger plain dictionari pull request",
        "Solution_link_count":0.0,
        "Solution_original_content":"agre easili add plain dictionari digantamisra logger care remov convers insid logger plain dictionari",
        "Solution_preprocessed_content":"agre easili add plain dictionari logger care remov convers insid logger plain dictionari",
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0524835989,
        "Challenge_watch_issue_ratio":0.0281162137
    },
    {
        "Challenge_adjusted_solved_time":166.3613888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770135000,
        "Challenge_created_time":1666171234000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":11,
        "Challenge_repo_fork_count":20,
        "Challenge_repo_issue_count":377,
        "Challenge_repo_star_count":23,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":166.3613888889,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":858,
        "Platform":"Github",
        "Solution_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Solution_gpt_summary":"log train select log test select consider smaller encod select data pipelin train test label column databas separ arrai log column implement log select reproduc prescript test event train test explicit arrai",
        "Solution_link_count":0.0,
        "Solution_original_content":"yeah logger limit navig option log train select log test select consider smaller encod select data pipelin train test label column databas separ arrai log column implement log select reproduc prescript test event train test explicit arrai option pretti straightforward",
        "Solution_preprocessed_content":"yeah limit navig option log train select log test select consider smaller encod select data pipelin label column databas separ arrai log column implement log select reproduc prescript explicit arrai option pretti straightforward",
        "Solution_readability":6.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_created_time":1660651109000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":161,
        "Platform":"Github",
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"updat write exp log metric run statu run run start server miss metric edit sorri",
        "Solution_preprocessed_content":"updat write metric run statu run run start server miss metric edit sorri",
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":9752.3283333333,
        "Challenge_answer_count":2,
        "Challenge_body":"> from azureml.core.model import Model\r\nmodel = Model.register(model_path = MODEL_FILENAME,\r\n                       model_name = \"MyONNXmodel\",\r\n                       tags = {\"onnx\":\"V0\"},\r\n                       description = \"test\",\r\n                       workspace = ws)\r\nthis is the python code I am using to register the model.",
        "Challenge_closed_time":1587148100000,
        "Challenge_created_time":1552039718000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/243",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.17,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9752.3283333333,
        "Challenge_title":"Unable to register the model using Jupyter Notebook with error message: \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\"",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":50,
        "Platform":"Github",
        "Solution_body":"Thanks for your report. Are you still experiencing this issue? Have you posted on the forum https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/home?forum=AzureMachineLearningService? Thank you for reaching out to us.  We see our answer this issue was delayed. Our apologies. We did not receive a response to our response, so will close this issue for now. Should you need further assistance, please submit a post on this forum. We are happy to help.\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"report experienc forum http social msdn com forum home forum azuremachinelearningservic reach delai apolog receiv respons respons close assist submit forum happi",
        "Solution_preprocessed_content":"report experienc forum reach delai apolog receiv respons respons close assist submit forum happi",
        "Solution_readability":6.6,
        "Solution_reading_time":5.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_created_time":1631554276000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37,
        "Challenge_repo_fork_count":101,
        "Challenge_repo_issue_count":1083,
        "Challenge_repo_star_count":153,
        "Challenge_repo_watch_count":24,
        "Challenge_score_count":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_gpt_summary":"notebook workspac unknown statu connect instanc autostop propag ddb log host account instanc stop state latest",
        "Solution_link_count":1.0,
        "Solution_original_content":"chanc instanc autostop propag ddb log host account instanc stop state latest http github com awslab servic workbench commit cbbfeddcaebebb nguyen instanc stop sate latest",
        "Solution_preprocessed_content":"chanc instanc autostop propag ddb log host account instanc state latest instanc stop sate latest",
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":22.5291666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThe banner message is shown on the top page of Studio Lab.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Studio Lab Top Page\r\n2. The message is shown.\r\n\r\n**Expected behavior**\r\nWe can use the Studio Lab as usual.\r\n\r\nI confirmed the following error.\r\n\r\n* We can start runtime but when clicking \"Open Project\", `ERR_EMPTY_RESPONSE` occurs in the browser.\r\n* When we click the start runtime, \"There was a problem when loading your project. This should be resolved shortly. Please try again later.\" occurred.\r\n\r\n**Screenshots**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n\r\n**Additional context**\r\n\r\nAs the message suggests, we are working to restore the service. We apologize for any inconvenience.\r\nI'll announce after the service is back. \r\n",
        "Challenge_closed_time":1668731619000,
        "Challenge_created_time":1668650514000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/166",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.0,
        "Challenge_reading_time":13.81,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":88,
        "Challenge_repo_issue_count":182,
        "Challenge_repo_star_count":300,
        "Challenge_repo_watch_count":15,
        "Challenge_score_count":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":22.5291666667,
        "Challenge_title":"Starting 16th Nov 2022 04:00 PM PST, we are experiencing elevated error starting runtimes. The SageMaker Studio Lab team is working to restore the service. We apologize for any inconvenience.",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":"studio lab team elev wait announc servic",
        "Solution_link_count":0.0,
        "Solution_original_content":"studio lab back patienc close",
        "Solution_preprocessed_content":"studio lab back patienc close",
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":0.4752777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600153381000,
        "Challenge_created_time":1600151670000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":4,
        "Challenge_repo_fork_count":0,
        "Challenge_repo_issue_count":211,
        "Challenge_repo_star_count":7,
        "Challenge_repo_watch_count":3,
        "Challenge_score_count":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4752777778,
        "Challenge_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":109,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"label bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link app homepag http github com marketplac label bot dashboard http mlbot net data khirotaka enchant http github com hamelsmu mlapp bot",
        "Solution_preprocessed_content":"bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link bot",
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":278.3016666667,
        "Challenge_answer_count":4,
        "Challenge_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Challenge_closed_time":1562359564000,
        "Challenge_created_time":1561357678000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":105,
        "Challenge_repo_issue_count":108,
        "Challenge_repo_star_count":236,
        "Challenge_repo_watch_count":16,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":278.3016666667,
        "Challenge_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":66,
        "Platform":"Github",
        "Solution_body":"```\r\nIndex: rl_coach\/src\/markov\/s3_client.py\r\nIDEA additional info:\r\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\r\n<+>UTF-8\r\n===================================================================\r\n--- rl_coach\/src\/markov\/s3_client.py\t(revision 789d7553bdfda74d10dcfbcc0c0286fdb0b5b57f)\r\n+++ rl_coach\/src\/markov\/s3_client.py\t(date 1561357411000)\r\n@@ -60,10 +60,12 @@\r\n \r\n     def download_model(self, checkpoint_dir):\r\n         s3_client = self.get_client()\r\n+        logger.info(\"Downloading pretrained model from %s\/%s %s\" % (self.bucket, self.model_checkpoints_prefix, checkpoint_dir))\r\n         filename = \"None\"\r\n         try:\r\n             filename = os.path.abspath(os.path.join(checkpoint_dir, \"checkpoint\"))\r\n             if not os.path.exists(checkpoint_dir):\r\n+                logger.info(\"Model folder %s does not exist, creating\" % filename)\r\n                 os.makedirs(checkpoint_dir)\r\n \r\n             while True:\r\n@@ -73,13 +75,17 @@\r\n                 if \"Contents\" not in response:\r\n                     # If no lock is found, try getting the checkpoint\r\n                     try:\r\n+                        key = self._get_s3_key(\"checkpoint\")\r\n+                        logger.info(\"Downloading %s\" % key)\r\n                         s3_client.download_file(Bucket=self.bucket,\r\n-                                                Key=self._get_s3_key(\"checkpoint\"),\r\n+                                                Key=key,\r\n                                                 Filename=filename)\r\n                     except Exception as e:\r\n+                        logger.info(\"Something went wrong, will retry in 2 seconds %s\" % e)\r\n                         time.sleep(2)\r\n                         continue\r\n                 else:\r\n+                    logger.info(\"Found a lock file, waiting\")\r\n                     time.sleep(2)\r\n                     continue\r\n \r\n@@ -98,6 +104,8 @@\r\n                             filename = os.path.abspath(os.path.join(checkpoint_dir,\r\n                                                                     obj[\"Key\"].replace(self.model_checkpoints_prefix,\r\n                                                                                        \"\")))\r\n+\r\n+                            logger.info(\"Downloading model file %s\" % filename)\r\n                             s3_client.download_file(Bucket=self.bucket,\r\n                                                     Key=obj[\"Key\"],\r\n                                                     Filename=filename)\r\n\r\n``` Sadly, no file upload is available in issues You can open a pull request if you like, do you need help in doing so? It's definitely an issue and I've encountered it myself. It also happens when the checkpoint file references files that don't exist. Thank you @bhannebipro , I lost track :)",
        "Solution_gpt_summary":"log verbos plan upload patch open pull request",
        "Solution_link_count":0.0,
        "Solution_original_content":"index coach src markov client idea addit subsystem com intellij openapi diff impl patch charsetep utf coach src markov client revis dbdfdaddcfbcccfdbbbf coach src markov client date download model checkpoint dir client client logger download pretrain model bucket model checkpoint prefix checkpoint dir filenam filenam path abspath path checkpoint dir checkpoint path checkpoint dir logger model folder creat filenam makedir checkpoint dir respons lock checkpoint kei kei checkpoint logger download kei client download file bucket bucket kei kei checkpoint kei kei filenam filenam except logger went retri time sleep logger lock file wait time sleep filenam path abspath path checkpoint dir obj kei replac model checkpoint prefix logger download model file filenam client download file bucket bucket kei obj kei filenam filenam sadli file upload open pull request definit checkpoint file file bhannebipro lost track",
        "Solution_preprocessed_content":"sadli file upload open pull request definit checkpoint file file lost track",
        "Solution_readability":12.4,
        "Solution_reading_time":24.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":164.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":4.5558333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Challenge_closed_time":1642203169000,
        "Challenge_created_time":1642186768000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":39.2,
        "Challenge_reading_time":31.45,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":715,
        "Challenge_repo_star_count":127,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":4.5558333333,
        "Challenge_title":"MlflowCatalog anonymous function is not registered ",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_created_time":1571075742000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7,
        "Challenge_repo_fork_count":54,
        "Challenge_repo_issue_count":91,
        "Challenge_repo_star_count":94,
        "Challenge_repo_watch_count":10,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_gpt_summary":"short term lower version kfp term updat leverag latest featur kfp plan file compat latest version kfp potenti plan updat merg close",
        "Solution_link_count":6.0,
        "Solution_original_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version pip instal http storag googleapi com pipelin releas kfp tar upgrad term updat leverag latest featur kfp http github com sampl ek kubeflow workshop blob dfeacbfcebea notebook kubeflow pipelin pipelin ipynb file leverag latest featur kfp master kubeflow pipelin http github com kubeflow pipelin tree master sampl contrib sampl mnist kmean master yaml file http github com kubeflow pipelin tree master compon latest version kfp compat potenti http github com kubeflow pipelin http github com kubeflow pipelin descript time team concentr updat merg close",
        "Solution_preprocessed_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version term updat leverag latest featur kfp file leverag latest featur kfp master latest version kfp compat potenti descript time team concentr updat merg close",
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0769230769,
        "Challenge_watch_issue_ratio":0.1098901099
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_gpt_summary":"modifi line packag remov faulti line initi catalog directli copi dataset catalog loader involv store model modul attribut properti attribut load modul fly remov deepcopi pipelinemodel intend separ process infer time note dataset deepcopi radic frequent",
        "Solution_link_count":0.0,
        "Solution_original_content":"remov faulti line directli initi catalog model loadabl option longer deepcopi initi catalog copi dataset catalog loader kera model clone model kera model dataset pipelinemodel intent separ process infer time remov deepcopi conflict function catalog come abstractmodeldataset model modul attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi underli valu dataset load safe radic on",
        "Solution_preprocessed_content":"remov faulti line directli model loadabl option longer deepcopi copi dataset catalog loader intent separ process remov deepcopi come abstractmodeldataset attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi safe radic on",
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_created_time":1665708717000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101,
        "Challenge_repo_fork_count":1786,
        "Challenge_repo_issue_count":1390,
        "Challenge_repo_star_count":10030,
        "Challenge_repo_watch_count":243,
        "Challenge_score_count":0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":583,
        "Platform":"Github",
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_gpt_summary":"downgrad version temporari exceed length limit paramet valu offici qlib perman accommod",
        "Solution_link_count":0.0,
        "Solution_original_content":"benchmark lightgbm limit length param wait offici qlib accommod downgrad temp limit length param wait offici qlib accommod downgrad temp wish dai",
        "Solution_preprocessed_content":"limit length param wait offici qlib accommod downgrad temp limit length param wait offici qlib accommod downgrad temp wish dai",
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":94.4830555556,
        "Challenge_answer_count":5,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387113000,
        "Challenge_created_time":1598046974000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":94.4830555556,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Solution_gpt_summary":"upgrad version automlstep pythonscriptstep submit automl run",
        "Solution_link_count":1.0,
        "Solution_original_content":"share stacktrac submit pipelin log pythonscriptstep usererror messag train automl runtim instal environ run local process run instal depend runconfigur detailsuri http aka debuginfo type userscriptexcept messag userscriptexcept tmessag train automl runtim instal environ run local process run instal depend runconfigur tinnerexcept optionaldependencymissingexcept tmessag train automl runtim instal environ run local process run instal depend runconfigur tinnerexcept terrorrespons usererror validationerror scenarionotsuport optionaldependencymiss messag train automl runtim instal environ run local process run instal depend runconfigur terrorrespons usererror messag train automl runtim instal environ run local process run instal depend runconfigur stacktrac file mnt batch task share root job pjx mlw model ddea dba dceaab mount workspaceblobstor ddea dba dceaab setup context injector line execut context rais userscriptexcept baseex traceback exceptioninfo file mnt batch task share root job pjx mlw model ddea dba dceaab mount workspaceblobstor ddea dba dceaab setup context injector line execut context runpi run path sy argv global run file env ebadebafccba lib runpi line run path pkg pkg fname file env ebadebafccba lib runpi line run modul mod mod spec pkg file env ebadebafccba lib runpi line run exec run global file run model line file run model line run submit config automl config tag tag file env ebadebafccba lib site packag core line submit run submit func config workspac kwarg file env ebadebafccba lib site packag train automl automlconfig line automl static submit output file env ebadebafccba lib site packag train automl automlconfig line start execut automl run default execut set obj fit param output parent run file env ebadebafccba lib site packag train automl automlconfig line default execut return automl estim fit fit param file env ebadebafccba lib site packag train automl azureautomlcli line fit train automl runtim instal environ run local messageformat train automl runtim instal environ run local process run instal depend runconfigur messageparamet time stack trace driver log txt traceback file run model line file run model line run submit config automl config tag tag file env ebadebafccba lib site packag core line submit run submit func config workspac kwarg file env ebadebafccba lib site packag train automl automlconfig line automl static submit output file env ebadebafccba lib site packag train automl automlconfig line start execut automl run default execut set obj fit param output parent run file env ebadebafccba lib site packag train automl automlconfig line default execut return automl estim fit fit param file env ebadebafccba lib site packag train automl azureautomlcli line fit train automl runtim instal environ run local userscriptexcept userscriptexcept messag train automl runtim instal environ run local process run instal depend runconfigur swatig billmanh experienc submit automl run pythonscriptstep automlstep year throw train automl runtim instal upgrad open",
        "Solution_preprocessed_content":"share stacktrac submit pipelin log stack trace experienc submit automl run year throw instal upgrad open",
        "Solution_readability":16.5,
        "Solution_reading_time":83.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":486.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1339.2047222222,
        "Challenge_answer_count":25,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330541000,
        "Challenge_created_time":1615509404000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":22,
        "Challenge_repo_fork_count":115,
        "Challenge_repo_issue_count":411,
        "Challenge_repo_star_count":500,
        "Challenge_repo_watch_count":33,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1339.2047222222,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Solution_gpt_summary":"widget updat ipython version lifecycl start aros train remov extra slash predict text notebook tri auth mode iam",
        "Solution_link_count":9.0,
        "Solution_original_content":"version ipython updat lifecycl start put eof stop start notebook sourc activ jupytersystemenv pip instal upgrad ipython sourc home anaconda bin deactiv updat lifecycl train statu failurereason clienterror download data preload preload output graph bin illeg char sub sequenc movi len databas step notebook add extra outputloc kristof nei screenshot reproduc kristof nei screenshot reproduc kristof nei screenshot reproduc kristof nei screenshot reproduc austinklin screenshot download data extra slash process uri str bucket uri preload ran notebook predict toi stori come thriller comedi notebook model classif graph convolut network recal see notebook repositori dgl model spend time reproduc reproduc run fresh notebook creat cloud format public doc imag http imag githubusercont com afac png kristof nei state notebook mix creat fresh notebook instanc workaround lifecycl configur releas pypi anymor reproduc run fresh notebook creat cloud format public doc imag http imag githubusercont com afac png kristof nei state notebook mix creat fresh notebook instanc workaround lifecycl configur releas pypi anymor austinklin run fresh figur gnn model model dgl austinklin run fresh figur gnn model model dgl team cloudwatch log job consol austinklin run fresh figur gnn model model dgl team cloudwatch log job consol yeah sai rgcn presum stand relat graph convolut network austinklin run fresh figur gnn model model dgl team cloudwatch log job consol yeah sai rgcn presum stand relat graph convolut network kristof nei updat recreat austinklin reach caught updat gui probabl tomorrow austinklin team final start export configur connectionerror maxretryerror httpsconnectionpool host port retri exceed url export newconnectionerror establish connect errno servic updat start notebook week miss authent token idea start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version graph notebook run graph notebook version configur care authent set note eras block host endpoint configur graph notebook config start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version graph notebook run graph notebook version configur care authent set note eras block host endpoint configur graph notebook config austinklin time effort cell run export start export url export servic host export iam wait store export export param messag miss authent token version graph notebook graph notebook config host dbcluster amazonaw com port auth mode default load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql week ago altho predict export train model predict export kristof nei believ deal flip iam auth config export compon graph notebook config host dbcluster amazonaw com port auth mode iam load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql note auth mode iam kristof nei believ deal flip iam auth config export compon graph notebook config host dbcluster amazonaw com port auth mode iam load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql note auth mode iam austinklin export train went predict genr kristof nei see text notebook drama come model gener creat track http github com graph notebook address addit feedback notebook futur kristof nei see text notebook drama come model gener creat track address addit feedback notebook futur understood close track report notebook date cut ticket run gui appli set auth mode iam llealgt http github com graph notebook issuecom michaelnchin nope run notebook introduct node classif gremlin notebook start gremlin guess relat notebook",
        "Solution_preprocessed_content":"version updat lifecycl start put stop start notebook updat lifecycl train illeg char preload miss authent token run configur care authent set note eras block host endpoint configur start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version run configur care authent set note eras block host endpoint configur time effort cell run version week ago altho predict export train model predict believ deal flip iam auth config compon note auth mode iam believ deal flip iam auth config compon note auth mode iam export train went predict img width alt imag see text notebook drama come model gener creat track address addit feedback notebook futur see text notebook drama come model gener creat track address addit feedback notebook futur understood close track report notebook date cut ticket run gui appli nope run notebook notebook guess relat notebook",
        "Solution_readability":9.2,
        "Solution_reading_time":145.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":102.0,
        "Solution_word_count":1555.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":43.3186111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104425000,
        "Challenge_created_time":1606948478000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22,
        "Challenge_repo_fork_count":115,
        "Challenge_repo_issue_count":411,
        "Challenge_repo_star_count":500,
        "Challenge_repo_watch_count":33,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.3186111111,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_gpt_summary":"set hostnam localhost endpoint graph notebook host updat host file add alia localhost localhost yourendpoint flush dn cach set hostnam jupyt notebook graph notebook config yourendpoint step run sparql queri complet successfulli document updat cover step",
        "Solution_link_count":1.0,
        "Solution_original_content":"miss piec walkthrough connect http github com graph notebook tree addit databas set hostnam localhost endpoint graph notebook host updat host mac alia localhost localhost yourendpoint flush dn cach set hostnam jupyt notebook graph notebook config yourendpoint ran sparql queri successfulli complet glad file expand document cover step close free open submit assist",
        "Solution_preprocessed_content":"miss piec walkthrough connect set hostnam localhost endpoint updat mac alia localhost localhost flush dn cach set hostnam jupyt notebook yourendpoint ran sparql queri successfulli complet glad file expand document cover step close free open submit assist",
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":110.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_created_time":1583773133000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":49,
        "Challenge_repo_issue_count":205,
        "Challenge_repo_star_count":144,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":193,
        "Platform":"Github",
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_gpt_summary":"invok sampl mnist train job involv verifi oper run successfulli cluster step instal oper trust json cluster region oidc account updat",
        "Solution_link_count":7.0,
        "Solution_original_content":"charlesa replac input output bucket role arn run output kubectl trainingjob mnist kubectl trainingjob mnist gautamkmr yeah bucket executor role kubectl trainingjob statu secondari statu creation time job mnist kubectl trainingjob mnist namespac default label annot kubectl kubernet appli configur apivers com trainingjob metadata annot mnist namespac default api version com trainingjob metadata creation timestamp gener resourc version link api com namespac default trainingjob mnist uid efd spec algorithm train imag dkr ecr amazonaw com latest train input mode file hyper paramet depth valu eta valu gamma valu child weight valu silent valu object valu multi softmax num class valu num round valu input data config channel train compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist train channel compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist output data config output path mnist model region resourc config instanc count instanc type xlarg volum size role arn arn iam role execut role stop condit runtim charlesa output oper run successfulli cluster verifi kubectl pod grep step http readthedoc stabl oper kubernet html setup oper deploy instal oper yeah notic kubectl pod oper readi statu restart ag oper control fdbd pend kubectl pod oper oper control fdbd namespac oper prioriti priorityclassnam node label control plane control pod templat hash fdbd annot kubernet psp ek privileg statu pend control replicaset oper control fdbd kube rbac proxi imag gcr kubebuild kube rbac proxi port tcp host port tcp arg secur listen address upstream http logtostderr environ role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn imag dkr ecr amazonaw com oper port host port arg metric addr limit cpu memori request cpu memori environ default endpoint role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn condit type statu podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond oper default token rwdkn type secret volum popul secret secretnam oper default token rwdkn option qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event type reason ag messag warn failedschedul default schedul node schedul pod ek ecr crd artifact come ek pull imag region worker node associ cluster that messag sai warn failedschedul default schedul node schedul pod run kubectl node charlesa chanc review kubectl node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek yeah recreat cluster charlesa previou output pod cluster worker node node schedul pod base output worker node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek node oper pod node node previou comment kubectl node comput intern kubectl node comput intern kubectl node comput intern oper pod kubectl pod grep kubectl pod oper oper deploi successfulli trainingjob run attach trainingjob kubectl trainingjob mnist tri oper pod log gautamkmr kubectl log oper control fdbd dkc oper control runtim metric metric server start listen addr control runtim control start eventsourc control trainingjob sourc sourc control runtim control start eventsourc control hyperparametertuningjob sourc sourc control runtim control start eventsourc control hostingdeploy sourc sourc control runtim control start eventsourc control model sourc sourc control runtim control start eventsourc control endpointconfig sourc sourc control runtim control start eventsourc control batchtransformjob sourc sourc setup start control runtim start metric server path metric control runtim control start control control trainingjob control runtim control start control control model control runtim control start control control batchtransformjob control runtim control start control control hostingdeploy control runtim control start control control endpointconfig control runtim control start control control hyperparametertuningjob control runtim control start worker control trainingjob worker count control runtim control start worker control model worker count control runtim control start worker control endpointconfig worker count control runtim control start worker control batchtransformjob worker count control runtim control start worker control hostingdeploy worker count control runtim control start worker control hyperparametertuningjob worker count control trainingjob resourc trainingjob default mnist control trainingjob job statu set intermedi statu trainingjob default mnist statu synchronizingksjobwith control trainingjob updat job statu trainingjob default mnist statu trainingjobstatu synchronizingksjobwith lastchecktim control trainingjob resourc trainingjob default mnist control trainingjob gener spec trainingjob default mnist mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request eab bae bcdee github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request eab bae bcdee lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request cceb github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request cceb lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist charlesa share log track oper pod retriev credenti iam servic unrecognizedclientexcept secur token request trust json http readthedoc stabl oper kubernet html creat iam role trust polici updat cluster region oidc add account charlesa close activ dai open exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output ubuntu imvaria repo model train kubectl pod namespac readi statu restart ag kube node tgx run kube node kqz run kube coredn dbc cwfvj run kube coredn dbc xld run kube kube proxi run kube kube proxi rjj run kube metric server cfbd nppx run job oper control fhkvv run ubuntu imvaria repo model train kubectl pod oper control fhkvv job oper control fhkvv namespac job prioriti node comput intern start time fri jun label control plane control pod templat hash annot kubernet psp ek privileg statu run ip control replicaset oper control docker dfcbeadfabfdaaabffacebc imag dkr ecr amazonaw com oper imag docker pullabl dkr ecr amazonaw com oper sha ffbbabfdbfeabbad port host port arg metric addr namespac job state run start fri jun readi restart count limit cpu memori request cpu memori environ default endpoint default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt kube rbac proxi docker ecdaafdcdceaddbffeaaddbaabbd imag gcr kubebuild kube rbac proxi imag docker pullabl gcr kubebuild kube rbac proxi sha dbbbcbabddabdcabaccceefbed port tcp host port tcp arg secur listen address upstream http logtostderr state run start fri jun readi restart count environ default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt condit type statu initi readi containersreadi podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond kube api access jrt type volum inject data multipl sourc tokenexpirationsecond configmapnam kube root crt configmapopt downwardapi qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event ubuntu imvaria repo model train kubectl log oper control fhkvv job request throttl request took request http api extens vbeta timeout control runtim metric metric server start listen addr start namespac job setup start control runtim start metric server path metric control start eventsourc reconcilergroup com reconcilerkind endpointconfig control endpointconfig sourc sourc control start eventsourc reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici sourc sourc control start eventsourc reconcilergroup com reconcilerkind model control model sourc sourc control start eventsourc reconcilergroup com reconcilerkind trainingjob control trainingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind processingjob control processingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy sourc sourc control start control reconcilergroup com reconcilerkind model control model control start control reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici control start control reconcilergroup com reconcilerkind endpointconfig control endpointconfig control start control reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob control start control reconcilergroup com reconcilerkind processingjob control processingjob control start control reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob control start control reconcilergroup com reconcilerkind trainingjob control trainingjob control start control reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy control start worker reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy worker count control start worker reconcilergroup com reconcilerkind model control model worker count control start worker reconcilergroup com reconcilerkind endpointconfig control endpointconfig worker count control start worker reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici worker count control start worker reconcilergroup com reconcilerkind processingjob control processingjob worker count control start worker reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob worker count control start worker reconcilergroup com reconcilerkind trainingjob control trainingjob worker count control start worker reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob worker count ubuntu imvaria repo model train kubectl trainingjob statu secondari statu creation time job osic test run ubuntu imvaria repo model train kubectl trainingjob osic test run osic test run namespac default label annot api version com trainingjob metadata creation timestamp gener field api version com field type fieldsv fieldsv metadata annot kubectl kubernet appli configur spec algorithmspecif trainingimag traininginputmod inputdataconfig outputdataconfig soutputpath region resourceconfig instancecount instancetyp volumesizeingb rolearn stoppingcondit maxruntimeinsecond trainingjobnam kubectl client appli oper updat time resourc version uid baf spec algorithm train imag dkr ecr amazonaw com model train latest train input mode file input data config channel train compress type data sourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri osic overrid output data config soutputpath osic overrid region resourc config instanc count instanc type xlarg volum size role arn arn iam role model train role stop condit runtim train job osic test run event",
        "Solution_preprocessed_content":"replac input output bucket role arn run output yeah bucket executor role output oper run successfulli cluster verifi step instal oper yeah notic warn failedschedul node schedul pod kubectl node kubectl node statu role ag version readi readi readi yeah recreat cluster previou output cluster worker node base output worker node statu role ag version readi readi readi node oper pod oper deploi successfulli trainingjob run attach trainingjob tri oper pod log share log track oper pod retriev credenti iam servic trust polici updat cluster region oidc add account close activ dai exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output",
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":2342.6461111111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Challenge_closed_time":1631108652000,
        "Challenge_created_time":1622675126000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.59,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":2342.6461111111,
        "Challenge_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":292,
        "Platform":"Github",
        "Solution_body":"After a long while I've tried the exact same script in the same compute instance again. I don't experience the connection error anymore so I will close this ticket. I don't know what changed.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tri exact comput instanc connect anymor close ticket",
        "Solution_preprocessed_content":"tri exact comput instanc connect anymor close ticket",
        "Solution_readability":5.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":457.1169444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1621931178000,
        "Challenge_created_time":1620285557000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":25,
        "Challenge_repo_issue_count":1008,
        "Challenge_repo_star_count":130,
        "Challenge_repo_watch_count":18,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.1169444444,
        "Challenge_title":"The data path inside sagemaker notebook does not work",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0099206349,
        "Challenge_watch_issue_ratio":0.0178571429
    },
    {
        "Challenge_adjusted_solved_time":1881.8983333333,
        "Challenge_answer_count":0,
        "Challenge_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Challenge_closed_time":1598336871000,
        "Challenge_created_time":1591562037000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":13.06,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1881.8983333333,
        "Challenge_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":126,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":7,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_created_time":1624292044000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":674.2494444444,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":249,
        "Platform":"Github",
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_gpt_summary":"version sdk updat sdk automl core pipelin core pipelin packag messag packag version conflict local environ pipelinerun wait complet keyword argument timeout packag version conflict local environ",
        "Solution_link_count":0.0,
        "Solution_original_content":"afogarti share version sdk import print core version afogarti reproduc see pipelin run warn run anywai workspac reproduc symptom open updat respons run previous weekli pipelin schedul run start mondai took coupl minut finish log june run hour time tri manual run pipelin hit exact steprun object sort updat middl june sdk edit updat sdk automl core pipelin core pipelin packag share engin hope activ packag version conflict local environ pipelinerun wait complet keyword argument timeout run rehydr receiv run object type argument",
        "Solution_preprocessed_content":"share version sdk reproduc see pipelin run anywai workspac reproduc symptom open updat respons run previous weekli pipelin schedul run start mondai took coupl minut finish log june run hour time tri manual run pipelin hit exact steprun object sort updat middl june sdk edit updat sdk pipelin packag share engin hope activ packag version conflict local environ keyword argument run rehydr receiv run object type argument",
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":122.7663888889,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nWhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createModel\" when I run the pipeline from scratch. The error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**To Reproduce**\r\nThe only steps I took was to unfold it as it naturally comes. This bug prevented me from creating a sagemaker model for both the batch and realtime pipelines.\r\n\r\n**Expected behavior**\r\nThe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**Solution to that moment**\r\nTry to fix the numpy versions issue by re-creating the `sagemaker_layer` layer, via pip installation of the libraries. However, there were conflicts with other modified libraries at the time of `pip install numpy`. For this reason, I had to choose to use the default AWS library that comes with numpy \"AWSLambda-Python38-SciPy1x-v29\". For this, I had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create_sagemaker_model - I add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python38-SciPy1x:29\u201d\r\n\r\nWith this, I stop throwing that error at me. I think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. Please check if it still exists in the new versions.\r\n\r\n**Please complete the following information about the solution:**\r\n- [ ] Version: [e.g. v1.1.0]\r\n\r\n\r\nTo get the version of the solution, you can look at the description of the created CloudFormation stack. For example, \"(SO0136) - AWS MLOps Framework. Version v1.1.0\".\r\n\r\n- [ ] Region: [e.g. us-east-1]\r\n- [ ] Was the solution modified from the version published on this repository? No\r\n- [ ] If the answer to the previous question was yes, are the changes available on GitHub? -\r\n- [ ] Have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] Were there any errors in the CloudWatch Logs? Yes\r\n\r\n**Additional context**\r\nI am a Solution Architect of an advanced AWS partner company, and we are running a proof of concept with a real client.",
        "Challenge_closed_time":1620839615000,
        "Challenge_created_time":1620397656000,
        "Challenge_link":"https:\/\/github.com\/aws-solutions\/mlops-workload-orchestrator\/issues\/6",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":10.2,
        "Challenge_reading_time":28.62,
        "Challenge_repo_contributor_count":8,
        "Challenge_repo_fork_count":45,
        "Challenge_repo_issue_count":23,
        "Challenge_repo_star_count":115,
        "Challenge_repo_watch_count":16,
        "Challenge_score_count":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":122.7663888889,
        "Challenge_title":"Error with sagemaker_layer in lambda \"create_sagemakermodel\"",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":321,
        "Platform":"Github",
        "Solution_body":"Please mention if this continues to support you in correcting this error with a pull request. Also to know if it is something from my environment or if it is really a bug in the layer. Greetings to all! Hi @CthompsonCL , thank you for raising this issue. In the newly released version (v1.2.0), the createmodel lambda does not exist anymore. The model is created within a CloudFromation template. So, this issue is resolved in the new release. We will investigate\/fix the build of the sagemaker layer in the previous release.     @CthompsonCL, one more note. if you build the solution locally (i.e, custom build), the sagemaker layer must be built using an Amazon Linux image. Otherwise, you will have the reported error. Perfect! thanks for all. ",
        "Solution_gpt_summary":"newli releas version createmodel lambda anymor model creat cloudform templat team build layer previou releas built local layer built linux imag avoid report",
        "Solution_link_count":0.0,
        "Solution_original_content":"pull request environ layer greet cthompsoncl rais newli releas version createmodel lambda anymor model creat cloudfrom templat releas build layer previou releas cthompsoncl note build local build layer built linux imag report perfect",
        "Solution_preprocessed_content":"pull request environ layer greet rais newli releas version createmodel lambda anymor model creat cloudfrom templat releas build layer previou releas note build local layer built linux imag report perfect",
        "Solution_readability":6.8,
        "Solution_reading_time":9.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":125.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.347826087,
        "Challenge_watch_issue_ratio":0.6956521739
    },
    {
        "Challenge_adjusted_solved_time":575.7216666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1630576282000,
        "Challenge_created_time":1628503684000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.28,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":139,
        "Challenge_repo_star_count":357,
        "Challenge_repo_watch_count":9,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":575.7216666667,
        "Challenge_title":"fsd clone for non-DVC repos throws an error",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":232,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":75.83,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIf I specify an experiment in `mlflow.yml`, and the set up the mlflow configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in mlflow \"Default\" (0) experiment. This works when running \"kedro run\" through the CLI.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# mlflow.yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: True  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import KedroSession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith KedroSession.create(project_path=r\"path\/to\/project\"):\r\n    config=get_mlflow_config()\r\n    config.setup()\r\n    \r\n    mlflow.log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"Default\".\r\n\r\n```\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe faulty line is: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L100\r\n\r\n[We should use mlflow ``mlflow.set_experiment`` method](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.set_experiment), but it does not restore deleted experiment. This wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L124-L132",
        "Challenge_closed_time":1636318265000,
        "Challenge_created_time":1636045277000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/256",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.6,
        "Challenge_reading_time":20.71,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":75.83,
        "Challenge_title":"Setting the mlflow experiment does not work in interactive mode",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":653.7947222222,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Deploy SWB in hongkong reigon\r\n2. Create a Sagemaker workspace\r\n3. Click \"Stop\" button.\r\n5. workspace status show \"UNKNOWN\"\r\n",
        "Challenge_closed_time":1659958133000,
        "Challenge_created_time":1657604472000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/45",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":38,
        "Challenge_repo_fork_count":5,
        "Challenge_repo_issue_count":119,
        "Challenge_repo_star_count":8,
        "Challenge_repo_watch_count":12,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":653.7947222222,
        "Challenge_title":"[Bug] In HongKong region, After user stop sagemaker workspace manually, web console show \"UNKNOWN\" status",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":54,
        "Platform":"Github",
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/cd99bd2a4f39291e3149b0abbc78ab5b3d650454)",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"commit http github com awslab servic workbench commit cdbdafebabbcabbd",
        "Solution_preprocessed_content":null,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_created_time":1566233711000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4111.2283333333,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_gpt_summary":"team member request run log team member automl setup time automl team random occurr consist dataset approxim featur",
        "Solution_link_count":0.0,
        "Solution_original_content":"nema unfortun access workspac pipelin log sonni run ebfd ecfbbfcf runid automl setup time time automl team random occurr lost track",
        "Solution_preprocessed_content":"nema unfortun access workspac log sonni run runid automl setup time time automl team random occurr lost track",
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":179.7819444444,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nSince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. This breaks the `kedro mlflow init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## Context\r\nWe should remove the `_get_project_globals` util function in kedromlflow and use `kedro.framework.context import get_static_project_data` as suggested in #86. **Beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch `kedro mlflow init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## Expected Result\r\nThe mlflow.yml file should be created\r\n\r\n## Actual Result\r\nAn error is raised.",
        "Challenge_closed_time":1603658563000,
        "Challenge_created_time":1603011348000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":179.7819444444,
        "Challenge_title":"Make mlflow init work when configuration is in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"Well spoted ! We can already solve this, by bundling kedro's `get_static_project_data` inside kedro_mlflow as long as we keep kedro < 0.16.5 retrocompatibility. We can switch then to `kedro.framework.context import get_static_project_data` when we drop this compatibility in the futur. I can add it to [Migrate 0.16.5 pull request](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/pull\/94) Ok let's do this!",
        "Solution_gpt_summary":"involv bundl static data insid maintain retrocompat retrocompat drop futur import switch framework context import static data migrat pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"spote bundl static data insid retrocompat switch framework context import static data drop compat futur add migrat pull request http github com galileo galilei pull",
        "Solution_preprocessed_content":"spote bundl insid retrocompat switch drop compat futur add",
        "Solution_readability":8.9,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":360.0336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943713000,
        "Challenge_created_time":1634647592000,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":18,
        "Challenge_repo_fork_count":60,
        "Challenge_repo_issue_count":1038,
        "Challenge_repo_star_count":652,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":360.0336111111,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":97,
        "Platform":"Github",
        "Solution_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Solution_gpt_summary":"fold column drop modifi line logger modifi line file",
        "Solution_link_count":2.0,
        "Solution_original_content":"kei http github com tinkoff etna blob debaccbddbedacc etna logger logger fold column drop http github com tinkoff etna blob master etna logger logger",
        "Solution_preprocessed_content":null,
        "Solution_readability":18.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0173410405,
        "Challenge_watch_issue_ratio":0.0057803468
    },
    {
        "Challenge_adjusted_solved_time":243.16,
        "Challenge_answer_count":3,
        "Challenge_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Challenge_closed_time":1654438640000,
        "Challenge_created_time":1653563264000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":19.4,
        "Challenge_reading_time":114.38,
        "Challenge_repo_contributor_count":62,
        "Challenge_repo_fork_count":369,
        "Challenge_repo_issue_count":1562,
        "Challenge_repo_star_count":527,
        "Challenge_repo_watch_count":26,
        "Challenge_score_count":0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":243.16,
        "Challenge_title":"Can not add AzureML extention on openshift cluster ",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":533,
        "Platform":"Github",
        "Solution_body":"Hey friend! Thanks for opening this issue. We appreciate your contribution and welcome you to our community! We are glad to have you here and to have your input on the Azure Arc Jumpstart.<p><\/p> Hello Johan, thanks for submitting feedback. Have you checked the [prerequisites specific to ARO](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-kubernetes-anywhere?tabs=studio#prerequisites) prior to attempting the extension installation?  Hello @rataxe , have you checked the pre-reqs above. If you already had and still facing a problem, we recommend you open a support case as this is not strictly related to the Jumpstart project but to the product itself. ",
        "Solution_gpt_summary":"prerequisit aro extens instal prerequisit persist open relat jumpstart",
        "Solution_link_count":1.0,
        "Solution_original_content":"friend open contribut welcom commun glad input arc jumpstart johan submit feedback prerequisit aro http doc com attach kubernet tab studio prerequisit prior extens instal ratax pre req open strictli relat jumpstart",
        "Solution_preprocessed_content":"friend open contribut welcom commun glad input arc johan submit feedback prior extens instal open strictli relat jumpstart",
        "Solution_readability":9.0,
        "Solution_reading_time":8.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0396927017,
        "Challenge_watch_issue_ratio":0.0166453265
    },
    {
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_created_time":1619181390000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16,
        "Challenge_repo_fork_count":0,
        "Challenge_repo_issue_count":909,
        "Challenge_repo_star_count":5,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":80,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":45.9877777778,
        "Challenge_answer_count":3,
        "Challenge_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Challenge_closed_time":1578199794000,
        "Challenge_created_time":1578034238000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.42,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":209,
        "Challenge_repo_issue_count":605,
        "Challenge_repo_star_count":1230,
        "Challenge_repo_watch_count":20,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":45.9877777778,
        "Challenge_title":"Wandb bug when running train long",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Currently working on this should be fixed. Just need to test it So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not. Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging. ",
        "Solution_gpt_summary":"test entir train session run termin loop end unclear concurr run train river time revert subprocess log debug",
        "Solution_link_count":0.0,
        "Solution_original_content":"test step anymor model run chart run termin loop end close concurr run aren train river time revert subprocess log debug",
        "Solution_preprocessed_content":"test step anymor model run chart run termin loop end close concurr run aren train river time revert subprocess log debug",
        "Solution_readability":6.3,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":139.3538888889,
        "Challenge_answer_count":5,
        "Challenge_body":"**Describe the bug**\r\nAmazon Sagemaker studio lab is not opening jupyter notebook. It is loading indefinitely at Preparing project run time after that i am getting `There was a problem when starting the project runtime. This should be resolved shortly.` Please try again later. It's been almost a week and it still hasn't been resolved. Even though I tried shifting the runtime from CPU to GPU but issue still persists. Any help would be appreciated.\r\n\r\n**The error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1643050102000,
        "Challenge_created_time":1642548428000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/52",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.2,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":88,
        "Challenge_repo_issue_count":182,
        "Challenge_repo_star_count":300,
        "Challenge_repo_watch_count":15,
        "Challenge_score_count":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":139.3538888889,
        "Challenge_title":"couldn't open Project on amazon sagemaker studio lab",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":"Hi - sorry you're running into this issue, we're taking a look.  I have also encountered this problem, which still exists at present.\r\n![image](https:\/\/user-images.githubusercontent.com\/37031974\/150719178-b4b46fa6-d912-44e3-a412-f605435d664c.png)\r\n I deleted my account and re-created it, which took around 5 minutes for them to accept my request for the second time. If need instance then this is the only approach I could find. I had not received any updates from this issue after a week, so I recreated my instance.\r\n I see - really sorry you're running into that. I've created a ticket with the team, but I'll close this issue for now since there is a known workaround.  Hello, I can't open project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? Thanks!",
        "Solution_gpt_summary":"delet account creat took minut accept request time workaround creat ticket team workaround",
        "Solution_link_count":1.0,
        "Solution_original_content":"sorri run present imag http imag githubusercont com bbfa fdc png delet account creat took minut accept request time instanc receiv updat week recreat instanc sorri run creat ticket team close workaround open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account",
        "Solution_preprocessed_content":"sorri run present delet account took minut accept request time instanc receiv updat week recreat instanc sorri run creat ticket team close workaround open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account",
        "Solution_readability":6.9,
        "Solution_reading_time":12.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":165.0966666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Challenge_closed_time":1654018977000,
        "Challenge_created_time":1653424629000,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":18,
        "Challenge_repo_fork_count":43,
        "Challenge_repo_issue_count":536,
        "Challenge_repo_star_count":131,
        "Challenge_repo_watch_count":10,
        "Challenge_score_count":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":165.0966666667,
        "Challenge_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":157,
        "Platform":"Github",
        "Solution_body":"Error in LaunchPad notebooks. There was a change in the triton upstream where you previously didn't need to specify the model suffix as the path. Now you do. ",
        "Solution_gpt_summary":"delet pod start captur print env output ipykernel launchpad helm deploy tritoncli sqlite corrupt note triton upstream model suffix path specifi",
        "Solution_link_count":0.0,
        "Solution_original_content":"launchpad notebook triton upstream previous specifi model suffix path",
        "Solution_preprocessed_content":"launchpad notebook triton upstream previous specifi model suffix path",
        "Solution_readability":5.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0335820896,
        "Challenge_watch_issue_ratio":0.0186567164
    },
    {
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":4,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_created_time":1667438077000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":88,
        "Challenge_repo_issue_count":182,
        "Challenge_repo_star_count":300,
        "Challenge_repo_watch_count":15,
        "Challenge_score_count":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_gpt_summary":"start runtim api studio lab run cpu gpu runtim unavail gpu instanc instanc alloc address",
        "Solution_link_count":0.0,
        "Solution_original_content":"final btw run cpu gpu todai saleemmalik sorri inconveni studio lab servic close aozorahim said gpu instanc unavail instanc alloc cours deal",
        "Solution_preprocessed_content":"final btw run cpu gpu todai sorri inconveni studio lab servic close said gpu instanc unavail instanc alloc cours deal",
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":124.4336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Challenge_closed_time":1600665871000,
        "Challenge_created_time":1600217910000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":209,
        "Challenge_repo_issue_count":605,
        "Challenge_repo_star_count":1230,
        "Challenge_repo_watch_count":20,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":124.4336111111,
        "Challenge_title":"Wandb Run stalling",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"So this appears to be a problem on the Weights and Biases end of things. https:\/\/github.com\/wandb\/client\/issues\/1243 This is fixed see original issue.",
        "Solution_gpt_summary":"sweep stall sweep complet identifi end origin updat reflect",
        "Solution_link_count":1.0,
        "Solution_original_content":"end http github com client origin",
        "Solution_preprocessed_content":null,
        "Solution_readability":7.6,
        "Solution_reading_time":1.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_created_time":1638880026000,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17,
        "Challenge_repo_fork_count":11,
        "Challenge_repo_issue_count":154,
        "Challenge_repo_star_count":15,
        "Challenge_repo_watch_count":14,
        "Challenge_score_count":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1034.8597222222,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":176,
        "Platform":"Github",
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_gpt_summary":"add txt remov tar xvzf data imag tar directori data line lock file date version repositori unclear document section",
        "Solution_link_count":5.0,
        "Solution_original_content":"high prioriti remov address instal pip instal txt virtual env instal separ txt bit intent instal bit prevent version conflict condit instal venv depend rout singl line add txt shcheklein intent txt add instruct instal instruct miss unlik peopl reach page tutori instal venv lil rememb left tar time extract start run frozen updat tar updat model remot prevent upload intend tapadipti creat separ pr doc updat iesahin pull gave pull data cloud checkout target tapadiptisitaula document test model model cach date metric model time file path displai remov file list anymor pull data cloud checkout target tapadiptisitaula document test model model doubl yesterdai push artifact complet successfulli sai push date line lock line http github com iter blob lock hash valu instal clone repositori imag http imag githubusercont com aef dab png tapadipti stage version http github com iter stage push iesahin http github com iter codif close",
        "Solution_preprocessed_content":"high prioriti remov address instal virtual env instal separ intent instruct miss unlik peopl reach page tutori instal lock codif close",
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1103896104,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_created_time":1580947939000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91,
        "Challenge_repo_fork_count":675,
        "Challenge_repo_issue_count":2354,
        "Challenge_repo_star_count":5152,
        "Challenge_repo_watch_count":96,
        "Challenge_score_count":0,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":8976.4125,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":999,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_gpt_summary":"upgrad newer version boto instal autogluon sourc wait autogluon releas",
        "Solution_link_count":0.0,
        "Solution_original_content":"submit haven close boto version function autogluon newer version zhuwenzhen latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_preprocessed_content":"submit haven close boto version function autogluon newer version latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":1032.455,
        "Challenge_answer_count":2,
        "Challenge_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Challenge_closed_time":1665527881000,
        "Challenge_created_time":1661811043000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.74,
        "Challenge_repo_contributor_count":19,
        "Challenge_repo_fork_count":94,
        "Challenge_repo_issue_count":1834,
        "Challenge_repo_star_count":281,
        "Challenge_repo_watch_count":36,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1032.455,
        "Challenge_title":"Can't use Azure ML features when remotely connected to a compute",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"Seems to work when remotely connected via VS Code desktop, but it definitely doesn't work when connected via vscode.dev. The azure ml remote extension is disabled in this case. Seems like we should be able to support this. Yes, this is only on web platforms like vscode.dev or codespaces.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"remot connect desktop definit connect vscode dev remot extens disabl web platform vscode dev codespac",
        "Solution_preprocessed_content":"remot connect desktop definit connect remot extens disabl web platform codespac",
        "Solution_readability":4.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":1618.0830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Challenge_closed_time":1624218172000,
        "Challenge_created_time":1618393073000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":44,
        "Challenge_repo_fork_count":385,
        "Challenge_repo_issue_count":712,
        "Challenge_repo_star_count":2929,
        "Challenge_repo_watch_count":75,
        "Challenge_score_count":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0830555556,
        "Challenge_title":"Local wandb logging is borked",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":35,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":3487.3783333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Challenge_closed_time":1620299767000,
        "Challenge_created_time":1607745205000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":25.9,
        "Challenge_reading_time":288.86,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":96,
        "Challenge_solved_time":3487.3783333333,
        "Challenge_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":1212,
        "Platform":"Github",
        "Solution_body":"@DXcarlos I have tried to reproduce the error on `jewellery` dataset available on our GitHub repository and I couldn't reproduce this error.\r\n\r\nIs it possible for you to share the Notebook along with the dataset so we can reproduce the error and troubleshoot what is causing this?\r\n\r\nI am including @Yard1 in this thread to see if he can understand what's going on with the log file you shared above. Antoni, maybe something specific to the dataset.  @pycaret @Yard1 How can I share you privately? @DXcarlos You can private message on our Slack channel. If you are still not there, you can join using the following link:\r\n\r\nhttps:\/\/join.slack.com\/t\/pycaret\/shared_invite\/zt-kdoe7hee-yvNANPHXPM9VtK7R6Npx4Q\r\n\r\n Stale issue message @DXcarlos , we will close out this issue for now. Please feel free to reopen if you want.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"dxcarlo tri reproduc jewelleri dataset github repositori reproduc share notebook dataset reproduc troubleshoot yard thread log file share antoni mayb dataset pycaret yard share privat dxcarlo privat messag slack channel link http slack com pycaret share invit kdoehe yvnanphxpmvtkrnpxq stale messag dxcarlo close free reopen",
        "Solution_preprocessed_content":"tri reproduc dataset github repositori reproduc share notebook dataset reproduc troubleshoot thread log file share antoni mayb dataset share privat privat messag slack channel link stale messag close free reopen",
        "Solution_readability":7.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":128.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_created_time":1620111416000,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":49,
        "Challenge_repo_fork_count":43,
        "Challenge_repo_issue_count":1491,
        "Challenge_repo_star_count":115,
        "Challenge_repo_watch_count":13,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0328638498,
        "Challenge_watch_issue_ratio":0.0087189805
    },
    {
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_created_time":1605982845000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_gpt_summary":"manual specifi run statu run replac line retriev run statu import note run termin manual avoid interfer",
        "Solution_link_count":0.0,
        "Solution_original_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli specifii statu termin run manual interact cli ti avoid interfer",
        "Solution_preprocessed_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli termin run manual interact avoid interfer",
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":2.6138888889,
        "Challenge_answer_count":1,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229646000,
        "Challenge_created_time":1641220236000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":88,
        "Challenge_repo_issue_count":182,
        "Challenge_repo_star_count":300,
        "Challenge_repo_watch_count":15,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6138888889,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":15,
        "Platform":"Github",
        "Solution_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Solution_gpt_summary":"direct templat report technic",
        "Solution_link_count":1.0,
        "Solution_original_content":"technic templat http github com studio lab assigne label templat report studio lab titl",
        "Solution_preprocessed_content":null,
        "Solution_readability":26.4,
        "Solution_reading_time":3.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":210.9636111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Challenge_closed_time":1617875123000,
        "Challenge_created_time":1617115654000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.5,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":210.9636111111,
        "Challenge_title":"mlflow run context is not logged when using MLFlowLogger",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":675.9419444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Issue in Vertex AI Automl training python API while using vertex dataset with source as foreign project's BQ table\r\n- What would you like to achieve: I would like to run automl training job(using automl api python) with vertex dataset created from foreign project's BigQuery table.\r\n- Even though 'vertex AI service agent' and 'Vertex AI Custom code service agent' added to respective bigquery dataset with 'bigquery data editor' role, we are receiving bigquery.tables.get persmission denied error when we try run automl training job. \r\n\r\nError:\r\n`INFO:google.cloud.aiplatform.training_jobs:No column transformations provided, so now retrieving columns from dataset in order to set default column transformations. \r\nTraceback (most recent call last): \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 190, in <module> \r\nautoml_tabular = create_training_pipeline_tabular(train_type, project_id, display_name, int(dataset_id), location, model_display_name, float(training_fraction_split), float(validation_fraction_split), float(test_fraction_split), int(budget_milli_node_hours), disable_early_stopping, sync, target_column) \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 57, in create_training_pipeline_tabular \r\nmodel = tabular_job.run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3461, in run \r\nreturn self._run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/base.py\", line 730, in wrapper \r\nreturn method(*args, **kwargs) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3645, in _run \r\n) = column_transformations_utils.get_default_column_transformations( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/utils\/column_transformations_utils.py\", line 42, in get_default_column_transformations \r\nfor column_name in dataset.column_names \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 81, in column_names \r\nself._retrieve_bq_source_columns( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 241, in _retrieve_bq_source_columns \r\ntable = client.get_table(bq_table_uri) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 1034, in get_table \r\napi_response = self._call_api( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 782, in _call_api \r\nreturn call() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 283, in retry_wrapped_func \r\nreturn retry_target( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 190, in retry_target \r\nreturn target() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/_http\/__init__.py\", line 480, in api_request \r\nraise exceptions.from_http_response(response) \r\ngoogle.api_core.exceptions.Forbidden: 403 GET https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/<<projectID>>\/datasets\/<<datasetID>>\/tables\/<<tableID>>?prettyPrint=false: Access Denied: Table <<TableID>>: Permission bigquery.tables.get denied on table <<TableID>> (or it may not exist). \r\n`\r\n\r\nAfter raising google support, we got a temporary workaround by adding column specification in API call (reference case# 29535929, 29310889).\r\n**Sample API call of the workaround:**\r\n`job = training_jobs.AutoMLTabularTrainingJob( \r\ndisplay_name=\"my_display_name\", \r\noptimization_prediction_type=\"classification\", \r\noptimization_objective=\"minimize-log-loss\", \r\ncolumn_specs={\"column_1\": \"auto\", \"column_2\": \"numeric\"}, \r\nlabels={'key': 'value'}, \r\n) \r\n`\r\n\r\nWe would want to have this fixed in in SDK. If possible, we would like to have a gcloud command for automl modules.\r\n",
        "Challenge_closed_time":1649696673000,
        "Challenge_created_time":1647263282000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1078",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.3,
        "Challenge_reading_time":53.0,
        "Challenge_repo_contributor_count":75,
        "Challenge_repo_fork_count":188,
        "Challenge_repo_issue_count":1846,
        "Challenge_repo_star_count":283,
        "Challenge_repo_watch_count":53,
        "Challenge_score_count":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":675.9419444444,
        "Challenge_title":"Vertex AI Automl training error when training dataset with foreign project's BQ table as source",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"I've updated the docstring to clarify that the service account needs certain permissions when neither `column_transformations` or `column_specs` is set.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"updat docstr clarifi servic account permiss column transform column spec set",
        "Solution_preprocessed_content":"updat docstr clarifi servic account permiss set",
        "Solution_readability":14.6,
        "Solution_reading_time":1.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0406283857,
        "Challenge_watch_issue_ratio":0.0287107259
    },
    {
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_created_time":1623592472000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":890.5602777778,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":130,
        "Platform":"Github",
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_gpt_summary":"creat environ extra docker step inject file sampl environ github repositori notebook",
        "Solution_link_count":1.0,
        "Solution_original_content":"extra docker step longer creat environ inject file wish environ deploy sampl http github com machinelearningnotebook blob master deploy deploi cloud model regist deploi ipynb",
        "Solution_preprocessed_content":"extra docker step longer creat environ inject file wish environ deploy sampl",
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":0.9458333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Challenge_closed_time":1633697794000,
        "Challenge_created_time":1633694389000,
        "Challenge_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.7,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":2,
        "Challenge_repo_fork_count":2,
        "Challenge_repo_issue_count":3,
        "Challenge_repo_star_count":3,
        "Challenge_repo_watch_count":18,
        "Challenge_score_count":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.9458333333,
        "Challenge_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":191,
        "Platform":"Github",
        "Solution_body":"Thanks for the catch @mike0sv ! No trouble (literally, no trouble at all since it was @mnrozhkov :))",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"catch mikesv troubl liter troubl mnrozhkov",
        "Solution_preprocessed_content":null,
        "Solution_readability":4.1,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.6666666667,
        "Challenge_watch_issue_ratio":6.0
    },
    {
        "Challenge_adjusted_solved_time":5.2394444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206718000,
        "Challenge_created_time":1642187856000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":715,
        "Challenge_repo_star_count":127,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.2394444444,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":41,
        "Platform":"Github",
        "Solution_body":"Duplicated to #496 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":347.7586111111,
        "Challenge_answer_count":8,
        "Challenge_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Challenge_closed_time":1559077203000,
        "Challenge_created_time":1557825272000,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":11.0,
        "Challenge_reading_time":29.59,
        "Challenge_repo_contributor_count":123,
        "Challenge_repo_fork_count":1021,
        "Challenge_repo_issue_count":2798,
        "Challenge_repo_star_count":8658,
        "Challenge_repo_watch_count":181,
        "Challenge_score_count":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":347.7586111111,
        "Challenge_title":"Logging issue when activating Comet contrib",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":315,
        "Platform":"Github",
        "Solution_body":"@Lothiraldan thanks for reporting this. It is indeed a bug to fix. Also notifying @dsblank as he is the main contributor of the comet integration.\r\nWould gladly accept your help offer on this. I'd like to avoid having a logger object that is passed around the whole codebase, but apart from that I'm open to suggestions.\r\n @w4nderlust Yes, and @Lothiraldan and I both are on the Comet team, so we have already been discussing this. @Lothiraldan has much expertise in loggers, so I look forward to his suggestions as well. Hi @w4nderlust, thank you for your prompt answer. I forgot to said that I'm working with @dsblank in the Comet team.\r\n\r\nHaving a non-global logger is ideal but not always feasible.\r\n\r\nThe approach I'm using in my projects is the following, use a logger per module: `LOGGER = logging.getLogger(__name__)`. As the __name__ often contains your project name, you get will get loggers like `dulwich`, `dulwich.experiment`, `dulwich.contribs.comet`. I think I have taken this idea from Django https:\/\/docs.djangoproject.com\/en\/2.1\/topics\/logging\/#using-logging.\r\n\r\nThis way you can configure the top-level logger for your project and every other loggers will just propagate the log messages to it and uses the configured handlers. This unlock having different log level on a module basis or even different handlers if needed.\r\n\r\nI would highly recommend having a central function where the top-level logger is configured, something like https:\/\/github.com\/Lothiraldan\/balto\/blob\/master\/balto\/_logging.py#L6, I found it that it really helps for maintaining a coherent logging configuration.\r\n\r\nApart from that, the Ludwig project seems to be only using a StreamHandler right now so there is no much expertise I can give you on the handlers subjects.\r\n\r\nDon't hesitate if you have some questions. Thanks for the detailed explanation @Lothiraldan . @msaisumanth Is on top of it. It looks pretty straightforward: have a single global logger setup function, add a logger in every module, use that logger instead of logging. I expect this to be solved pretty quickly.\r\nThank you again! @Lothiraldan please take a look at https:\/\/github.com\/uber\/ludwig\/pull\/352\r\nI was able to verify that the output is getting printed as expected. @w4nderlust if this makes sense, I'll modify the other modules as well.  @Lothiraldan it would be great if you could take a look at the PR, it should solve the issue, but wanted to doublecheck with you before merging it. We merged the PR as we believe it works fine, @Lothiraldan if you could take a look at it to confirm it's fine for you too, that owuld be great. I made some comments, sorry for the delay, I was busy with some other stuff.",
        "Solution_gpt_summary":"move root logger ludwig logger configur manual avoid log logger modul configur central function ludwig implement singl global logger setup function logger modul logger log pull request creat merg review team",
        "Solution_link_count":3.0,
        "Solution_original_content":"lothiraldan report notifi dsblank contributor integr gladli accept avoid logger object pass codebas apart open wnderlust lothiraldan team lothiraldan expertis logger forward wnderlust prompt forgot said dsblank team global logger ideal feasibl logger modul logger log getlogg logger dulwich dulwich dulwich contrib taken idea django http doc djangoproject com log log configur level logger logger propag log messag configur handler unlock log level modul basi handler highli central function level logger configur http github com lothiraldan balto blob master balto log maintain coher log configur apart ludwig streamhandl expertis handler subject hesit explan lothiraldan msaisumanth pretti straightforward singl global logger setup function add logger modul logger log pretti quickli lothiraldan http github com uber ludwig pull verifi output print wnderlust sens modifi modul lothiraldan great doublecheck merg merg believ lothiraldan owuld great comment sorri delai busi stuff",
        "Solution_preprocessed_content":"report notifi contributor integr gladli accept avoid logger object pass codebas apart open team expertis logger forward prompt forgot said team logger ideal feasibl logger modul logger taken idea django configur logger logger propag log messag configur handler unlock log level modul basi handler highli central function logger configur maintain coher log configur apart ludwig streamhandl expertis handler subject hesit explan pretti straightforward singl global logger setup function add logger modul logger log pretti quickli verifi output print sens modifi modul great doublecheck merg merg believ owuld great comment sorri delai busi stuff",
        "Solution_readability":7.6,
        "Solution_reading_time":32.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":426.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0439599714,
        "Challenge_watch_issue_ratio":0.0646890636
    },
    {
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_created_time":1617627646000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":18.9641666667,
        "Challenge_answer_count":1,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Challenge_closed_time":1622440667000,
        "Challenge_created_time":1622372396000,
        "Challenge_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":6,
        "Challenge_repo_fork_count":6,
        "Challenge_repo_issue_count":43,
        "Challenge_repo_star_count":11,
        "Challenge_repo_watch_count":0,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9641666667,
        "Challenge_title":"[BUG] wandb value doesn't update",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":25,
        "Platform":"Github",
        "Solution_body":"![image](https:\/\/user-images.githubusercontent.com\/26226101\/120102333-71439e00-c185-11eb-8a20-b113112b0b3f.png)\r\n\r\n\uc544\uc774\uac70 \uc65c \uadf8\ub7f0\uac8c \ud588\ub124\uc694.... \uc218\uc815\ud574 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!",
        "Solution_gpt_summary":"index grow list list",
        "Solution_link_count":1.0,
        "Solution_original_content":"imag http imag githubusercont com bbbf png",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.5,
        "Solution_reading_time":2.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.1395348837,
        "Challenge_watch_issue_ratio":0.0
    },
    {
        "Challenge_adjusted_solved_time":123.0280555556,
        "Challenge_answer_count":1,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847372000,
        "Challenge_created_time":1583404471000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":123.0280555556,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":201,
        "Platform":"Github",
        "Solution_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"robinvdheijden reach forum notebook open forum thread msdn forum http social msdn com forum home forum azuremachinelearningservic forum commun member expert guidanc base proce close thread matter yutongti msft gladli",
        "Solution_preprocessed_content":"reach forum notebook open forum thread forum commun member expert guidanc base proce close thread matter gladli",
        "Solution_readability":8.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":790.3911111111,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1662449543000,
        "Challenge_created_time":1659604135000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.42,
        "Challenge_repo_contributor_count":38,
        "Challenge_repo_fork_count":5,
        "Challenge_repo_issue_count":119,
        "Challenge_repo_star_count":8,
        "Challenge_repo_watch_count":12,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":790.3911111111,
        "Challenge_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":98,
        "Platform":"Github",
        "Solution_body":"Already fixed",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.9,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":14.3083333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Challenge_closed_time":1626409391000,
        "Challenge_created_time":1626357881000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":18.4,
        "Challenge_reading_time":11.61,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.3083333333,
        "Challenge_title":"mlflow run_name unexpected argument error",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":69,
        "Platform":"Github",
        "Solution_body":"That's because you are looking at the docs under \"latest\" which is the development version. On the master branch, there is a run_name argument but 1.3.x does not have that. You are probably using Lightning 1.3.x. \r\nIf you want, you can install Lightning rc0 to test the features before the 1.4 release.   Here are the docs for the stable version (1.3.x) https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html Okay got it. Thanks for clarifying ",
        "Solution_gpt_summary":"document version stabl version stabl version run argument logger instal lightn test featur releas stabl version document link",
        "Solution_link_count":1.0,
        "Solution_original_content":"doc latest version master branch run argument probabl lightn instal lightn test featur releas doc stabl version http pytorch lightn readthedoc stabl api pytorch lightn logger html okai clarifi",
        "Solution_preprocessed_content":"doc latest version master branch argument probabl lightn instal lightn test featur releas doc stabl version okai clarifi",
        "Solution_readability":6.6,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":28.55,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Challenge_closed_time":1653501835000,
        "Challenge_created_time":1653399055000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":13.0,
        "Challenge_reading_time":21.61,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":28.55,
        "Challenge_title":"mlflow ui doesn't show any models",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"Creating a clean env and installing pycaret again solved the issue.",
        "Solution_gpt_summary":"creat clean environ reinstal pycaret",
        "Solution_link_count":0.0,
        "Solution_original_content":"creat clean env instal pycaret",
        "Solution_preprocessed_content":"creat clean env instal pycaret",
        "Solution_readability":6.4,
        "Solution_reading_time":0.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":263.7483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1569234937000,
        "Challenge_created_time":1568285443000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.1,
        "Challenge_reading_time":224.21,
        "Challenge_repo_contributor_count":40,
        "Challenge_repo_fork_count":1102,
        "Challenge_repo_issue_count":681,
        "Challenge_repo_star_count":8768,
        "Challenge_repo_watch_count":287,
        "Challenge_score_count":0,
        "Challenge_sentence_count":155,
        "Challenge_solved_time":263.7483333333,
        "Challenge_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1547,
        "Platform":"Github",
        "Solution_body":"fixed with new pipeline and test machines",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.5,
        "Solution_reading_time":0.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":426.2658333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Challenge_closed_time":1626683431000,
        "Challenge_created_time":1625148874000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.8,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":7,
        "Challenge_repo_issue_count":644,
        "Challenge_repo_star_count":29,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":426.2658333333,
        "Challenge_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":118,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":1510.63,
        "Challenge_answer_count":1,
        "Challenge_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Challenge_closed_time":1657547192000,
        "Challenge_created_time":1652108924000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":17,
        "Challenge_repo_fork_count":22,
        "Challenge_repo_issue_count":693,
        "Challenge_repo_star_count":111,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1510.63,
        "Challenge_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":120,
        "Platform":"Github",
        "Solution_body":"Resolved in #420",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.9,
        "Solution_reading_time":0.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0245310245,
        "Challenge_watch_issue_ratio":0.0101010101
    },
    {
        "Challenge_adjusted_solved_time":222.8511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Challenge_closed_time":1602278580000,
        "Challenge_created_time":1601476316000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":222.8511111111,
        "Challenge_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":106,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":913.5883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Challenge_closed_time":1653922922000,
        "Challenge_created_time":1650634004000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.2,
        "Challenge_reading_time":50.05,
        "Challenge_repo_contributor_count":439,
        "Challenge_repo_fork_count":17219,
        "Challenge_repo_issue_count":20692,
        "Challenge_repo_star_count":76135,
        "Challenge_repo_watch_count":860,
        "Challenge_score_count":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":913.5883333333,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"cc @philschmid  I would also suggest @kanwari3 to\r\n- try to use the same Python\/PyTorch\/transformers versions (and other libraries) on SageMaker that work locally (if possible)\r\n- if the above doesn't work, try to use on local machine the same versions as those used on SageMaker, and see if you still get errors\r\n\r\nSo we have a better idea about if this is indeed a SageMaker issue or libraries issue This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. cc @philschmid  , cc @ydshieh ,  cc @sgugger \r\nHi, \r\n\r\nThis is a follow up on this post with the same title. We are trying to fix the issue and are still getting the same error after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. We tried unsqueezing the input_ids and attention_masks but it didn\u2019t fix the error.\r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\n    return {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.",
        "Solution_gpt_summary":"tri match transform pytorch version unsqueez input id attent mask persist feedback assist",
        "Solution_link_count":1.0,
        "Solution_original_content":"philschmid kanwari pytorch transform version librari local local version idea librari automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor philschmid ydshieh sgugger titl match transform pytorch version accord respect valueerror valu unpack model led transform modul input id shape tri unsqueez input id attent mask didnt updat tri unsqueez input tensor model led unsqueez col return input id torch unsqueez input id pubm train pubm train map unsqueez col greatli feedback",
        "Solution_preprocessed_content":"version local local version idea librari automat mark stale activ address comment thread note ignor titl match transform pytorch version accord valueerror valu unpack transform modul shape tri unsqueez didnt updat tri unsqueez input tensor return greatli feedback",
        "Solution_readability":10.8,
        "Solution_reading_time":20.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":97.4961111111,
        "Challenge_answer_count":0,
        "Challenge_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Challenge_closed_time":1635155013000,
        "Challenge_created_time":1634804027000,
        "Challenge_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":6,
        "Challenge_repo_fork_count":5,
        "Challenge_repo_issue_count":65,
        "Challenge_repo_star_count":5,
        "Challenge_repo_watch_count":5,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":97.4961111111,
        "Challenge_title":"wandb create index name error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0923076923,
        "Challenge_watch_issue_ratio":0.0769230769
    },
    {
        "Challenge_adjusted_solved_time":341.4130555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Challenge_closed_time":1642711286000,
        "Challenge_created_time":1641482199000,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":135,
        "Challenge_repo_fork_count":498,
        "Challenge_repo_issue_count":3155,
        "Challenge_repo_star_count":4302,
        "Challenge_repo_watch_count":60,
        "Challenge_score_count":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":341.4130555556,
        "Challenge_title":"MLflow pyfunc model can't be loaded",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":187,
        "Platform":"Github",
        "Solution_body":"I think this version is not yet up-to-date with our 1.0 branch. cc @parano Line 85 is different from `main` branch @alexdivet @aarnphm the new 1.0.0a2 was just released, could you help confirm the issue has been resolved? ![Screenshot 2022-01-20 at 15 39 52](https:\/\/user-images.githubusercontent.com\/29749331\/150418600-d75d6b01-679d-4fa7-9a66-395262c13569.png)\r\nWorks just fine for me\r\nRunning on M1 Max, Python 3.9.10, with Rosetta 2. Please let me know if you still run into problems @alexdivet It works on my end too. Thanks for resolving it \ud83d\ude4f\ud83c\udffb",
        "Solution_gpt_summary":"version date branch version releas rosetta end",
        "Solution_link_count":1.0,
        "Solution_original_content":"version date branch parano line branch alexdivet aarnphm releas screenshot http imag githubusercont com ddb png run rosetta run alexdivet end",
        "Solution_preprocessed_content":"version branch line branch releas run rosetta run end",
        "Solution_readability":4.8,
        "Solution_reading_time":6.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0427892235,
        "Challenge_watch_issue_ratio":0.0190174326
    },
    {
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":2,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_created_time":1657726485000,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56,
        "Challenge_repo_fork_count":246,
        "Challenge_repo_issue_count":1160,
        "Challenge_repo_star_count":2570,
        "Challenge_repo_watch_count":37,
        "Challenge_score_count":0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":829,
        "Platform":"Github",
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_gpt_summary":"instal zenml branch pip instal git http github com zenml zenml git bugfix window sourc util close",
        "Solution_link_count":1.0,
        "Solution_original_content":"danguitavina guess stack trace run window local orchestr guess instal zenml branch pip instal git http github com zenml zenml git bugfix window sourc util schustmi close",
        "Solution_preprocessed_content":"guess stack trace run window local orchestr guess instal zenml branch close",
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0482758621,
        "Challenge_watch_issue_ratio":0.0318965517
    },
    {
        "Challenge_adjusted_solved_time":291.4263888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Challenge_closed_time":1605715301000,
        "Challenge_created_time":1604666166000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.6,
        "Challenge_reading_time":65.61,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":291.4263888889,
        "Challenge_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":337,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_created_time":1593379921000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":2.4247222222,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668600473000,
        "Challenge_created_time":1668591744000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.18,
        "Challenge_repo_contributor_count":92,
        "Challenge_repo_fork_count":2591,
        "Challenge_repo_issue_count":1867,
        "Challenge_repo_star_count":14671,
        "Challenge_repo_watch_count":266,
        "Challenge_score_count":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.4247222222,
        "Challenge_title":"[BUG] xdeepfm error in AzureML test",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":162,
        "Platform":"Github",
        "Solution_body":"```\r\n pytest tests\/integration\/examples\/test_notebooks_gpu.py::test_xdeepfm_integration --disable-warnings --durations 0\r\n```\r\nwith \r\n```\r\n@pytest.mark.gpu\r\n@pytest.mark.notebooks\r\n@pytest.mark.integration\r\n@pytest.mark.parametrize(\r\n    \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n    [\r\n        (\r\n            15,\r\n            10,\r\n            {\r\n                \"res_syn\": {\"auc\": 0.9716, \"logloss\": 0.699},\r\n                \"res_real\": {\"auc\": 0.749, \"logloss\": 0.4926},\r\n            },\r\n            42,\r\n        )\r\n    ],\r\n)\r\ndef test_xdeepfm_integration(\r\n    notebooks,\r\n    output_notebook,\r\n    kernel_name,\r\n    syn_epochs,\r\n    criteo_epochs,\r\n    expected_values,\r\n    seed,\r\n):\r\n    notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n    pm.execute_notebook(\r\n        notebook_path,\r\n        output_notebook,\r\n        kernel_name=kernel_name,\r\n        parameters=dict(\r\n            EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n            EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n            BATCH_SIZE_SYNTHETIC=1024,\r\n            BATCH_SIZE_CRITEO=1024,\r\n            RANDOM_SEED=seed,\r\n        ),\r\n    )\r\n    results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n        \"data\"\r\n    ]\r\n\r\n    for key, value in expected_values.items():\r\n        assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\n        assert results[key][\"logloss\"] == pytest.approx(\r\n            value[\"logloss\"], rel=TOL, abs=ABS_TOL\r\n        )\r\n```",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pytest test integr test notebook gpu test xdeepfm integr disabl warn durat pytest mark gpu pytest mark notebook pytest mark integr pytest mark parametr syn epoch criteo epoch valu seed re syn auc logloss re auc logloss test xdeepfm integr notebook output notebook kernel syn epoch criteo epoch valu seed notebook path notebook xdeepfm quickstart execut notebook notebook path output notebook kernel kernel paramet dict epoch synthet run syn epoch epoch criteo run criteo epoch batch size synthet batch size criteo random seed seed read notebook output notebook scrap datafram set index data kei valu valu item assert kei auc pytest approx valu auc rel tol ab ab tol assert kei logloss pytest approx valu logloss rel tol ab ab tol",
        "Solution_preprocessed_content":null,
        "Solution_readability":19.8,
        "Solution_reading_time":15.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":1.3086111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Challenge_closed_time":1634915290000,
        "Challenge_created_time":1634910579000,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":2.4,
        "Challenge_reading_time":3.53,
        "Challenge_repo_contributor_count":4,
        "Challenge_repo_fork_count":9,
        "Challenge_repo_issue_count":124,
        "Challenge_repo_star_count":94,
        "Challenge_repo_watch_count":2,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.3086111111,
        "Challenge_title":"wrong wandb dataset file name",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":49,
        "Platform":"Github",
        "Solution_body":"feature_68\uc5d0 \uc5c5\ub370\uc774\ud2b8\ub41c \uba54\uc778 \ube0c\ub79c\uce58\uac00 \uc801\uc6a9 \uc548\ub418\uc11c \uadf8\ub7f0\uac83.\r\nPR \uc0dd\uc131\ud574\uc11c \uc218\uc815\ud558\uc790",
        "Solution_gpt_summary":"creat pull request updat branch file data load file train tsv data load",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-0.8,
        "Solution_reading_time":0.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0322580645,
        "Challenge_watch_issue_ratio":0.0161290323
    },
    {
        "Challenge_adjusted_solved_time":47.865,
        "Challenge_answer_count":6,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449956000,
        "Challenge_created_time":1650277642000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":47.865,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":11.0,
        "Solution_original_content":"maverick scientist recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview http loivilrq http axkmilrr creat fake data minim reproduc reproduc publicli dataset pycaret nikhil team share file save time mon apr nikhil gupta wrote maverick scientist recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview http loivilrq http axkmilrr creat fake data minim reproduc reproduc publicli dataset pycaret repli email directli github unsubscrib receiv messag maverick scientist honestli prefer set preced open sourc commun sustain run global accept practic minim reproduc encourag nikhil attach file reproduc abhinav zip http github com pycaret pycaret file zip maverick scientist pycaret gener relat pycaret repo nikhil repli read carefulli clearli dilemma forum anywai what prevent run properli wed apr nikhil gupta wrote maverick scientist pycaret gener relat pycaret repo repli email directli github unsubscrib receiv messag",
        "Solution_preprocessed_content":"recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview creat fake data minim reproduc reproduc publicli dataset pycaret nikhil team share file save time mon apr nikhil gupta wrote recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview creat fake data minim reproduc reproduc publicli dataset pycaret repli email directli github unsubscrib receiv honestli prefer set preced commun sustain run global accept practic minim reproduc encourag nikhil attach file reproduc abhinav pycaret gener relat pycaret repo nikhil repli read carefulli clearli dilemma forum anywai what prevent run properli wed apr nikhil gupta wrote pycaret gener relat pycaret repo repli email directli github unsubscrib receiv",
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":511.6538888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Challenge_closed_time":1610230183000,
        "Challenge_created_time":1608388229000,
        "Challenge_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":23.7,
        "Challenge_reading_time":251.52,
        "Challenge_repo_contributor_count":4,
        "Challenge_repo_fork_count":4,
        "Challenge_repo_issue_count":210,
        "Challenge_repo_star_count":8,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":290,
        "Challenge_solved_time":511.6538888889,
        "Challenge_title":"Error when starting new experiment in mlflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1131,
        "Platform":"Github",
        "Solution_body":"Already fixed in #79 by introducing delay between mlflow server start and starting experiments in mlflow",
        "Solution_gpt_summary":"pipelin github action test pass start introduc delai server start start",
        "Solution_link_count":0.0,
        "Solution_original_content":"introduc delai server start start",
        "Solution_preprocessed_content":"introduc delai server start start",
        "Solution_readability":10.7,
        "Solution_reading_time":1.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.019047619,
        "Challenge_watch_issue_ratio":0.0285714286
    },
    {
        "Challenge_adjusted_solved_time":1317.2311111111,
        "Challenge_answer_count":13,
        "Challenge_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Challenge_closed_time":1563269566000,
        "Challenge_created_time":1558527534000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":21.6,
        "Challenge_reading_time":18.74,
        "Challenge_repo_contributor_count":326,
        "Challenge_repo_fork_count":1350,
        "Challenge_repo_issue_count":8555,
        "Challenge_repo_star_count":3062,
        "Challenge_repo_watch_count":103,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1317.2311111111,
        "Challenge_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":116,
        "Platform":"Github",
        "Solution_body":"Hi @Jeffwan ,\r\n\r\nneed your support on this.\r\n\r\nI am using training image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" and it is throwing an error for mising values for parameters K and feature_dim. Although we are not using these parameters anywhere in pipeline.\r\n\r\nCan you please provide the solution ?\r\n\r\nRegards,\r\nVarun em. I may delete the configuration fields in clean up. Let me double check and come back to you @vackysh  I can reproduce this issue. \r\n\r\n`HyperParameters` was removed by me in this commit\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/commit\/26f2719c28a731d8925ae2ce96252be1df2562aa\r\n\r\nAdd it back will solve this problem Image has been rebuilt and it should be good now.  Hi @Jeffwan ,\r\n\r\nThanks for your response.\r\n\r\nI again executed the pipeline using image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" , but getting the same issue\r\n\r\n\"Training failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\"\r\n\r\nThe pipeline parameters are:\r\n\r\n@dsl.pipeline(\r\nname='MNIST Classification pipeline',\r\ndescription='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\nimage='382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1',\r\ndataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\ninstance_type='ml.c4.8xlarge',\r\ninstance_count='2',\r\nvolume_size='50',\r\nmodel_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\nbatch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\nbatch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\nrole_arn=''\r\n):\r\n\r\nPlease suggest how to get through it if issue has already fixed at your end.\r\n @vackysh I think the problem is your machine already has this image. could you go to the machine and do a force pull? \r\n```\r\nseedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05\r\n``` HI @Jeffwan ,\r\n\r\nI refreshed the image It is now working fine.\r\nThank you so much.\r\n\r\nRegards,\r\nVarun Hi @Jeffwan,\r\n\r\nWhere can i get the actual source code (ML code ) reading from the image seedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05 (not a docker file, but actual logic for train, predction) ?\r\n\r\nI actually working on similar automation and want to analyse the source code.\r\n\r\nRegards,\r\nVarun @vackysh This is a component example for training. \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/train.py\r\n\r\nNot sure if your work is internal or public. It would be perfect if you can make some contribution! Feel free to ping me on Slack or shoot me an email. Hi @Jeffwan  ,\r\n\r\nThanks for information. But i am looking for the main Kmean algorithms code that is used for training the model. I couldn't find that anywhere on path.\r\n\r\nI have a requirement where Scikit SVM model to get deploy on kubeflow pipeline using aws sagemaker services and S3. So i want to have a look on source ML code that has been passed through image as an input to pipeline.\r\n\r\nRegards,\r\nVarun\r\n\r\n @vackysh Now I get your point, in the example, I am using the container images from SageMaker. I think KMEANS one is first-party models and you might don't have access to it. What I suggest you to do is bring your own training image if you have customization request. This issue is resolved. Now closing > This issue is resolved. Now closing\r\n\r\nHave you figured out a way to make it? Did you try bring your own container? ",
        "Solution_gpt_summary":"remov hyperparamet field commit refresh imag sourc kmean algorithm train model path train imag request",
        "Solution_link_count":2.0,
        "Solution_original_content":"jeffwan train imag dkr ecr amazonaw com kmean throw mise valu paramet featur dim paramet pipelin varun delet configur field clean doubl come vackysh reproduc hyperparamet remov commit http github com kubeflow pipelin commit fcadaecebedfaa add imag rebuilt jeffwan respons execut pipelin imag dkr ecr amazonaw com kmean train clienterror valu specifi featur dim hyperparamet validationerror pipelin paramet dsl pipelin mnist classif pipelin descript mnist classif kmean mnist classif region imag dkr ecr amazonaw com kmean dataset path mnist kmean data instanc type xlarg instanc count volum size model output path mnist kmean model batch transform input mnist kmean input batch transform ouput mnist kmean output role arn end vackysh imag forc pull seedjeffwan kubeflow pipelin jeffwan refresh imag varun jeffwan sourc read imag seedjeffwan kubeflow pipelin docker file logic train predction autom analys sourc varun vackysh compon train http github com kubeflow pipelin blob master compon train src train intern public perfect contribut free ping slack shoot email jeffwan kmean algorithm train model path scikit svm model deploi kubeflow pipelin servic sourc pass imag input pipelin varun vackysh imag kmean parti model access train imag request close close figur",
        "Solution_preprocessed_content":"train imag throw mise valu paramet paramet pipelin varun delet configur field clean doubl come reproduc remov commit add imag rebuilt respons execut pipelin imag train clienterror valu specifi hyperparamet pipelin paramet mnist classif pipelin descript mnist classif kmean end imag forc pull refresh imag varun sourc read imag autom analys sourc varun compon train intern public perfect contribut free ping slack shoot email kmean algorithm train model path scikit svm model deploi kubeflow pipelin servic sourc pass imag input pipelin varun imag kmean model access train imag request close close figur",
        "Solution_readability":9.1,
        "Solution_reading_time":43.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":64.4975,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Challenge_closed_time":1663248037000,
        "Challenge_created_time":1663015846000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":16.2,
        "Challenge_reading_time":63.87,
        "Challenge_repo_contributor_count":44,
        "Challenge_repo_fork_count":385,
        "Challenge_repo_issue_count":712,
        "Challenge_repo_star_count":2929,
        "Challenge_repo_watch_count":75,
        "Challenge_score_count":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":64.4975,
        "Challenge_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":2,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_created_time":1581438052000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.5533333333,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_gpt_summary":"upgrad sdk newest version dataset class evolv significantli version",
        "Solution_link_count":1.0,
        "Solution_original_content":"andrewkinsella version year ago http doc com releas note time dataset class evolv significantli upgrad sdk newest version maymsft swanderz",
        "Solution_preprocessed_content":"version time class evolv significantli upgrad sdk newest version",
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_created_time":1576464430000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":409,
        "Platform":"Github",
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_gpt_summary":"logger distribut backend ddp gpu slurm multiprocess pickl nest function function rest store implement git temporari review merg version instal master pickl merg master branch coupl dai ago",
        "Solution_link_count":4.0,
        "Solution_original_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu lt core gpu pytorch pytorch lightn tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard ttdummyfilewrit logdir pickl thread lock copi process report build dbczumar smurch neggert open pytorch lightn ident pickl ddp logger reproduc gave led lightn version gist reproduc pytorch lightn http gist github com polyphenolx efcffaeb easili reproduc output home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn log packag renam logger deprec packag remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn logger modul renam deprec modul remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn data loader decor deprec remov warn warn arg kwarg gpu environ variabl node rank defin set cuda visibl devic traceback file test line trainer fit model file home anaconda env pytorch lib site packag pytorch lightn trainer trainer line fit spawn ddp train nproc num process arg model file home anaconda env pytorch lib site packag torch multiprocess spawn line spawn return start process arg nproc daemon start spawn file home anaconda env pytorch lib site packag torch multiprocess spawn line start process process start file home anaconda env pytorch lib multiprocess process line start popen popen file home anaconda env pytorch lib multiprocess context line popen return popen process obj file home anaconda env pytorch lib multiprocess popen spawn posix line init init process obj file home anaconda env pytorch lib multiprocess popen fork line init launch process obj file home anaconda env pytorch lib multiprocess popen spawn posix line launch reduct dump process obj file home anaconda env pytorch lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object rest store default host cred environ cuda gpu geforc gtx geforc gtx geforc gtx geforc gtx version packag numpi pytorch debug pytorch version pytorch lightn tensorboard tqdm linux architectur bit processor version ubuntu smp mon utc add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari review merg pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run http github com minimaxir aitextgen imag http imag githubusercont com cac fdd png imag http imag githubusercont com facb bfde png strategi version version",
        "Solution_preprocessed_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu core gpu pytorch tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard pickl copi process report build ident pickl ddp logger reproduc gave led gist reproduc easili reproduc output environ add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run strategi version version",
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":72.0,
        "Solution_word_count":675.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":2.1180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Challenge_closed_time":1620393602000,
        "Challenge_created_time":1620385977000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":7,
        "Challenge_repo_issue_count":644,
        "Challenge_repo_star_count":29,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.1180555556,
        "Challenge_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":166,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":1434.0155555556,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using `PyTorchLightningPruningCallback` to search best hyperparams, it reports `AttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'`\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom typing import List, Optional\r\n\r\nimport optuna\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\r\nfrom torch.utils.data import random_split, DataLoader\r\n\r\n\r\nclass FashionDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str, batch_size: int):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.test_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=False, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.train_set, self.valid_set = random_split(self.train_set, [55000, 5000])\r\n\r\n    def train_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n    def test_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super(SimpleNet, self).__init__()\r\n\r\n        hidden_layers = []\r\n        d_inp = 28 * 28\r\n        for d_hid in d_hids:\r\n            hidden_layers.append(nn.Linear(d_inp, d_hid))\r\n            hidden_layers.append(nn.ReLU())\r\n            hidden_layers.append(nn.Dropout(p_drop))\r\n            d_inp = d_hid\r\n        hidden_layers.append(nn.Linear(d_inp, 10))\r\n\r\n        self.layers = nn.Sequential(*hidden_layers)\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(inputs)\r\n\r\n\r\nclass LitSimpleNet(pl.LightningModule):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super().__init__()\r\n        self.model = SimpleNet(d_hids, p_drop)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.accuracy = torchmetrics.Accuracy()\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.model(inputs.view(-1, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx) -> torch.Tensor:\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        return self.criterion(outputs, targets)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        self.accuracy(outputs, targets)\r\n        self.log(\"valid_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=3e-4, weight_decay=1e-5)\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial) -> float:\r\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n    p_drop = trial.suggest_float(\"p_drop\", 0.1, 0.5)\r\n    d_hids = [trial.suggest_int(f\"d_hid_{i}\", 16, 128, log=True) for i in range(n_layers)]\r\n\r\n    datamodule = FashionDataModule(\".\", 128)\r\n    model = LitSimpleNet(d_hids, p_drop)\r\n    trainer = pl.Trainer(\r\n        max_epochs=20,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        enable_checkpointing=False,\r\n        logger=True,\r\n        default_root_dir=\".\",\r\n        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"valid_acc\")]\r\n    )\r\n\r\n    hparams = dict(n_layers=n_layers, d_hids=d_hids, p_drop=p_drop)\r\n    trainer.logger.log_hyperparams(hparams)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    return trainer.callback_metrics[\"valid_acc\"].item()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pruner = optuna.pruners.MedianPruner()\r\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\r\n    study.optimize(objective, n_trials=100, timeout=1000)\r\n\r\n    print(\"Number of Finished Trials:\", len(study.trials))\r\n\r\n    trial = study.best_trial\r\n    print(\"Best Trial:\")\r\n    print(\"\\tValue:\", trial.value)\r\n    print(\"\\tParams:\")\r\n    for key, value in trial.params.items():\r\n        print(f\"\\t\\t{key}: {value}\")\r\n\r\n```\r\n\r\n```bash\r\n[W 2022-09-08 20:14:45,294] Trial 0 failed because of the following error: AttributeError(\"'AcceleratorConnector' object has no attribute 'distributed_backend'\")\r\nTraceback (most recent call last):\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/study\/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"optuna_examples\/optuna_lightning_example.py\", line 89, in objective\r\n    trainer = pl.Trainer(\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/argparse.py\", line 345, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 497, in __init__\r\n    self._call_callback_hooks(\"on_init_start\")\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1585, in _call_callback_hooks\r\n    fn(self, *args, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/integration\/pytorch_lightning.py\", line 61, in on_init_start\r\n    trainer._accelerator_connector.distributed_backend is not None  # type: ignore\r\nAttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'\r\n```\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/github\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.ipynb\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n\r\n### Expected behavior\r\n\r\nShould not report any errors.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n\r\n```\r\n\r\n\r\n<details>\r\n  <summary>Details<\/summary>\r\n    Paste the output here and move this toggle outside of the comment block.\r\n<\/details>\r\n\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- Lightning Component:  Trainer\r\n- PyTorch Lightning Version:  1.7.5\r\n- PyTorch Version:  1.12.1\r\n- Python version: 3.8.13\r\n- OS: Linux (Ubuntu 20.04)\r\n- CUDA\/cuDNN version: 11.3.1\r\n- How you installed PyTorch: conda\r\n\r\n\n\ncc @akihironitta",
        "Challenge_closed_time":1667802669000,
        "Challenge_created_time":1662640213000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/14604",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":17.8,
        "Challenge_reading_time":88.49,
        "Challenge_repo_contributor_count":447,
        "Challenge_repo_fork_count":2788,
        "Challenge_repo_issue_count":14589,
        "Challenge_repo_star_count":22027,
        "Challenge_repo_watch_count":231,
        "Challenge_score_count":0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":1434.0155555556,
        "Challenge_title":"Optuna integration reports AttributeError",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"Hey, @RegiusQuant. \r\n\r\nSide answer, you might be interested by Lightning HPO: https:\/\/github.com\/Lightning-AI\/lightning-hpo. This enables to run Optuna with PyTorch Lightning without friction and scalable in the cloud.\r\n\r\n Hey, @RegiusQuant - Thanks for the question. Can you please point me to the version of `optuna` that you are using?  For reference: https:\/\/github.com\/optuna\/optuna\/issues\/3978 @krshrimali Optuna version\uff1a3.0.0 I'm observing the same issue with Optuna 3.0.2 @hrzn Hi, I'm from the Optuna-dev team. Optuna's pytorch-lightning (PL) integration module doesn't support PL>=1.6 because it broke backwards-compatibility as investigated in https:\/\/github.com\/optuna\/optuna\/issues\/3418. Unfortunately, Optuna team doesn't have time to fix the module soon to support recent PL; we would like to wait for a PR from optuna and PL users.\r\n\r\n@tchaton I believe you can close this issue because the issue comes from Optuna... With Optuna==3.0.2 with lightning==1.5.10, I got \r\n`ValueError: optuna.integration.PyTorchLightningPruningCallback supports only optuna.storages.RDBStorage in DDP.`\r\nAfter downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.  @mikiotada Again, the error does not relate to PL. As the error message said, Optuna's integration does not support DDP without RDBStorage. \r\n\r\n> After downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.\r\n\r\nIn my understanding, Optuna 2.x didn't officially support DDP; it does not work as you expected, I'm afraid even though there was no error. Closing this issue as there seems nothing we can address from our side. Please refer to https:\/\/github.com\/optuna\/optuna\/issues\/3418.",
        "Solution_gpt_summary":"pytorch lightn integr modul broke backward compat downgrad keep lightn offici ddp team time modul soon",
        "Solution_link_count":4.0,
        "Solution_original_content":"regiusqu lightn hpo http github com lightn lightn hpo enabl run pytorch lightn friction scalabl cloud regiusqu version http github com krshrimali version observ hrzn dev team pytorch lightn integr modul broke backward compat http github com unfortun team time modul soon wait tchaton believ close come lightn valueerror integr pytorchlightningpruningcallback storag rdbstorag ddp downgrad arbitrari version keep lightn ran mikiotada relat messag said integr ddp rdbstorag downgrad arbitrari version keep lightn ran offici ddp afraid close address http github com",
        "Solution_preprocessed_content":"lightn hpo enabl run pytorch lightn friction scalabl cloud version valueerror downgrad keep ran relat messag said integr ddp rdbstorag downgrad keep ran offici ddp afraid close address",
        "Solution_readability":8.9,
        "Solution_reading_time":21.93,
        "Solution_score_count":5.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":232.0,
        "Tool":"Optuna",
        "Challenge_contributor_issue_ratio":0.0306395229,
        "Challenge_watch_issue_ratio":0.0158338474
    },
    {
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_created_time":1586647457000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":2,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":95,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great pytorchlightn core contributor alsrgv submit catch happi pointer broken log aggreg flush end flush call test pass alexeykarnachev idea sorri gui miss messag borda end idea github action hang http github com pytorchlightn pytorch lightn pull run",
        "Solution_preprocessed_content":"contribut great submit catch happi pointer broken log aggreg flush end flush call test pass idea sorri gui miss messag end idea github action",
        "Solution_readability":5.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":88.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":1.4025,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1656583953000,
        "Challenge_created_time":1656578904000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.8,
        "Challenge_reading_time":18.48,
        "Challenge_repo_contributor_count":5,
        "Challenge_repo_fork_count":8,
        "Challenge_repo_issue_count":39,
        "Challenge_repo_star_count":9,
        "Challenge_repo_watch_count":1,
        "Challenge_score_count":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4025,
        "Challenge_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":147,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":0.8413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1599658056000,
        "Challenge_created_time":1599655027000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":31.44,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.8413888889,
        "Challenge_title":"CometLogger failing without save_dir",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":206,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue!",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.7,
        "Solution_reading_time":0.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":28.415,
        "Challenge_answer_count":0,
        "Challenge_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Challenge_closed_time":1580353817000,
        "Challenge_created_time":1580251523000,
        "Challenge_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.0,
        "Challenge_repo_contributor_count":7,
        "Challenge_repo_fork_count":30,
        "Challenge_repo_issue_count":92,
        "Challenge_repo_star_count":255,
        "Challenge_repo_watch_count":12,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.415,
        "Challenge_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0760869565,
        "Challenge_watch_issue_ratio":0.1304347826
    },
    {
        "Challenge_adjusted_solved_time":719.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Challenge_closed_time":1591023634000,
        "Challenge_created_time":1588434434000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.8,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":6,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":719.2222222222,
        "Challenge_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":171,
        "Platform":"Github",
        "Solution_body":"@ceyzaguirre4 pls ^^",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.8,
        "Solution_reading_time":0.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":24.1527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Challenge_closed_time":1661948109000,
        "Challenge_created_time":1661861159000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.9,
        "Challenge_reading_time":59.04,
        "Challenge_repo_contributor_count":11,
        "Challenge_repo_fork_count":20,
        "Challenge_repo_issue_count":377,
        "Challenge_repo_star_count":23,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":24.1527777778,
        "Challenge_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":330,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":524.3688888889,
        "Challenge_answer_count":10,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654301000,
        "Challenge_created_time":1620766573000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_open_time":null,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":524.3688888889,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":112,
        "Platform":"Github",
        "Solution_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Solution_gpt_summary":"set environ env environ workspac tutori myenv env clone automl env environ process displai messag valueerror list tri reproduc step creat workaround pass train env train env environ workspac automl releas forecast time column pass automl set plan regress",
        "Solution_link_count":3.0,
        "Solution_original_content":"updat set environ core environ import environ env environ workspac tutori myenv env clone automl env train step automlpipelinebuild model train step automl set automl set train data dataset input comput target comput target partit column partit column node count process count node run invoc timeout train env myenv environ process displai messag valueerror list imag http imag githubusercont com ddb png load run type load entrypoint automl train automl run auto run dto except pyarrow env feaebbeefad lib site packag pars pyarrow dataset runtim traceback file env feaebbeefad lib site packag train automl runtim model train model line log run data file path arg automl set step run file env feaebbeefad lib site packag train automl runtim model train model line run data read csv file path pars date timestamp column file env feaebbeefad lib site packag panda parser line parser return read filepath buffer kwd file env feaebbeefad lib site packag panda parser line read parser textfileread buf kwd file env feaebbeefad lib site packag panda parser line init engin engin file env feaebbeefad lib site packag panda parser line engin engin cparserwrapp option file env feaebbeefad lib site packag panda parser line init set noconvert column file env feaebbeefad lib site packag panda parser line set noconvert column set val file env feaebbeefad lib site packag panda parser line set index valueerror list cristina crisans surfac driver log shbijlan delet workspac tri reproduc step creat messag version imag http imag githubusercont com befe cfafaec png relat realiz comput instanc version pip instal upgrad sdk automl pip instal contrib automl pipelin step imag http imag githubusercont com beff png crisans surfac driver log core import environ train env environ workspac automl pass train env workaround releas forecast time column pass automl set deeptim instruct environ run cell pipelin end regress model releas addit regress core import environ train env environ workspac automl pass train env workaround releas forecast time column pass automl set plan regress cartacio visibl deeptim import add function regress classif plan regress cartacio visibl crisans roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact sabina cartacio com cartacio start multipl model regress send email crisans roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact sabina cartacio com mailto sabina cartacio com close track offlin featur request model sabina",
        "Solution_preprocessed_content":"updat set environ import environ env myenv environ process displai messag valueerror list load load entrypoint automl except traceback file line log arg file line run data file line return kwd file line parser kwd file line file line file line file line file line valueerror list cristina surfac delet workspac tri reproduc step creat messag version relat realiz comput instanc version pip instal sdk pip instal surfac core import environ pass workaround releas forecast pass automl set instruct environ run cell pipelin end regress model releas addit regress core import environ pass workaround releas forecast pass automl set plan regress visibl import add function regress classif plan regress visibl roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact start multipl model regress send email roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact close track offlin featur request model sabina",
        "Solution_readability":11.4,
        "Solution_reading_time":82.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":664.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":15.6752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Challenge_closed_time":1576023863000,
        "Challenge_created_time":1575967432000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.6752777778,
        "Challenge_title":"Comet PAPI Depreciated",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":5.8786111111,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1615314058000,
        "Challenge_created_time":1615292895000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/175",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.4,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":49,
        "Challenge_repo_issue_count":205,
        "Challenge_repo_star_count":144,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.8786111111,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":12,
        "Platform":"Github",
        "Solution_body":"Closing in favour of #174 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.5,
        "Solution_reading_time":0.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":3846.9952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Challenge_closed_time":1636988013000,
        "Challenge_created_time":1623138830000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3846.9952777778,
        "Challenge_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":172,
        "Platform":"Github",
        "Solution_body":"@tchaton Is this a lightning issue? Closing this issue as there is no progress nor manifestation from the Comet Team.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tchaton lightn close progress manifest team",
        "Solution_preprocessed_content":"lightn close progress manifest team",
        "Solution_readability":6.0,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_created_time":1587712154000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_gpt_summary":"version specifi version data tabulardataset panda datafram deprec core dataset dataset panda datafram version shown review articl dataset version version histori maintain pull data sql dataset",
        "Solution_link_count":5.0,
        "Solution_original_content":"swaticolab version specifi version http doc com api core data abstract dataset abstractdataset workspac version latest note core dataset dataset document http doc com api core core dataset dataset panda datafram panda datafram http doc com api core core dataset dataset panda datafram deprec replac data tabulardataset panda datafram http doc com api core data tabulardataset panda datafram null rang datetim null implement version shown rohitmungi msft maymsft dataset pointer data storag articl explain dataset version http doc com version track dataset",
        "Solution_preprocessed_content":"version specifi version note deprec replac implement version shown dataset pointer data storag articl explain dataset version",
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_created_time":1580225794000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2674,
        "Challenge_repo_issue_count":13570,
        "Challenge_repo_star_count":20939,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":703.9719444444,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_gpt_summary":"creat trainer object logger existingexperi object sdk workaround modifi logger class accept kei creat existingexperi sdk paramet present refactor store kei directli logger object instanti logger implement pull request report logger",
        "Solution_link_count":1.0,
        "Solution_original_content":"submit fdelrio hacki eleg haven time kei creat logger trainer logger logger trainer trainer logger logger model model trainer fit model kei logger kei logger logger kei kei trainer trainer logger logger trainer test model modifi logger class accept kei creat existingexperi sdk param present class logger lightningloggerbas properti mode onlin kei api kei api kei workspac workspac kwarg existingexperi api kei api kei workspac workspac previou kei kwarg return happili accept gui achiev haven time williamfalcon williamfalcon progress fdelrio logger object lifetim trainer mayb refactor store kei directli logger object instanti logger xsschauhan idea submit http github com pytorchlightn pytorch lightn pull free reopen creat robot dvirginz latest master minim dvirginz latest master minim sorri build sourc probabl open logger haven time delv",
        "Solution_preprocessed_content":"submit hacki eleg haven time kei creat logger trainer modifi class accept creat sdk param present happili accept gui achiev haven time progress logger object lifetim trainer mayb refactor store directli logger object logger idea submit free reopen creat robot latest master minim latest master minim sorri build sourc probabl open logger haven time delv",
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":2491.9302777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Challenge_closed_time":1655127391000,
        "Challenge_created_time":1646156442000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":86,
        "Challenge_repo_issue_count":1012,
        "Challenge_repo_star_count":1924,
        "Challenge_repo_watch_count":28,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2491.9302777778,
        "Challenge_title":"Support writing out dataset profiles as json format with mlflow",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"How can i retrieve the profile while inside the start_run()? This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"retriev profil insid start run stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"retriev profil insid stale remov stale label close tomorrow",
        "Solution_readability":2.8,
        "Solution_reading_time":1.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":316.8641666667,
        "Challenge_answer_count":8,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539227000,
        "Challenge_created_time":1660398516000,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":28,
        "Challenge_repo_fork_count":79,
        "Challenge_repo_issue_count":582,
        "Challenge_repo_star_count":246,
        "Challenge_repo_watch_count":16,
        "Challenge_score_count":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":316.8641666667,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Solution_gpt_summary":"upgrad latest version dbx creat artifact manual locat artifact delet artifact creat id github action verifi profil configur step public doc creat artifact time",
        "Solution_link_count":1.0,
        "Solution_original_content":"zermelozf stack trace dbx deploi environ default dbx start deploy environ default dbx profil file dbx auth config profileenvconfigprovid verifi dbx auth config profileenvconfigprovid verif dbx profil default deploy traceback file opt homebrew anaconda env bin dbx line sy exit cli file opt homebrew anaconda env lib site packag click core line return arg kwarg file opt homebrew anaconda env lib site packag click core line invok ctx file opt homebrew anaconda env lib site packag click core line invok return process sub ctx invok sub ctx file opt homebrew anaconda env lib site packag click core line invok return ctx invok callback ctx param file opt homebrew anaconda env lib site packag click core line invok return callback arg kwarg file opt homebrew anaconda env lib site packag dbx deploi line deploi api client prepar environ environ file opt homebrew anaconda env lib site packag dbx util common line prepar environ storageconfigurationmanag prepar file opt homebrew anaconda env lib site packag dbx api storag base line prepar cl setup properti file opt homebrew anaconda env lib site packag dbx api storag base line setup option properti workspac dir file opt homebrew anaconda env lib site packag track fluent line return client file opt homebrew anaconda env lib site packag track client line return track client file opt homebrew anaconda env lib site packag track track servic client line return store file opt homebrew anaconda env lib site packag store track rest store line rais file opt homebrew anaconda env lib site packag store track rest store line respons proto endpoint getexperimentbynam req bodi file opt homebrew anaconda env lib site packag store track rest store line endpoint return endpoint host cred endpoint json bodi respons proto file opt homebrew anaconda env lib site packag util rest util line endpoint respons verifi rest respons respons endpoint file opt homebrew anaconda env lib site packag util rest util line verifi rest respons rais restexcept json load respons text except restexcept paramet valu zermelozf version dbx upgrad latest renardeinsid upgrad dbx restexcept paramet valu deploi job time deploi job frida version run test verifi profil configur step public doc http doc com dev id html step instal sampl depend deploi dbx deploi debug option free omit host url come renardeinsid txt profil configur deploy json file environ default profil default workspac dir share dbx artifact locat dbf share dbx dbx deploi environ default deploy file conf deploy json job workaround sorri time invest creat artifact manual locat artifact delet artifact creat id github action creat artifact time frida pretti anywai client logic accordingli",
        "Solution_preprocessed_content":"stack trace version upgrad latest upgrad dbx restexcept deploi job time deploi job version run test verifi deploi option come profil configur file environ dbx deploi default workaround sorri time invest creat artifact manual locat artifact delet artifact creat id github action creat artifact time pretti anywai client logic accordingli",
        "Solution_readability":14.8,
        "Solution_reading_time":77.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":61.0,
        "Solution_word_count":510.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0481099656,
        "Challenge_watch_issue_ratio":0.0274914089
    },
    {
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_created_time":1630081759000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11,
        "Challenge_repo_fork_count":7,
        "Challenge_repo_issue_count":270,
        "Challenge_repo_star_count":13,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":27,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0407407407,
        "Challenge_watch_issue_ratio":0.0222222222
    },
    {
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":1,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_created_time":1600305917000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4,
        "Challenge_repo_fork_count":0,
        "Challenge_repo_issue_count":211,
        "Challenge_repo_star_count":7,
        "Challenge_repo_watch_count":3,
        "Challenge_score_count":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":96,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"label bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link app homepag http github com marketplac label bot dashboard http mlbot net data khirotaka enchant http github com hamelsmu mlapp bot",
        "Solution_preprocessed_content":"bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link bot",
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_created_time":1641941774000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":86,
        "Challenge_repo_issue_count":1012,
        "Challenge_repo_star_count":1924,
        "Challenge_repo_watch_count":28,
        "Challenge_score_count":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3662.6730555556,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_gpt_summary":"initi session creation updat creat session whylog session depend instal writer configur binder writer allow implicitli specifi config throw except option disabl choos cell",
        "Solution_link_count":2.0,
        "Solution_original_content":"close default session later whylog wrapper close session creat logger updat initi session creation whylog import creat session session creat session summari session profil datafram train train data flat summari summari summari updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer obviou reason http github com whylab whylog blob mainlin whylog yaml imag http imag githubusercont com cbccd png notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason took bit debug config relat vote throw except option disabl except choos path ignor drop cell config pipelin pipelin verbos writer save format protobuf output path type cfg file config yaml echo config cfg file whylog import creat session session creat session cfg file assert whylog version later integr whylog enabl session stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"close default session later whylog wrapper close session creat logger updat initi session creation updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason vote throw except option disabl except choos path ignor drop cell stale remov stale label close tomorrow",
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":261.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":44.3683333333,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Challenge_closed_time":1660908206000,
        "Challenge_created_time":1660748480000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.0,
        "Challenge_reading_time":27.79,
        "Challenge_repo_contributor_count":5,
        "Challenge_repo_fork_count":8,
        "Challenge_repo_issue_count":39,
        "Challenge_repo_star_count":9,
        "Challenge_repo_watch_count":1,
        "Challenge_score_count":0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":44.3683333333,
        "Challenge_title":"[mlflow] Migration Job should run before upgrade",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Hi @faceless7171 \r\n\r\nThank you very much for reporting the issue. Yes, your suggestion can work. Let me create a PR and test it. Well, we can't use the pre-hook option because we need a configuration file for the DB connection. And I don't want to make secrets visible in the container.\r\n\r\n```console\r\n\u2502 Events:                                                                                                                                                          \u2502\r\n\u2502   Type     Reason       Age               From               Message                                                                                             \u2502\r\n\u2502   ----     ------       ----              ----               -------                                                                                             \u2502\r\n\u2502   Normal   Scheduled    22s               default-scheduler  Successfully assigned default\/mlflow-bzb8s to minikube                                              \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"migrations-config\" : configmap \"mlflow-migrations\" not found   \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"dbchecker\" : configmap \"mlflow-dbchecker\" not found            \u2502\r\n\u2502                                                                                                                                                                  \u2502\r\n```\r\n\r\nSo, we have another option. Maybe we can use the init container pattern for this purpose. Let me try. @all-contributors please add @faceless7171  for bug @burakince \n\nI've put up [a pull request](https:\/\/github.com\/community-charts\/helm-charts\/pull\/37) to add @faceless7171! :tada: Hi @faceless7171 \r\n\r\nCould you please try again with mlflow chart minimum 0.7.0 version? @burakince tested on 0.7.1 version. Everything is working now. Thanks for the fix.",
        "Solution_gpt_summary":"migrat job run pod upgrad commun agre pre hook option configur file connect decid init pattern commun member creat pull request add minimum version chart",
        "Solution_link_count":1.0,
        "Solution_original_content":"faceless report creat test pre hook option configur file connect secret visibl consol event type reason ag messag normal schedul default schedul successfulli assign default bzb minikub warn failedmount kubelet mountvolum setup volum migrat config configmap migrat warn failedmount kubelet mountvolum setup volum dbchecker configmap dbchecker option mayb init pattern purpos contributor add faceless burakinc pull request http github com commun chart helm chart pull add faceless tada faceless chart minimum version burakinc test version",
        "Solution_preprocessed_content":"report creat test option configur file connect secret visibl option mayb init pattern purpos add add tada chart minimum version test version",
        "Solution_readability":7.2,
        "Solution_reading_time":15.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_created_time":1601583593000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2291,
        "Challenge_repo_issue_count":1857,
        "Challenge_repo_star_count":3523,
        "Challenge_repo_watch_count":2031,
        "Challenge_score_count":0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":338,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_created_time":1609957275000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":3,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7538.9758333333,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_gpt_summary":"sdk rang version older newer version switch older version pip improv perform version updat metadata sdk core sdk updat allow sdk releas sdk preview",
        "Solution_link_count":4.0,
        "Solution_original_content":"klawrawkz pip version sdk rang version older newer version repli harneetvirk pretti version version win version imag http imag githubusercont com bfd acb png ran farther pip instal upgrad upgrad strategi eager sdk stuck pip multipl version core determin version compat pip multipl version mgmt containerregistri determin version compat collect mgmt containerregistri download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl pip multipl version mgmt core determin version compat download mgmt containerregistri whl download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip hundr subpackag sdk depend come core compat sdk instal pip ignor option probabl johan environ local env local environ core unfortun core instal sdk creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon adamjstewart sdk preview http doc com concept sdk date",
        "Solution_preprocessed_content":"pip version sdk version version repli pretti version win version ran farther stuck advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip notic perform gotcha klaw sdk metadata updat hundr subpackag sdk depend come core compat sdk instal pip option probabl environ local env local environ unfortun instal creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon sdk preview sdk date",
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score_count":16.0,
        "Solution_sentence_count":77.0,
        "Solution_word_count":608.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":3.7161111111,
        "Challenge_answer_count":0,
        "Challenge_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Challenge_closed_time":1621785253000,
        "Challenge_created_time":1621771875000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":10,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":139,
        "Challenge_repo_star_count":357,
        "Challenge_repo_watch_count":9,
        "Challenge_score_count":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.7161111111,
        "Challenge_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":147.3688888889,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522860000,
        "Challenge_created_time":1588992332000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326,
        "Challenge_repo_fork_count":1350,
        "Challenge_repo_issue_count":8555,
        "Challenge_repo_star_count":3062,
        "Challenge_repo_watch_count":103,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":147.3688888889,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":141,
        "Platform":"Github",
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_gpt_summary":"imag host ecr imag instruct link ecr",
        "Solution_link_count":6.0,
        "Solution_original_content":"marwan oper imag host ecr framework tensorflow pytorch built algorithm http doc com latest algo html imag http doc com amazonecr latest userguid docker push ecr imag html instruct ecr http github com kubeflow pipelin marwan close free reopen reopen comment close bobgi close respons http github com kubeflow pipelin issuecom marwan close free reopen reopen comment close instruct interact comment http git commun contributor guid pull request relat file kubernet test infra http github com kubernet test infra titl prow repositori",
        "Solution_preprocessed_content":"oper imag host ecr framework tensorflow pytorch imag instruct ecr close free reopen comment close close respons close free reopen comment close instruct interact comment relat file repositori",
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":0.4194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065545000,
        "Challenge_created_time":1581064035000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2291,
        "Challenge_repo_issue_count":1857,
        "Challenge_repo_star_count":3523,
        "Challenge_repo_watch_count":2031,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4194444444,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":59.9219444444,
        "Challenge_answer_count":3,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488847000,
        "Challenge_created_time":1597273128000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":59.9219444444,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":82,
        "Platform":"Github",
        "Solution_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Solution_gpt_summary":"latest version pytorch lightn version checkpoint subfold save locat guid checkpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"david waterworth latest checkpoint subfold guid checkpoint david waterworth awaelchli",
        "Solution_preprocessed_content":null,
        "Solution_readability":5.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":52.535,
        "Challenge_answer_count":6,
        "Challenge_body":"## Expected Behavior\r\nCode example  from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## Actual Behavior\r\nCode example below from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # THIS IS THE METHOD THAT DOESN'T APPEAR TO WORK\r\n    model_upload_op = gcc_aip.ModelUploadOp(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\nOn the other hand, the method below worked:\r\n```\r\n # THIS METHOD DOES WORK\r\n    # aiplatform.Model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\nI'm currently using Vertex AI Pipelines to train a model and upload to Vertex AI. Currently in the pipeline, I'm attempting to use the ModelUploadOp class to upload a custom model to Vertex AI models. The logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## Specifications\r\n\r\nVersion: \r\n- Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp\r\n- Pipelines Version: kfp==1.8.11\r\n- Platform: Google Cloud Vertex AI \r\n\r\n[1]: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Challenge_closed_time":1646371495000,
        "Challenge_created_time":1646182369000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/349",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.2,
        "Challenge_reading_time":22.2,
        "Challenge_repo_contributor_count":84,
        "Challenge_repo_fork_count":365,
        "Challenge_repo_issue_count":1349,
        "Challenge_repo_star_count":544,
        "Challenge_repo_watch_count":33,
        "Challenge_score_count":2,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":52.535,
        "Challenge_title":"ModelUploadOp from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\"  does not work",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":174,
        "Platform":"Github",
        "Solution_body":"There was a recent breaking change. Will update notebook accordingly. Notebook has been updated, tested and merged. @andrewferlitsch can we close this issue since the notebook is merged ? yes, I will close it. I thought I had. Hi all,\r\nI saw this thread and the updated notebook - thanks for fixing it. \r\n\r\nI can't help but think that using `artifact_uri=WORKING_DIR` in the Importer Node seems misaligned with Kubeflow's focus on Artifacts and Artifact management. Is it possible to set `artifact_uri` to the Artifact location without hardcoding `WORKING_DIR`?\r\n\r\n```\r\nimport_unmanaged_model_task = importer_node.importer(\r\n        artifact_uri=WORKING_DIR,        <--- Can we set this without hardcoding WORKING_DIR?\r\n        artifact_class=artifact_types.UnmanagedContainerModel,\r\n        metadata={\r\n            \"containerSpec\": {\r\n                \"imageUri\": \"us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest\",\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nTo make things simple, let's say that instead of putting the training code in CustomTrainingJobOp, you define a custom function (below). In this case wouldn't it be more Kubeflow-ish to replace save the file to `Output[Model].path` instead of hard coding `WORKING_DIR` , like the following? \r\n\r\n```\r\ndef train_model(dataset: Input[Dataset],  model: Output[Model]):\r\n      ...\r\n\r\n       # then save model\r\n       # model.save(WORKING_DIR)   <---- This is the way outlined in the notebook\r\n       model.save(model.path)         <--- This seems more aligned with KFP than above\r\n```\r\n\r\nThen, when you wanted to upload the model, you would again replace `WORKING_DIR` with the location of the Artifact set by Kubeflow.\r\n\r\n```\r\n@kfp.dsl.pipeline(...)\r\ndef pipeline(...):\r\n        ...\r\n\r\n        # train model\r\n        train_model_op = train_model(...)\r\n\r\n        import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=train_model_op.outputs[\"model\"].uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nBut unfortunately you can't actually do this because you get an error: \"AttributeError: 'PipelinParam' object has no attribute uri'\". To avoid this error, you could also nest the Importer Node into a custom Component that has Input[Model] as one of the parameters. Then you could set `artifact_uri=model.uri`. \r\n\r\n```\r\n@component(...)\r\ndef custom_importer(trained_model: Input[Model], vertex_model: Output[Model]):\r\n     import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=trained_model.uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nUnfortunately, you can't do this as you get \"TypeError: There are no registered serializers for type \"google.UnmanagedContainerModel\".\" @natetsang If you want an easy way to upload models to Vertex Model Registry, you can use my components: https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/KFPv2_hell\/components\/google-cloud\/Vertex_AI\/Models \r\nExample usage: https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/05b2f255f8ccd7d8588f8143a76536bf83c2c7c7\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI\/pipeline.py#L51\r\n\r\n```Python\r\nupload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https:\/\/raw.githubusercontent.com\/Ark-kun\/pipeline_components\/719783ef44c04348ea23e247a93021d91cfe602d\/components\/google-cloud\/Vertex_AI\/Models\/Upload_Tensorflow_model\/component.yaml\")\r\n\r\n...\r\n    vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\r\n        model=model,\r\n    ).outputs[\"model_name\"]\r\n```\r\nWhere `model` is a `TensorflowSavedModel` artifact that was produced by `model.save(model_path)`.\r\n\r\nPlease open an issue in my repo if you have any issues or feature requests for the components I've linked.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"break updat notebook accordingli notebook updat test merg andrewferlitsch close notebook merg close thread updat notebook artifact uri dir import node misalign kubeflow focu artifact artifact set artifact uri artifact locat hardcod dir import unmanag model task import node import artifact uri dir set hardcod dir artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri docker pkg dev cloud aiplatform predict cpu latest put train customtrainingjobop defin function kubeflow ish replac save file output model path dir train model dataset input dataset model output model save model model save dir outlin notebook model save model path align kfp upload model replac dir locat artifact set kubeflow kfp dsl pipelin pipelin train model train model train model import unmanag model task import node import artifact uri train model output model uri artifact locat artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri serv imag uri unfortun attributeerror pipelinparam object attribut uri avoid nest import node compon input model paramet set artifact uri model uri compon import train model input model vertex model output model import unmanag model task import node import artifact uri train model uri artifact locat artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri serv imag uri unfortun typeerror regist serial type unmanagedcontainermodel natetsang upload model vertex model registri compon http github com ark kun pipelin compon tree kfpv hell compon cloud vertex model usag http github com ark kun pipelin compon blob bffccddfabfccc sampl cloud vertex train tabular regress model tensorflow import vertex pipelin upload tensorflow model cloud vertex compon load compon url http raw githubusercont com ark kun pipelin compon efceaeadcf compon cloud vertex model upload tensorflow model compon yaml vertex model upload tensorflow model cloud vertex model model output model model tensorflowsavedmodel artifact produc model save model path open repo featur request compon link",
        "Solution_preprocessed_content":"break updat notebook accordingli notebook updat test merg close notebook merg close thread updat notebook import node misalign kubeflow focu artifact artifact set artifact locat hardcod put train customtrainingjobop defin function replac save file upload model replac locat artifact set kubeflow unfortun attributeerror pipelinparam object attribut uri avoid nest import node compon input paramet set unfortun typeerror regist serial type upload model vertex model registri compon usag artifact produc open repo featur request compon link",
        "Solution_readability":15.8,
        "Solution_reading_time":49.27,
        "Solution_score_count":5.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":353.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0622683469,
        "Challenge_watch_issue_ratio":0.0244625649
    },
    {
        "Challenge_adjusted_solved_time":23.5547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Challenge_closed_time":1603222865000,
        "Challenge_created_time":1603138068000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":86,
        "Challenge_repo_issue_count":1012,
        "Challenge_repo_star_count":1924,
        "Challenge_repo_watch_count":28,
        "Challenge_score_count":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.5547222222,
        "Challenge_title":"MLFlow NameError",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":108,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":50.4269444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Challenge_closed_time":1615408051000,
        "Challenge_created_time":1615226514000,
        "Challenge_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":2,
        "Challenge_repo_fork_count":2,
        "Challenge_repo_issue_count":47,
        "Challenge_repo_star_count":6,
        "Challenge_repo_watch_count":4,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":50.4269444444,
        "Challenge_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":70,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0425531915,
        "Challenge_watch_issue_ratio":0.085106383
    },
    {
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_created_time":1592335014000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":49,
        "Challenge_repo_issue_count":205,
        "Challenge_repo_star_count":144,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2039.8183333333,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_gpt_summary":"pin master branch build type api backwardli compat master",
        "Solution_link_count":0.0,
        "Solution_original_content":"believ gener deepcopi branch backport previous modul pick reason pin master branch api backwardli compat master",
        "Solution_preprocessed_content":"believ gener deepcopi branch backport previous modul pick reason pin branch api backwardli compat",
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":8082.0786111111,
        "Challenge_answer_count":3,
        "Challenge_body":"It fails at the \"apply patch\" stage",
        "Challenge_closed_time":1624956881000,
        "Challenge_created_time":1595861398000,
        "Challenge_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.94,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":12,
        "Challenge_repo_issue_count":219,
        "Challenge_repo_star_count":42,
        "Challenge_repo_watch_count":4,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8082.0786111111,
        "Challenge_title":"Comet \"Reproduce\" feature doesn't work",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":11,
        "Platform":"Github",
        "Solution_body":"is this still an issue @51N84D ? Yeah, this still doesn't work. I don't think anyone has tried to resolve it yet Ok ; should we in your opinion?",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.1,
        "Solution_reading_time":1.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0410958904,
        "Challenge_watch_issue_ratio":0.0182648402
    },
    {
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":1,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_created_time":1645497684000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135,
        "Challenge_repo_fork_count":650,
        "Challenge_repo_issue_count":1974,
        "Challenge_repo_star_count":882,
        "Challenge_repo_watch_count":2756,
        "Challenge_score_count":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_gpt_summary":"pull request merg github repositori",
        "Solution_link_count":1.0,
        "Solution_original_content":"mitig merg http github com pull",
        "Solution_preprocessed_content":null,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0683890578,
        "Challenge_watch_issue_ratio":1.3961499493
    },
    {
        "Challenge_adjusted_solved_time":267.6955555556,
        "Challenge_answer_count":2,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342987000,
        "Challenge_created_time":1612379283000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6,
        "Challenge_repo_fork_count":2,
        "Challenge_repo_issue_count":604,
        "Challenge_repo_star_count":35,
        "Challenge_repo_watch_count":2,
        "Challenge_score_count":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":267.6955555556,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"that test gone latest verifi imipenem close observ",
        "Solution_preprocessed_content":"that test gone latest verifi close observ",
        "Solution_readability":3.3,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":2.7277777778,
        "Challenge_answer_count":4,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1658854769000,
        "Challenge_created_time":1658844949000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.3,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":5,
        "Challenge_repo_fork_count":8,
        "Challenge_repo_issue_count":39,
        "Challenge_repo_star_count":9,
        "Challenge_repo_watch_count":1,
        "Challenge_score_count":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.7277777778,
        "Challenge_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Hi @mikwieczorek \r\n\r\nThanks to inform us. Yes, probably it's my mistake. I changed it to the name some time ago. Let's write some tests and fix the problem. Well, it looks like your link refers to the port (service port) rather than targetPort (pod's port. This is currently what we use.). But we can even make it optional (port or targetPort selection) and use a name rather than a port number. It looks like it works with the latest version of Prometheus but I think we need to support all versions together.\r\n\r\nAnd [this is the full schema of endpoints field](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/api.md#monitoring.coreos.com\/v1.Endpoint).\r\n\r\nI will do some additional manual tests and send the PR. Hi @mikwieczorek \r\n\r\nChart version 0.3.0 should solve your problem. If it still accrues, feel free to reopen this issue. You can use the following command to update your deployment without the need for additional changes.\r\n\r\n```\r\nhelm repo update\r\nhelm upgrade --install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\r\n```\r\n\r\nBest,\r\nBurak Thank you @burakince for your prompt fix. It works correctly after the update. \r\nNext time, I will make an MR instead of just reporting the issue",
        "Solution_gpt_summary":"manual targetport port http deploi servicemonitor manifest propos targetport port http templat servicemonitor yaml port port chart version updat deploy addit helm repo updat helm upgrad instal namespac track server commun chart set servicemonitor enabl bias summari",
        "Solution_link_count":1.0,
        "Solution_original_content":"mikwieczorek probabl time ago write test link port servic port targetport pod port option port targetport select port latest version prometheu version schema endpoint field http github com prometheu oper prometheu oper blob document api monitor coreo com endpoint addit manual test send mikwieczorek chart version accru free reopen updat deploy addit helm repo updat helm upgrad instal namespac track server commun chart set servicemonitor enabl burak burakinc prompt updat time report",
        "Solution_preprocessed_content":"probabl time ago write test link port targetport option port latest version prometheu version addit manual test send chart version accru free reopen updat deploy addit burak prompt updat time report",
        "Solution_readability":7.3,
        "Solution_reading_time":15.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":187.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":519.1605555556,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Challenge_closed_time":1626388114000,
        "Challenge_created_time":1624519136000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":519.1605555556,
        "Challenge_title":"Warning while loading the azureml.core",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":101,
        "Platform":"Github",
        "Solution_body":"Can you try installing [azureml-core](https:\/\/pypi.org\/project\/azureml-core\/) instead? This is unfortunately a package conflict issue. I was able to create the conda environment by removing all the version pinning on `matplotlib`, `numpy`, ... all the way to `pytorch`, and changed `azureml-sdk` to `azureml-core`.  \r\n\r\n```yml\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib\r\n- numpy\r\n- seaborn\r\n- urllib3\r\n- scipy\r\n- scikit-learn\r\n- pandas\r\n- py-xgboost\r\n- jupyterlab\r\n- ipykernel\r\n- pytorch\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-core\r\n\r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]                               \r\n```\r\n\r\n```bash\r\n(base) \u279c  jiazho_playground \u2717 conda activate ati_reranking_automl_py36\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground git:(split-merge) \u2717 python\r\nPython 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)\r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import azureml.core\r\n>>>\r\n\r\n\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground \u2717 pip freeze | grep azureml.core\r\nazureml-core==1.32.0\r\n```\r\n",
        "Solution_gpt_summary":"instal core sdk remov version pin packag matplotlib numpi sdk core conda environ yml file",
        "Solution_link_count":1.0,
        "Solution_original_content":"instal core http pypi org core unfortun packag conflict creat conda environ remov version pin matplotlib numpi pytorch sdk core yml ati rerank automl depend interpret version later pip conda matplotlib numpi seaborn urllib scipi scikit panda jupyterlab ipykernel pytorch pip base sdk core pytorch transform score dep infer schema numpi bash base jiazho playground conda activ ati rerank automl ati rerank automl jiazho playground git split merg anaconda default jun gcc linux type copyright credit licens import core ati rerank automl jiazho playground pip freez grep core core",
        "Solution_preprocessed_content":"instal unfortun packag conflict creat conda environ remov version pin",
        "Solution_readability":10.4,
        "Solution_reading_time":15.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":2,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_gpt_summary":"setup panda iri starter add yml manual conf local folder instal version remov global group on bias summari",
        "Solution_link_count":3.0,
        "Solution_original_content":"wil weekend version brand releas yesterdai past version updat broken auto discoveri mechan releas note cli discoveri enal overrid plugin updat panda iri starter match version compliant setup panda iri starter add od test plugin glad plugin viz cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin viz viz global properli hook properli load add yml manual conf local folder folder conf short term ven conveni allow kei document http readthedoc latest sourc experiment track configur html yml file irectli copi past http github com galileo galilei blob master templat yml test pass test relat cli plugin local starter quickli read messag detect consist independ panda iri starter viz workaround directli mondai morn nice plugin hook load function depend yml present init copi file templat conf local instal version consol pip uninstal pip instal git http github com galileo galilei git cli bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur group global init level unknown reason account group version report core team affect plugin viz docker airflow global quickest hacki remov global group on branch cli repo plugin hook load function depend yml present init copi file templat conf local exactli init render templat copi past replac jinja tag dynam valu folder conf folder default local specifi environ init env hook logic yml file pass paramet wrapper paramet track uri port host defin yml file quick test patch charm dmb deploi patch pypi pip instal close free reopen version",
        "Solution_preprocessed_content":"wil weekend version brand past version updat broken mechan updat starter match version compliant setup starter add od test plugin glad plugin cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin global properli hook properli load add manual folder short term ven conveni allow kei irectli test pass test relat cli plugin local quickli read messag detect consist independ starter workaround directli mondai morn nice plugin hook load function depend present copi file templat instal version bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur account group version report core team affect plugin global quickest remov global group on branch repo plugin hook load function depend yml present init copi file templat exactli render templat folder folder hook logic yml file pass paramet wrapper paramet defin file quick test patch charm deploi patch pypi close free reopen version",
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":572.4141666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956142000,
        "Challenge_created_time":1661895451000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19,
        "Challenge_repo_fork_count":94,
        "Challenge_repo_issue_count":1834,
        "Challenge_repo_star_count":281,
        "Challenge_repo_watch_count":36,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":572.4141666667,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":338.3591666667,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Challenge_closed_time":1647350309000,
        "Challenge_created_time":1646132216000,
        "Challenge_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":14,
        "Challenge_repo_fork_count":6,
        "Challenge_repo_issue_count":79,
        "Challenge_repo_star_count":6,
        "Challenge_repo_watch_count":6,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":338.3591666667,
        "Challenge_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":47,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1772151899,
        "Challenge_watch_issue_ratio":0.0759493671
    },
    {
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":9,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_created_time":1647978773000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15,
        "Challenge_repo_fork_count":88,
        "Challenge_repo_issue_count":182,
        "Challenge_repo_star_count":300,
        "Challenge_repo_watch_count":15,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_gpt_summary":"recreat account option delet open account report open databas file save file instal librari pip creat file start kernel",
        "Solution_link_count":5.0,
        "Solution_original_content":"dev sorri wanna start gpu runtim kindli lorazabora launch gpu instanc runtim environ imag http imag githubusercont com faf png experienc week cpu runtim arent workload sens there free cloud cpu option hope soon someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv messag someshfengd delet account imag http imag githubusercont com abba ceb png dear someshfengd delet account close icoxfog delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_preprocessed_content":"dev sorri wanna start gpu runtim kindli launch gpu instanc runtim environ experienc week cpu runtim arent workload sens there free cloud cpu option hope soon report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv delet account dear delet account close delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":11.7994444444,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Challenge_closed_time":1573208758000,
        "Challenge_created_time":1573166280000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":24.8,
        "Challenge_reading_time":190.21,
        "Challenge_repo_contributor_count":91,
        "Challenge_repo_fork_count":651,
        "Challenge_repo_issue_count":2147,
        "Challenge_repo_star_count":3215,
        "Challenge_repo_watch_count":70,
        "Challenge_score_count":0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":11.7994444444,
        "Challenge_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":785,
        "Platform":"Github",
        "Solution_body":"After installing GluonTS, could you try running in a cell \r\n\r\n```\r\n!pip show mxnet\r\n```\r\n\r\nand report the result? `!pip show mxnet` results in:\r\n\r\n```python\r\nName: mxnet\r\nVersion: 1.4.1\r\nSummary: MXNet is an ultra-scalable deep learning framework. This version uses openblas.\r\nHome-page: https:\/\/github.com\/apache\/incubator-mxnet\r\nAuthor: UNKNOWN\r\nAuthor-email: UNKNOWN\r\nLicense: Apache 2.0\r\nLocation: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\r\nRequires: numpy, graphviz, requests\r\nRequired-by: gluonts\r\nYou are using pip version 10.0.1, however version 19.3.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\n`!pip install mxnet --upgrade` led to conflicts with gluonts and numpy and the same error message with `GPU is not enabled`. \r\n @tm1611 hopefully this is solved with the upcoming `0.4` release (to be relased very soon), since #428 was merged. I think the problem is that with `gluonts<0.4` the vanilla mxnet package gets installed and replaces the one with built-in cuda for GPU processing. @tm1611 can you check if the problem is gone now when you install GluonTS from scratch on your instance? Yes, the problem with mxnet and dependencies was removed. Thanks a lot for the quick fix. \r\n\r\nUnrelated to this issue, but related to `m4_benchmark.py` and the sake of completeness: Using gluonts-version 0.4., one has to change `num_eval_samples` to `num_sampels` (see #421 ). ",
        "Solution_gpt_summary":"upgrad gluont version mxnet depend complet benchmark gluont version num eval sampl num sampl",
        "Solution_link_count":1.0,
        "Solution_original_content":"instal gluont run cell pip mxnet report pip mxnet mxnet version summari mxnet ultra scalabl framework version openbla home page http github com apach incub mxnet author unknown author email unknown licens apach locat home anaconda env mxnet lib site packag numpi graphviz request gluont pip version version upgrad pip instal upgrad pip pip instal mxnet upgrad led conflict gluont numpi messag gpu enabl hopefulli releas relas soon merg gluont vanilla mxnet packag instal replac built cuda gpu process gone instal gluont scratch instanc mxnet depend remov quick unrel relat benchmark sake complet gluont version num eval sampl num sampel",
        "Solution_preprocessed_content":"instal gluont run cell report led conflict gluont numpi messag hopefulli releas merg vanilla mxnet packag instal replac cuda gpu process gone instal gluont scratch instanc mxnet depend remov quick unrel relat sake complet",
        "Solution_readability":5.7,
        "Solution_reading_time":17.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":202.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0423847229,
        "Challenge_watch_issue_ratio":0.032603633
    },
    {
        "Challenge_adjusted_solved_time":9822.3969444444,
        "Challenge_answer_count":8,
        "Challenge_body":"### What steps did you take: Removed the HPO and Training Jobs only creating the model and batch transforming in SageMaker \r\n[Automatically taking the HPO and Training on SageMaker facing some issue while kfp compile]\r\n\r\n### What happened: Getting output properly in Kubefow. But I want to to see custom  model (ANY) output without HPO and Model training in Sagemaker\r\n\r\n### What did you expect to happen: Without HPO and batch job in SageMaker\r\n\r\n\r\n\r\n\r\n### Anything else you would like to add:\r\nAny open source loan data model using KF would be appriciated \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Challenge_closed_time":1632453568000,
        "Challenge_created_time":1597092939000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":326,
        "Challenge_repo_fork_count":1350,
        "Challenge_repo_issue_count":8555,
        "Challenge_repo_star_count":3062,
        "Challenge_repo_watch_count":103,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9822.3969444444,
        "Challenge_title":"Want to create ANY model and do batch transform in without Training in Amazon SageMaker ",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"\/assign @Jeffwan @PatrickXYS \r\nlooks like aws specific SageMaker KFP should works fine as regular KFP installed on EKS.\r\n\r\n@RedbackThomson @akartsky Any idea you can share here?  Hi @swarnaditya \r\nThank you for using SageMaker Components for Kubeflow Pipelines. Could you provide more details about the issue:\r\n- are you bringing your own model? Is it uploaded to S3?\r\n- are you bringing your own container or using one of the [pre-built SageMaker algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html)?\r\n- sample reproducible code\r\n\r\nHere is a sample pipeline with only model and batch transform job - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/definition\/transform_job_pipeline.py\r\n\r\nand here are the sample inputs for this pipeline - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/config\/kmeans-mnist-batch-transform\/config.yaml#L6-L23. Note: the variables enclosed in `(( ))` are replaced at runtime by our integration tests so you will not be able to run with these inputs directly but we can help you with your use-case\r\n\r\nIn case this helps, here is a customer blog who created a pipeline with model+Hosting - https:\/\/aws.amazon.com\/blogs\/machine-learning\/cisco-uses-amazon-sagemaker-and-kubeflow-to-create-a-hybrid-machine-learning-workflow\/ This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/assign @surajkota  @RedbackThomson  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/close\r\n\r\nsince there is no updates on the issue @surajkota: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352#issuecomment-926312361):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_gpt_summary":"model upload pre built algorithm sampl pipelin model batch transform job share sampl input pipelin blog share pipelin model host",
        "Solution_link_count":7.0,
        "Solution_original_content":"assign jeffwan patrickxi kfp regular kfp instal ek redbackthomson akartski idea share swarnaditya compon kubeflow pipelin model upload pre built algorithm http doc com latest algo html sampl reproduc sampl pipelin model batch transform job http github com kubeflow pipelin blob master compon test integr test resourc definit transform job pipelin sampl input pipelin http github com kubeflow pipelin blob master compon test integr test resourc config kmean mnist batch transform config yaml note variabl enclos replac runtim integr test run input directli blog creat pipelin model host http com blog cisco kubeflow creat hybrid workflow automat mark stale activ close activ contribut assign surajkota redbackthomson automat mark stale activ close activ contribut close updat surajkota close respons http github com kubeflow pipelin issuecom close instruct interact comment http git commun contributor guid pull request relat file kubernet test infra http github com kubernet test infra titl prow repositori",
        "Solution_preprocessed_content":"assign kfp regular kfp instal ek idea share compon kubeflow pipelin model upload sampl reproduc sampl pipelin model batch transform job sampl input pipelin note variabl enclos replac runtim integr test run input directli blog creat pipelin model host automat mark stale activ close activ contribut assign automat mark stale activ close activ contribut close updat close respons close instruct interact comment relat file repositori",
        "Solution_readability":14.4,
        "Solution_reading_time":30.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":255.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":560.8433333333,
        "Challenge_answer_count":3,
        "Challenge_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Challenge_closed_time":1669124369000,
        "Challenge_created_time":1667105333000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":37.27,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":560.8433333333,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"In addition, runs of `compare_models `and `create_model` get nested recursively as well in pycaret > 2.3.6.\r\nSee screenshot below where the red line shows the behaviour in pycaret==2.3.6 (which is the wanted and expected behaviour) and in orange the nested unwanted behaviour in pycaret 2.3.8 , 2.3.9 and 2.3.10\r\n![image](https:\/\/user-images.githubusercontent.com\/50994394\/200543740-0883c8ba-9f4a-4d1f-8abc-560ae7dbd54e.png)\r\n @nagamatz @tdekelver-bd This issue was recently fixed on last rc release. Can you try installing pycaret with `pip install --pre pycaret` and let me know if you still face the issue. It's not yet fixed with 3.0.0rc4. This is the results with the code in the first post. Now, mlflow is 2.0.1\r\n![\u7121\u984c](https:\/\/user-images.githubusercontent.com\/1991802\/206426156-4b19a4cc-b865-4af3-8281-1b89fc099f28.png)\r\n",
        "Solution_gpt_summary":"releas pycaret instal pycaret pip instal pre pycaret",
        "Solution_link_count":2.0,
        "Solution_original_content":"addit run compar model creat model nest recurs pycaret screenshot red line pycaret orang nest unwant pycaret imag http imag githubusercont com cba abc aedbd png nagamatz tdekelv releas instal pycaret pip instal pre pycaret http imag githubusercont com bacc bfcf png",
        "Solution_preprocessed_content":"addit run nest recurs pycaret screenshot red line orang nest unwant pycaret releas instal pycaret",
        "Solution_readability":7.7,
        "Solution_reading_time":10.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_gpt_summary":"modifi line packag remov faulti line initi catalog directli copi dataset catalog loader involv store model modul attribut properti attribut load modul fly remov deepcopi pipelinemodel intend separ process infer time note dataset deepcopi radic frequent",
        "Solution_link_count":0.0,
        "Solution_original_content":"remov faulti line directli initi catalog model loadabl option longer deepcopi initi catalog copi dataset catalog loader kera model clone model kera model dataset pipelinemodel intent separ process infer time remov deepcopi conflict function catalog come abstractmodeldataset model modul attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi underli valu dataset load safe radic on",
        "Solution_preprocessed_content":"remov faulti line directli model loadabl option longer deepcopi copi dataset catalog loader intent separ process remov deepcopi come abstractmodeldataset attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi safe radic on",
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":40.4833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859588000,
        "Challenge_created_time":1666713848000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":18,
        "Challenge_repo_fork_count":22,
        "Challenge_repo_issue_count":807,
        "Challenge_repo_star_count":97,
        "Challenge_repo_watch_count":33,
        "Challenge_score_count":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":40.4833333333,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":127,
        "Platform":"Github",
        "Solution_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Solution_gpt_summary":"token session token extra config field access kei secret session token authent async oper",
        "Solution_link_count":0.0,
        "Solution_original_content":"session token secret access kei connect field extra config",
        "Solution_preprocessed_content":"session token secret access kei connect field extra config",
        "Solution_readability":13.0,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":2,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_created_time":1589926894000,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13,
        "Challenge_repo_fork_count":10,
        "Challenge_repo_issue_count":614,
        "Challenge_repo_star_count":37,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pl reproduc latest reproduc",
        "Solution_preprocessed_content":"pl reproduc latest reproduc",
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0211726384,
        "Challenge_watch_issue_ratio":0.013029316
    },
    {
        "Challenge_adjusted_solved_time":2106.4869444444,
        "Challenge_answer_count":0,
        "Challenge_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Challenge_closed_time":1602948810000,
        "Challenge_created_time":1595365457000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":374,
        "Challenge_repo_star_count":126,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2106.4869444444,
        "Challenge_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":31,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_created_time":1651909943000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24,
        "Challenge_repo_fork_count":341,
        "Challenge_repo_issue_count":412,
        "Challenge_repo_star_count":2043,
        "Challenge_repo_watch_count":19,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1666.9041666667,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":37,
        "Platform":"Github",
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_gpt_summary":"add finish test close properli",
        "Solution_link_count":0.0,
        "Solution_original_content":"finish test close properli",
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":219.2780555556,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686697000,
        "Challenge_created_time":1658897296000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":37,
        "Challenge_repo_fork_count":101,
        "Challenge_repo_issue_count":1083,
        "Challenge_repo_star_count":153,
        "Challenge_repo_watch_count":24,
        "Challenge_score_count":2,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":219.2780555556,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":223,
        "Platform":"Github",
        "Solution_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Solution_gpt_summary":"line submit pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"context start approx hour ago guess mayb iam api yesterdai tdmalon kcadett arn st role dev syd timswb launchconstraint servicecatalog author perform addtag resourc arn southeast notebook instanc basicnotebookinst lqerepcrnmaw ident base polici allow addtag action servic statu accessdeniedexcept request eecbb proxi null line submit http github com awslab servic workbench pull",
        "Solution_preprocessed_content":"context start approx hour ago mayb iam api yesterdai submit",
        "Solution_readability":13.8,
        "Solution_reading_time":11.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":4.5216666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":1659051271000,
        "Challenge_created_time":1659034993000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.8,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":425,
        "Challenge_repo_fork_count":4048,
        "Challenge_repo_issue_count":30819,
        "Challenge_repo_star_count":23050,
        "Challenge_repo_watch_count":435,
        "Challenge_score_count":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5216666667,
        "Challenge_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":61,
        "Platform":"Github",
        "Solution_body":"Duplicate of https:\/\/github.com\/ray-project\/ray\/issues\/26567",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"duplic http github com rai rai",
        "Solution_preprocessed_content":null,
        "Solution_readability":29.2,
        "Solution_reading_time":0.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":18.5819444444,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Challenge_closed_time":1657616964000,
        "Challenge_created_time":1657550069000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.1,
        "Challenge_reading_time":23.2,
        "Challenge_repo_contributor_count":5,
        "Challenge_repo_fork_count":8,
        "Challenge_repo_issue_count":39,
        "Challenge_repo_star_count":9,
        "Challenge_repo_watch_count":1,
        "Challenge_score_count":0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":18.5819444444,
        "Challenge_title":"[mlflow] Extra args broken",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":213,
        "Platform":"Github",
        "Solution_body":"Hi @subramaniam20jan \r\n\r\nCould you please share your values.yaml file with me? Do you have any additional change in it? Hi @subramaniam20jan \r\n\r\nI really didn't understand the problem. Static prefix is valid argument for mlflow server. You can find more information [here](https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-static-prefix).\r\n\r\nAlso, it tested with argument and without argument in [the unit tests](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/charts\/mlflow\/unittests\/deployment_test.yaml#L65). Also, it tested without argument in the integration test which we run it with [kind here](https:\/\/github.com\/community-charts\/helm-charts\/runs\/7283774204?check_suite_focus=true#step:12:175).\r\n\r\nI'm really not able to recreate the issue. Could you please share the error message? Well, if you use `mlflow ui` command, you must change it to `mlflow server` command for production usage. You can find same explanation from here: https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui Actually it was my bad. This wasnt really an issue but I appreciate the addition of the extra parameters to the readiness and liveness probe :) btw, @burakince great job on the chart and image! I was something I have made many times in individual assignments and missed having in open source somewhere. Came across your project when I was about to create one myself. Saves me a lot of work :) Thanks @subramaniam20jan :) I just added an example usage to [here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/liveness-probe-and-readiness-probe-example).\r\n\r\nBest",
        "Solution_gpt_summary":"staticprefix separ argument extraenv start server smoother end static prefix argument server link unit test integr test argument later misunderstand",
        "Solution_link_count":5.0,
        "Solution_original_content":"subramaniamjan share valu yaml file addit subramaniamjan static prefix argument server http org doc latest cli html cmdoption server static prefix test argument argument unit test http github com commun chart helm chart blob chart unittest deploy test yaml test argument integr test run http github com commun chart helm chart run suit focu step recreat share messag server usag explan http org doc latest cli html wasnt addit extra paramet readi live probe btw burakinc great job chart imag time individu assign miss open sourc came creat save subramaniamjan usag http github com commun chart tree live probe readi probe",
        "Solution_preprocessed_content":"share file addit static prefix argument server test argument argument test argument integr test run recreat share messag usag explan wasnt addit extra paramet readi live probe btw great job chart imag time individu assign miss open sourc came creat save usag",
        "Solution_readability":11.2,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":191.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":2,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_gpt_summary":"setup panda iri starter add yml manual conf local folder instal version remov global group on bias summari",
        "Solution_link_count":3.0,
        "Solution_original_content":"wil weekend version brand releas yesterdai past version updat broken auto discoveri mechan releas note cli discoveri enal overrid plugin updat panda iri starter match version compliant setup panda iri starter add od test plugin glad plugin viz cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin viz viz global properli hook properli load add yml manual conf local folder folder conf short term ven conveni allow kei document http readthedoc latest sourc experiment track configur html yml file irectli copi past http github com galileo galilei blob master templat yml test pass test relat cli plugin local starter quickli read messag detect consist independ panda iri starter viz workaround directli mondai morn nice plugin hook load function depend yml present init copi file templat conf local instal version consol pip uninstal pip instal git http github com galileo galilei git cli bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur group global init level unknown reason account group version report core team affect plugin viz docker airflow global quickest hacki remov global group on branch cli repo plugin hook load function depend yml present init copi file templat conf local exactli init render templat copi past replac jinja tag dynam valu folder conf folder default local specifi environ init env hook logic yml file pass paramet wrapper paramet track uri port host defin yml file quick test patch charm dmb deploi patch pypi pip instal close free reopen version",
        "Solution_preprocessed_content":"wil weekend version brand past version updat broken mechan updat starter match version compliant setup starter add od test plugin glad plugin cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin global properli hook properli load add manual folder short term ven conveni allow kei irectli test pass test relat cli plugin local quickli read messag detect consist independ starter workaround directli mondai morn nice plugin hook load function depend present copi file templat instal version bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur account group version report core team affect plugin global quickest remov global group on branch repo plugin hook load function depend yml present init copi file templat exactli render templat folder folder hook logic yml file pass paramet wrapper paramet defin file quick test patch charm deploi patch pypi close free reopen version",
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1017.9547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053107000,
        "Challenge_created_time":1619388470000,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8,
        "Challenge_repo_fork_count":428,
        "Challenge_repo_issue_count":16,
        "Challenge_repo_star_count":350,
        "Challenge_repo_watch_count":18,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1017.9547222222,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":136,
        "Platform":"Github",
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_gpt_summary":"specifi version downgrad version",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.5,
        "Challenge_watch_issue_ratio":1.125
    },
    {
        "Challenge_adjusted_solved_time":367.1658333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Challenge_closed_time":1657912525000,
        "Challenge_created_time":1656590728000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":17.7,
        "Challenge_reading_time":99.61,
        "Challenge_repo_contributor_count":24,
        "Challenge_repo_fork_count":341,
        "Challenge_repo_issue_count":412,
        "Challenge_repo_star_count":2043,
        "Challenge_repo_watch_count":19,
        "Challenge_score_count":1,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":367.1658333333,
        "Challenge_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":523,
        "Platform":"Github",
        "Solution_body":"Hi @GillianGrayson \r\n\r\nFor **1. hydra-optuna-sweeper problem**, it has been modified in release_1.4. You can find [here](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/7e67c4692590550e7b703655845e59508eb071bb\/configs\/hparams_search\/mnist_optuna.yaml#L49)\r\n\r\n @GillianGrayson ty for reporting, the problems have been fixed on the current `main` branch.",
        "Solution_gpt_summary":"hydra sweeper releas link logger run train process",
        "Solution_link_count":1.0,
        "Solution_original_content":"gilliangrayson hydra sweeper modifi releas http github com ashlev lightn hydra templat blob ecebeebbb config hparam search mnist yaml gilliangrayson report branch",
        "Solution_preprocessed_content":null,
        "Solution_readability":12.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":0.6055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Challenge_closed_time":1605720463000,
        "Challenge_created_time":1605718283000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.6055555556,
        "Challenge_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":218,
        "Platform":"Github",
        "Solution_body":"The issue is not confirmed and was due to adding a Pipeline and a PipelineML object.\r\nI close it.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pipelin pipelineml object close",
        "Solution_preprocessed_content":"pipelin pipelineml object close",
        "Solution_readability":2.3,
        "Solution_reading_time":1.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_created_time":1610404594000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9,
        "Challenge_repo_fork_count":18,
        "Challenge_repo_issue_count":385,
        "Challenge_repo_star_count":132,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_gpt_summary":"wait migrat mileston remov boilerpl plugin consist futur",
        "Solution_link_count":0.0,
        "Solution_original_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_preprocessed_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_created_time":1585108798000,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91,
        "Challenge_repo_fork_count":254,
        "Challenge_repo_issue_count":266,
        "Challenge_repo_star_count":224,
        "Challenge_repo_watch_count":35,
        "Challenge_score_count":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":146,
        "Platform":"Github",
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_gpt_summary":"link servic limit increas page prerequisit section notebook instanc type endpoint default servic limit notebook valu child weight resourcelimitexceed account level servic limit xlarg endpoint usag instanc util instanc request delta instanc default instanc train job account increas tour notebook updat contact limit increas account run notebook",
        "Solution_link_count":1.0,
        "Solution_original_content":"cell call child weight tri child weight default instanc train job account increas tour add prerequsit section notebook valu child weight resourcelimitexceed resourcelimitexceed call createtrainingjob oper account level servic limit instanc train job instanc util instanc request delta instanc contact request increas limit doc notebook updat http doc com gener latest html state default limit xlarg typic account run notebook administr contact limit increas account run notebook",
        "Solution_preprocessed_content":"cell call child weight tri default instanc train job account increas tour add prerequsit section notebook valu doc notebook updat state default limit typic account run notebook administr contact limit increas account run notebook",
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3421052632,
        "Challenge_watch_issue_ratio":0.1315789474
    },
    {
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_created_time":1603479422000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425,
        "Challenge_repo_fork_count":4048,
        "Challenge_repo_issue_count":30819,
        "Challenge_repo_star_count":23050,
        "Challenge_repo_watch_count":435,
        "Challenge_score_count":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":102,
        "Platform":"Github",
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"consequ search metric multi metric krfrick ping folk api kei pick releas",
        "Solution_preprocessed_content":"consequ search metric ping folk api pick releas",
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_created_time":1655132901000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439,
        "Challenge_repo_fork_count":17217,
        "Challenge_repo_issue_count":20687,
        "Challenge_repo_star_count":76119,
        "Challenge_repo_watch_count":860,
        "Challenge_score_count":0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":1943.8975,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":298,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_gpt_summary":"messag instal report run pip instal instal persist instal set api kei mode environ variabl persist open team",
        "Solution_link_count":5.0,
        "Solution_original_content":"sgugger messag instal report runtimeerror callback instal run pip instal exactli run pip instal instal pip edit edit titl clear mon jul sylvain gugger wrote messag instal report runtimeerror callback instal run pip instal exactli run pip instal repli email directli github unsubscrib receiv author thread messag properli initi api kei automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor sgugger http huggingfac doc transform class callback doc api kei callback tri set api kei mode insid function run spawn luck downgrad till environ api kei environ mode onlin environ mayb open write integr maintain written team automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"messag instal exactli run instal edit edit titl clear mon jul sylvain gugger wrote messag instal runtimeerror callback instal run exactli run pip instal repli email directli github unsubscrib receiv author properli initi api kei automat mark stale activ address comment thread note ignor doc api kei callback tri set insid function run spawn luck downgrad till mayb open write integr maintain written team automat mark stale activ address comment thread note ignor",
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0212210567,
        "Challenge_watch_issue_ratio":0.0415720017
    },
    {
        "Challenge_adjusted_solved_time":35.6138888889,
        "Challenge_answer_count":5,
        "Challenge_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Challenge_closed_time":1650470329000,
        "Challenge_created_time":1650342119000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":35.6138888889,
        "Challenge_title":"plots are not saving through MLFLOW",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":18,
        "Platform":"Github",
        "Solution_body":"@akashg116414 We can't help you with this. Can you please describe a little bit more, show the code, show the complete error, etc. this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\nexample:\r\n\r\n```python\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('juice')\r\nfrom pycaret.classification import *\r\nclf1 = setup(data, target = 'Purchase', session_id=123, log_experiment=True, experiment_name='juice1',log_plots=True)\r\n``` > this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\n\r\nAre you installing both of them in the same environment? That should not be done as there may be conflicts at this time (until we officially release the 3.0.0 pycaret version which will have time-series integrated with the main pycaret package).\r\n\r\nTry installing one of them in a fresh (clean) environment and see if it works. @akashg116414 By the way, I am not able to recreate the issue with the information (code) you have provided. It works fine for me (see attached notebook below).\r\n\r\nhttps:\/\/gist.github.com\/ngupta23\/f7f33a5361928cac2f36f855f7398c88\r\n\r\nPlease provide a completely reproducible example that shows the steps to get to the error you are getting. Since we are unable to recreate this and information seems to be missing, we will close this for now. Feel free to create a new issue. We add a new Bug template that will guide you through the process of submitting a reproducible example. \r\n\r\nThanks!",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"akashg littl bit complet instal pycaret pycaret alpha tri classif pycaret dataset import data data data juic pycaret classif import clf setup data target purchas session log juic log plot instal pycaret pycaret alpha tri classif instal environ conflict time offici releas pycaret version time seri integr pycaret packag instal fresh clean environ akashg recreat attach notebook http gist github com ngupta ffacacfffc complet reproduc step recreat miss close free creat add templat guid process submit reproduc",
        "Solution_preprocessed_content":"littl bit complet instal tri classif instal tri classif instal environ conflict time instal fresh environ recreat complet reproduc step recreat miss close free creat add templat guid process submit reproduc",
        "Solution_readability":8.6,
        "Solution_reading_time":19.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":219.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_created_time":1663557360000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":93,
        "Challenge_repo_fork_count":1518,
        "Challenge_repo_issue_count":2643,
        "Challenge_repo_star_count":6633,
        "Challenge_repo_watch_count":124,
        "Challenge_score_count":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":145,
        "Platform":"Github",
        "Solution_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Solution_gpt_summary":"instal latest version pycaret pip instal pre pycaret persist reopen",
        "Solution_link_count":0.0,
        "Solution_original_content":"reproduc master pip instal pre pycaret nagamatz reopen persist",
        "Solution_preprocessed_content":"reproduc master reopen persist",
        "Solution_readability":6.2,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":4565.0625,
        "Challenge_answer_count":3,
        "Challenge_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Challenge_closed_time":1657980432000,
        "Challenge_created_time":1641546207000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":6,
        "Challenge_repo_fork_count":3,
        "Challenge_repo_issue_count":147,
        "Challenge_repo_star_count":19,
        "Challenge_repo_watch_count":8,
        "Challenge_score_count":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4565.0625,
        "Challenge_title":"Execute translate script without creating ClearML task",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"I think that this error might be preventing me from using the Translate script locally.\r\n\r\nWhen I try I get the following error:\r\nInstalling the current project: silnlp (1.0.0)\r\n(silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate BT-English\/cba-en\/cba-en_cp01 --src-project cba --trg-iso en --books EXO --output-usfm BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT --checkpoint best\r\n2022-06-28 21:02:08,808 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-06-28 21:02:09,149 - silnlp.common.utils - INFO - Git commit: f46a63c3b3\r\nRetrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4556fa0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\nRetrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569190>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n^CRetrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569340>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n\r\nprint(args):\r\nNamespace(books=['EXO'], checkpoint='best', clearml_queue=None, eager_execution=False, end_seq=None, experiment='BT-English\/cba-en\/cba-en_cp01', memory_growth=False, output_usfm='BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT', src=None, src_prefix=None, src_project='cbaNT', start_seq=None, trg=None, trg_iso='en', trg_prefix=None)\r\n Tested this for translating and it worked fine.   (silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate --checkpoint last --src-project tl-TCB --src \/home\/david\/disk2\/gutenberg\/Paratext\/projects\/TCB\/091SAtlASD15.SFM --trg-iso blx --output-usfm \/home\/david\/disk2\/gutenberg\/BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\/results\/091SAAMIU_last.sfm BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\r\n2022-07-16 15:03:40,452 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-07-16 15:03:40,828 - silnlp.common.utils - INFO - Git commit: 8cd5b9c649\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 231, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 225, in main\r\n    translator.translate_text_file(args.src, args.trg_iso, args.trg)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 151, in translate_text_file\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{os.path.basename(src_file_path)}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 79, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 9, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml_connection.py\", line 24, in __post_init__\r\n    self.name = self.name.replace(\"\\\\\", \"\/\")\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"prevent translat local instal silnlp silnlp vmne david pop silnlp silnlp nmt translat english cba cba src cba trg iso book exo output usfm english cba cba exocb checkpoint silnlp common environ workspac home david disk gutenberg environ variabl sil nlp data path silnlp common util git commit facb retri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login retri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login cretri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login print arg namespac book exo checkpoint queue eager execut end seq english cba cba memori growth output usfm english cba cba exocb src src prefix src cbant start seq trg trg iso trg prefix test translat silnlp vmne david pop silnlp silnlp nmt translat checkpoint src tcb src home david disk gutenberg paratext tcb satlasd sfm trg iso blx output usfm home david disk gutenberg tagalog blx blx uni dup share preserv saamiu sfm tagalog blx blx uni dup share preserv silnlp common environ workspac home david disk gutenberg environ variabl sil nlp data path silnlp common util git commit cdbc traceback file usr lib runpi line run modul return run global file usr lib runpi line run exec run global file home david silnlp silnlp nmt translat line file home david silnlp silnlp nmt translat line translat translat text file arg src arg trg iso arg trg file home david silnlp silnlp nmt translat line translat text file init translat task suffix checkpoint path basenam src file path file home david silnlp silnlp nmt translat line init translat task sil file line init file home david silnlp silnlp common connect line init replac attributeerror nonetyp object attribut replac",
        "Solution_preprocessed_content":"prevent translat local instal silnlp cba exo workspac environ variabl git commit retri connect broken object establish connect address associ hostnam retri connect broken object establish connect address associ hostnam cretri connect broken object establish connect address associ hostnam print namespac test translat blx workspac environ variabl git commit traceback file line return file line exec file line file line file line file line sil attributeerror nonetyp object attribut replac",
        "Solution_readability":15.1,
        "Solution_reading_time":46.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":282.0,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_created_time":1585713762000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_open_time":null,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164,
        "Challenge_repo_fork_count":740,
        "Challenge_repo_issue_count":1942,
        "Challenge_repo_star_count":7884,
        "Challenge_repo_watch_count":102,
        "Challenge_score_count":0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_gpt_summary":"activ termin instal packag instal ipykernel termin environ creat conda environ ipykernel instal run alpha version plugin",
        "Solution_link_count":2.0,
        "Solution_original_content":"asid coupl environ match termin notebook familiar panda csvdataset start packag instal notebook team environ creat termin activ termin instal note file browser left hand notebook mount bash sourc activ conda activ unrel window activ test notebook instal instal ipykernel termin env conda environ notebook dropdown select ipykernel instal doc http ipython readthedoc stabl instal kernel instal html conda creat test pip instal ipykernel typic instal ipykernel doc ipykernel instal test notebook instal note shut notebook loos packag environ default notic panda idea idea waylonwalk slightli adapt straight conda creat env ipykernel sourc activ env ipykernel instal env displai test pipelin test run reason close massiv waylonwalk direct cheer tom tjcuddihi team produc knowledg document keen plai nice access context yetudada alpha version plugin plai nice allow run process job uwaisiqb knowledg articl publish http com blog opensourc pipelin train model",
        "Solution_preprocessed_content":"asid coupl environ match termin notebook familiar start packag instal notebook team environ creat termin activ termin instal note file browser left hand notebook mount instal ipykernel termin env conda environ notebook dropdown select instal note shut notebook loos packag environ default notic panda idea idea slightli adapt straight reason close massiv direct cheer tom team produc knowledg document keen plai nice access context alpha version plugin plai nice allow run process job knowledg articl publish",
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":397.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0844490216,
        "Challenge_watch_issue_ratio":0.052523172
    },
    {
        "Challenge_adjusted_solved_time":622.6719444444,
        "Challenge_answer_count":6,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248052000,
        "Challenge_created_time":1630006433000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":55,
        "Challenge_repo_fork_count":2296,
        "Challenge_repo_issue_count":1858,
        "Challenge_repo_star_count":3528,
        "Challenge_repo_watch_count":2030,
        "Challenge_score_count":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":622.6719444444,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":204,
        "Platform":"Github",
        "Solution_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Solution_gpt_summary":"updat comput enabl ident grant ident access data storag creat comput instanc ident configur datastor creat workspac creat cluster ident base access featur sdk",
        "Solution_link_count":1.0,
        "Solution_original_content":"driver log txt http github com machinelearningnotebook file driver log txt gener comput sdk updat creat aml workspac disappear subscript doesnt log perfectli sdk updat sort ident sdk driver log messag comput ident provis updat comput enabl ident grant ident access data storag impress comput cluster comput instanc doc configur datastor rudizh featur comput instanc ident creat automat updat chunyli cool end creat workspac move file recreat cluster guess that start close",
        "Solution_preprocessed_content":"gener comput sdk updat creat aml workspac disappear subscript doesnt log perfectli sdk updat sort ident sdk driver log messag comput ident updat comput enabl ident grant ident access data storag impress comput cluster comput instanc doc datastor featur comput instanc ident automat updat cool end creat workspac move file recreat cluster guess that start close",
        "Solution_readability":8.9,
        "Solution_reading_time":15.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_created_time":1635429600000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16,
        "Challenge_repo_fork_count":0,
        "Challenge_repo_issue_count":909,
        "Challenge_repo_star_count":5,
        "Challenge_repo_watch_count":7,
        "Challenge_score_count":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":6.9419444444,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Challenge_closed_time":1602141183000,
        "Challenge_created_time":1602116192000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.0,
        "Challenge_reading_time":59.98,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":2,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":6.9419444444,
        "Challenge_title":"mlflow logger complains about missing run_id",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":304,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Here, mlflow logger is actually an MlflowClient object. so you ll need to use the function calls specified in this doc - https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/tracking\/client.html . These functions needs run_id as first argument which can be accessed as self.logger.run_id @nazim1021 thx for clarification! @qianyu-berkeley feel free to reopen if needed... Thanks! Same problem here, working with @nazim1021 suggestion. What about adding it to the doc? > What about adding it to the doc?\r\n\r\ngood idea, mind send a PR? :]",
        "Solution_gpt_summary":"function call specifi client object run argument access logger run add document encourag send pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great logger client object function call specifi doc http org doc latest modul track client html function run argument access logger run nazim thx clarif qianyu berkelei free reopen nazim doc doc idea send",
        "Solution_preprocessed_content":"contribut great logger client object function call specifi doc function argument access thx clarif free reopen doc doc idea send",
        "Solution_readability":5.0,
        "Solution_reading_time":7.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":2,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_created_time":1649286157000,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":3,
        "Challenge_repo_fork_count":6,
        "Challenge_repo_issue_count":41,
        "Challenge_repo_star_count":24,
        "Challenge_repo_watch_count":3,
        "Challenge_score_count":0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":326,
        "Platform":"Github",
        "Solution_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"soon crazili busi moment happend callback pipelin yaml callback arg usernam lannguyen theseu classif resum didnt specifi repo havent fulli document",
        "Solution_preprocessed_content":"soon crazili busi moment happend callback repo havent document",
        "Solution_readability":9.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0731707317,
        "Challenge_watch_issue_ratio":0.0731707317
    },
    {
        "Challenge_adjusted_solved_time":9.3011111111,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848054000,
        "Challenge_created_time":1597814570000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449,
        "Challenge_repo_fork_count":2665,
        "Challenge_repo_issue_count":13532,
        "Challenge_repo_star_count":20903,
        "Challenge_repo_watch_count":227,
        "Challenge_score_count":0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":9.3011111111,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":299,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great submit reproduc instruct run sampl logger logger test track uri http trainer trainer logger logger trainer logger track uri run bash verifi latest version quick respons awaelchli morn log metric paramet close reopen",
        "Solution_preprocessed_content":"contribut great submit reproduc instruct run sampl track uri run verifi latest version quick respons morn log close reopen",
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    }
]