[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1587712154000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an internal server error while deploying a container to AKS using Azure ML CLI. The error occurs sporadically and there is no clear pattern to it. The error message suggests creating a retry loop, but this would not address the underlying issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":560.8433333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Challenge_closed_time":1669124369000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1667105333000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with pycaret and mlflow integration where they are unable to create probabilities in addition to predicted values for binary response models using the scikit learn function \"predict_model\". The issue occurs when they call the calibrated algorithm in a separate notebook for scoring new data. The expected behavior is to see the probabilities model.predict_prob(X), but the code errors out.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":37.27,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":560.8433333333,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":296,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In addition, runs of `compare_models `and `create_model` get nested recursively as well in pycaret > 2.3.6.\r\nSee screenshot below where the red line shows the behaviour in pycaret==2.3.6 (which is the wanted and expected behaviour) and in orange the nested unwanted behaviour in pycaret 2.3.8 , 2.3.9 and 2.3.10\r\n![image](https:\/\/user-images.githubusercontent.com\/50994394\/200543740-0883c8ba-9f4a-4d1f-8abc-560ae7dbd54e.png)\r\n @nagamatz @tdekelver-bd This issue was recently fixed on last rc release. Can you try installing pycaret with `pip install --pre pycaret` and let me know if you still face the issue. It's not yet fixed with 3.0.0rc4. This is the results with the code in the first post. Now, mlflow is 2.0.1\r\n![\u7121\u984c](https:\/\/user-images.githubusercontent.com\/1991802\/206426156-4b19a4cc-b865-4af3-8281-1b89fc099f28.png)\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.7,
        "Solution_reading_time":10.56,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":112.1258333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714116000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601310463000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an inconsistency issue with the MLFlowLogger.log_metrics method within steps of a LightningModule. The documentation states that the method should accept a dictionary of metric names and values, but the user found that using log_metrics or log_metric with the dictionary resulted in errors. The user was able to get the method to work by using log_metric with separate arguments for the key and value, but this behavior is not consistent with the documentation. The user has provided a minimum code example to reproduce the bug and has listed their environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":22.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":112.1258333333,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"0332ede5",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-3.5,
        "Solution_reading_time":0.12,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":1.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":116.0143763889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to start a sweep using this yaml file.<\/p>\n<p>sweep.yaml<\/p>\n<pre><code class=\"lang-auto\">method: bayes\nmetric:\n  goal: maximize\n  name: val_f1_score\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    value: 42\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 30\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    value: adam\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  paths:\n    - \n      data:\n        value: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  -\n    use:\n      value: True\n    project:\n      value: Whats-this-rock\n\ndataset:\n  -\n    id:\n      value: [1, 2, 3, 4]\n    dir:\n      value: data\/3_consume\/\n    image:\n      size:\n        value: 124\n      channels:\n        value: 3\n    classes:\n      value: 10\n    sampling:\n      value: None\n\nmodel:\n  -\n    backbone:\n      value: efficientnetv2m\n    use_pretrained_weights:\n      value: True\n    trainable:\n      value: True\n    preprocess:\n      value: True\n    dropout_rate:\n      value: 0.3\n\ncallback:\n  -\n    monitor:\n      value: \"val_f1_score\"\n    earlystopping:\n      patience:\n        value: 10\n    reduce_lr:\n      factor:\n        values: [.9, .7, .5]\n      min_lr: 0.00001\n      patience:\n        values: [1, 2, 3, 4]\n    save_model:\n      status:\n        value: True\n      best_only:\n        value: True\n\nprogram: src\/models\/train.py\n\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">Error: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>Here\u2019s the full traceback of the error:-<\/p>\n<pre><code class=\"lang-auto\">During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 942, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2178, in upsert_sweep\n    raise e\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2175, in upsert_sweep\n    check_retry_fn=no_retry_4xx,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2153, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>I am using hydra and trying to replicate a config.yaml for wandb sweeps<\/p>\n<p>config.yaml<\/p>\n<pre><code class=\"lang-auto\">notes: \"\"\nseed: 42\nlr: 0.001\nepochs: 30\naugmentation: True\nclass_weights: True\noptimizer: adam\nloss: categorical_crossentropy\nmetrics: [\"accuracy\"]\nbatch_size: 64\nnum_classes: 7\n\npaths:\n  data: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  use: True\n  project: Whats-this-rock\n\ndataset:\n  id: [1, 2, 3, 4]\n  dir: data\/3_consume\/\n  image:\n    size: 124\n    channels: 3\n  classes: 10\n  sampling: None\n\nmodel:\n  backbone: efficientnetv2m\n  use_pretrained_weights: True\n  trainable: True\n  preprocess: True\n  dropout_rate: 0.3\n\ncallback:\n  monitor: \"val_f1_score\"\n  earlystopping:\n    patience: 10\n  reduce_lr:\n    factor: 0.4\n    min_lr: 0.00001\n    patience: 2\n  save_model:\n    status: True\n    best_only: True\n\n<\/code><\/pre>",
        "Challenge_closed_time":1663515880848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663098229093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to start a sweep using a YAML file with multi-level nesting. The error message indicates that the \"paths\" hyperparameter configuration is invalid. The user is using Hydra and trying to replicate a config.yaml for wandb sweeps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multi-level-nesting-in-yaml-for-sweeps\/3108",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":116.0143763889,
        "Challenge_title":"Multi-level nesting in yaml for sweeps",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1087.0,
        "Challenge_word_count":350,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is to use dot notation instead of nested parameters as wandb (v0.13.3) sweeps doesn\u2019t support nested parameters.<\/p>\n<pre><code class=\"lang-auto\">sweep.yaml\n\nmethod: bayes\nmetric:\n  goal: maximize\n  name: val_accuracy\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    values: [1, 42, 100]\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 100\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    values: [adam, adamax]\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.70\n      - 0.75\n      - 0.80\n  data_path:\n    value: data\/4_tfds_dataset\/\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv3\n  dataset_id:\n    values:\n      - [1]\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None, oversampling, undersampling]\n  backbone:\n    values:\n      [\n        efficientnetv2m,\n        efficientnetv2,\n        resnet,\n        mobilenetv2,\n        inceptionresnetv2,\n        xception,\n      ]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [True, False]\n  preprocess:\n    value: True\n  dropout_rate:\n    values: [0.3]\n  monitor:\n    value: \"val_accuracy\"\n  earlystopping.use:\n    value: True\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [True]\n  reduce_lr.factor:\n    values: [.9, .7, .5, .3]\n  reduce_lr.patience:\n    values: [1, 3, 5, 7, 13]\n  reduce_lr.min_lr:\n    value: 1e-5\n  save_model:\n    value: False\n\nprogram: src\/models\/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.77,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":159.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.6854797222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Challenge_closed_time":1584005785480,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583933260433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created a score.py file and want to pass an argument to it using argparse, but they are unable to find a way to pass arguments. They have shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Challenge_last_edit_time":1584005920356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":20.1458463889,
        "Challenge_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":196,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1584011988083,
        "Solution_link_count":3.0,
        "Solution_readability":35.1,
        "Solution_reading_time":25.25,
        "Solution_score_count":-2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":82.4554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.<\/p>\n<p><strong>training code (whentrain.ipynb) :<\/strong><\/p>\n<pre><code>#import libs and packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom azureml.core import Workspace, Dataset\n\n# get existing workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n\n# load the dataset which is placed in the data folder\ndataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\ndataset = dataset.to_pandas_dataframe()\n\n# Create the outputs directories to save the model and images\nos.makedirs('outputs\/model', exist_ok=True)\nos.makedirs('outputs\/output', exist_ok=True)\ndataset['Date'] = pd.to_datetime(dataset['Date'])\ndataset = dataset.set_index('Date')\n###\nscaler = MinMaxScaler()\n\n#inputs\nX = dataset.iloc[:, 1:]\n#output\ny = dataset.iloc[:, :1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n###\n\nmodel1 = RandomForestRegressor(n_estimators = 6,\n                                   max_depth = 10,\n                                   min_samples_leaf= 1,\n                                   oob_score = 'True',\n                                   random_state=42)\nmodel1.fit(X_train, y_train.values.ravel())\n\ny_pred2 = model1.predict(X_test)\n<\/code><\/pre>\n<p><strong>And here is the code on the estimator part (estimator.ipynb):<\/strong><\/p>\n<pre><code>from azureml.core import Experiment\nfrom azureml.core import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import TensorFlow\nfrom azureml.widgets import RunDetails\n\nimport os\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\nexp = Experiment(workspace=workspace, name='azure-exp')\ncluster_name = &quot;gpucluster&quot;\n\ntry:\n    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                           max_nodes=1)\n\n    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\nfrom azureml.core import ScriptRunConfig\nsource_directory = os.getcwd()\n\nfrom azureml.core import Environment\n\nmyenv = Environment(&quot;user-managed-env&quot;)\nmyenv.python.user_managed_dependencies =True\nfrom azureml.core import Dataset\ntest_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n\nsrc = ScriptRunConfig(source_directory=source_directory,\n                      script='whentrain.ipynb',\n\n                      arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                      compute_target=compute_target,\n                      environment=myenv)\nrun = exp.submit(src)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>The error that happens in <strong>run.wait_for_completion<\/strong> states :<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 107, in &lt;module&gt;\n[stderr]    &quot;notebookHasBeenCompleted&quot;: true\n[stderr]NameError: name 'true' is not defined\n[stderr]\n<\/code><\/pre>\n<p>As you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?<\/p>\n<p>I'm running the Notebook on Python 3.<\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>Okay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 115, in &lt;module&gt;\n[stderr]    &quot;source_hidden&quot;: false,\n[stderr]NameError: name 'false' is not defined\n[stderr]\n<\/code><\/pre>",
        "Challenge_closed_time":1640628230640,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640331391010,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a NameError when trying to run a ScriptRunConfig in Azure Machine Learning. The error occurs in the whentrain.ipynb file and states that the name 'true' or 'false' is not defined. The user is unsure of the source of the error and is seeking assistance in resolving it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":61.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":82.4554527778,
        "Challenge_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":413,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b72317d8-d212-487c-8510-7d965e8d135f\">@Ash  <\/a> Ok, I think the issue is here.     <\/p>\n<pre><code>src = ScriptRunConfig(source_directory=source_directory,  \n                       script='whentrain.ipynb',  \n                            \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],  \n                       compute_target=compute_target,  \n                       environment=myenv)  \n<\/code><\/pre>\n<p>The script parameter is set to the notebook &quot;whentrain.ipynb&quot;, This should be a python script *.py which can train your model. Since you are using the notebook filename the entire source of jupyter notebook is loaded and it fails with these errors. You can lookup samples on azure ml notebook github repo for <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train.py\">reference<\/a>. I think if you can convert your whentrain.ipynb file to a python script whentrain.py and save it the current folder structure you should be able to use it in this step.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":12.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":97.4961111111,
        "Challenge_answer_count":0,
        "Challenge_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Challenge_closed_time":1635155013000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634804027000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with local wandb logging as it assumes the presence of an API key, which is not required for running wandb locally. The user suggests configuring it to work with wandb locally as well.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":65.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":5.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":97.4961111111,
        "Challenge_title":"wandb create index name error",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":21,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1545131500420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4473.0,
        "Answerer_view_count":745.0,
        "Challenge_adjusted_solved_time":213.3802761111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model in Azure ML and kept on getting the error 'model not found' from my score.py. So I decided to start from scratch again. I had my custom environment registered, and the Azure ML API for Environment class doesn't seem to have anything like 'delete' or 'unregister'. is there a way to work around this? Thanks<\/p>",
        "Challenge_closed_time":1604151289467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603383120473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a model in Azure ML but keeps getting an error message. They want to start from scratch but are unable to unregister their custom environment as there is no option available in the Azure ML API for Environment class. The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":1604274985247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64486262",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":213.3802761111,
        "Challenge_title":"Is there a way to un-register an environment in Azure ML studio",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595118700083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py&amp;preserve-view=true#delete--\" rel=\"nofollow noreferrer\">delete<\/a> method in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py\" rel=\"nofollow noreferrer\">Model<\/a> class to delete a registered model.<\/p>\n<p>This can also be done via the Azure CLI as:<\/p>\n<pre><code>az ml model delete &lt;model id&gt;\n<\/code><\/pre>\n<p>Other commands can be found here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest\" rel=\"nofollow noreferrer\">az ml model<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":260.4842072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Challenge_closed_time":1656410088380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656241190023,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use Tensorboard to monitor their model's performance within a Jupyter notebook running on Amazon SageMaker Studio Lab, but the commands they have used to set up Tensorboard are not working and nothing shows up in the output of the cell where the commands are executed. The user is seeking suggestions to solve this problem.",
        "Challenge_last_edit_time":1656243378470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":12.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":46.9162102778,
        "Challenge_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420286650807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Trondheim, Norway",
        "Poster_reputation_count":720.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1657181121616,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":11.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":108.1698811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Challenge_closed_time":1559896269912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559506858340,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is working with data scientists who want to gain insight into the neural network models they train using Azure Machine Learning Studio\/Service. They are looking for a way to dump out and inspect the internal representation of a neural network model and access the nodes and weights of a trained neural network to visualize the network as a graph structure. The user has tried to use the Train Model output and Save as Trained Model options, but has not found a way to access the internals of the trained model. They are seeking advice on how to access this data or suggestions for a different machine learning framework that might be more appropriate for this kind of analysis.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56418684",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":19.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":108.1698811111,
        "Challenge_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":250,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427643193808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":24.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":301.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424791933163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2483.0,
        "Answerer_view_count":192.0,
        "Challenge_adjusted_solved_time":10.4979555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Challenge_closed_time":1631093676416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630949196040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has used Optuna to optimize their network in Python by making multiple activation trials for each layer. However, when they return the study.bestparams object, it only displays one parameter and does not show where the parameter was used or the other activation functions used. The user is looking for a way to precisely display the best settings used by their model and at which layers.",
        "Challenge_last_edit_time":1631055883776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":25.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":40.1334377778,
        "Challenge_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":211,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597774835507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.8,
        "Solution_reading_time":12.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":5.6297925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Microsoft Azure Machine Learning and was wondering if anyone had done some experiments on date time features. Doe sit automatically derive additional features like \"day of week\", \"day of month\", \"hour of day\" from them, or do I have to provide these?<\/p>\n\n<p>I could not find any info in the official documentation (and a lack of a Microsoft support forum =)<\/p>",
        "Challenge_closed_time":1434736380420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1434716113167,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Microsoft Azure Machine Learning and is seeking information on whether the platform automatically derives additional features like \"day of week\", \"day of month\", \"hour of day\" from date time features or if they need to provide these features themselves. The user was unable to find any information in the official documentation or a Microsoft support forum.",
        "Challenge_last_edit_time":1445833326870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30937903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6297925,
        "Challenge_title":"How are date features utilized in Microsoft Azure Machine Studio",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1221999894423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delaware",
        "Poster_reputation_count":2603.0,
        "Poster_view_count":225.0,
        "Solution_body":"<p>Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples below<\/p>\n\n<p>x&lt;-as.Date(\"12\/3\/2009\", \"%m\/%d\/%Y\")<\/p>\n\n<blockquote>\n  <p>months.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"December\"<\/p>\n\n<blockquote>\n  <p>weekdays.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Thursday\"<\/p>\n\n<blockquote>\n  <p>quarters(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Q4\"<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":19.9010738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I ran into this issue yesterday, while trying to use the same sqlite script I used in \"Apply SQL Transformation\" module in Azure ML, in Sqlite over Python module in Azure ML:<\/p>\n\n<pre><code>with tbl as (select * from t1)\nselect * from tbl\n<\/code><\/pre>\n\n<p>Here is the error I got:<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\n  File \"C:\\server\\invokepy.py\", line 169, in batch\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 388, in read_sql\n  File \"C:\\temp\\azuremod.py\", line 193, in azureml_main\n    results = pd.read_sql(query,con)\n    coerce_float=coerce_float, parse_dates=parse_dates)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1017, in execute\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1022, in read_sql\n    cursor = self.execute(*args)\n    raise_with_traceback(ex)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1006, in execute\n---------- End of error message from Python  interpreter  ----------\n    cur.execute(*args)\nDatabaseError: Execution failed on sql:  with tbl as (select * from t1)\n                    select * from tbl\n<\/code><\/pre>\n\n<p>and the Python code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pandas as pd\n    import sqlite3 as lite\n    import sys\n    con = lite.connect('data1.db')\n    con.text_factory = str\n    with con:\n        cur = con.cursor()\n\n        if (dataframe1 is not None):\n            cur.execute(\"DROP TABLE IF EXISTS t1\")\n            dataframe1.to_sql('t1',con)\n        query = '''with tbl as (select * from t1)\n                    select * from tbl'''                      \n        results = pd.read_sql(query,con)    \n\n    return results,\n<\/code><\/pre>\n\n<p>when replacing the query with:<\/p>\n\n<pre><code>select * from t1\n<\/code><\/pre>\n\n<p>It worked as expected.\nAs you probably know, Common table expressions is a key feature in Sqlite, the ability to run recursive code is a \"must have\" in any functional language such as Sqlite.<\/p>\n\n<p>I also tried to run my Python script in Jupyter Notebook in Azure, that also worked as expected.<\/p>\n\n<p>Is it possible we have a different configuration for Sqlite in the Python module than in Jupyter Notebook and in \"Apply SQL Transformation\" module?<\/p>",
        "Challenge_closed_time":1456485065183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1456413421317,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while using the same SQLite script in the \"Apply SQL Transformation\" module and the SQLite over Python module in Azure ML. The error occurred due to the use of common table expressions, which is not supported in SQLite3. The user tried running the Python script in Jupyter Notebook in Azure, which worked as expected. The user questioned if there is a different configuration for SQLite in the Python module than in Jupyter Notebook and the \"Apply SQL Transformation\" module.",
        "Challenge_last_edit_time":1456812883803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35631267",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":19.9010738889,
        "Challenge_title":"Azure ML in \"Execute Python Script\" module :Common table expressions is not supported in sqlite3",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>I reproduced your issue and reviewed the <code>SQL Queries<\/code> doc of <code>pandas.io.sql<\/code> at <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries\" rel=\"nofollow\">http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries<\/a>. I tried to use <code>read_sql_query<\/code> to solve it, but failed.<\/p>\n\n<p>According to the <code>pandas<\/code> doc, tt seems that <code>Pandas<\/code> not support the usage for this SQL syntax.<\/p>\n\n<p>Base on my experience and according to your SQL, I tried to do the SQL <code>select * from (select * from t1) as tbl<\/code> instead of your SQL that work for <code>Pandas<\/code>.<\/p>\n\n<p>Hope it helps. Best Regards. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":53.7180736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Challenge_closed_time":1569671934552,
        "Challenge_comment_count":4,
        "Challenge_created_time":1569478549487,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a mobile application using React Native and needs to access a machine learning model to send pictures for segmentation. They want to use Amazon Sagemaker to receive a segmented picture back, but are unsure how to create an endpoint that they can access through React Native without running a separate server.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":16.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":53.7180736111,
        "Challenge_title":"How do I access Amazon Sagemaker through React Native?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":717.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488334447848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":17.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0333333333,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI was wondering if there is CLI option to \"Update the Job status\" similar to:\n\nI have found polyaxon ops update --help\n\nUsage: polyaxon ops update [OPTIONS]\n\n  Update run.\n\n  Uses \/docs\/core\/cli\/#caching\n\n  Examples:\n\n  $ polyaxon ops update --uid 8aac02e3a62a4f0aaa257c59da5eab80\n  --description=\"new description for my runs\"\n\n  $ polyaxon ops update --project=cats-vs-dogs -uid 8aac02e3a62a4f0aaa257c59da5eab80 --tags=\"foo, bar\" --name=\"unique-name\"\n\nOptions:\n  -p, --project TEXT  The project name, e.g. 'mnist' or 'acme\/mnist'.\n  -uid, --uid TEXT    The run uuid.\n  -n, --name TEXT     Name of the run (optional).\n  --description TEXT  Description of the run (optional).\n  --tags TEXT         Tags of the run (comma separated values).\n  --help              Show this message and exit.\n\nLong story short is that lot of my jobs finished working with status \"Running\". Don't know why, but in logs I see they finished.\nSo I decided to stop them manually, and I see \"Stopping\".\nSome jobs are being stopped but did not change status\nSo I want to clean that and \"update the status\" manually --> let's mark them as stopped.",
        "Challenge_closed_time":1649332733000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649332613000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where some of their jobs have finished working but are still showing a status of \"Running\". They tried to stop them manually but some jobs did not change status. The user is looking for a CLI option to update the job status manually and mark them as stopped.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1478",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0333333333,
        "Challenge_title":"Update jobs status from CLI or Client",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The first thing is the check the release notes with this kind of problems, as the issue could have been solved in subsequent releases.\n\nNow going back to the solution, It's better to use client to iterate over the runs and mark them as finished:\n\nfrom polyaxon.client import ProjectClient\n\npclient = ProjectClient(project=\"PROJECT_NAME\")\nfor r in pclient.list_runs(query=\"status: stopping\").results:\n    print(\"cleaning {}\".format(r.uuid)\n    RunClient(\"PROJECT_NAME\", run_uuid=r.uuid).log_stopped(message=\"manual cleaning\")",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":6.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":235.0291777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using Inference Schema to autogenerate the swagger doc for my AzureML endpoint (as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">here<\/a>), I see that it creates a wrapper around my input_sample. Is there a way to\nnot wrap the input inside this &quot;data&quot; wrapper?<\/p>\n<p>Here is what my score.py looks like:<\/p>\n<pre><code>input_sample = {\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\noutput_sample = [{'prediction': 'true', 'predictionConfidence': 0.8279970776764844}]\n\n@input_schema('data', StandardPythonParameterType(input_sample))\n@output_schema(StandardPythonParameterType(output_sample))\ndef run(data):\n&quot;&quot;&quot;\n    {\n        data: { --&gt; DON'T WANT this &quot;data&quot; wrapper\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\n    }\n    &quot;&quot;&quot;\n    try:\n        id = data['id']\n        ...\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1601883755636,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600982458753,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Inference Schema to autogenerate the swagger doc for their AzureML endpoint. However, they are encountering a problem where Inference Schema creates a wrapper around their input_sample. The user is looking for a way to remove this \"data\" wrapper.",
        "Challenge_last_edit_time":1601039285472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64054587",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":16.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":250.3602452778,
        "Challenge_title":"How can I remove the wrapper around the input when using Inference Schema",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406298639460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":435.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>InferenceSchema used with Azure Machine Learning deployments, then the code for this package was recently published at <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/InferenceSchema<\/a> under an MIT license. So you could possibly use that to create a version specific to your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601885390512,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":120.6901825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Challenge_closed_time":1646329084320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646326739290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand the process of getting a prediction from a Sagemaker endpoint. They are aware of the 4 specific functions required in the inference script and have noticed that the <code>Predictor.predict()<\/code> method calls the Sagemaker session to post a request to the endpoint. However, they are unable to find where the inference script is run and are curious about the code that Sagemaker runs when it receives the request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.6513972222,
        "Challenge_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578932319743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ireland",
        "Poster_reputation_count":1012.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646761223947,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3846.9952777778,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Challenge_closed_time":1636988013000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1623138830000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that when `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place, which can lead to confusing errors. The other loggers do not change `metrics` in-place when `log_metrics` is called. The user is willing to submit a PR to fix this issue and has some questions regarding the changes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3846.9952777778,
        "Challenge_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@tchaton Is this a lightning issue? Closing this issue as there is no progress nor manifestation from the Comet Team.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.44,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1505166133223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":73.5019452778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says \"Endpoint successfully built\". Only the below is shown:<\/p>\n\n<pre><code>--------------------------------------------------------------------------------!\n<\/code><\/pre>\n\n<p>Instead, I found the endpoint number from my console. <\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ndata= test_out2\npredictor.predict(data)\n<\/code><\/pre>\n\n<p>Then I try to invoke the endpoint by using 2D array:\n(1) If my 2D array is in size of (5000, 170), I am getting the error:<\/p>\n\n<pre><code>ConnectionResetError: [Errno 104] Connection reset by peer\n<\/code><\/pre>\n\n<p>(2) If reducing the array to size of (10,170), error is :<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.\n<\/code><\/pre>\n\n<p>Any suggestion please? Found similar case in github, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589<\/a>.<\/p>\n\n<p>Is it the similar case please?<\/p>\n\n<p>Thank you very much in advance!<\/p>",
        "Challenge_closed_time":1556735063680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556470456677,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a well-trained TensorFlow model in Sagemaker using JSON and weight file. However, the endpoint was not successfully built and the user had to find the endpoint number from the console. When trying to invoke the endpoint using a 2D array, the user encountered a \"Connection reset by peer\" error for an array of size (5000, 170) and a \"ModelError\" for an array of size (10, 170). The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55892554",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":21.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":73.5019452778,
        "Challenge_title":"Invoke endpoint after model deployment : [Err 104] Connection reset by peer",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":465.0,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401287882528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The first error with data size (5000, 170) might be due to a capacity issue. SageMaker endpoint prediction has a size limit of 5mb. So if your data is larger than 5mb, you need to chop it into pieces and call predict multiple times. <\/p>\n\n<p>For the second error with data size (10, 170), the error message asks you to look into logs. Did you find anything interesting in the cloudwatch log? Anything can be shared in this question?<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1397589101936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dominican Republic",
        "Answerer_reputation_count":563.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":10.2489016667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im creating a model using optuna lightgbm integration, My training set has some categorical features and i pass those features to the model using the <code>lgb.Dataset<\/code> class, here is the code im using ( NOTE: X_train, X_val, y_train, y_val are all pandas dataframes ).<\/p>\n<pre><code>\nimport lightgbm as lgb \n\n        grid = {\n            \n       \n            'boosting': 'gbdt',\n            'metric': ['huber', 'rmse' , 'mape'],\n            'verbose':1\n\n        }\n        \n        X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n        cat_features = [ col for col in X_train if col.startswith('cat') ]\n\n        dval = Dataset(X_val, label=y_val, categorical_feature=cat_features)\n        dtrain = Dataset(X_train, label=y_train,  categorical_feature=cat_features)\n        \n        model = lgb.train(      \n                                    grid,\n                                    dtrain,\n                                    valid_sets=[dval],\n                                    early_stopping_rounds=100)\n                                    \n\n<\/code><\/pre>\n<p>Every time the <code>lgb.train<\/code> function is called, i get the following user warning<\/p>\n<pre><code>\n UserWarning: categorical_column in param dict is overridden.\n\n<\/code><\/pre>\n<p>I believe that lighgbm is not treating my categorical features the way it should, someone knows how to fix this issue? Am i using the parameter correctly?<\/p>",
        "Challenge_closed_time":1613830364163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613793468117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating a model using Optuna lightgbm integration. The training set has categorical features, and the user is passing those features to the model using the lgb.Dataset class. However, every time the lgb.train function is called, the user gets a UserWarning that the categorical_column in the param dict is overridden. The user is seeking help to fix this issue and wants to know if they are using the parameter correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66287854",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":10.2489016667,
        "Challenge_title":"Optuna lightgbm integration giving categorical features error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586625057632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>In case of picking the name (not indexes) of those columns, add as well the <code>feature_name<\/code> parameters as the <a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Dataset.html#lightgbm.Dataset.__init__\" rel=\"nofollow noreferrer\">documentation states<\/a><\/p>\n<p>That said, your <code>dval<\/code> and <code>dtrain<\/code> will be initialized as follow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>dval = Dataset(X_val, label=y_val, feature_name=cat_features, categorical_feature=cat_features)\ndtrain = Dataset(X_train, label=y_train, feature_name=cat_features, categorical_feature=cat_features)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.8,
        "Solution_reading_time":8.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":0,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a ResourceLimitExceeded error when running code cell [17] to create an endpoint to host the model while walking through the SageMaker Studio tour for the first time in a new AWS account. The error occurred due to the account-level service limit 'ml.m4.xlarge for endpoint usage' being 0 Instances. The user suggests that the Prerequisites section could address this proactively with a link to the service limit increase page or the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of 0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2.4045722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Challenge_closed_time":1652649592320,
        "Challenge_comment_count":3,
        "Challenge_created_time":1652637573657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble making a PowerShell Get request with a body to an API endpoint. They have tried to append the body as a query string and encode the query parameter, but both attempts have failed. They have successfully made the same request in Python and Postman.",
        "Challenge_last_edit_time":1652640935860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":16.7,
        "Challenge_reading_time":43.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3385175,
        "Challenge_title":"PowerShell Get request with body",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.6,
        "Solution_reading_time":13.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.3012636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've a azure synapse analytics workspace in region North Europe, as the region has hardware Accelerated pools, GPU base pools so to say. But i don't see the packages setting.     <br \/>\nhere is the comparison for 2 workspace, 1 in north Europe and other one in West Europe.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209418-screenshot-2022-06-08-at-120209.png?platform=QnA\" alt=\"209418-screenshot-2022-06-08-at-120209.png\" \/> vs <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209494-screenshot-2022-06-08-at-120550.png?platform=QnA\" alt=\"209494-screenshot-2022-06-08-at-120550.png\" \/>    <\/p>\n<p>Even the package setting in the Workspace itself is disabled for me: here is the screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209434-screenshot-2022-06-08-at-120143.png?platform=QnA\" alt=\"209434-screenshot-2022-06-08-at-120143.png\" \/>    <\/p>\n<p>I've 2 questions in this reagrd:     <\/p>\n<ul>\n<li> Am I missing any configuration for the GPU pool or this feature is not released?    <\/li>\n<li> Is there any alternate way to install a package? <code>pip install<\/code> or <code>pip3 install<\/code> are not working.      <\/li>\n<\/ul>",
        "Challenge_closed_time":1654770659816,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654683175267,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing a Python package in a hardware accelerated GPU Spark pool in their Azure Synapse Analytics workspace located in North Europe. They are unable to find the package setting and it is disabled in the workspace. The user has two questions regarding this issue: whether they are missing any configuration for the GPU pool or if this feature is not yet released, and if there is an alternate way to install the package as pip install or pip3 install are not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/881432\/how-to-install-python-package-in-hardware-accelera",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":24.3012636111,
        "Challenge_title":"How to install python package in hardware accelerated GPU spark pool ?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=f236076b-e057-4c60-b0fd-0068a6492053\">@Prateek Narula  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>(UPDATE:6\/10\/2022): Unfortunately, we do not have Library Management (Package) support for GPU spark pools in Azure Synapse Analytics.    <\/p>\n<\/blockquote>\n<p>---------------------------------------------------    <\/p>\n<p>As per the repro, I had noticed similar behaviour.     <\/p>\n<blockquote>\n<p>Looks like packages are only supported for Node size family: &quot;Memory Optimized&quot; - let me get a confirmation from the product team.    <\/p>\n<\/blockquote>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209808-synape-gpu.gif?platform=QnA\" alt=\"209808-synape-gpu.gif\" \/>    <\/p>\n<p>We are reaching out to internal team to get more information related to this issue and will get back to you as soon as we have an update.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.6,
        "Solution_reading_time":26.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":213.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1412515367427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":682.3236119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559693969323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557237604320,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has followed an Amazon tutorial for using SageMaker to create a model, but is unsure how to view the final formula and graphs of plotted factors. They are seeking guidance on how to review the model in a similar way to a GLM.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":9.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":682.3236119445,
        "Challenge_title":"Beginners guide to Sagemaker",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":748.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554397763220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.6,
        "Solution_reading_time":18.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601004709207,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":1074.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":11.2046330556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running MLflow Project for a model using following command from my ubuntu 20.04 terminal<\/p>\n<pre><code>mlflow run . --no-conda -P alpha=0.5\n<\/code><\/pre>\n<p>My system doesn't have conda or python (It does however have python3). So, I added alias for python using terminal<\/p>\n<pre><code>alias python='python3'\n<\/code><\/pre>\n<p>After which I could open python in terminal using <code>python<\/code>. However, I still got the same error<\/p>\n<pre><code>2021\/11\/21 08:07:34 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpp4h595ql for downloading remote URIs passed to arguments of type 'path' ===\n2021\/11\/21 08:07:34 INFO mlflow.projects.backend.local: === Running command 'python tracking.py 0.5 0.1' in run with ID 'e50ca47b3f8848a083906be6220c26fc' === \nbash: python: command not found\n2021\/11\/21 08:07:34 ERROR mlflow.cli: === Run (ID 'e50ca47b3f8848a083906be6220c26fc') failed ===\n<\/code><\/pre>\n<p>How to get rid of this error?<\/p>",
        "Challenge_closed_time":1637463754447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637462657297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running an MLflow project for a model on Ubuntu 20.04 terminal due to the absence of conda or python. The user added an alias for python using the terminal, but still received the error \"bash: python: command not found\" while running the project. The user is seeking a solution to resolve this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70051420",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3047638889,
        "Challenge_title":"MLFlow projects; bash: python: command not found",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601004709207,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":1074.0,
        "Poster_view_count":143.0,
        "Solution_body":"<p>Change <code>python<\/code> to <code>python3<\/code> in the <code>MLproject<\/code> file to the resolve error.<\/p>\n<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1637502993976,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1645792458310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":1465.312585,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Challenge_closed_time":1645794046503,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640518921197,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is facing conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel. The notebook appears to use a separate distribution of Python 3.7, and the Python instance inside the notebook is different. The user is unable to import tensorflow as it isn't preinstalled in the Python environment that the notebook invokes. The user is seeking a solution to address this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70486162",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":12.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1465.312585,
        "Challenge_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331727483732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1060.0,
        "Poster_view_count":139.0,
        "Solution_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":7.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11.8299888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Challenge_closed_time":1579719419647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579686060327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in throwing an exception from within an MLflow project. They are executing a function using mlflow.run but are getting an ExecutionException. The user is seeking a way to get the exception being raised where they are executing mlflow.run or to send an mlflow.exceptions.ExecutionException with a custom message set from within the project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.2664777778,
        "Challenge_title":"How can I throw an exception from within an MLflow project?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472932425400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1579728648287,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":4.8079869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling to correctly set Vertex AI pipeline which does the following:<\/p>\n<ol>\n<li>read data from API and store to GCS and as as input for batch prediction.<\/li>\n<li>get an existing model (Video classification on Vertex AI)<\/li>\n<li>create Batch prediction job with input from point 1.<br \/>\nAs it will be seen, I don't have much experience with Vertex Pipelines\/Kubeflow thus I'm asking for help\/advice, hope it's just some beginner mistake.\nthis is the gist of the code I'm using as pipeline<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import dsl\n\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Artifact,\n    Model,\n)\n\nPROJECT_ID = 'my-gcp-project'\nBUCKET_NAME = &quot;mybucket&quot;\nPIPELINE_ROOT = &quot;{}\/pipeline_root&quot;.format(BUCKET_NAME)\n\n\n@component\ndef get_input_data() -&gt; str:\n    # getting data from API, save to Cloud Storage\n    # return GS URI\n    gcs_batch_input_path = 'gs:\/\/somebucket\/file'\n    return gcs_batch_input_path\n\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=['google-cloud-aiplatform==1.8.0']\n)\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    &quot;&quot;&quot;Load existing Vertex model&quot;&quot;&quot;\n    import google.cloud.aiplatform as aip\n\n    model_id = '1234'\n    model = aip.Model(model_name=model_id, project=project_id, location='us-central1')\n\n\n\n@dsl.pipeline(\n    name=&quot;batch-pipeline&quot;, pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(gcp_project: str):\n    input_data = get_input_data()\n    ml_model = load_ml_model(gcp_project)\n\n    gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n\n\nif __name__ == '__main__':\n    from kfp.v2 import compiler\n    import google.cloud.aiplatform as aip\n    pipeline_export_filepath = 'test-pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_export_filepath)\n    # pipeline_params = {\n    #     'gcp_project': PROJECT_ID,\n    # }\n    # job = aip.PipelineJob(\n    #     display_name='test-pipeline',\n    #     template_path=pipeline_export_filepath,\n    #     pipeline_root=f'gs:\/\/{PIPELINE_ROOT}',\n    #     project=PROJECT_ID,\n    #     parameter_values=pipeline_params,\n    # )\n\n    # job.run()\n<\/code><\/pre>\n<p>When running the pipeline it throws this exception when running Batch prediction:<br \/>\n<code>details = &quot;List of found errors: 1.Field: batch_prediction_job.model; Message: Invalid Model resource name. <\/code>\nso I'm not sure what could be wrong. I tried to load model in the notebook (outside of component) and it correctly returns.<\/p>\n<p>Second issue I'm having is referencing GCS URI as output from component to batch job input.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>   input_data = get_input_data2()\n   gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n<\/code><\/pre>\n<p>During compilation, I get following exception <code>TypeError: Object of type PipelineParam is not JSON serializable<\/code>, though I think this could be issue of ModelBatchPredictOp component.<\/p>\n<p>Again any help\/advice appreciated, I'm dealing with this from yesterday, so maybe I missed something obvious.<\/p>\n<p>libraries I'm using:<\/p>\n<pre><code>google-cloud-aiplatform==1.8.0  \ngoogle-cloud-pipeline-components==0.2.0  \nkfp==1.8.10  \nkfp-pipeline-spec==0.1.13  \nkfp-server-api==1.7.1\n<\/code><\/pre>\n<p><strong>UPDATE<\/strong>\nAfter comments, some research and tuning, for referencing model this works:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    region = 'us-central1'\n    model_id = '1234'\n    model_uid = f'projects\/{project_id}\/locations\/{region}\/models\/{model_id}'\n    model.uri = model_uid\n    model.metadata['resourceName'] = model_uid\n<\/code><\/pre>\n<p>and then I can use it as intended:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        job_display_name=f'batch-prediction-test',\n        model=ml_model.outputs['model'],\n        gcs_source_uris=[input_batch_gcs_path],\ngcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/test'\n    )\n<\/code><\/pre>\n<p><strong>UPDATE 2<\/strong>\nregarding GCS path, a workaround is to define path outside of the component and pass it as an input parameter, for example (abbreviated):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@dsl.pipeline(\n    name=&quot;my-pipeline&quot;,\n    pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(\n        gcp_project: str,\n        region: str,\n        bucket: str\n):\n    ts = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    \n    gcs_prediction_input_path = f'gs:\/\/{BUCKET_NAME}\/prediction_input\/video_batch_prediction_input_{ts}.jsonl'\n    batch_input_data_op = get_input_data(gcs_prediction_input_path)  # this loads input data to GCS path\n\n    batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        model=training_job_run_op.outputs[&quot;model&quot;],\n        job_display_name='batch-prediction',\n        # gcs_source_uris=[batch_input_data_op.output],\n        gcs_source_uris=[gcs_prediction_input_path],\n        gcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/',\n    ).after(batch_input_data_op)  # we need to add 'after' so it runs after input data is prepared since get_input_data doesn't returns anything\n\n<\/code><\/pre>\n<p>still not sure, why it doesn't work\/compile when I return GCS path from <code>get_input_data<\/code> component<\/p>",
        "Challenge_closed_time":1640097300176,
        "Challenge_comment_count":7,
        "Challenge_created_time":1639525350357,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing issues with referencing an existing model and input file on Cloud Storage while using Vertex AI Model Batch prediction. They are using a pipeline to read data from an API and store it to GCS as input for batch prediction. The user is encountering errors while running the pipeline, and they are seeking help to resolve the issues. The user has updated their code to resolve the issue with referencing the existing model, but they are still facing issues with referencing the GCS path.",
        "Challenge_last_edit_time":1640079991423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70356856",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":17.4,
        "Challenge_reading_time":78.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":158.8749497222,
        "Challenge_title":"Vertex AI Model Batch prediction, issue with referencing existing model and input file on Cloud Storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1202.0,
        "Challenge_word_count":495,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372196724860,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Prague, Czech Republic",
        "Poster_reputation_count":276.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.<\/p>\n<p>For your <code>input.output<\/code> observation on <code>gcs_source_uris<\/code>, the reason behind it is because the way the function\/class returns the value. If you dig inside the class\/methods of <code>google_cloud_pipeline_components<\/code>  you will find that it implements a structure that will allow you to use <code>.outputs<\/code> from the returned value of the function called.<\/p>\n<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component<\/code> function. So, in order to have that implemented in your custom class\/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.<\/p>\n<pre><code>class CustomClass():\n     def __init__(self):\n       self.return_val = {'path':'custompath','desc':'a desc'}\n      \n     @property\n     def output(self):\n       return self.return_val \n\nhello = CustomClass()\nprint(hello.output['path'])\n<\/code><\/pre>\n<p>If you want to dig more about it you can go to the following pages:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/github.com\/bharathdsce\/kubeflow\/blob\/fcd627714664956b2c280b0109b64633bc99fa05\/components\/google-cloud\/google_cloud_pipeline_components\/aiplatform\/utils.py#L383\" rel=\"nofollow noreferrer\">convert_method_to_component<\/a>, which is the implementation of <code>convert_method_to_component<\/code><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/www.programiz.com\/python-programming\/property\" rel=\"nofollow noreferrer\">Properties<\/a>, basics of property in python.<\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":177.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":52.9681911111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1590563891208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590373205720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to execute AWS Sagemaker Notebook and is receiving a \"Failed to start kernel\" error with a \"ThrottlingException\" message. The user suspects that the issue may be related to an app that is stuck in the process of being deleted.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.9681911111,
        "Challenge_title":"Cannot execute AWS Sagemaker Notebook",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":965.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2438519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How could I request quota for the NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs  <\/p>\n<p>They both do not appear as an option on the usual form to request quota increase in any of the 4 US regions I looked  <\/p>\n<p>Thanks  <\/p>\n<p>Manuel<\/p>",
        "Challenge_closed_time":1629922551630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629921673763,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in requesting quota for NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs as they are not appearing as an option on the usual form to request quota increase in any of the 4 US regions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/528034\/access-to-ncast4-v3-series-and-nd-a100-v4-series-v",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.2438519445,
        "Challenge_title":"Access to NCasT4_v3-series and ND A100 v4-series VMs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=8de1093a-60f2-479b-b811-ff2bab24e3cd\">@Manuel Reyes Gomez  <\/a> ,    <\/p>\n<p>the VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/126428-image.png?platform=QnA\" alt=\"126428-image.png\" \/>    <\/p>\n<p>Source: <a href=\"https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&amp;regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\">https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&amp;regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3<\/a>    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.6,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":58.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1870847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used to use Azure Machine Learning Studio (classic).  <br \/>\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.  <br \/>\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).  <br \/>\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.  <\/p>\n<p>How can I process at the same speed as classic?<\/p>",
        "Challenge_closed_time":1635458665072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635432791567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing a significant difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic). The virtual machine size is Standard_DS3_v2, and steps that have been changed even slightly take 20 times longer to process than in classic. The user is seeking advice on how to process at the same speed as classic.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.1870847222,
        "Challenge_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\">Optimize Data Processing<\/a>. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/170450\/\">(1)<\/a>, <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html\">(2)<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":188.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1679.0761111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Challenge_closed_time":1632957644000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626912970000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing issues with Neptune_catalyst.ipynb failing and suspects that there may be a typo or missing `run` object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1679.0761111111,
        "Challenge_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @Roshin29, thank you for the bug report! \r\n\r\nThe machine learning sample notebooks received substantial revisions in [Release 3.0.1](https:\/\/github.com\/aws\/graph-notebook\/releases\/tag\/v3.0.1). This release also included a number of changes under the hood to support the general availability release of Amazon Neptune ML.\r\n\r\nThe Gremlin query listed is only seen in older versions of the `Neptune-ML-01-Introduction-to-Node-Classification-Gremlin` sample notebook, and is now replaced by the one below:\r\n```\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"${endpoint}\").\r\n  with(\"Neptune#ml.limit\",3).\r\n  V().has('title', 'Apollo 13 (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\n```\r\nI am not able to reproduce the listed exception when running this query using graph-notebook v3.0.6, so the issue appears to have been resolved with the latest changes.\r\n\r\nClosing this issue out, as there are no further action items at this time. Please feel free to re-open if you have any further questions.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.78,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":20.4671011111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Challenge_closed_time":1603790179687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603715439387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker's limit on the number of models that can be trained in parallel, which is currently set at 30. They need to scale up their system to train individual models for hundreds of thousands of units and host the resulting model artifacts on a multi-model endpoint. The user is seeking suggestions on how to implement a system in AWS Sagemaker to have a separate trained model artifact for each unit and whether it is possible to output multiple model artifacts for one Sagemaker training job using an SKLearn estimator. Additionally, the user is seeking clarification on how Sagemaker makes use of multiple CPUs when a training script is submitted.",
        "Challenge_last_edit_time":1603716498123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":15.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":20.7611944445,
        "Challenge_title":"AWS Sagemaker Multiple Training Jobs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1053.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":23.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":281.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":602.3895386111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Challenge_closed_time":1524806701632,
        "Challenge_comment_count":1,
        "Challenge_created_time":1522638099293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the differences between Azure Machine Learning Studio and Azure Machine Learning Workbench, including their intended differences and whether Workbench is being phased out in favor of Studio. The user has found some differences, such as Studio having a hard limit on input data and a more user-friendly GUI, while Workbench has more powerful deployment tools. However, there are also scattered references claiming that Studio is simply a renamed and updated version of Workbench. The user is seeking guidance on which offering to choose as a fresh data scientist looking to adopt the Microsoft stack for enterprise-scale use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":15.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":602.3895386111,
        "Challenge_title":"Azure Machine Learning Studio vs. Workbench",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3387.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434736108840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dallas, TX, United States",
        "Poster_reputation_count":2045.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":10.52,
        "Solution_score_count":6.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.9666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi everyone I have a step in vertex ai pipelines that looks like this:\n\ntranscribe_task = transcribe_audios(audio_files=download_task.output)\ntranscribe_task.set_cpu_limit(\"2\").set_memory_limit(\n\"8G\"\n).add_node_selector_constraint(\"NVIDIA_TESLA_T4\").set_gpu_limit(\"1\")\n\nyet that task is not executed due to:\n\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/custom_model_training_nvidia_t4_gpus, cause=null; Failed to create custom job for the task.\n\nBut that quota is not listed anywhere in the quota manager, how can I enable GPU in Vertex AI pipelines?",
        "Challenge_closed_time":1675667340000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675635060000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Vertex AI pipelines where a task is not being executed due to exceeding quota limits for custom model training with NVIDIA T4 GPUs. However, the quota is not listed in the quota manager, and the user is seeking guidance on how to enable GPU in Vertex AI pipelines.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-not-listed-in-Vertex-AI-pipelines\/m-p\/518466#M1211",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.9666666667,
        "Challenge_title":"Quota not listed in Vertex AI pipelines?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":35.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":20.4471097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1507014239332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506940629737,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a machine learning model as a web service using Azure Machine Learning Studio. They trained a LogisticRegressionCV model from sklearn.linear_model locally on their machine and tried to publish it as a web service. However, they encountered an error stating that 'module' object has no attribute 'LogisticRegressionCV'. The user wants to update sklearn on their workspace and is seeking advice on how to install any python module on Azure Machine Learning Studio to use predict functions of any model developed locally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46523924",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":25.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":20.4471097222,
        "Challenge_title":"Adding python modules to AzureML workspace",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2578.0,
        "Challenge_word_count":249,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458750704640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":186.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.6,
        "Solution_reading_time":17.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":456.5955555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units? \n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Challenge_closed_time":1614090886000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612447142000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidelines for provisioning SageMaker Studio domains across large global enterprises with multiple business units. They are looking for guidance on scoping and designing the deployment process to ensure scalability, taking into account factors such as number of users, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, and security.",
        "Challenge_last_edit_time":1667925623704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sagemaker-studio-enterprise-deployment-guidelines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":7.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":456.5955555556,
        "Challenge_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925545476,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1225.01,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830449000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664420413000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where using mlflow without an mlflow writer configured fails silently. The expected behavior is for mlflow integration to write to mlflow by default and warn if missing or inconsistent config is set. The user suggests that whylogs should mention the missing mlflow writer in a warning and automatically add the mlflow writer (with a warning) to draw attention to where the behavior can be modified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1225.01,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":222,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":12.75,
        "Solution_score_count":null,
        "Solution_sentence_count":18.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1448314895790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":1769.0,
        "Answerer_view_count":272.0,
        "Challenge_adjusted_solved_time":15.0059675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can AWS SageMaker handle binary classification using TFidf vectorized text as prediction base?<\/p>",
        "Challenge_closed_time":1567815655550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567761634067,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if AWS SageMaker can perform binary classification using TFidf vectorized text for prediction.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57819173",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":1.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":15.0059675,
        "Challenge_title":"SageMaker AWS Binary Text Classification",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":17,
        "Platform":"Stack Overflow",
        "Poster_created_time":1339151552347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Netherlands",
        "Poster_reputation_count":1125.0,
        "Poster_view_count":319.0,
        "Solution_body":"<p>You would have to use inference pipeline for your use case. What that means is that you will need to use a pre-processing step to featurize your text into tfidf and then feed into Sagemaker classification. Here's a <a href=\"https:\/\/stackoverflow.com\/questions\/57767899\/how-to-create-a-pipeline-in-sagemaker-with-pytorch\">SO answer<\/a> with more details around this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":1.0301077778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the following data:<\/p>\n\n<ul>\n<li>Identifier of a person<\/li>\n<li>Days in location (starts at 1 and runs until event)<\/li>\n<li>Age of person in months at that time (so this increases as the days in location increase too).<\/li>\n<li>Smoker (boolean), doesn't change over time in our case<\/li>\n<li>Sex, doesn't change over time<\/li>\n<li>Fall (boolean) this is an event that may never happen, or can happen multiple times during the complete period for a certain person<\/li>\n<li>Number of wounds: (this can go from 0 to 8), a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time<\/li>\n<li>Event we want to predict (boolean), only the last row of a person will have value true for this<\/li>\n<\/ul>\n\n<p>I have this data for 1500 people (in total 1500000 records so on average about 1000 records per person). For some people the event I want to predict takes place after a couple of days, for some after 10 years.  For everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as 1.<\/p>\n\n<p>I'm new to this and all the documentation I have found so far doesn't demonstrate time series for multiple persons or objects. When I for example split the data in the machine learning studio, I want to keep records of the same person over time together.<\/p>\n\n<p>Would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next 5 days?<\/p>\n\n<p>Edit: sample data of 2 persons: <a href=\"http:\/\/pastebin.com\/KU4bjKwJ\" rel=\"nofollow\">http:\/\/pastebin.com\/KU4bjKwJ<\/a><\/p>",
        "Challenge_closed_time":1477404012396,
        "Challenge_comment_count":2,
        "Challenge_created_time":1477382328947,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has a dataset of 1500 people with various properties such as age, smoker, sex, number of wounds, and fall events. They want to create a model that predicts an event based on other time series events and properties of an object. The user is struggling to find documentation on time series for multiple persons or objects and wants to keep records of the same person over time together. They also want to know if it's possible to feed the system with new records and get an estimate of the event taking place in the next 5 days.",
        "Challenge_last_edit_time":1477400304008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40234432",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.0231802778,
        "Challenge_title":"Create a model that predicts an event based on other time series events and properties of an object",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":596.0,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1345413556180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1946.0,
        "Poster_view_count":211.0,
        "Solution_body":"<p>sounds like very similar to this sample:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f<\/a><\/p>\n\n<p>Unfortunately there is going to be a bit of R code involved. Yes you should be able to retrain the model with new data.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":5.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221667848150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":849.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":1013.8935497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Challenge_closed_time":1608159311612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604509294833,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether AWS Sagemaker charges extra for every API inference request made to the model, in addition to the hourly cost of running the service.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1013.8935497222,
        "Challenge_title":"Does AWS Sagemaker charges you per API request?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":187.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710710163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Poster_reputation_count":404.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":247.7520736111,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Challenge_closed_time":1656556961635,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655665054170,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble associating their sweeps with a project in wandb. They have tried setting the project name in the sweep configuration and using wandb.init with the project name, but the sweeps are still showing up as uncategorized in the projects interface. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.6,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":247.7520736111,
        "Challenge_title":"Can't associate sweeps with project",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1324808381143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":9050.0,
        "Answerer_view_count":1750.0,
        "Challenge_adjusted_solved_time":2198.1168433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1623834809928,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to implement distributed training using SageMaker v2.29.2 and Tensorflow v2.3.2, following a blog post. However, they are having difficulties importing the smdistributed script and are receiving a \"ModuleNotFoundError\".",
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":2203.8220902778,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.9730555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that [in EC2 it's possible][1], however I don't see it mentioned in Amazon SageMaker documentation.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/elastic-inference\/latest\/developerguide\/basics.html",
        "Challenge_closed_time":1604506639000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604477936000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of fitting multiple Amazon Elastic Inference accelerators in Amazon SageMaker endpoints, as it is possible in EC2, but cannot find any documentation regarding this feature.",
        "Challenge_last_edit_time":1667926687134,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sagemaker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":4.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.9730555556,
        "Challenge_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, they cant be; multi-attach is only supported with EC2.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612946445792,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":131.5469241667,
        "Challenge_answer_count":10,
        "Challenge_body":"<p>Hi,<br>\nI\u2019m currently working on a self-supervised representation learning project, and to evaluate the quality of my models I train a linear classifier on the outputs of my (frozen) trained encoder and look at the downstream classification accuracy.<\/p>\n<p>This evaluation procedure is done separately from the training of the encoder, however is there still a way to add the metrics computed during this evaluation phase to the standard metrics I log during the training phase, in the same run panel?<\/p>\n<p>More generally, can I add metrics to a run that is already finished?<\/p>\n<p>Thanks a lot!<\/p>",
        "Challenge_closed_time":1674087539911,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673613970984,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is working on a self-supervised representation learning project and evaluates the quality of the models by training a linear classifier on the outputs of the trained encoder. The evaluation is done separately from the training of the encoder, and the user wants to know if there is a way to add the metrics computed during this evaluation phase to the standard metrics logged during the training phase in the same run panel. The user also wants to know if it is possible to add metrics to a run that is already finished.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/log-custom-metrics-for-a-run-outside-of-the-training-loop\/3696",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":131.5469241667,
        "Challenge_title":"Log custom metrics for a run outside of the training loop",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":249.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a> , appreciate your your additional feedback.<\/p>\n<p>This approach of first logging , <code>loss<\/code>, to a run, then revisiting\/resuming a run to log different metric, <code>accuracy<\/code>, starting from <strong>step zero<\/strong> again is not supported. The wandb logging step must be monotonically increasing in each call, otherwise the <code>step<\/code> value is ignored during your call to <code>log()<\/code>. Now if you are not interested in logging accuracy at step 0, you <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#resuming-guidance\">could resume<\/a> the previously finished run using its un id and log additional metrics to the run. this however is problematic as the new metric is logged starting at the last known\/registered step for the run.<\/p>\n<p>One approach to get around the issue you are running into  is to assign each of the runs to a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\">specific group<\/a>. Example set <code>group = version_0<\/code> for any runs that logs metrics for this specific version of the model. You could then set grouping in the workspace to help with tracking  the different metrics for each experiment, <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Group-Viz-Test\/groups\/L2\/workspace?workspace=user-mohammadbakir\">see this example workspace<\/a>.<\/p>\n<p>Hope this helps and please let us know if you have additional questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.63,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":183.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.3011111111,
        "Challenge_answer_count":0,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848054000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1597814570000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying the MLflow UI, specifically an AttributeError related to a missing attribute called 'mlModelFilterPattern'. The user needs to review the configuration parameter being used.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":9.3011111111,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":299,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":170.0166666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1659198312000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658586252000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an exception while using `WandbLogger` in a backtest with `aggregate_metrics=True`. The error occurred in `tslogger.log_backtest_metrics` while constructing `metrics_df` and it was unable to group by \"segment\". The bug appears in both `Pipeline.backtest` and `TimeSeriesCrossValidation` class. The expected behavior is no error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":261.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":170.0166666667,
        "Challenge_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2554944445,
        "Challenge_answer_count":6,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/105827-image.png?platform=QnA\" alt=\"105827-image.png\" \/>    <\/p>\n<p>I tried to deploy a VM to Azure Machine Learning, but I get the error message &quot;You do not have enough quota for the following VM sizes. Click here to view and request quota.&quot; And the VM cannot be deployed.    <\/p>\n<p>But I have enough quota (24 CPUs).    <\/p>\n<p>What is causing the problem?    <\/p>\n<p>I'm using Azure's Free trial plan.<\/p>",
        "Challenge_closed_time":1623772934643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to deploy a VM on Azure Machine Learning due to an error message stating that they do not have enough quota for certain VM sizes, despite having enough quota (24 CPUs). The user is using Azure's Free trial plan and is seeking to understand the cause of the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437136\/cant-deploy-a-vm-on-the-azure-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2554944445,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=80eeb45c-8aa8-49ab-81df-1bb291fc79a5\">@ShoM  <\/a> ,    <\/p>\n<p>there are different quotas in Azure:    <\/p>\n<ul>\n<li> There are quotas for <code>vCPUs per Azure Region<\/code>    <\/li>\n<li> In addition there are quotas for <code>vCPUs per VM Series<\/code>    <\/li>\n<\/ul>\n<p>Both quotas (for Azure Region and VM Series) must fit the requirements.    <\/p>\n<p>It seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.    <br \/>\nYou can check your quotas by the link you marked with the red line in your screenshot.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":9.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1531218624572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":16.6869011111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to test different set of parameters in a ML algorithm using Optuna.<\/p>\n\n<p>The automatic sampling of Optuna is very useful, but is there any way to force one specific set of parameters into the proposed batch defined by Optuna?<\/p>\n\n<p>For example if I have a x,y parameters:<\/p>\n\n<pre><code>def objective(trial)\n   x = trial.suggest_uniform('x', -10, 10)\n   y = trial.suggest_uniform('x', -5, 5)\n   return (x+y-2)**2\nstudy = optuna.create_study(study_name='study_name')\nstudy.optimize(objective, n_trials=10)\n<\/code><\/pre>\n\n<p>I would also like to define one set of x=0.1, y=0.2 into the automatic generated one. Is this possible? <\/p>\n\n<p>It could be interesting to compare the \"intuitive\" values of some ML algorithm with other values.<\/p>",
        "Challenge_closed_time":1590642371747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590582298903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to test different sets of parameters in a machine learning algorithm using Optuna. They are looking for a way to force one specific set of parameters into the proposed batch defined by Optuna, in order to compare the \"intuitive\" values of some ML algorithm with other values.",
        "Challenge_last_edit_time":1590673455660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62043096",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":16.6869011111,
        "Challenge_title":"Force one specific set of parameters into the sampled batch",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes. One way to do this would be to use a <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/trial.html#optuna.trial.FixedTrial\" rel=\"nofollow noreferrer\">FixedTrial<\/a>, which would show you the result of your intuitive guess.<\/p>\n\n<p><code>print(objective(optuna.trial.FixedTrial({'x': 0.1, 'y': 0.2})))<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":4.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1527682322812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":31.9027575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Challenge_closed_time":1587720289803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587603987047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to log a SageMaker DeepAR model with MLFlow for version tracking, but the mlflow.sagemaker module does not have a \"log_model\" function. The user tried to use the mlflow.pyfunc module, but it did not work. The user is seeking help to log the SageMaker model and generate cloudpickle and yaml files with MLFlow.",
        "Challenge_last_edit_time":1587605439876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61377643",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":32.3063211111,
        "Challenge_title":"Tracking SageMaker Estimator with MLFlow",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548023586667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":336.0223277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Challenge_closed_time":1661377323147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660154954560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running a Sagemaker pipeline with two steps, tuning and training, and wants to use the best hyperparameters obtained from the tuning step in the next training step. They have tried to retrieve the hyperparameters using HyperparameterTuningJobAnalytics but are unable to pass them to the next training step's estimator due to an error message. The user is seeking suggestions to resolve this issue.",
        "Challenge_last_edit_time":1660167642767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":339.5468297222,
        "Challenge_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542606952710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.0410916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am encountering an issue of error 0138, while training the data, at the end it shows memory has been exhausted exception  <\/p>\n<p>I do not think my data has exceed the limit of azure ML studio, is there any way to solve this?<\/p>",
        "Challenge_closed_time":1653942581987,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653902834057,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an error 0138 while training data, which indicates a memory outage. They are unsure why this is happening as they believe their data has not exceeded the limit of Azure ML studio. They are seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869594\/memory-outage-while-running-module",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":3.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":11.0410916667,
        "Challenge_title":"memory outage while running module",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=d4454683-c6fe-4103-a1a8-d167ab8d04de\">@darya  <\/a>     <\/p>\n<p>Thanks for reaching out to us. This issue seldoms happen.  Could you please share your structure to us and how is your dataset size? Based on the error info, too many steps in your experiment may cause that.     <\/p>\n<p>I would suggest you try to remove some unnecessary one to try and see. If you believe your structure is reasonable, please share it to us. But it should be fine if you have not put too much.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer to help the community if you feel helpful, thanks.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":50.4269444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Challenge_closed_time":1615408051000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615226514000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the hydra config is no longer being saved to the wandb logger's config.yaml file. The user has provided examples of the previous and current states of the file and suspects that it may be related to a specific line of code in their project. The user is seeking advice on how to restore the previous state.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":47.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":50.4269444444,
        "Challenge_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":121.7916666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531232000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573092782000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the learning rate plot in Comet while trying to keep track of learning rate updates. The learning rate being plotted is not the expected one, especially when using the learning_rate_warmup_epochs option. The plotted learning rate is constant for the first few epochs and eventually decreases due to reduce_learning_rate_on_plateau. The user is unsure if this issue is related to the error message \"Failed to extract parameters from Optimizer.init()\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":121.7916666667,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":53.7558333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345866000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1660152345000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running the command `mlflow run . --experiement-name=psystock_data_pipelines` in Chapter 7 of a book. The error message indicated that a variable 'x' was not defined. The solution was to delete a line of code that contained a stray `raise` statement referencing the undefined variable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":53.7558333333,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":261,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":null,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1243547542743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6438.0,
        "Answerer_view_count":922.0,
        "Challenge_adjusted_solved_time":193.0081575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Challenge_closed_time":1449768300887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449073471520,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the Jupyter notebook kernel dies when attempting to create a dummy column for a column with 5,196 unique values. The resulting dataframe should have a shape of (647054, 5196), but the kernel dies almost every time the user runs the code. The user is unsure if this is a Jupyter notebook or pandas bug and has tried running the code on a machine with >100 GB of RAM with no success.",
        "Challenge_last_edit_time":1454300528203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":193.0081575,
        "Challenge_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5063.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373475615300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":2037.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":8.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":6.3089888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an R script in Azure Machine Learning that takes two inputs. I have since been working on a project that will take advantage of the webservice I created within Azure. When I use whole numbers as the values, everything works fine. In my C# code, these values are still doubles, and I use ToString to format them for the HTTP request.  I can send the data, and get 100% accurate results back. However, when I send values that actually contain digits after the decimal, I get a bad request response. I think the issue is with how the R script reads in from Azure Machine Learning inputs. So far I have this:<\/p>\n\n<pre><code>#R Script in Azure ML:\n1:    objCoFrame &lt;- maml.mapInputPort(2) # class: data.frame\n2:    objCoVector &lt;- as.vector(objCoFrame[1,])\n<\/code><\/pre>\n\n<p>which was doing the trick with integers. I have also tried <\/p>\n\n<pre><code>2:    objCoVector &lt;- as.vector(as.numeric(objCoFrame[1,]))\n<\/code><\/pre>\n\n<p>but got the same result.<\/p>\n\n<p>The Bad Request Response Content reads:<\/p>\n\n<pre><code>{\n    \"error\":\n    {\n        \"code\":\"BadArgument\",\n        \"message\":\"Invalid argument provided.\",\n        \"details\":\n        [{\n            \"code\":\"InputParseError\",\n            \"target\":\"rhsValues\",\n            \"message\":\"Parsing of input vector failed.  Verify the input vector has the correct number of columns and data types.  Additional details: Input string was not in a correct format..\"\n        }]\n    }\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1458012604667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1457989892307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Azure Machine Learning where they receive a bad request response when sending values with digits after the decimal point. The R script in Azure ML reads inputs in a certain way that works with integers but not with decimal values. The error message suggests that the input vector has the incorrect number of columns and data types.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35998155",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":17.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.3089888889,
        "Challenge_title":"Bad Request Response from Azure Machine Learning",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457988600436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Omaha, NE, USA",
        "Poster_reputation_count":380.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Can you force the type using Meta-Editor before passing to execute-R<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":0.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.0455230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1660860269032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to store weights locally and use the sweep id and\/or run id for naming. They are seeking guidance on how to access these within the train function they are using for the agent.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":39.0455230556,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":31.6572594445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Challenge_closed_time":1453832406127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453718439993,
        "Challenge_favorite_count":5.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Experiment that works fine, but when deployed and accessed through the front-end of their application, the API-call takes up to 30 seconds to calculate a simple linear regression. The user suspects that it may be due to the experiment still being in a development slot or being run on a slow machine. They are looking for a way to speed up the execution.",
        "Challenge_last_edit_time":1453911336527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":31.6572594445,
        "Challenge_title":"Azure Machine Learning Request Response latency",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1128.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446116840792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":311.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Solution_comment_count":11.0,
        "Solution_last_edit_time":1453911048927,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":16.44,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":218.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MlflowMetricsDataSet ignores the specified run_id when the prefix is not specified in the catalog, and instead uses the name in the catalog. This results in the current run_id overriding the specified run_id, causing the metric to be logged in a new run instead of the expected run. The bug also occurs in the latest version on develop.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":162.2517566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Challenge_closed_time":1611382397500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1610971789293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use Optuna for hyperparameter tuning of their model but is stuck on defining a search space with lognormal\/normal distribution, which is possible in hyperopt using hp.lognormal. The user is asking if it is possible to define such a space using a combination of the existing suggest_ api of Optuna.",
        "Challenge_last_edit_time":1610981620636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":114.0578352778,
        "Challenge_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445250318392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2164.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1611565726960,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":17.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":133.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5388561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650552651672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650439111790,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while fine-tuning multiple models using a for loop with Huggingface Trainer and Wandb. Although the code is training and saving models for all files in the data directory, Wandb is only showing logs for the first file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.1,
        "Challenge_reading_time":17.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.5388561111,
        "Challenge_title":"Wandb for Huggingface Trainer saves only first model",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.5462247223,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Challenge_closed_time":1652695869278,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652672302869,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using Sweeps to run through different configuration models and wants to know if there is an easier way to query the best model configuration and run predictions off it, instead of creating a new sweep with the best performing parameter set.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.5,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.5462247223,
        "Challenge_title":"Run best model off sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":8.17,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1498252453503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":298.7422783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Challenge_closed_time":1530057890672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529660522477,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to build a hyperparameter optimization job in Amazon Sagemaker using Python, but is encountering an error related to the 'tuner.py' file. The error message suggests that a 'str' object has no attribute 'keys', and the issue seems to be related to a 'list' object not having the required attribute.",
        "Challenge_last_edit_time":1530173488990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.8,
        "Challenge_reading_time":31.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":110.3800541667,
        "Challenge_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1531248961192,
        "Solution_link_count":1.0,
        "Solution_readability":25.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1592673441292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":38.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":0.1245758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Challenge_closed_time":1593695267950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593694819477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a problem with \"Execute Python\" scripts in Azure ML Studio (classic) as it takes additional time to perform internal tasks before executing the actual Python code. This delay is causing an increased time of 40-60 seconds per module, which is aggregating and causing a delay of 400-500 seconds per execution. The user is seeking help to understand the reason behind this delay and any optimization that can be done.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":11.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.1245758334,
        "Challenge_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":166.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582179684312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":601.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":12.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":162.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1598030987107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":171.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":76.2785638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Challenge_closed_time":1598032113043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597757510213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering issues while using Weights&Biases Cloud-based sweeps with Keras for K-Fold cross-validation. The user has created a new Sweep within a W&B Project with a specific configuration and a sweep.py file. However, the user is facing issues with the wandb.init() call, which seems to lose track of the sweep it is running in after the first call. The user's goal is to have a run for each fold of the K-Fold cross-validation, and they are seeking advice on how to combine sweeps with Keras K-Fold cross-validation.",
        "Challenge_last_edit_time":1661849871016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":76.2785638889,
        "Challenge_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1043.0,
        "Challenge_word_count":289,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486549300030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1599772735680,
        "Solution_link_count":6.0,
        "Solution_readability":16.9,
        "Solution_reading_time":22.33,
        "Solution_score_count":6.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":177.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1317.2311111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Challenge_closed_time":1563269566000,
        "Challenge_comment_count":13,
        "Challenge_created_time":1558527534000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the Sagemaker Remote Test log not being reported correctly. The CodeBuild logs show a \"Failed\" status, but the actual logs do not show a failure and terminate abruptly. The expected behavior is for the PR commit status to say \"Failed\" if the CodeBuild log says \"Failed,\" and for the CodeBuild log to print out the error instead of terminating abruptly. The issue is observed in two commits of the PR and is related to the MX 1.6 DLC image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Challenge_link_count":1,
        "Challenge_participation_count":13,
        "Challenge_readability":21.6,
        "Challenge_reading_time":18.74,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1317.2311111111,
        "Challenge_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @Jeffwan ,\r\n\r\nneed your support on this.\r\n\r\nI am using training image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" and it is throwing an error for mising values for parameters K and feature_dim. Although we are not using these parameters anywhere in pipeline.\r\n\r\nCan you please provide the solution ?\r\n\r\nRegards,\r\nVarun em. I may delete the configuration fields in clean up. Let me double check and come back to you @vackysh  I can reproduce this issue. \r\n\r\n`HyperParameters` was removed by me in this commit\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/commit\/26f2719c28a731d8925ae2ce96252be1df2562aa\r\n\r\nAdd it back will solve this problem Image has been rebuilt and it should be good now.  Hi @Jeffwan ,\r\n\r\nThanks for your response.\r\n\r\nI again executed the pipeline using image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" , but getting the same issue\r\n\r\n\"Training failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\"\r\n\r\nThe pipeline parameters are:\r\n\r\n@dsl.pipeline(\r\nname='MNIST Classification pipeline',\r\ndescription='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\nimage='382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1',\r\ndataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\ninstance_type='ml.c4.8xlarge',\r\ninstance_count='2',\r\nvolume_size='50',\r\nmodel_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\nbatch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\nbatch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\nrole_arn=''\r\n):\r\n\r\nPlease suggest how to get through it if issue has already fixed at your end.\r\n @vackysh I think the problem is your machine already has this image. could you go to the machine and do a force pull? \r\n```\r\nseedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05\r\n``` HI @Jeffwan ,\r\n\r\nI refreshed the image It is now working fine.\r\nThank you so much.\r\n\r\nRegards,\r\nVarun Hi @Jeffwan,\r\n\r\nWhere can i get the actual source code (ML code ) reading from the image seedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05 (not a docker file, but actual logic for train, predction) ?\r\n\r\nI actually working on similar automation and want to analyse the source code.\r\n\r\nRegards,\r\nVarun @vackysh This is a component example for training. \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/train.py\r\n\r\nNot sure if your work is internal or public. It would be perfect if you can make some contribution! Feel free to ping me on Slack or shoot me an email. Hi @Jeffwan  ,\r\n\r\nThanks for information. But i am looking for the main Kmean algorithms code that is used for training the model. I couldn't find that anywhere on path.\r\n\r\nI have a requirement where Scikit SVM model to get deploy on kubeflow pipeline using aws sagemaker services and S3. So i want to have a look on source ML code that has been passed through image as an input to pipeline.\r\n\r\nRegards,\r\nVarun\r\n\r\n @vackysh Now I get your point, in the example, I am using the container images from SageMaker. I think KMEANS one is first-party models and you might don't have access to it. What I suggest you to do is bring your own training image if you have customization request. This issue is resolved. Now closing > This issue is resolved. Now closing\r\n\r\nHave you figured out a way to make it? Did you try bring your own container? ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.1,
        "Solution_reading_time":43.32,
        "Solution_score_count":null,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.5733333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Challenge_closed_time":1666184788000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1666114324000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The Neptune ML Export widget is throwing an error when the user tries to export data using a specific command from the Node Classification notebook. The error message states that the credential should be scoped to the correct service, 'execute-api'. The expected behavior is for the export to run to completion.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.5733333333,
        "Challenge_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for opening your first issue here!\n Any solution for this? > Any solution for this?\r\n\r\nThe error message was a bit of a red herring. The actual problem was amundsen-gremlin isn't installed as part of the base image creation `amundsendev\/amundsen-metadata`.\r\n\r\nSolution 1 - add the package to the requirements files and rebuild your own Amundsen image\r\n\r\nSolution 2 - build on the base image and add a `RUN pip install amundsen-gremlin` to your bespoke dockerfile. For my use case I've gone with the latter.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.16,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":22.0598044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use optuna lib in Python to optimise parameters for recommender systems' models. Those models are custom and look like standard fit-predict sklearn models (with methods get\/set params). <\/p>\n\n<p>What I do: simple objective function that selects two parameters from uniform int distribution, set these params to model, predicts the model (there no fit stage as it simple model that uses params only in predict stage) and calculates some metric. <\/p>\n\n<p>What I get: the first trial runs normal, it samples params and prints results to log. But on the second and next trial I have some strange errors (look code below) that I can't solve or google. When I run study on just 1 trial everything is okay.<\/p>\n\n<p>What I tried: to rearrange parts of objective function, put fit stage inside, try to calculate more simpler metrics - nothing helps. <\/p>\n\n<p>Here is my objective function: <\/p>\n\n<pre><code># getting train, test\n# fitting model\nself.model = SomeRecommender()\nself.model.fit(train, some_other_params)\n\ndef objective(trial: optuna.Trial):\n    # save study\n    if path is not None:\n        joblib.dump(study, some_path)\n\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # setting params to model\n    params = {'alpha': alpha,\n              'beta': beta}\n    self.model.set_params(**params)\n\n    # getting predict\n    recs = self.model.predict(some_other_params)\n\n    # metric computing\n    metric_result = Metrics.hit_rate_at_k(recs, test, k=k)\n\n    return metric_result\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>That's what I get on three trials:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>[I 2019-10-01 12:53:59,019] Finished trial#0 resulted in value: 0.1. Current best value is 0.1 with parameters: {'alpha': 59.6135986324444, 'beta': 40.714559720597585}.\n[W 2019-10-01 13:39:58,140] Setting status of trial#1 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n[W 2019-10-01 13:39:58,206] Setting status of trial#2 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n<\/code><\/pre>\n\n<p>I can't understand where is the problem and why the first trial is working. Please, help. <\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1570006754996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569926663223,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the optuna library in Python to optimize parameters for recommender systems' models. The first trial runs normally, but on the second and subsequent trials, the user encounters an error that they cannot solve or find on Google. The error message is \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\". The user has tried rearranging parts of the objective function, putting the fit stage inside, and calculating simpler metrics, but nothing has helped.",
        "Challenge_last_edit_time":1569927339700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58183158",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":61.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.2477147222,
        "Challenge_title":"How to fix error \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\" - strange behaviour in optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":666.0,
        "Challenge_word_count":446,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Your code seems to have no problems.<\/p>\n\n<p>I ran a simplified version of your code (see below), and it worked well in my environment:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # evaluating params\n    return alpha + beta\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>Could you tell me about your environment in order to investigate the problem? (e.g., OS, Python version, Python interpreter (CPython, PyPy, IronPython or Jython), Optuna version)<\/p>\n\n<blockquote>\n  <p>why the first trial is working.<\/p>\n<\/blockquote>\n\n<p>This error is raised by <a href=\"https:\/\/github.com\/pfnet\/optuna\/blob\/389a176c8cd1c860001a7a4562670006643e5e11\/optuna\/samplers\/tpe\/sampler.py#L558\" rel=\"noreferrer\">optuna\/samplers\/tpe\/sampler.py#558<\/a>, and this line is only executed when the number of completed trials in the study is greater than zero.<\/p>\n\n<p>BTW, you might be able to avoid this problem by using <code>RandomSampler<\/code> as follows:<\/p>\n\n<pre><code>sampler = optuna.samplers.RandomSampler()\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\n<\/code><\/pre>\n\n<p>Notice that the optimization performance of <code>RandomSampler<\/code> tends to be worse than <code>TPESampler<\/code> that is the default sampler of Optuna.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":18.86,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":153.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1416075035300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL",
        "Answerer_reputation_count":3492.0,
        "Answerer_view_count":133.0,
        "Challenge_adjusted_solved_time":6.5771875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to hyper-parameter optimize multiple time series forecasting models on the same data. I'm using the Optuna Sweeper plugin for Hydra. The different models have different hyper-parameters and therefore different search spaces. At the moment my config file looks like this:<\/p>\n<pre><code>defaults:\n  - datasets: data\n  - models: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n#  launcher:\n#    n_jobs: 10\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: \n\n# Ets\n    models.damped_trend:\n      type: categorical\n      choices:\n      - 'True'\n      - 'False'\n\n  # Theta\n    # models.method:\n    #   type: categorical\n    #   choices:\n    #   - 'additive'\n    #   - 'multiplicative' \n<\/code><\/pre>\n<p>Now, when I run the main_val.py file with --multirun, I get the optimal hyper-parameters for Ets. Great. But when I want to run the optimization for another model, in this example Theta, I have to manually comment out the search space for Ets and uncomment the search space for Theta. In reality, each model has much more parameters to optimize and I'm working with 10 different models. This makes my config file quite long and confusing and this commenting\/uncommenting stuff is both annoying and error-prone.<\/p>\n<p>I would like to import the search space for each model from another yaml file. Is that possible?<\/p>\n<p>I tried the following:<\/p>\n<pre><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n#  launcher:\n#    n_jobs: 10\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: search_spaces\n<\/code><\/pre>\n<p>with the file search_spaces\/Ets.yaml looking like this:<\/p>\n<pre><code>models.damped_trend:\n  type: categorical\n  choices:\n  - 'True'\n  - 'False'\n<\/code><\/pre>\n<p>But I got the error:<\/p>\n<pre><code>Validation error while composing config:\n    Cannot assign str to Dict[str, Any]\n        full_key: hydra.sweeper.search_space\n        object_type=OptunaSweeperConf\n<\/code><\/pre>",
        "Challenge_closed_time":1646682876732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646659198857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using the Optuna Sweeper plugin for Hydra to optimize hyper-parameters for multiple time series forecasting models. Each model has different hyper-parameters and search spaces. The user wants to import the search space for each model from another YAML file to avoid manually commenting and uncommenting the search space for each model in the config file. However, the user encountered an error while trying to import the search space from another YAML file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71381726",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":6.5771875,
        "Challenge_title":"When using the optuna plugin for hydra, can I import the search space from another config file?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":404.0,
        "Challenge_word_count":284,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646657863040,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Here are two options:<\/p>\n<ol>\n<li>Use a <a href=\"https:\/\/hydra.cc\/docs\/advanced\/overriding_packages\/#default-list-package-keywords\" rel=\"nofollow noreferrer\"><code>@package<\/code><\/a> directive<\/li>\n<li>Use a <a href=\"https:\/\/omegaconf.readthedocs.io\/en\/2.1_branch\/usage.html#variable-interpolation\" rel=\"nofollow noreferrer\">variable interpolation<\/a><\/li>\n<\/ol>\n<p>In detail:<\/p>\n<h2>Using an <code>@package<\/code> directive<\/h2>\n<p>An <code>@package<\/code> directive can be used to place <code>Ets.yaml<\/code> in the <code>hydra.sweeper.search_space<\/code> package:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces@hydra.sweeper.search_space: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n<\/code><\/pre>\n<h2>Using a variable interpolation:<\/h2>\n<p>A string interpolation can be used to create a reference from <code>hydra.sweeper.search_spaces<\/code> to the top-level <code>search_spaces<\/code> config.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: ${search_spaces}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":21.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":130.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.8680555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can I train models in parallel? Is is possible to train model in parallel on like hyperdrive?<\/p>",
        "Challenge_closed_time":1653922130807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653904605807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training models in parallel, similar to using hyperdrive.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869619\/parallel-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.8680555556,
        "Challenge_title":"Parallel training",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3d6a9d61-6cf9-45d8-870d-2fbbf147f56d\">@Chungsun  <\/a>  Thanks for the question. The max number of parallel tasks is limited by number of cores in the cluster (excluding master node).    <br \/>\nThe demand for parallelism comes from two sources: 1. The cross validation which address multiple combination of train-val datasets &amp; parameters 2. The training algorithm itself which can be parallelized.    <\/p>\n<p>\u2022\tYou can run multiple runs in a distributed fashion across AML clusters, meaning that each cluster node can be running a run in parallel to other nodes running other runs. For instance, that\u2019s what we also do with Pipeline steps, HyperParameter Tunning child runs and for Azure AutoML child runs.    <\/p>\n<p> <a href=\"https:\/\/github.com\/microsoft\/solution-accelerator-many-models\"> https:\/\/aka.ms\/many-models<\/a> is a solution accelerator that will help you walk through to run many models.     <br \/>\nIn the HyperDriveConfig there is AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run. So there would be 1 execution per node.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4162452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/172214-image.png?platform=QnA\" alt=\"172214-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1644316889220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644315390737,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's Azure subscription cost is decreasing every day, even though they have deleted everything from their workspace and do not have any cluster in their AzureML workspace. The user is unsure why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/726898\/azure-subscription-cost",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":4.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.4162452778,
        "Challenge_title":"Azure Subscription Cost",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>If you want to review your costs and what resources are being charged, then the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview#understand-cost-management\">Cost Analysis blade<\/a> will allow you to drill down work this out. Please let us know if this helps    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1612454694036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mexico City, CDMX, Mexico",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3418.0769397223,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Challenge_closed_time":1614800344830,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602495267847,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has encountered an error \"list index out of range\" while sending data to an ML model deployed on Azure ML Studio via REST API after adding an inference schema to allow compatibility with Power BI. The deployment is designated as \"healthy\" with no error messages. The user has provided the entry and prediction scripts for reference.",
        "Challenge_last_edit_time":1615561798207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":48.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":3418.0769397223,
        "Challenge_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":785.0,
        "Challenge_word_count":386,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1614801695372,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":40.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":253.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.5791947222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>FLAML looks like it performs better than Azure AutoML for hyperparameter tuning (based on the benchmarking in the Arxiv paper): <a href=\"https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf\">https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf<\/a>  <\/p>\n<p>Is it now being used or is there a plan to integrate it for the hyperparameter tuning in Azure Machine Learning Services? If so, when is that expected to become available?<\/p>",
        "Challenge_closed_time":1623391032688,
        "Challenge_comment_count":1,
        "Challenge_created_time":1623298947587,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether Azure AutoML uses or plans to use FLAML for hyperparameter tuning, as FLAML has been shown to perform better in benchmarking. The user is asking if there is a plan to integrate FLAML into Azure Machine Learning Services and when it will become available.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/429832\/does-azure-automl-use-(or-plan-to-use)-flaml-for-t",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":25.5791947222,
        "Challenge_title":"Does Azure AutoML use (or plan to use) FLAML for the hyperparameter tuning?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=44a7ffc5-e97c-4dec-95a0-445a9835aab3\">@Rainer Hillermann  <\/a> Thanks, We are not using the FLAML for Azure AutoML for the hyperparameter tuning, You can raise a user voice request <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\">here<\/a> so the community can vote and provide their feedback, the product team then checks this feedback and implements the feature in future releases.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":5.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":945.8408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Challenge_closed_time":1564215404000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1560810377000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a pipeline compile error while trying to run a Kubeflow\/SageMaker notebook in their workshop.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":41.03,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":945.8408333333,
        "Challenge_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":266,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Note: adding tensorflow-gpu==1.11.0 and rebuilding the image solves the issue Image has been updated for this",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.38,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":755.2722222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221269000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1612502289000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error with CometLogger when using an API key without a save directory. The error occurs because the train loop tries to read the save directory, which is not set. The issue can be resolved by setting the save directory to None.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":755.2722222222,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":21.92,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":326.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1303910479480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4196.0,
        "Answerer_view_count":67.0,
        "Challenge_adjusted_solved_time":3788.8637213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Challenge_closed_time":1631884865500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618244956103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running experiments on AWS spot instances and sometimes the experiments are stopped. They want to know how to continue logging to the same run and set the run-id of the active run. They have provided pseudocode but it is not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3788.8637213889,
        "Challenge_title":"Continue stopped run in MLflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6278808333,
        "Challenge_answer_count":1,
        "Challenge_body":"[This is a duplicate of a question I asked on stack overflow](https:\/\/stackoverflow.com\/questions\/75043118\/sagemaker-batch-transform-job-upstream-prematurely-closed-connection-when-surp)\n\nI am serving a sagemaker model through a custom docker container using [the guide that AWS provides](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html#When-should-I-build-my-own-algorithm-container%3F). This is a docker container that runs a simple nginx->gunicorn\/wsgi->flask server\n\nI am facing an issue where my transform requests time out around 30 minutes in all instances, despite should being able to continue to 60 minutes. I need requests to be able to go to sagemaker maximum of 60 minutes due to data intense nature of request.\n\n\n----------\n\n\nThrough experience working with this setup for some months, I know that there are 3 factors that should affect the time my server has to respond to requests:\n\n 1. Sagemaker itself will cap invocations requests according to the\n    `InvocationsTimeoutInSeconds` paremeter set when [creating the batch\n    transform\n    job](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ModelClientConfig.html#sagemaker-Type-ModelClientConfig-InvocationsTimeoutInSeconds).\n 2. The `nginx.conf` file must be configured such that `keepalive_timeout`, `proxy_read_timeout`, `proxy_send_timeout`, and `proxy_connect_timeout` are all equal or greater than maximum timeout\n 3. gunicorn server must its timeout configured to be equal or greater than maximum timeout\n\n\n----------\n\n\nI have verified that when I create my batch transform job `InvocationsTimeoutInSeconds` is set to 3600 (1 hour)\n\nMy nginx.conf looks like this:\n\n    worker_processes 1;\n    daemon off; # Prevent forking\n    \n    \n    pid \/tmp\/nginx.pid;\n    error_log \/var\/log\/nginx\/error.log;\n    \n    events {\n      # defaults\n    }\n    \n    http {\n      include \/etc\/nginx\/mime.types;\n      default_type application\/octet-stream;\n      access_log \/var\/log\/nginx\/access.log combined;\n    \n      sendfile        on;\n      client_max_body_size 30M;\n      keepalive_timeout  3920s;\n      \n      upstream gunicorn {\n        server unix:\/tmp\/gunicorn.sock;\n      }\n    \n      server {\n        listen 8080 deferred;\n        client_max_body_size 80m;\n    \n        keepalive_timeout 3920s;\n        proxy_read_timeout 3920s;\n        proxy_send_timeout 3920s;\n        proxy_connect_timeout 3920s;\n        send_timeout 3920s;\n    \n        location ~ ^\/(ping|invocations) {\n          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n          proxy_set_header Host $http_host;\n          proxy_redirect off;\n          proxy_pass http:\/\/gunicorn;\n        }\n    \n        location \/ {\n          return 404 \"{}\";\n        }\n      }\n    }`\n\nI start the gunicorn server like this:\n\n    def start_server():\n        print('Starting the inference server with {} workers.'.format(model_server_workers))\n        print('Model server timeout {}.'.format(model_server_timeout))\n    \n        # link the log streams to stdout\/err so they will be logged to the container logs\n        subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n        subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n    \n        nginx = subprocess.Popen(['nginx', '-c', '\/opt\/program\/nginx.conf'])\n        gunicorn = subprocess.Popen(['gunicorn',\n                                     '--timeout', str(3600),\n                                     '-k', 'sync',\n                                     '-b', 'unix:\/tmp\/gunicorn.sock',\n                                     '--log-level', 'debug',\n                                     '-w', str(1),\n                                     'wsgi:app'])\n    \n        signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n    \n        # If either subprocess exits, so do we.\n        pids = set([nginx.pid, gunicorn.pid])\n        while True:\n            pid, _ = os.wait()\n            if pid in pids:\n                break\n    \n        sigterm_handler(nginx.pid, gunicorn.pid)\n        print('Inference server exiting')\n\nDespite all this, whenever a transform job takes longer than approx 30 minutes I will see this message in my logs and the transform job status becomes failed: \n\n    2023\/01\/07 08:23:14 [error] 11#11: *4 upstream prematurely closed connection while reading response header from upstream, client: 169.254.255.130, server: , request: \"POST \/invocations HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/invocations\", host: \"169.254.255.131:8080\"\n\nI am close to thinking there is a bug in AWS batch transform, but perhaps I am missing some other variable (perhaps in the nginx.conf) that could lead to premature upstream termination of my request.",
        "Challenge_closed_time":1673126209340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673120348969,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker Batch Transform where requests time out around 30 minutes despite being able to continue to 60 minutes. The user has verified that the `InvocationsTimeoutInSeconds` parameter is set to 3600, and the `nginx.conf` file and gunicorn server are configured to have a timeout equal to or greater than the maximum timeout. However, whenever a transform job takes longer than approximately 30 minutes, the transform job status becomes failed with the error message \"upstream prematurely closed connection while reading response header from upstream.\" The user is unsure if there is a bug in AWS batch transform or if they are missing some other variable in the `nginx.conf` file that could lead to premature upstream termination of the",
        "Challenge_last_edit_time":1673467665391,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbey_TyxSRrSsZ0XG99jnDQ\/sagemaker-batch-transform-upstream-prematurely-closed-connection-unable-to-serve-requests-that-take-longer-than-30-minutes",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":54.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":1.6278808333,
        "Challenge_title":"Sagemaker Batch Transform - \"upstream prematurely closed connection\" - Unable to serve requests that take longer than 30 minutes",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":450,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"By looking at hardware metrics was able to determine that the upstream termination only happens when the server was near its memory limit. So my guess is that the OS was killing the gunicorn worker and the 30 minute mark was just a coincidence that happened on my long running test cases.\n\nMy solution was to increase the memory available on the server",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1673126209340,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.7425963889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>For runs I do:<\/p>\n<pre><code class=\"lang-auto\">wandb.run.get_url()\n<\/code><\/pre>\n<p>how do I do the same but for sweeps given the <code>sweep_id<\/code>?<\/p>\n<hr>\n<p>fulls sample run:<\/p>\n<pre><code class=\"lang-auto\">\"\"\"\nMain Idea:\n- create sweep with a sweep config &amp; get sweep_id for the agents (note, this creates a sweep in wandb's website)\n- create agent to run a setting of hps by giving it the sweep_id (that mataches the sweep in the wandb website)\n- keep running agents with sweep_id until you're done\n\nnote:\n    - Each individual training session with a specific set of hyperparameters in a sweep is considered a wandb run.\n\nref:\n    - read: https:\/\/docs.wandb.ai\/guides\/sweeps\n\"\"\"\n\nimport wandb\nfrom pprint import pprint\nimport math\nimport torch\n\nsweep_config: dict = {\n    \"project\": \"playground\",\n    \"entity\": \"your_wanbd_username\",\n    \"name\": \"my-ultimate-sweep\",\n    \"metric\":\n        {\"name\": \"train_loss\",\n         \"goal\": \"minimize\"}\n    ,\n    \"method\": \"random\",\n    \"parameters\": None,  # not set yet\n}\n\nparameters = {\n    'optimizer': {\n        'values': ['adam', 'adafactor']}\n    ,\n    'scheduler': {\n        'values': ['cosine', 'none']}  # todo, think how to do\n    ,\n    'lr': {\n        \"distribution\": \"log_uniform_values\",\n        \"min\": 1e-6,\n        \"max\": 0.2}\n    ,\n    'batch_size': {\n        # integers between 32 and 256\n        # with evenly-distributed logarithms\n        'distribution': 'q_log_uniform_values',\n        'q': 8,\n        'min': 32,\n        'max': 256,\n    }\n    ,\n    # it's often the case that some hps we don't want to vary in the run e.g. num_its\n    'num_its': {'value': 5}\n}\nsweep_config['parameters'] = parameters\npprint(sweep_config)\n\n# create sweep in wandb's website &amp; get sweep_id to create agents that run a single agent with a set of hps\nsweep_id = wandb.sweep(sweep_config)\nprint(f'{sweep_id=}')\n\n\ndef my_train_func():\n    # read the current value of parameter \"a\" from wandb.config\n    # I don't think we need the group since the sweep name is already the group\n    run = wandb.init(config=sweep_config)\n    print(f'{run=}')\n    pprint(f'{wandb.config=}')\n    lr = wandb.config.lr\n    num_its = wandb.config.num_its\n\n    train_loss: float = 8.0 + torch.rand(1).item()\n    for i in range(num_its):\n        # get a random update step from the range [0.0, 1.0] using torch\n        update_step: float = lr * torch.rand(1).item()\n        wandb.log({\"lr\": lr, \"train_loss\": train_loss - update_step})\n    run.finish()\n\n\n# run the sweep, The cell below will launch an agent that runs train 5 times, usingly the randomly-generated hyperparameter values returned by the Sweep Controller.\nwandb.agent(sweep_id, function=my_train_func, count=5)\n<\/code><\/pre>\n<p>cross: <a href=\"https:\/\/stackoverflow.com\/questions\/75852199\/how-do-i-print-the-wandb-sweep-url-in-python\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">machine learning - How do I print the wandb sweep url in python? - Stack Overflow<\/a><\/p>",
        "Challenge_closed_time":1679934449664,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679892176317,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to print the URL for a sweep in WandB using Python, similar to how they print the URL for a run. They have provided a sample code for creating a sweep and running agents with a set of hyperparameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-print-the-wandb-sweep-url-in-python\/4133",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":11.0,
        "Challenge_reading_time":34.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":11.7425963889,
        "Challenge_title":"How do I print the wandb sweep url in python?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":227.0,
        "Challenge_word_count":335,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>like <code>wandb.get_sweep_url()<\/code>? Thanks!<\/p>\n<p><a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run?_gl=1*uk130d*_ga*MTYwMTE3MDYzNS4xNjUyMjI2MTE1*_ga_JH1SJHJQXJ*MTY4MDAxNTk2Ny4yNjguMS4xNjgwMDE2MTMyLjQ3LjAuMA\" class=\"inline-onebox\">Run | Weights &amp; Biases Documentation<\/a>\u2026<span class=\"hashtag\">#get_sweep_url<\/span><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":36.1,
        "Solution_reading_time":4.66,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1631000506000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running train_model from examples after installing graphnet from scratch and signing up to WandB. The error occurred due to the absence of a directory called \"wandb\" and can be fixed by creating the folder manually. The user suggests automatically creating the folder if it is not present.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":10471.2408333333,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":188.0286397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Challenge_closed_time":1645569051340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892148237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble creating a sagemaker endpoint using an image from the AWS deep learning containers repository due to the image size being greater than the supported size. The user is seeking a way to determine the size of images provided by AWS managed images or the deep learning containers repository.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":18.6,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":188.0286397222,
        "Challenge_title":"How to determine size of images available in aws?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2191666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\n**Do we have a best practice for that?**\nIn the [`create-presigned-notebook-instance-url`][1] command, what is the `--session-expiration-duration-in-seconds`: is it the validity duration of the URL or the max session duration once the URL has been clicked?\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html",
        "Challenge_closed_time":1543951736000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543947347000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on accessing SageMaker Notebooks without accessing the console and is asking for best practices. They also have a question regarding the `create-presigned-notebook-instance-url` command and the `--session-expiration-duration-in-seconds` parameter.",
        "Challenge_last_edit_time":1668556320118,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sagemaker-notebooks-without-accessing-the-console",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.2191666667,
        "Challenge_title":"Accessing SageMaker Notebooks without accessing the console",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":481.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form:\n`https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>`\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of  `[1800, 43200]`  in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565195,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":8.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":118.9246497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've run an automl job at AWS Autopilot using the F1 metric. I'd like to see the value of precision and recall too. How?<\/p>",
        "Challenge_closed_time":1578755923276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578327794537,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run an automl job at AWS Autopilot using the F1 metric and wants to know how to view the precision and recall values.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59615549",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":118.9246497222,
        "Challenge_title":"Precision and recall at AWS Autopilot",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499272954707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Depending on the algo used, they might be visible in the training log of the top candidate.<\/p>\n\n<p>What you could also do is keep a test set on the side, and use it to compute precision, recall and other metrics on the trained model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":2.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464391892936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Challenge_adjusted_solved_time":14.8198297223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've gone through the pricing page of Sagemaker at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a>. I could not find the information on how it charges the cost of emitting logs to cloudwatch. Does the price listed include the price of emitting logs or are they charged according to Cloudwatch log pricing separately?<\/p>",
        "Challenge_closed_time":1565290116270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565236764883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether the pricing for Amazon Sagemaker includes the cost of emitting logs to Cloudwatch or if it is charged separately.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57405022",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":5.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":14.8198297223,
        "Challenge_title":"Amazon Sagemaker pricing of cloudwatch",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1391126150232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":322.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>All AWS Services that uses Cloudwatch you will be charged according to Cloudwatch log pricing.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.28,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":5.6782083333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Challenge_closed_time":1561838537720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561818096170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, they encountered an error stating that the SKLearn NearestNeighbors learner does not have a predict method. The user is now trying to find a way to call this endpoint within Sagemaker or determine if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":5.6782083333,
        "Challenge_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":694.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511812140112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":169.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458983415440,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cluj-Napoca, Romania",
        "Answerer_reputation_count":12439.0,
        "Answerer_view_count":1795.0,
        "Challenge_adjusted_solved_time":30.66359,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Challenge_closed_time":1659781694627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659671305703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up a trainer with arguments to report to wandb, but when they set up the trainer, it shows an error stating that \"w\" is not supported, and only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.",
        "Challenge_last_edit_time":1660033763876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.9,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":30.66359,
        "Challenge_title":"HuggingFace Trainer() cannot report to wandb",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587186468848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Taipei, Taiwan R.O.C",
        "Poster_reputation_count":163.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":5.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1373018880576,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":17.1053752778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Challenge_closed_time":1543906305888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543844726537,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is following AWS Sagemaker documentation to train and deploy an ML model using the high-level Python library provided by Amazon SageMaker. However, the deployment fails with an error message indicating that the account-level service limit for endpoint usage has been exceeded. The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.1053752778,
        "Challenge_title":"AWS Sagemaker Deploy fails",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5932.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373018880576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":305.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.0,
        "Solution_reading_time":2.34,
        "Solution_score_count":7.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2754447222,
        "Challenge_answer_count":14,
        "Challenge_body":"<p>I followed the usual instructions:<\/p>\n<pre><code class=\"lang-auto\">pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code class=\"lang-auto\">(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<aside class=\"onebox stackexchange\" data-onebox-src=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Charlie Parker\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e9c0a0caedbda92f5ad9bc087e52e143936f9f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  <\/a>\n\n<h4>\n  <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">Wandb automatically logeed into the wrong user -- why?<\/a>\n<\/h4>\n\n<div class=\"tags\">\n  <strong>wand<\/strong>\n<\/div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Charlie Parker\n  <\/a>\n  on <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">02:32PM - 12 Aug 22 UTC<\/a>\n<\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1660319389712,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660314798111,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user installed Wandb and logged in, but the API key was automatically logged in as the wrong user, and the user is unsure how to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916",
        "Challenge_link_count":9,
        "Challenge_participation_count":14,
        "Challenge_readability":14.3,
        "Challenge_reading_time":34.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":1.2754447222,
        "Challenge_title":"Wandb automatically logeed into the wrong user -- why?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1783.0,
        "Challenge_word_count":225,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<aside class=\"quote no-group\" data-username=\"brando\" data-post=\"9\" data-topic=\"2916\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/brando\/40\/199_2.png\" class=\"avatar\"> brando:<\/div>\n<blockquote>\n<p>But that is weird, I\u2019ve ran it from pycharm debugger before\u2026<img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/confused.png?v=12\" title=\":confused:\" class=\"emoji\" alt=\":confused:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<\/blockquote>\n<\/aside>\n<p>First make sure your <code>.netrc<\/code> file looks right. Login in as instructed in wandb and its fine to relogin. Make sure it still looks fine. Make sure you update wandb with pip. You can set the env variable with your key too. Update pycharm. Remove all the .idea folders in your projects and start from scratch. Make sure your pycharm projs have the right path to the python interpreter form pycharm. Thats I think what worked\u2026<\/p>\n<pre><code class=\"lang-auto\">(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/ultimate-utils\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/iit-term-synthesis\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/pycoq\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/data\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/proverbot\/.idea\n<\/code><\/pre>\n<p>this was useful:<\/p>\n<pre><code class=\"lang-auto\">run_bash_command('pip install wandb --upgrade')\ncat_file('~\/.zshrc')\ncat_file('~\/.netrc')\n\nwandb.init(project=\"proof-term-synthesis\", entity=\"brando\", name='run_name', group='expt_name')\n\nprint('success!\\a')\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":23.42,
        "Solution_score_count":null,
        "Solution_sentence_count":18.0,
        "Solution_word_count":158.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":5.0310547222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When performing a single-objective optimization with <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Optuna<\/a>, the best parameters of the study are accessible using:<\/p>\n<pre><code>import optuna\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    return (x - 2) ** 2\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2.002108042}\n<\/code><\/pre>\n<p>If I want to perform a multi-objective optimization, this would be become for example :<\/p>\n<pre><code>import optuna\ndef multi_objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    f1 = (x - 2) ** 2\n    f2 = -f1\n    return f1, f2\n\nstudy = optuna.create_study(directions=['minimize', 'maximize'])\nstudy.optimize(multi_objective, n_trials=100)\n<\/code><\/pre>\n<p>This works, but the command <code>study.best_params<\/code> fails with <code>RuntimeError: The best trial of a 'study' is only supported for single-objective optimization.<\/code><\/p>\n<p>How can I get the best parameters for a multi-objective optimization ?<\/p>",
        "Challenge_closed_time":1611273369707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611255257910,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in obtaining the best parameters for a multi-objective optimization using Optuna. While the best parameters can be easily accessed for a single-objective optimization, the command fails for multi-objective optimization, resulting in a runtime error. The user is seeking a solution to obtain the best parameters for multi-objective optimization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65833998",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.0310547222,
        "Challenge_title":"Best parameters of an Optuna multi-objective optimization",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2571.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588846413276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":108.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>In multi-objective optimization, you often end up with more than one best trial, but rather a set of trials. This set if often referred to as the Pareto front. You can get this Pareto front, or the list of trials, via <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.Study.html#optuna.study.Study.best_trials\" rel=\"noreferrer\"><code>study.best_trials<\/code><\/a>, then look at the parameters from each individual trial i.e. <code>study.best_trials[some_index].params<\/code>.<\/p>\n<p>For instance, given your directions of minimizing <code>f1<\/code> and maximizing <code>f2<\/code>, you might end up with a trial that has a small value for <code>f1<\/code> (good) but at the same time small value for <code>f2<\/code> (bad) while another trial might have a large value for both <code>f1<\/code> (bad) and <code>f2<\/code> (good). Both of these trials could be returned from <code>study.best_trials<\/code>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":12.09,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":115.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1396913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi team,     <br \/>\nI am learning about the quota for machine learning service and I have a general doubt.    <\/p>\n<p>I can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.    <br \/>\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.     <\/p>\n<p>so that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.    <\/p>\n<p>I am able to find out  the-  get details query but this one doesnt give me details specific to each resource-group or the workspace.    <\/p>\n<p>HTTP query -&gt; <a href=\"https:\/\/management.azure.com\/subscriptions\/%7Bsubs_id%7D\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01\">https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01<\/a><\/p>",
        "Challenge_closed_time":1667069947336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667069444447,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to set quotas for CPU cores at the resource group level in Azure Machine Learning Service. They have a subscription level total CPU cores quota of 10 and want to assign 5 cores each to two resource groups under that subscription to ensure that if all the cores are taken up by the resources under one resource group, the other resource group or the ML workspace under it should not suffer. The user is unable to find specific details for each resource group or workspace using the provided HTTP query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":13.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1396913889,
        "Challenge_title":"how to set Quota at resource group level?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7bafa6a3-9285-4b1c-a003-4490a74d05b5\">@JA  <\/a> ,    <\/p>\n<p>quotas can be set on Azure Subscription level only.    <br \/>\nThere is no option to apply quotas for different Azure Resource Groups.    <br \/>\nThere are 2 options I can see for your requirement:    <br \/>\nUse 2 Azure Subscriptions for each Resource Group    <br \/>\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.6524075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.<\/p>",
        "Challenge_closed_time":1666266282800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666227934133,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up distributed dask on Azure Machine Learning compute due to the need for manual configuration. They are seeking information on whether there is native support available for this process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1055350\/ray-dask-native-support-be-added-to-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":10.6524075,
        "Challenge_title":"ray+dask native support  be added to Azure Machine Learning",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. you can do is to setup the compute cluster &amp; compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.    <\/p>\n<p>Here is the document Library to turn Azure ML Compute into Ray and Dask cluster.    <br \/>\n<a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784\">https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":5.6903283333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Challenge_closed_time":1648538467292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648517982110,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to view all the service quota limits for different SageMaker services, including training, CreateTransformJob, and spot vs not dedicated. They are seeking information on where to find the current service limits for SageMaker services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6903283333,
        "Challenge_title":"How to see all SageMaker service quota limits?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.6,
        "Solution_reading_time":5.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":2.4121063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Azure Machine Learning has an item called <code>Train Matchbox Recommender<\/code>. It can be configured with a <code>Number of traits<\/code>. Unfortunately, the documentation does not describe what such a trait is.<\/p>\n\n<p>What are traits? Is this related to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_variable\" rel=\"nofollow noreferrer\">latent variables<\/a>?<\/p>",
        "Challenge_closed_time":1525171563376,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525162879793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the meaning of \"traits\" in Azure ML's Train Matchbox Recommender item, as the documentation does not provide a clear definition.",
        "Challenge_last_edit_time":1526046231816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50113374",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.4121063889,
        "Challenge_title":"What is a trait in Azure ML matchbox recommender?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":859.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1290750251812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nimes, France",
        "Poster_reputation_count":55864.0,
        "Poster_view_count":4960.0,
        "Solution_body":"<p><a href=\"http:\/\/apprize.info\/microsoft\/azure_1\/9.html\" rel=\"nofollow noreferrer\">This<\/a> page may have better descriptions on it.<\/p>\n\n<p>Basically, traits are the features the algorithm will learn about each user related to each item. For example, in the <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Recommender-Restaurant-ratings-2\" rel=\"nofollow noreferrer\">restaurant ratings recommender<\/a> traits could include a user's birth year, if they're a student or working professional, martial status, etc.<\/p>\n\n<p>Hope that helps!<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":7.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1308581761180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":7201.0,
        "Answerer_view_count":861.0,
        "Challenge_adjusted_solved_time":69.4949169444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I deployed a large 3D model to aws sagemaker. Inference will take 2 minutes or more. I get the following error while calling the predictor from Python:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;'\n<\/code><\/pre>\n<p>In Cloud Watch I also see some PING time outs while the container is processing:<\/p>\n<pre><code>2020-10-07T16:02:39.718+02:00 2020\/10\/07 14:02:39 https:\/\/forums.aws.amazon.com\/ 106#106: *251 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/unix:\/tmp\/gunicorn.sock\/ping&quot;, host: &quot;model.aws.local:8080&quot;\n<\/code><\/pre>\n<p>How do I increase the invocation time out?<\/p>\n<p>Or is there a way to make async invocations to an sagemaker endpoint?<\/p>",
        "Challenge_closed_time":1602331706568,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602081524867,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed a large 3D model to AWS Sagemaker and is facing an error while calling the predictor from Python due to invocation time out. The user also sees PING time outs in Cloud Watch while the container is processing. The user is seeking a way to increase the invocation time out or make async invocations to an Sagemaker endpoint.",
        "Challenge_last_edit_time":1602331765720,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64246437",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":15.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":10.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":69.4949169444,
        "Challenge_title":"How to increase AWS Sagemaker invocation time out while waiting for a response",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":8464.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>It\u2019s currently not possible to increase timeout\u2014this is an open issue in GitHub. Looking through the issue and similar questions on SO, it seems like you may be able to use batch transforms in conjunction with inference.<\/p>\n<h1>References<\/h1>\n<p><a href=\"https:\/\/stackoverflow.com\/a\/55642675\/806876\">https:\/\/stackoverflow.com\/a\/55642675\/806876<\/a><\/p>\n<p>Sagemaker Python SDK timeout issue: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":7.27,
        "Solution_score_count":6.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":9.3958480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Challenge_closed_time":1643080918688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643047926007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on GCP Vertex AI and deployed it on an endpoint. They are able to execute predictions from a sample within their GCP project, but they are unsure if it is possible to request the endpoint from another GCP project with the use of a service account and IAM role in both projects.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.1646336111,
        "Challenge_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":417.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598873976143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Versailles, France",
        "Poster_reputation_count":140.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1643081751060,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":163.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":41.0861802778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure Machine Learning Service, when we deploy a Model as an AKS Webservice Endpoint, how can we raise exceptions to let the end-user get proper feedback if their API call is unsuccessful? Azure mentions using <code>azureml.exceptions.WebserviceException<\/code> in their <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.webserviceexception?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation<\/a>. However, how do we use this class to raise exceptions in case the API call cannot be processed properly and the end-user is responsible for it?<\/p>",
        "Challenge_closed_time":1617344350896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617196440647,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use azureml.exceptions.WebserviceException to effectively manage REST API errors when deploying a model as an AKS Webservice Endpoint in Azure Machine Learning Service. They are looking for ways to raise exceptions to provide proper feedback to end-users in case their API call is unsuccessful.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66888622",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.6,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":41.0861802778,
        "Challenge_title":"How to effectively use azureml.exceptions.WebserviceException for efficient REST API Error Management?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":250.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>To raise exceptions to let the end-user get proper feedback if their API call is unsuccessful, we use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-services\/azureml.contrib.services.aml_response.amlresponse?view=azure-ml-py\" rel=\"nofollow noreferrer\"><code>azureml.contrib.services.aml_response.AMLResponse<\/code> Class<\/a>.<\/p>\n<p>Example of use in <code>score.py<\/code>:<\/p>\n<pre><code>if [some-condition]:    \n    return AMLResponse(&quot;bad request&quot;, 500)\n<\/code><\/pre>\n<p>Documentation Link can be found <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#binary-data\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.5,
        "Solution_reading_time":9.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1621374020000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with importing `CometLogger` due to a recent refactoring of logger imports. The `comet_ml` library needs to be imported before `torch` and `tensorboard` for it to work properly. However, since the refactoring, `torch` is now imported before `comet_ml` in `loggers\/comet.py`, which forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the `ImportError`. The expected behavior is for the `comet_ml` import inside `loggers\/comet.py` to come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_participation_count":7,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":2840.7811111111,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":325,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score_count":null,
        "Solution_sentence_count":28.0,
        "Solution_word_count":260.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651909943000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where Wandb is only logging one run instead of three runs when using DDP and multirun in their `test.py` file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1666.9041666667,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1642181068947,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":22.2882166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a HyperparameterTuner on an Estimator for an LDA model in a SageMaker notebook using mxnet but am running into errors related to the feature_dim hyperparameter in my code. I believe this is related to the differing dimensions of the train and test datasets but I'm not 100% certain if this is the case or how to fix it.<\/p>\n<h2>Estimator Code<\/h2>\n<p>[note that I'm setting the feature_dim to the training dataset's dimensions]<\/p>\n<pre><code>vocabulary_size = doc_term_matrix_train.shape[1]\n\nlda = sagemaker.estimator.Estimator(\n        container,\n        role,\n        output_path=&quot;s3:\/\/{}\/{}\/output&quot;.format(bucket, prefix),\n        train_instance_count=1,\n        train_instance_type=&quot;ml.c4.2xlarge&quot;,\n        sagemaker_session=session\n        )\n\nlda.set_hyperparameters(\n    mini_batch_size=40\n    feature_dim=vocabulary_size,\n    )\n\n<\/code><\/pre>\n<h2>Hyperparameter Tuning Job<\/h2>\n<pre><code>#s3_input_train and s3_input_test hold doc_term matrices of the test\/train corpus \n\ns3_input_train = 's3:\/\/{}\/{}\/train'.format(bucket, prefix)\ns3_input_test ='s3:\/\/{}\/{}\/test\/'.format(bucket, prefix)\ndata_channels = {'train': s3_input_train, 'test': s3_input_test}\n\nhyperparameter_ranges = {\n    &quot;alpha0&quot;: ContinuousParameter(0.1, 1.5, scaling_type=&quot;Logarithmic&quot;),\n    &quot;num_topics&quot;:IntegerParameter(3, 10)}\n\n# Configure HyperparameterTuner\nmy_tuner = HyperparameterTuner(estimator=lda,  \n                               objective_metric_name='test:pwll',\n                               hyperparameter_ranges=hyperparameter_ranges,\n                               max_jobs=5,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(data_channels, job_name='run-3', include_cls_metadata=False)\n<\/code><\/pre>\n<h2>Cloudwatch Logs<\/h2>\n<p>When I run the above, the tunings fail and when I look into Cloudwatch to see the logs, the error is typically:<\/p>\n<p><em>[01\/19\/2022 19:42:22 ERROR 140234465695552] Algorithm Error: index 11873 is out of bounds for axis 1 with size 11873 (caused by IndexError)<\/em><\/p>\n<p>I replicated the above because 11873 is the number of features in my test dataset, so I think there's a connection but I'm not sure exactly what's going on. When I try &quot;11873&quot; as the value for feature_dim, the error complains that the data has 32465 features (corresponding to the training set). Summing the two values also gives the following error:<\/p>\n<p><em>[01\/20\/2022 13:44:01 ERROR 140125082621760] Customer Error: The supplied feature_dim parameter does not have the same dimensionality of the data. (feature_dim) 44338 != 32465 (data).<\/em><\/p>\n<p>Lastly, one of the last logs in Cloudwatch reports the following, suggesting that &quot;all data&quot; is being fit into a matrix with the dimensions of the test data:<\/p>\n<p><em>[01\/20\/2022 14:49:52 INFO 140411440904000] Loaded all data into matrix with shape: (11, 11873)<\/em><\/p>\n<p>How do I define feature_dim given the test and training data sets?<\/p>",
        "Challenge_closed_time":1642773499220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642643048200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering errors related to the feature_dim hyperparameter while running a HyperparameterTuner on an Estimator for an LDA model in a SageMaker notebook using mxnet. The user believes that the errors are related to the differing dimensions of the train and test datasets, but is unsure how to fix it. The Cloudwatch logs show that the algorithm error is caused by an index out of bounds for axis 1 with size 11873, which is the number of features in the test dataset. The user has tried setting feature_dim to 11873 and 32465, but both values result in errors. The logs also suggest that \"all data\" is being fit into a matrix with the dimensions of the test data. The user",
        "Challenge_last_edit_time":1642693261640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70779880",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":37.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":36.2363944444,
        "Challenge_title":"SageMaker Hyperparameter Tuning for LDA, clarifying feature_dim",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":325,
        "Platform":"Stack Overflow",
        "Poster_created_time":1642181068947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>I have resolved this issue. My problem was that I was splitting the data into test and train BEFORE converting the data into doc-term matrices, which resulted in test and train datasets of different dimensionality, which threw off SageMaker's algorithm. Once I convereted all of the input data into a doc-term matrix, and THEN split it into test and train, the hyperparameter optimization operation completed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1609843354000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the LED model in SageMaker SMP training. They have tried several fixes, including matching the python, transformers, and pytorch versions, but are still facing issues. The error is in the \"modeling_led\" within the transformers module, which is expecting a different input_ids shape. The user tried to unsqueeze input tensors to the \"modeling_led\" to solve the above error, which helped move forward in the process, but they got another error further down in the code. The error message is \"Tensors must have the same number of dimensions: got 4 and 3.\" The user is seeking feedback and assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":195.695,
        "Challenge_answer_count":0,
        "Challenge_body":"After https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. It starts using `COMET_EXPERIMENT_KEY`. But it doesn't respect it if it is set already.\r\nSo the bug is in the following.\r\nI already set this variable \r\nThen logger overwrites my value here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L189\r\nThen it deletes this variable at all here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L215\r\nThis way it ignores my variable and deletes it at all later\r\nMoreover in version function it also ignores my set variable\r\nI will create a pull request to fix it ",
        "Challenge_closed_time":1603809056000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603104554000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running on ddp mode with comet logger. The error message indicates that the local object 'SummaryTopic' cannot be pickled, resulting in an AttributeError. The code runs when the logger is detached from the trainer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4229",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":12.2,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":195.695,
        "Challenge_title":"Comet logger overrides COMET_EXPERIMENT_KEY env variable",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":86,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.1061447222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with <code>wandb.sweep<\/code>. For now, in order to get the best group of hyper-params, I have to look for the best group on my own and record those params manually. I wonder whether there is a way to extract or collect reuslts of hyper-params automatically by <code>wandb<\/code>?<br>\nThanks a lot!<\/p>",
        "Challenge_closed_time":1682009719808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681937337687,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using wandb.sweep to tune hyper-parameters and is manually recording the best group of hyper-parameters. They are seeking a way to automatically extract or collect results of hyper-parameters using wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/collect-results-from-sweep\/4238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.1061447222,
        "Challenge_title":"Collect results from sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello!<\/p>\n<p>We have <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=G01IM4yVkc6u\" rel=\"noopener nofollow ugc\">Parallel Coordinate plots and Hyper Parameter Importance Plots<\/a> in the UI that can help with looking for the best group! In terms of collecting results of sweeps, the hyperparameters are automatically logged to the <code>config.yaml<\/code> file in your run\u2019s file tab.  However, if you want to collect the hyperparameters  yourself, you can also access individual hyperparameter values using <code>wandb.config['hyperparameter-name']<\/code> within the <code>main()<\/code> function you are running your sweep on. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/config\">Here<\/a> is our documentation on ways to use access and update the config file.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":11.56,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8071302778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a state-machine workflow with 3 following states:  \n\n[screenshot-of-my-workflow](https:\/\/i.stack.imgur.com\/4xJTE.png)   \n\n1. A 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (*this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.*) \n2. Use map to call SageMaker endpoints dictated by the array(or list) from above result.\n3. Send the result of above 'Map' to a Lambda function and exit the workflow.\n\n\nHere's the entire workflow in .asl.json, inspired from [this aws blog](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-map-state.html).\n```\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n```\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n```\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n```\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.  \n[screenshot-of-graph-inspector](https:\/\/i.stack.imgur.com\/3gXsM.jpg)\n\nWhat's causing the error and How do I fix this?",
        "Challenge_closed_time":1647513967263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647503861594,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a validation error when trying to fetch parameters for the SageMaker's InvokeEndpoint block inside the iterator of the Map block in AWS StepFunctions. The error occurs when trying to access the parameters inside the iteration, and the user presumes that the error is because \"$.EndpointName\" is not being filled with the relevant value. However, when checking the failed execution and inspecting the InvokeEndpoint block, the input appears to be correct. The user is seeking guidance on how to fix this issue.",
        "Challenge_last_edit_time":1668586573828,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-stepfunctions-sagemaker-s-invokeendpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":41.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.8071302778,
        "Challenge_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":336,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In general (as mentioned [here in the parameters doc](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-parameters.html)), you also need to **end the parameter name** with `.$` when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. `\"InvocationBody.$\": \"$.body.InputData\"`), but not in others (`\"EndpointName\": \"$.EndpointName\"`), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret `$.EndpointName` as literally the name of the endpoint (which doesn't satisfy `^[a-zA-Z0-9](-*[a-zA-Z0-9])*`!)\n\nSo suggest you change to `EndpointName.$` and `Body.$` in your InvokeEndpoint parameters",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647513967264,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7800052778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi! The following error happens while trying to create an endpoint from a successful trained model:\n\n* In the web console: \n> The customer:primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.\n * CloudWatch logs: \n> exec: \"serve\": executable file not found in $PATH\n\nIm deploying the model using a Lambda step, just as in this [notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb). The Lambda step is successful, and I can see in the AWS web console that the model configuration is created with success. \n\nThe exact same error happens when I  create an endpoint for the registered model in the AWS web console, under Inference -> Models. In the console I can see that an inference container was created for the model, with the following characteristics:\n* Image: 763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39\n* Mode: single model\n* Environment variables (Key Value): \n> SAGEMAKER_CONTAINER_LOG_LEVEL\t20\n\n> SAGEMAKER_PROGRAM\tinference.py\n\n> SAGEMAKER_REGION\teu-west-3\n\n> SAGEMAKER_SUBMIT_DIRECTORY\t\/opt\/ml\/model\/code\n \nI absolutely have no clue what is wrong and I could not find anything relevant online about this problem. Is it necessary to provide an custom docker image for inference or something?\n\nFor more details, please find below the pipeline model steps code. Any help would be much appreciated!\n```\nmodel = Model(\n    image_uri=estimator.training_image_uri(),\n    model_data=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=sagemaker_session,\n    role=sagemaker_role,\n    source_dir='code',\n    entry_point='inference.py'\n)\nstep_model_create = ModelStep(\n        name=\"CreateModelStep\",\n        step_args=model.create(instance_type=\"ml.m5.large\")\n )\n\nregister_args = model.register(\n        content_types=[\"*\"],\n        response_types=[\"application\/json\"],\n        inference_instances=[\"ml.m5.large\"],\n        transform_instances=[\"ml.m5.large\"],\n        model_package_group_name=\"test\",\n        approval_status=\"Approved\"\n)\nstep_model_register = ModelStep(name=\"RegisterModelStep\", step_args=register_args)\n```",
        "Challenge_closed_time":1670290340636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670280332617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create an endpoint from a trained model. The error message states that the customer:primary container for production variant AllTraffic did not pass the ping health check and to check CloudWatch logs for this endpoint. The CloudWatch logs show an error message \"exec: \"serve\": executable file not found in $PATH\". The user is deploying the model using a Lambda step and has created the model configuration successfully. The same error occurs when creating an endpoint for the registered model in the AWS web console. The user is unsure of what is wrong and is seeking help.",
        "Challenge_last_edit_time":1670628195195,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0JgbfwUoS5m6VH4TvtZmkg\/error-creating-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":29.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":2.7800052778,
        "Challenge_title":"Error Creating Endpoint",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":218,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, the problem here is that your inference model's container URI `763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39` is using a **training** image, not an **inference** image for TensorFlow. Because the images are each optimized for their own function, the serving executable is not available in the training container in this case.\n\nUsually, the framework-specific SDK classes will handle this lookup for you (for example `TensorFlowModel(...)` as used in the notebook you linked, or when calling `sagemaker.tensorflow.TensorFlow.deploy(...)` from the Estimator class.\n\nI see here though that you're using the generic `Model`, so guess you don't know (or don't want to commit to) the framework and version at the point the Lambda function runs?\n\nMy suggestions would be:\n\n- Can you use the Pipelines `ModelStep` to create your model before calling the Lambda deployment function? Similarly to how your linked notebook uses `CreateModelStep`. This would build your framework & version into the pipeline definition itself, but should mean that the selection of inference container image gets handled properly & automatically.\n- If you really need to be dynamic, I think you might need to find a way of looking up at least the *framework* from the training job. From my testing, you can use `estimator = sagemaker.tensorflow.TensorFlow.attach(\"training-job-name\")` and then `model = estimator.create_model(...)` to correctly infer the specific inference container *version* from a training job, but it still relies on knowing that TensorFlow is the correct framework. I'm not aware of a framework-agnostic equivalent? So could e.g. try describing the training job, manually inferring which framework it uses from that information, and then using the relevant framework estimator class' [attach()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.attach) method to figure out the specifics and create your model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1670290340638,
        "Solution_link_count":1.0,
        "Solution_readability":11.4,
        "Solution_reading_time":25.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":266.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.1083627778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to use the Azure Pricing estimate in the Azure Pricing Calculator, the &quot;Estimated monthly costs&quot; seems to include but also far exceeds the compute cost.  Does this Estimated Monthly cost include the other resources that get created?     <br \/>\neg. Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), Key Vault    <\/p>\n<p>Thank you    <br \/>\nPeter    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/52085-image.png?platform=QnA\" alt=\"52085-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1609285909056,
        "Challenge_comment_count":1,
        "Challenge_created_time":1609267518950,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is questioning whether the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator includes all other non-compute \"additional resources\" created in the workspace, such as Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), and Key Vault. The user notes that the estimated monthly cost seems to include but also far exceeds the compute cost.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/213635\/does-the-estimated-monthly-costs-for-azure-machine",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.3,
        "Challenge_reading_time":8.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.1083627778,
        "Challenge_title":"Does the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator include all other non-compute \"additional resources\" created in the workspace",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Peter.    <\/p>\n<p>Thanks for reaching out. I tried your selections but I don't have the same service as you. Have you selected other services in you calculator?     <\/p>\n<p>For your question, the estimated price is only for Azure Machine Learning Service. You need to select all services you need in the calculator like below:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51998-image.png?platform=QnA\" alt=\"51998-image.png\" \/>    <\/p>\n<p>Please note I only use random number for the example.     <\/p>\n<p><strong>From the number I guess you have selected 2 Machine Learning Services and also other services since they added to your basket when you clicked them,<\/strong> you can click the button to see what you have all as below.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51969-image.png?platform=QnA\" alt=\"51969-image.png\" \/>    <\/p>\n<p>Also you are selecting Reservation service, detail: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations\">https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.0,
        "Solution_reading_time":15.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":129.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":248.2981122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1638931046247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a managed notebook on Vertex AI due to a quota limit error related to 'Create Runtime API requests per minute' of the 'notebooks.googleapis.com' service. The user has tried to edit the quota limit but is unable to increase it. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":248.2981122222,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.2166666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am carrying out the OAuth verification in Google Cloud Platform, I received an email that said:\n\"Thanks for your patience while we reviewed your project.\n\nYour project pc-api-XXXXXXXXXXXXXXX-XX has multiple unique domains in the redirect URI and origin URLs, many of which have\u00a0unrelated applications. This is in direct violation of the\u00a0Google API Services: User Data Policy, which requires that projects accurately represent their identity and intent to Google and to our users when they request access to Google user data.\n\nPlease follow the instructions on the\u00a0Google API Console\u00a0to:\n\nCreate new projects\nMigrate your redirect URIs with distinct brands to different projects, and\/or\nEnsure that these projects accurately represent their true identity to Google users\n\nYou can find more information in the\u00a0OAuth Application Verification FAQ. \u00a0To make sure we don't miss your messages, respond directly to this email to continue with the verification process.\"\n\n\n\nI have a web server, which checks the validity (domain-1.com) in-app purchases, and I also have a site with a different domain containing: privacy-policy and terms-of-service (domain-2.com).\n\nMy settings are as follows:\n\nOAuth consent screen:\n- Home page application:\u00a0https:\/\/www.domain-2.com\/\n- Privacy Policy:\u00a0https:\/\/www.domain-2.com\/privacy-policy\/\n- Terms of Service:\u00a0https:\/\/www.domain-2.com\/terms-of-service\/\n\nAuthorized domains:\n- domain-2.com\n- domain-1.com\n\nID client OAuth 2.0 -> Authorized Redirect URIs:\n-\u00a0https:\/\/game.domain-1.com:8443\n\nI have a working service account.\nI have successfully verified all 2 domains.\n\n\nWhere is the mistake?",
        "Challenge_closed_time":1649106480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649040900000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user received an email stating that their project violates the Google API Services: User Data Policy due to having multiple unique domains in the redirect URI and origin URLs. The user has a web server that checks the validity of in-app purchases on domain-1.com and a site with a different domain containing privacy-policy and terms-of-service on domain-2.com. The user needs to create new projects, migrate redirect URIs with distinct brands to different projects, and ensure that these projects accurately represent their true identity to Google users.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Action-Needed-OAuth-Google-Cloud-platform-multiple-unique\/m-p\/410024#M254",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":21.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":18.2166666667,
        "Challenge_title":"Action Needed | OAuth Google Cloud platform | multiple unique domains",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":233,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I have found the solution:\n\nI had 2 Google Cloud Platform projects for the same application.\n\nI deleted a Google Cloud Platform.\nI implemented all the configurations of the deleted project in the other project, and it worked!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":3.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2063763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <br \/>\nI created custom Azure AI model what I would like to use in PowerBI.    <br \/>\nWhen I open a dataset in PowerBI and after select the &quot;Azure Machine learning&quot; after the pop-up window is empty but I suppose it should contain my custom model(s).    <br \/>\nI followed the below articles:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\">https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate<\/a>    <\/p>\n<p>Kind regards    <br \/>\nTom    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162671-powerbi-azure-ai.png?platform=QnA\" alt=\"162671-powerbi-azure-ai.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162606-azure-ai-model.png?platform=QnA\" alt=\"162606-azure-ai-model.png\" \/>    <\/p>",
        "Challenge_closed_time":1641419590032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641415247077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a custom Azure AI model and wants to use it in PowerBI. However, when they select \"Azure Machine learning\" in PowerBI, the pop-up window is empty and does not show their custom model. The user has followed the provided articles but is still facing this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/684845\/why-does-powerbi-not-see-my-custom-azure-ai-model",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":19.6,
        "Challenge_reading_time":14.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2063763889,
        "Challenge_title":"Why does PowerBI not see my custom Azure AI model?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The product group for Power Bi actively monitors questions over at    <br \/>\n<a href=\"https:\/\/community.powerbi.com\/\">https:\/\/community.powerbi.com\/<\/a>       <\/p>\n<p>--please don't forget to <code>upvote<\/code> and <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/145510-image.png?platform=QnA\" alt=\"145510-image.png\" \/> if the reply is helpful--    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.6893194445,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>I was trying to use Sweep for hyperparameter tuning.   And I want to do grid sweep in tuning. Coincidently I happend to use the Bayes Sweep (since last time I use the bayes for tuning). Then something weird happened.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/e\/eab8bc4fae8510d8bd8187cc1a1434ecb341a135.png\" data-download-href=\"\/uploads\/short-url\/xurwnzUO5SdxA25PtlOLTx1c0L3.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/eab8bc4fae8510d8bd8187cc1a1434ecb341a135_2_690x132.png\" alt=\"image\" data-base62-sha1=\"xurwnzUO5SdxA25PtlOLTx1c0L3\" width=\"690\" height=\"132\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/eab8bc4fae8510d8bd8187cc1a1434ecb341a135_2_690x132.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/eab8bc4fae8510d8bd8187cc1a1434ecb341a135_2_1035x198.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/eab8bc4fae8510d8bd8187cc1a1434ecb341a135_2_1380x264.png 2x\" data-dominant-color=\"F9F9F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1868\u00d7358 27.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nI can understand the Bayes search may choose same combination of hyperparameters, But why the same hyperparameters come into different results? And I check my code, I definitely have set the seed. Is there anything I missed?<br>\nAnd this is the yaml config I use:<\/p>\n<blockquote><\/blockquote>\n<p>method: bayes<br>\nproject: classify<br>\nname: roberta-large<br>\nmetric:<br>\ngoal: maximize<br>\nname: best_valid_metric<br>\nparameters:<br>\ntask:<br>\nvalues: [\u201cemotion\u201d]<br>\nbatch_size:<br>\nvalues: [8, 16, 32]<br>\nplm_learning_rate:<br>\nvalues: [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]<br>\nother_learning_rate:<br>\nvalues: [1e-4, 2e-4, 3e-4, 4e-4, 5e-4]<br>\ndropout:<br>\nvalues: [0, 0.3, 0.5]<br>\nmodel_name:<br>\nvalue: 1<br>\nnum_labels:<br>\nvalue: 8<br>\ncommand:<\/p>\n<ul>\n<li>${env}<\/li>\n<li>${interpreter}<\/li>\n<li>${program}<\/li>\n<li>\u201c\u2013use_wandb\u201d<\/li>\n<li>${args}<\/li>\n<\/ul>",
        "Challenge_closed_time":1680683372199,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680666490649,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while using the Bayes Sweep for hyperparameter tuning. Despite using the same hyperparameters and setting the seed, the results produced were different. The user is seeking clarification on why this happened and if they missed anything. The user provided a YAML config for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/use-the-same-parameter-but-produce-different-results-in-bayesian-sweep\/4186",
        "Challenge_link_count":5,
        "Challenge_participation_count":8,
        "Challenge_readability":18.0,
        "Challenge_reading_time":33.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4.6893194445,
        "Challenge_title":"Use the same parameter but produce different results in Bayesian Sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":187,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Dear Zhuojun,<\/p>\n<p>would you be able to confirm if you are using wandb server locally or our public cloud offering?<\/p>\n<p>This is a known bug that has now been fixed in our latest version of wandb serve 0.31.0 which was released yesterday.<\/p>\n<p>Upgrading to this version should fix the issue that you are experiencing with sweep combinations (repetition) of what should be permutations of parameters.<\/p>\n<p>Warm regards,<\/p>\n<p>Frida<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":5.57,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.8516666667,
        "Challenge_answer_count":0,
        "Challenge_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Challenge_closed_time":1642173581000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1642148915000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where Wandb is only logging one run instead of three runs when using DDP and multirun in their test.py file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":2.13,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.8516666667,
        "Challenge_title":"wandb api key for github ci",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":14,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/settings\/secrets\r\n\r\n```\r\njobs:\r\n  weekday_job:\r\n    runs-on: ubuntu-latest\r\n    env:\r\n      DAY_OF_WEEK: Mon\r\n    steps:\r\n      - name: \"Hello world when it's Monday\"\r\n        if: ${{ env.DAY_OF_WEEK == 'Mon' }}\r\n        run: echo \"Hello $FIRST_NAME $middle_name $Last_Name, today is Monday!\"\r\n        env:\r\n          WANDB_API_KEY: $github.SECRETS.WANDB_API\r\n          WANDB_NAME: github_ci_tests\r\n          WANDB_NAME: Octocat\r\n```\r\n\r\n\r\nhttps:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":5.33,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":40.8553036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Challenge_closed_time":1550668517447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550539380057,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive and wants to register the model while adding a description to it. However, the register_model method does not seem to have a description flag, and the user is unsure how to add a description to the model so that it appears in the Azure Portal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":11.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":35.8714972222,
        "Challenge_title":"Add model description when registering model after hyperdrive successful run",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408574571227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1550686459150,
        "Solution_link_count":3.0,
        "Solution_readability":21.2,
        "Solution_reading_time":18.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1086111111,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI would like to compare the top 4 experiments based on a specific metrics.\n\nCurrently I query the top experiment using the cli:\n\npolyaxon ops ls -q \"name: GROUP_NAME, metrics.loss:<0.002\"  -s \"metrics.loss\" -l 5\n\nAnd then I copy\/paste the run UUIDs to:\n\npolyaxon run --hub tensorboard:mulit-run -P uuids=UUID1,UUID2,UUID3,UUID4,UUID5",
        "Challenge_closed_time":1649336377000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649335986000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to compare the top 4 experiments based on a specific metric and is currently using the CLI to query the top experiment. They then copy and paste the run UUIDs to start a Tensorboard for the top 5 experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1483",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1086111111,
        "Challenge_title":"How can I start a Tensorboard for the top 5 experiments",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"From the UI or using a YAML file, you can run:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"name: GROUP_NAME, metrics.loss:<0.002,  kind:job\"  \n  sort: \"metrics.loss\"\n  limit: 5\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nNote that in the UI if create a filter \/ sort configuration, you can automatically create a multi-run Tensorboard based on that query, for example:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1535382420716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.9537672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Challenge_closed_time":1641216477460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641209554070,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has been charged for 6 days of AWS Sagemaker Data Wrangler usage, even though they only used it for 3 hours a week ago. The tool seems to have continued running in the background, and the user is unsure how to stop it as they have no endpoints to close. The user is not concerned about the cost but is frustrated with the lack of help from AWS support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9231638889,
        "Challenge_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535382420716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641216587632,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1076.4220325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Challenge_closed_time":1617006916700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611574816563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an event rule in Cloudwatch to monitor Sagemaker training job state changes and trigger a lambda function to send messages in a telegram group. However, the events are fired multiple times with the same payload, resulting in duplicate messages. Enabling the Sagemaker profiler causes the number of identical rule invocations to increase. The user suspects a bug in AWS and is looking for a simple solution to avoid duplicate messages.",
        "Challenge_last_edit_time":1613131797383,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1508.9167047222,
        "Challenge_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":780.0,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":13.3565036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Challenge_closed_time":1655849768796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655801685383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy AWS using CDK and Sagemaker, but is encountering an error when running pytest. They are stuck on step 3 of the process and are unsure of what to add in the infrastructure code. They are also unsure if they should do the whole process on their local terminal or on Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":13.3565036111,
        "Challenge_title":"How to deploy AWS using CDK, sagemaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604312740476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553609947192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":221.1468555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Challenge_closed_time":1616165232248,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615369010763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered a brokenpipeerror when attempting to use a deepar model to predict from a test.json dataset.",
        "Challenge_last_edit_time":1615369103568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.1,
        "Challenge_reading_time":216.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":142,
        "Challenge_solved_time":221.1726347222,
        "Challenge_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":1241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1607069622448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":10,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug in the logger behavior of PyTorchLightning after a recent update. The logger starts using `COMET_EXPERIMENT_KEY` but does not respect it if it is already set. The logger overwrites the user's value, deletes the variable, and ignores the set variable in the version function. The user plans to create a pull request to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":3215.9254886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Challenge_closed_time":1626975605176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615398273417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"Resource limit Error\" while setting up their first SageMaker Studio to run post-processing scripts in a shared environment. They followed the steps in a video tutorial but are unable to proceed due to the error. They have checked their registered domains and S2 instances but are unable to locate the domains being utilized. The user is seeking help to resolve the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3215.9254886111,
        "Challenge_title":"How to setup AWS sagemaker - Resource limit Error",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530468231707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":357.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605982845000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a warning message when calling \"kedro mlflow init\" which states that the project is not initialized yet and that the command must be called before any other command. However, this warning can be ignored as the command works as intended. The issue is due to the dynamic creation of the command.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1627764232887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.7981325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Challenge_closed_time":1648038217500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647989575803,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to compile a SageMaker semantic segmentation model with SageMaker Neo for use on an edge device. The error message states that there is no valid Mxnet model file -symbol.json found. The user is unsure if gluon models are supported by Neo and is seeking help to recreate a -symbol.json file or convert the compiled sagemaker model. The user encountered the same problem with another built-in algorithm, k-Nearest Neighbour (k-NN).",
        "Challenge_last_edit_time":1648020944223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":23.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":13.5115825,
        "Challenge_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":236,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586928819952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.5499419444,
        "Challenge_answer_count":1,
        "Challenge_body":"I had made my custom training image so It can be conducted through CreateTrainingJob, not sagemaker training took kit (requiring \"ContainerEntrypoint\" option).\n\nBut when I'm trying to run HyperParameter Tuning Job, it is not allowed to add \"ContainerEntrypoint\" option in \"AlgorithmSpecification\" field.\n\nIs it impossible to run HyperParameter Tuning Job with training images that can not be run without sagemaker training toolkit?\n\nThanks!",
        "Challenge_closed_time":1668561111672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668501531881,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a custom training image that can be run through CreateTrainingJob without the sagemaker training toolkit. However, they are unable to add the \"ContainerEntrypoint\" option in the \"AlgorithmSpecification\" field when running a HyperParameter Tuning Job. The user is questioning if it is possible to run a HyperParameter Tuning Job with training images that cannot be run without the sagemaker training toolkit.",
        "Challenge_last_edit_time":1668848150584,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrrvbV_wJRuqG9TX1KyoZnQ\/is-it-possible-sagemaker-hyperparameter-tuning-job-without-sagemaker-training-tool-kit",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":16.5499419444,
        "Challenge_title":"Is it possible SageMaker HyperParameter Tuning Job without sagemaker-training tool kit",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there!\n\nFor custom training image, you can specify the entrypoint in your Dockerfile.\n\nBelow are some code snippet as well as links you can use as reference:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/rapids_bring_your_own\/code\/Dockerfile\n```\nENTRYPOINT [\".\/entrypoint.sh\"]\n```\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/keras_bring_your_own\/Dockerfile\n```\nENTRYPOINT [\"python\", \"-m\", \"trainer.start\"]\n```\n\nFurthermore, SageMaker Training Toolkit is a nicely wrapped up python package for you to use and eases the process of creating a custom training image, it's no different from implementing the logic yourself.\n\nSo it is definitely possible to run HyperParameter Tuning Job using custom containers without using our SageMaker Training Toolkit.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668561111672,
        "Solution_link_count":2.0,
        "Solution_readability":18.4,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530826195963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":202.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":4541.5040575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Challenge_closed_time":1660920166467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644569855830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information about the mechanism behind hyperparameter tuning job in AWS Sagemaker. They are specifically trying to bring their own container and minimize cross entropy loss. They are questioning whether the hyperparameters defined in the HyperParameterTuner class get copied into \/opt\/ml\/input\/config\/hyperparameters.json and whether the training image should be adjusted to use the hyperparameters from that file. The user is also confused about how hyperparameters are passed into the training code, as some sample HPO notebooks use argparser to pass in the hyperparameters.",
        "Challenge_last_edit_time":1644570751860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4541.7529547222,
        "Challenge_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446577693503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.88302,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Challenge_closed_time":1676070775536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676042396664,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to rerun a few runs in a wandb sweep, but after deleting them from the dashboard, wandb is not able to rerun those runs even after running the command again. The user suggests adding a feature to rerun the deleted runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.8,
        "Challenge_reading_time":5.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.88302,
        "Challenge_title":"Rerun a deleted run in wandb sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Challenge_closed_time":1548817091420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548817091420,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while starting a SageMaker hyperparameter tuning job using the Python SDK for the built-in Image Classifier algorithm. The error message indicates that additional hyperparameters are not allowed, specifically the 'sagemaker_estimator_module' and 'sagemaker_estimator_class_name' properties. The user is unsure why this error is occurring as they are not explicitly passing these properties. The user is seeking guidance on the correct way to start the tuning job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1224733422316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":7707.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":3.7756897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Challenge_closed_time":1653626541543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653612949060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to query a list of frequently used compute instance sizes under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. However, the list of resources that can be queried does not include compute under microsoft.machinelearningservices\/ (not classic studio) and Microsoft.Databricks\/workspaces. The user has tried a Kusto query to get VM instance size but it does not show what they have under Azure Machine Learning\/Azure Databricks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.7756897222,
        "Challenge_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":183.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":197.9953861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently I am exploring AWS sagemaker and I am facing a problem e.g. If I want to train my network on 1000s of epochs I cant stay active all the time. But as I logout my the notebook instance also stop execution. Is there any way to keep the instance active even after you logout ? <\/p>",
        "Challenge_closed_time":1541803691470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541090908080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Amazon Sagemaker Notebook instance where the execution stops as they log out, making it difficult to train networks on multiple epochs. They are seeking a solution to keep the instance active even after logging out.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53105741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":4.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":197.9953861111,
        "Challenge_title":"Amazon Sagemaker Notebook instance stop execution as I logout",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2598.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492699347027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.<\/p>\n\n<p>[1]<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":7.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.2021597222,
        "Challenge_answer_count":13,
        "Challenge_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Challenge_closed_time":1661461646292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661403318517,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where all records of their runs in a project have been lost without any action on their part. They have tried to use \"wandb sync\" and upload the runs to a new project, but the results are still the same. The user did not perform any specific operations regarding the wandb logging process or on the website. The project consists of runs uploaded from different machines, and the phenomenon of lost data also occurs on old runs that finished weeks ago. The user is seeking suggestions on how to recover the lost records.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Challenge_link_count":6,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":39.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":16.2021597222,
        "Challenge_title":"All records are lost in a project without any action",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":297.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":103.1438980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to make a GPT-2 model with deepspeed on an azure VM. I found ~2 bugs which I was able to patch, but I have stumbled upon a really tough one. You see, it says I need pytorch. No surprise. I install pytorch. It still says I don't have it. I used both pip and pip3 many times. I install pytorch from github and run setup.py. It says I need python 3. When I get python 3 it says the same. When I try google colab it gives me the following error:  <br \/>\n<code>Traceback (most recent call last):   <\/code>  File &quot;pretrain_gpt2.py&quot;, line 709, in &lt;module&gt;  <br \/>\n<code>   main()  <\/code>  File &quot;pretrain_gpt2.py&quot;, line 654, in main  <br \/>\n<code>   args.eod_token = get_train_val_test_data(args)  <\/code>  File &quot;pretrain_gpt2.py&quot;, line 600, in get_train_val_test_data  <br \/>\n<code>   args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py&quot;, line 34, in apply  <br \/>\n<code>   return make_loaders(args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py&quot;, line 170, in make_loaders  <br \/>\n<code>   train, tokenizer = data_utils.make_dataset(**data_set_args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/<strong>init<\/strong>.py&quot;, line 109, in make_dataset  <br \/>\n<code>   ds = split_ds(ds, split)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py&quot;, line 194, in split_ds  <br \/>\n<code>   rtn_ds[i] = SplitDataset(ds, split_inds)   <\/code> File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py&quot;, line 134, in <strong>init<\/strong>  <br \/>\n <code>  self.lens = itemgetter(*self.split_inds)(list(self.wrapped_data.lens))  <\/code>TypeError: itemgetter expected 1 arguments, got 0  <\/p>\n<p>How do I fix both the google colab and the azure VM errors?<\/p>",
        "Challenge_closed_time":1610409087243,
        "Challenge_comment_count":2,
        "Challenge_created_time":1610037769210,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while trying to create a GPT-2 model with deepspeed on an Azure VM. They have encountered bugs while installing pytorch and are unable to resolve the issue even after trying to install it from Github and running setup.py. The user is also facing errors while using Google Colab. They are seeking help to fix both the Google Colab and Azure VM errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/222550\/deepspeed-gpt-2-megatron-lm-problems",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":23.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":103.1438980556,
        "Challenge_title":"Deepspeed gpt-2 megatron-LM problems",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":213,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b4907c12-a726-468e-8576-0c4a9c876196\">@sammyboy123  <\/a>     <br \/>\nI would start by installing PyTorch via pip. Instructions can be found <a href=\"https:\/\/pytorch.org\/get-started\/locally\/\">here<\/a>. There is also a verification section which will test if you have PyTorch installed correctly. You also might find the DeepSpeed <a href=\"https:\/\/www.deepspeed.ai\/getting-started\/\">Getting Started page<\/a> helpful. There are specific tutorials for Azure and also a docker image available.    <\/p>\n<p>Let me know if this doesn't work for you or you are still facing issues.    <\/p>\n<p>-------------------------------    <\/p>\n<p>Please don\u2019t forget to <strong>&quot;Accept the answer&quot;<\/strong> and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1638953828923,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":607.0,
        "Answerer_view_count":770.0,
        "Challenge_adjusted_solved_time":41.5038786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Challenge_closed_time":1645427212643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645277798680,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while trying to use a serialized XGBClassifier object, trained and generated using xgboost=1.5.2, to predict on some data in an Azure environment. The error message states that the 'XGBModel' object has no attribute 'enable_categorical'. The user has confirmed that the same version of xgboost is being used in both the pipelines that produce\/serialize the model and in the pipeline that deserializes the model to predict on new data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71185505",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":32.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":41.5038786111,
        "Challenge_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443017464707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sweden",
        "Poster_reputation_count":644.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1463745452883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":248.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":2.2081222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently playing around with tensorboard on Sagemaker studio. According to aws, it is supposedly possible. However, I keep encountering error code 500 after launching tensorboard and changing the file path to .....\/proxy\/{port number}<\/p>\n<p>Great if somebody can assists on this topic :)<\/p>",
        "Challenge_closed_time":1657296138023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657288188783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use tensorboard on Sagemaker studio but keeps encountering error code 500 after launching tensorboard and changing the file path to ...\/proxy\/{port number}. They are seeking assistance to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72912418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.2081222222,
        "Challenge_title":"How do I get tensorboard to work with sagemaker studio?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":22.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=\"https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample\" rel=\"nofollow noreferrer\">https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample<\/a><\/p>\n<p>Did you follow those instructions?<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":3.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.1594822222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello everyone, I was trying to execute the example mentioned in the docs - [https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html]().\nI was able to successfully run this example but as soon as I changed the target_device  to `jetson_tx2`, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\n```\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n```\nAccording to the troubleshoot docs [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html](), this seems to be an issue of **model_fn**() function.\nThe inference script used by this example is mentioned here [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py]() , which itself doesn't contain any model_fn() definition but it still worked for target device `ml_c5`.\nSo could anyone please help me with the following questions:\n1. What changes does SageMaker Neo do to the model depending on `target_device` type? Since it seems the same model is loaded in a different way for different target device.\n2. Is there any way to determine how the model is expected to load for a certain target_device type so that I could define the **model_fn**() function myself in the same inference script mentioned above?\n3. At-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for `jetson_tx2` device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Challenge_closed_time":1668766092280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668430718144,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues with the inference script for Amazon Sagemaker Neo compiled models. They were able to successfully run an example script but encountered errors when changing the target device to `jetson_tx2`. The error message suggests an issue with the `model_fn()` function. The user is seeking help with understanding the changes made by SageMaker Neo to the model based on the target device, determining how the model is expected to load for a certain target device, and help with the inference script for the same model that works for `jetson_tx2` device.",
        "Challenge_last_edit_time":1668778694208,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":93.1594822222,
        "Challenge_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":282,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As you mentioned, you changed the Neo compiling target from `ml_c5` to `jetson_tx2`, the compiled model will require runtime from `jetson_tx2`. If you kept other code unchanged, the model will be deployed to a `ml.c5.9xlarge` EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment. \n\nIf you compile the model with `jeston_tx2` as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668766092280,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":6.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":16.0570572222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Challenge_closed_time":1598536233436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598478428030,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on how to use trains with hyper-parameter optimization tools like RayTune and is wondering if it is possible to call Task.init() multiple times in the same execution.",
        "Challenge_last_edit_time":1609427499190,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":16.0570572222,
        "Challenge_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":154.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.9617827778,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1654627313182,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654584250764,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is exploring sweeps for a university project and wants to know if it is possible to stop bad runs after a few epochs to save time. They have provided a report showing a run that does not look promising and would like to cancel it and start a new one instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Challenge_link_count":10,
        "Challenge_participation_count":5,
        "Challenge_readability":17.8,
        "Challenge_reading_time":37.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":11.9617827778,
        "Challenge_title":"How to early stop bad runs in sweeps to save time",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":663.0,
        "Challenge_word_count":193,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":90.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1274693802127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, India",
        "Answerer_reputation_count":1561.0,
        "Answerer_view_count":243.0,
        "Challenge_adjusted_solved_time":93.8230269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When running the Azure ML Online endpoint commands, it works locally. But when I try to deploy it to Azure I get this error.\nCommand - <code>az ml online-deployment create --name blue --endpoint &quot;unique-name&quot; -f endpoints\/online\/managed\/sample\/blue-deployment.yml --all-traffic<\/code><\/p>\n<pre><code>{\n    &quot;status&quot;: &quot;Failed&quot;,\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n        &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.&quot;,\n        &quot;details&quot;: [\n            {\n                &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n                &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.\\nThe build log is available in the workspace blob store \\&quot;coloraiamlsa\\&quot; under the path \\&quot;\/azureml\/ImageLogs\/1673692e-e30b-4306-ab81-2eed9dfd4020\/build.log\\&quot;&quot;,\n                &quot;details&quot;: [],\n                &quot;additionalInfo&quot;: []\n            }\n        ],\n        \n<\/code><\/pre>\n<p>This is the deployment YAML taken straight from <a href=\"https:\/\/github.com\/Azure\/azureml-examples\" rel=\"nofollow noreferrer\">azureml-examples<\/a> repo<\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  local_path: ..\/..\/model-1\/model\/sklearn_regression_model.pkl\ncode_configuration:\n  code: \n    local_path: ..\/..\/model-1\/onlinescoring\/\n  scoring_script: score.py\nenvironment: \n  conda_file: ..\/..\/model-1\/environment\/conda.yml\n  image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\ninstance_type: Standard_F2s_v2\ninstance_count: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1642392913590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642055150693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"DriverFileNotFound\" error while deploying Azure ML Online endpoint commands to Azure. The error message indicates that the driver file named \"score.py\" is not found in the provided dependencies. The user has shared the deployment YAML taken from the Azure ML examples repository.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70692270",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":23.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":93.8230269445,
        "Challenge_title":"Azure ML Online Endpoint deployment DriverFileNotFound Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274693802127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, India",
        "Poster_reputation_count":1561.0,
        "Poster_view_count":243.0,
        "Solution_body":"<p>Finally after lot of head banging, I have been able to consistently repro this bug in another Azure ML Workspace.<\/p>\n<p>I tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly.<\/p>\n<p>At this point I remembered that I had upgraded the Storage Account of my previous AML Workspace to DataLake Gen2.<\/p>\n<p>So I did the same upgrade in this new workspace\u2019s storage account. After the upgrade, when I try to deploy the same endpoint, I get the same <code>DriverFileNotFoundError<\/code>!<\/p>\n<p>It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types<\/a>.<\/p>\n<p>At this point my only option is to recreate a new workspace and deploy my code there. Hope Azure team fixes this soon.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":13.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":21.6879675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Challenge_closed_time":1539831993640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539753916957,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to set hyperparameters for image classification training in Amazon SageMaker. The error message stated that additional hyperparameters are not allowed and specifically mentioned \"precission_dtype\" as unexpected. The user is seeking help to overcome this issue and has reported it to AWS support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":15.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":21.6879675,
        "Challenge_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403185902747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation_count":169.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":71.6,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606374398152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":54.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":336.329535,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Am working on a project of consumer behaviour analysis on websites and predict the malicious activity of users in real-time.\nClick data is being collected for each click made by users.<\/p>\n<p>Am using multiple AWS services like kinesis stream, Lambda and sagemaker. I have created an autoencoder model and\ndeployed it as sagemaker endpoint which will be invoked using lambda when it receives new click data from the website through\nKinesis stream.<\/p>\n<p>Since sagemaker endpoint contains the only model but click data which lambda function receives is raw data with URLs, texts and\ndate. How can I pass raw data into required preprocessing steps and send processed data to sagemaker endpoint in the required format?<\/p>\n<p>Example of raw data:-<\/p>\n<p>{'URL':'www.amazon.com.au\/ref=nav_logo', 'Text':'Home', 'Information':'Computers'}<\/p>",
        "Challenge_closed_time":1624254151543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623043365217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a project to analyze consumer behavior on websites and predict malicious activity in real-time. They are using AWS services like Kinesis stream, Lambda, and SageMaker. They have created an autoencoder model and deployed it as a SageMaker endpoint, which will be invoked using Lambda when it receives new click data from the website through Kinesis stream. However, the raw data received by the Lambda function contains URLs, texts, and dates, and the user needs to preprocess this data before sending it to the SageMaker endpoint in the required format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67866286",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":11.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":336.329535,
        "Challenge_title":"Real-time Data Pre-processing in Lambda for SageMaker Endpoint",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":475.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604911047620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Victoria, Australia",
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.<\/p>\n<p>Reference:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a><\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":44.6,
        "Solution_reading_time":14.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":388.0879419444,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Thanks for your good product.<\/p>\n<p>It would be good to add an archive feature for runs.<\/p>\n<p>In a project, we may try many ideas. But most of them result in no outcomes. It would be good to archive those runs to keep the workspace clean.<\/p>\n<p>It is not a good option to delete them, because we may check them in future for some cases, such as ablation study.<\/p>",
        "Challenge_closed_time":1676670702464,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675273585873,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user suggests adding an archive feature for runs in a project to keep the workspace clean. They explain that deleting runs is not a good option as they may need to refer to them in the future for certain cases.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/archive-runs\/3793",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":3.1,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":388.0879419444,
        "Challenge_title":"Archive runs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I believe currently wandb does support multiple selection. But not in the workspace view. In the table view I can select and tag multiple runs at once.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1209.0019444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1640642583000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636290176000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug where external MLFlow logging failures cause the training job to fail. When the Databricks updates, the user loses access to MLFlow for a brief period, causing logging to fail. This error not only causes logging to fail but also causes the entire training pipeline to fail, losing progress on a potentially long-running job with limited error handling options currently available. The user is requesting flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":45.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":1209.0019444444,
        "Challenge_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":314.0,
        "Challenge_answer_count":40,
        "Challenge_body":"Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error:\u00a0\n\nQuota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
        "Challenge_closed_time":1684921140000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683790740000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a quota error while trying to use the bison chat model in Vertex AI. The error message indicates that the quota for online prediction requests per base model has been exceeded and the user needs to submit a quota increase request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-bison-chat-model-in\/m-p\/552421#M1857",
        "Challenge_link_count":0,
        "Challenge_participation_count":40,
        "Challenge_readability":9.2,
        "Challenge_reading_time":4.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":314.0,
        "Challenge_title":"Receiving quota error when trying to use bison chat model in Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":0.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"UPDATE: We have raised the default quotas for everyone.\u00a0 This roll out may take a day to reach everyone so.\u00a0 Thank you everyone for your patience and flagging this to us!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1501040260887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":2802.1180147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1625508026043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to use hyperparameters tuning on AWS Sagemaker. All 5 failed training jobs have the same error related to KeyError: 'SM_CHANNEL_TRAINING'. The issue is occurring at Step 4 of the project. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2802.1180147222,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588429621780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation_count":21.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.5540683334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:  <\/p>\n<p>best_run, fitted_model = remote_run.get_output()  <br \/>\nfitted_model  <\/p>\n<p>When runnning the code, I get the following error:   <\/p>\n<p>AttributeError: 'DataTransformer' object has no attribute 'enable_dnn'  <\/p>\n<p>Any help would be much appreciated.   <\/p>",
        "Challenge_closed_time":1613128985603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613083790957,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a remote run model built using the AutoML function. The code is producing an error message stating that the 'DataTransformer' object has no attribute 'enable_dnn'. The user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":12.5540683334,
        "Challenge_title":"Remote run model unable to be saved",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=f0b99777-9086-42ac-b2e2-2b069641e943\">@Bernardo Jaccoud  <\/a> Did your run configure enable_dnn i.e bert <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#bert-integration-in-automated-ml\">settings<\/a> of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":90.6846269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Challenge_closed_time":1593530314240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593203486503,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to train a TensorFlow model on multiple .npy files stored in an S3 bucket. They want to spin up separate AWS training instances for each training job to access the files from S3. The user is encountering an error when calling the fit function of the estimator, stating that the file is not found, even though it is present on S3.",
        "Challenge_last_edit_time":1593203849583,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":21.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":90.7854825,
        "Challenge_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":340.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":27.3,
        "Solution_reading_time":9.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":167.0766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Challenge_closed_time":1635333937000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634732461000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the wandb logger while using a wandb-callbacks branch. After running `python train.py logger=wandb`, the user gets an error message stating that the job was cancelled by the user after 130 iterations because the wandb login does not appear. Changing `logger: wandb` in train.yaml does not work either. The user has tried different conda envs with different torch and pl versions but is still unable to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":167.0766666667,
        "Challenge_title":"wandb import failure",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":45,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":498.5483013889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML provides client libraries (e.g. azureml for Python) for dataset management and model deploying. From what I understand, the custom algorithm would be serialized as a Pickle file, but I'm not sure what happens after that. If I have a custom model with a deep NN architecture and set up a web service for training and another for scoring, do I still need the machine that the model was developed on for the web services to run? I found this on the azureml documentation that was helpful:<\/p>\n<blockquote>\n<p>If a function has no source file associated with it (for example, you're developing inside of a REPL environment) then the functions byte code is serialized. If the function refers to any global variables those will also be serialized using Pickle. In this mode all of the state which you're referring to needs to be already defined (e.g. your published function should come after any other functions you are calling).<\/p>\n<p>If a function is saved on disk then the entire module the function is defined in will be serialized and re-executed on the server to get the function back. In this mode the entire contents of the file is serialized and the order of the function definitions don't matter.<\/p>\n<\/blockquote>\n<p>What if the function uses a library like TensorFlow or Keras? Can someone explain what happens after the Pickle model is created?<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1532614432912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530819659027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on how Azure web service deployment works locally, particularly with regards to custom models with deep neural network architecture and the use of libraries like TensorFlow or Keras. They are unsure if the machine that the model was developed on is still needed for the web services to run and are seeking further explanation on the process after the model is serialized as a Pickle file.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51198775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":498.5483013889,
        "Challenge_title":"How does Azure web service deployment work locally?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":126.0,
        "Challenge_word_count":239,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338127253383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.<\/p>\n\n<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]]),\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/blogs.technet.microsoft.com\/uktechnet\/2018\/04\/25\/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio\/\" rel=\"nofollow noreferrer\">You may find this post useful<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645548595867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1.8311227778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Challenge_closed_time":1645548706456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645542424610,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using wandb sweep to explore the results of different parameter settings on their python script \"train.py\". The wandb agent executes the file \"train.py\" and passes parameters to it, but the file expects parameter values by position and not with \"--param1=\" in front. The user wants to know how to get wandb to pass the values without \"--param1=\" in front without changing the \"train.py\" file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":12.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.7449572222,
        "Challenge_title":"How to get wandb to pass arguments by position?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":270.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440414980200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":5.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1645549016652,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":23.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":173.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":23.4444030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am setting up deployment pipelines for our models and I wanted to support this scenario:<\/p>\n\n<ol>\n<li>User registers model in <code>test<\/code> AML workspace in test subscription, checks in deployment code\/configs that references the model version (there is a <code>requirements.txt<\/code>-like file that specifies the model ID - name and version)<\/li>\n<li>Azure DevOps CI is triggered after code checkin to run <code>az ml model deploy<\/code> to a test environment.<\/li>\n<li>User decides after that endpoint works well, wants to deploy to prod. In Azure DevOps, manually invokes a prod pipeline that will use the same checked-in code\/configs (with the same referenced model):\n\n<ul>\n<li>copy the model from the <code>test<\/code> AML workspace to a new registered model in the <code>prod<\/code> AML workspace in a different subscription, <em>with the same version<\/em><\/li>\n<li>run <code>az ml model deploy<\/code> with different variables corresponding to the <code>prod<\/code> env, but using the same checked-in AML code\/configs<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>I've looked at the MLOps references but can't seem to figure out how to support step 3 in the above scenario. <\/p>\n\n<p>I thought I could do an <code>az ml model download<\/code> to download the model from the <code>test<\/code> env and register it in the <code>prod<\/code> env. The registration process automatically sets the version number so, e.g. the config that references <code>myModel:12<\/code> is no longer valid since in <code>prod<\/code> the ID is <code>myModel:1<\/code><\/p>\n\n<p>How can I copy the model from one workspace in one subscription to another and preserve the ID?<\/p>",
        "Challenge_closed_time":1567190010648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567105610797,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to set up deployment pipelines for their models and wants to copy a model from one workspace in one subscription to another while preserving the ID. They have tried using \"az ml model download\" to download the model from the test environment and register it in the prod environment, but the registration process automatically sets a new version number, making the config that references the model invalid. The user is seeking guidance on how to support this scenario.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57716459",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":21.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":23.4444030556,
        "Challenge_title":"Copying models between workspaces",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":515.0,
        "Challenge_word_count":242,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254279877887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":153.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You could use model tags to set up your own identifiers that are shared across workspace, and query models with specific tags:<\/p>\n\n<pre><code>az ml model update --add-tag\naz ml model list --tag\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":2.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.3538888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Challenge_closed_time":1624287519000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622734645000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where calling `stop_training_job` from the SageMaker client against an existing job that is not \"InProgress\" causes the client to hang. This issue only seems to happen within the sm-executor. The DEBUG output from the sm-executor shows that the request was rejected because the training job is in status Stopped.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":431.3538888889,
        "Challenge_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":210,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The relevant code snippet for this issue is [here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/7988c933d5f4d6c9878249a7521d64d891707824\/addons\/addon-base-raas-ui\/packages\/base-raas-ui\/src\/parts\/environments-sc\/parts\/ScEnvironmentHttpConnections.js#L60).\r\n\r\nI was unable to reproduce this issue (I'm using Chrome 90.0.4430.212) , but let's try to track this issue down.\r\n\r\nAre we unable to get the Sagemaker URL or is the browser failing to refer the user to the Sagemaker url?\r\n\r\nTo check if the browser is getting the Sagemaker URL correctly:\r\n1. Go to the Sagemaker workspace and click the connection button to open the \"HTTP Connections\" card \r\n2. Open the network tab in Chrome Inspect tool\r\n3. Clear all of the network activity\r\n4. Click on the `url` row on the sidebar\r\n5. In the new tab that opens up, select the sub tab for `response`.\r\n6. Check if a `url` is provided and try navigating to that `url` in the browser.\r\n\r\nIf the URL is valid and you can can navigate to the Sagemaker instance that way, then the issue is with the browser failing to redirect the user.\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/3661906\/120824482-84a88d80-c526-11eb-883b-13d9b4cfa7f4.mov\r\nHere's a video of the process.\r\n\r\nPlease follow the instructions above to help us debug what is the cause of the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":16.2,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":183.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.6833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"While working on simulator of Google Agent assist, the checkbox to enable FAQ or Article Suggestion or smart reply option is DISABLED.\n\nThe chat on simulator is working with virtual agent I have configured via Dialog flow with intents that I have built, but can't use agent assist feature since I am not able to enable it.\n\nPlease note I have configured smart reply, article suggestion and Knowledge Base option in the agent assist in my google project.\n\nLooking for help to fix the issue.",
        "Challenge_closed_time":1675226640000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674702180000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to enable the checkbox for FAQ, Article Suggestion, or Smart Reply options while working on the simulator of Google Agent Assist, despite having configured these features in their Google project. They are seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Agent-Assist-Simulator-view-feature-FAQ-article\/m-p\/514223#M1143",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":145.6833333333,
        "Challenge_title":"Google Agent Assist - Simulator view - feature FAQ, article suggestion CHECKBOX disabled",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Smart reply, FAQ and Articlet Suggestion of google agent assist became visible after I disabled the \"Choose to use Dialogflow\" option in my selected \"Conversation Profile\".\n\nHowever there is another issue. The \"smart reply\"\/\"FAQ\"\/\"Article Suggestion\" though enabled and visible in UI but they are not working. I am not getting any suggestion or smart reply for the customer chat. Any idea what could be wrong ?\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.9,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1705477778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I want to find the best hyperparameters using wandb, however, some combinations of them raises cuda memory error, how could I tell wandb that still runs with new hyperparameter combinations if there are these errors? So I do not need to check that all possible combinations do not raise a memory cuda error. I am afraid that  the whole sweep will stop after a specific number of runs has failed.<\/p>\n<p>Could I use some try except or  tell wandb to always execute new runs (like \u201callow unlimited failed runs\u201d) ?<\/p>\n<p>Thanks in advance<\/p>",
        "Challenge_closed_time":1685374082966,
        "Challenge_comment_count":0,
        "Challenge_created_time":1685369868994,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to find the best hyperparameters using wandb, but some combinations of them raise cuda memory errors. The user is looking for a way to tell wandb to continue running with new hyperparameter combinations even if there are errors, so they do not have to manually check all possible combinations. The user is asking if they can use a try-except or tell wandb to allow unlimited failed runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/allow-unlimited-failed-runs\/4482",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1705477778,
        "Challenge_title":"Allow unlimited failed runs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":11.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is WANDB_AGENT_MAX_INITIAL_FAILURES=1000<\/p>\n<p>Solved <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.7,
        "Solution_reading_time":3.41,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1045.5080555556,
        "Challenge_answer_count":0,
        "Challenge_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_closed_time":1582730951000,
        "Challenge_comment_count":14,
        "Challenge_created_time":1578967122000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to import the ML library in an Azure Notebook VM. The error is related to the attribute error of the TensorFlow logging module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Challenge_link_count":0,
        "Challenge_participation_count":14,
        "Challenge_readability":15.3,
        "Challenge_reading_time":1.92,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1045.5080555556,
        "Challenge_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@alla15747 Hi, thanks for reaching out to us. Could you please share your environment file so that we can know the details of this issue? @alla15747  please make sure that azureml-train-automl-runtime is installed in your environment if you using sdk>=1.0.76 or azureml-train-automl if using older version I'm running the code on the compute target and not my local machine. SDK 1.0.72\r\nHow to install packages in Azure Devops environment like azureml-train-automl? (base) C:\\Users\\aabdel137>pip freeze\r\nabsl-py==0.8.1\r\nadal==1.2.2\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.7\r\nanaconda-project==0.8.3\r\nansiwrap==0.8.4\r\napplicationinsights==0.11.9\r\nasn1crypto==0.24.0\r\nastor==0.8.0\r\nastroid==2.3.1\r\nastropy==3.2.1\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nazure-common==1.1.23\r\nazure-graphrbac==0.61.1\r\nazure-mgmt-authorization==0.60.0\r\nazure-mgmt-containerregistry==2.8.0\r\nazure-mgmt-keyvault==2.0.0\r\nazure-mgmt-resource==5.1.0\r\nazure-mgmt-storage==6.0.0\r\nazureml-contrib-interpret==1.0.72\r\nazureml-contrib-notebook==1.0.72\r\nazureml-core==1.0.72\r\nazureml-dataprep==1.1.29\r\nazureml-dataprep-native==13.1.0\r\nazureml-explain-model==1.0.72\r\nazureml-interpret==1.0.72.1\r\nazureml-pipeline==1.0.72\r\nazureml-pipeline-core==1.0.72\r\nazureml-pipeline-steps==1.0.72\r\nazureml-sdk==1.0.72\r\nazureml-telemetry==1.0.72\r\nazureml-train==1.0.72\r\nazureml-train-core==1.0.72\r\nazureml-train-restclients-hyperdrive==1.0.72\r\nazureml-widgets==1.0.72\r\nBabel==2.7.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.5\r\nbackports.os==0.1.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.7.1\r\nbitarray==0.9.3\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nbokeh==1.2.0\r\nboto==2.49.0\r\nBottleneck==1.2.1\r\ncertifi==2019.6.16\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.2.1\r\nclyent==1.2.2\r\ncolorama==0.4.1\r\ncomtypes==1.1.7\r\nconda==4.7.10\r\nconda-build==3.18.8\r\nconda-package-handling==1.3.11\r\nconda-verify==3.4.2\r\ncontextlib2==0.5.5\r\ncoverage==4.5.4\r\ncryptography==2.7\r\ncycler==0.10.0\r\nCython==0.29.12\r\ncytoolz==0.10.0\r\ndask==2.1.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\ndistributed==2.1.0\r\ndistro==1.4.0\r\ndocker==4.1.0\r\ndocutils==0.14\r\ndotnetcore2==2.1.10\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nflake8-formatter-junit-xml==0.0.6\r\nFlask==1.1.1\r\nfusepy==3.0.1\r\nfuture==0.17.1\r\ngast==0.3.2\r\ngevent==1.4.0\r\nglob2==0.7\r\ngoogle-pasta==0.1.7\r\ngreenlet==0.4.15\r\ngrpcio==1.24.3\r\nh5py==2.9.0\r\nheapdict==1.0.0\r\nhtml5lib==1.0.1\r\nidna==2.8\r\nimageio==2.5.0\r\nimagesize==1.1.0\r\nimportlib-metadata==0.23\r\ninterpret-community==0.1.0.3.3\r\ninterpret-core==0.1.18\r\nipykernel==5.1.1\r\nipython==7.6.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.0\r\nisodate==0.6.0\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.13.3\r\njeepney==0.4.1\r\nJinja2==2.10.1\r\njmespath==0.9.4\r\njoblib==0.13.2\r\njson5==0.8.4\r\njsonpickle==1.2\r\njsonschema==3.0.1\r\njunit-xml==1.8\r\njupyter==1.0.0\r\njupyter-client==5.3.1\r\njupyter-console==6.0.0\r\njupyter-core==4.5.0\r\njupyterlab==1.0.2\r\njupyterlab-server==1.0.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==18.0.0\r\nkiwisolver==1.1.0\r\nkmodes==0.10.1\r\nlazy-object-proxy==1.4.2\r\nlibarchive-c==2.8\r\nllvmlite==0.29.0\r\nlocket==0.2.0\r\nlxml==4.3.4\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.0\r\nmccabe==0.6.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmkl-service==2.0.2\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmsrest==0.6.10\r\nmsrestazure==0.6.2\r\nmultipledispatch==0.6.0\r\nnavigator-updater==0.2.1\r\nnbconvert==5.5.0\r\nnbformat==4.4.0\r\nndg-httpsclient==0.5.1\r\nnetworkx==2.3\r\nnltk==3.4.4\r\nnose==1.3.7\r\nnotebook==6.0.0\r\nnumba==0.44.1\r\nnumexpr==2.6.9\r\nnumpy==1.16.4\r\nnumpydoc==0.9.1\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopenpyxl==2.6.2\r\npackaging==19.2\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\npapermill==1.2.1\r\nparso==0.5.0\r\npartd==1.0.0\r\npath.py==12.0.1\r\npathlib2==2.3.4\r\npathspec==0.6.0\r\npatsy==0.5.1\r\npep8==1.7.1\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.0\r\nply==3.11\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.10.0\r\npsutil==5.6.3\r\npy==1.8.0\r\npy4j==0.10.7\r\npyasn1==0.4.7\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.3\r\npyflakes==2.1.1\r\nPygments==2.4.2\r\nPyJWT==1.7.1\r\npylint==2.4.2\r\npyodbc==4.0.26\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npypiwin32==223\r\npyreadline==2.1\r\npyrsistent==0.14.11\r\nPySocks==1.7.0\r\npyspark==2.4.4\r\npytest==5.2.2\r\npytest-arraydiff==0.3\r\npytest-astropy==0.5.0\r\npytest-cov==2.7.1\r\npytest-doctestplus==0.3.0\r\npytest-openfiles==0.3.2\r\npytest-remotedata==0.3.1\r\npython-dateutil==2.8.0\r\npython-dotenv==0.10.3\r\npytz==2019.1\r\nPyWavelets==1.0.3\r\npywin32==223\r\npywinpty==0.5.5\r\nPyYAML==5.1.1\r\npyzmq==18.0.0\r\nQtAwesome==0.5.7\r\nqtconsole==4.5.1\r\nQtPy==1.8.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.2.0\r\nrope==0.14.0\r\nruamel-yaml==0.15.46\r\nruamel.yaml==0.15.89\r\nscikit-image==0.15.0\r\nscikit-learn==0.21.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nSecretStorage==3.1.1\r\nSend2Trash==1.5.0\r\nshap==0.29.3\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.12.0\r\nsklearn==0.0\r\nsnowballstemmer==1.9.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.8\r\nSphinx==2.1.2\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.1.2\r\nspyder==3.3.6\r\nspyder-kernels==0.5.1\r\nSQLAlchemy==1.3.5\r\nstatsmodels==0.10.0\r\nsympy==1.4\r\ntables==3.5.2\r\ntblib==1.4.0\r\ntenacity==5.1.5\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-gpu==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntextwrap3==0.9.2\r\ntf-estimator-nightly==1.14.0.dev2019031401\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.37.0\r\ntraitlets==4.3.2\r\ntyped-ast==1.4.0\r\nunicodecsv==0.14.1\r\nunittest-xml-reporting==2.5.2\r\nurllib3==1.24.2\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.15.4\r\nwidgetsnbextension==3.5.0\r\nwin-inet-pton==1.1.0\r\nwin-unicode-console==0.5\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nxlrd==1.2.0\r\nXlsxWriter==1.1.8\r\nxlwings==0.15.8\r\nxlwt==1.3.0\r\nzict==1.0.0\r\nzipp==0.6.0\r\n AutoML became a part of default distribution (azureml-sdk) since 1.0.83\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2020-01-06\r\n\r\nif your client, that I believe pins the version of azureml sdk packages for the remote environment is 1.0.83, you will have automl on remote. \r\n\r\nIf you want to stay with 1.0.72 you can either reference automl extras azureml-sdk[automl] or explicitly reference azureml-train-automl (prefered).\r\n\r\nI would recommend to do both, upgrade client to the latest version and explicitly reference packages you need for your particular scenario not relying on metapackages like azureml-sdk\r\n\r\nOur reference doc will help you to get a set of the packages needed for your scenario\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py\r\n\r\nBy design every AzureML Python SDK package will bring necessary internal dependencies (of course except some corner cases :) )\r\n Thanks Vizhur, do you mind showing an example on how to reference azureml-train-automl in Azure Devops or Portal? \r\nThank you for the links! Not sure about your particular scenario, would you mind to share your ADO scenario so I can think of how to update it? MY scenario is implementing MLOPs example with automl step. Let me try to pull some relevant folks into the thread For AutoML, all the remote dependencies will get taken care of and will match whatever local dependencies are installed, e.g. if you have azureml-train-automl==1.0.72 installed, that version will be installed remotely for the training job.\r\nWe provide 2 clients to submitting these remote jobs currently, a thin client for submitting some types of remote jobs which is included as part of azureml-sdk, and a fuller client which enables more experiences such as Pipeline runs as part of azureml-train-automl. Since it looks like you are trying to use Pipelines, you will need to install the full azureml-train-automl client.\r\n\r\nFurthermore, the namespace for AutoMLStep changed recently, if you are using <1.0.76 the namespace would be \"from azureml.train.automl import AutoMLStep\", for >=1.0.76, you'll want to use \"from azureml.train.automl.runtime import AutoMLStep\" instead. I'm have sdk 1.0.72 installed. And I'm using from azureml.train.automl import AutoMLStep. Is there anyway to check the sdk version on the compute target machine? From your pip freeze, it doesn't look like you have the AutoML SDK installed. For the pipelines experience, you will need to have the SDK installed locally, not just on the target compute. Could you run \"pip install azureml-train-automl\"? @alla15747 \r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion. @SKrupa - Are you running your own code or a particular notebook sample from this repo?\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":117.81,
        "Solution_score_count":null,
        "Solution_sentence_count":38.0,
        "Solution_word_count":786.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":45.3303,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Challenge_closed_time":1611831268823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611668079743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying a training job in their AWS Sagemaker notebook instance. The error message stated that the data download failed due to insufficient disk space. The user tried running the training job on different instances with two sets of training data, and while it worked initially, they encountered the same error when trying to run it on a different instance. The user tried refreshing everything and restarting the instances, but the issue persisted.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":45.3303,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570029753880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Philippines",
        "Poster_reputation_count":98.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":3.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":134.9044,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>if I tune a model with the LightGBMTunerCV I always get this massive result of the cv_agg's binary_logloss. If I do this with a bigger dataset, this (unnecessary) io slows down the performance of the optimization process.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nimport optuna\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nbreast_cancer = load_breast_cancer()\n\nX_train, X_test, Y_train, Y_test = train_test_split(breast_cancer.data, breast_cancer.target)\n\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=breast_cancer.feature_names.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=breast_cancer.feature_names.tolist())\ncallbacks = [lgb.log_evaluation(period=0)]\ntuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True)\n\n\ntuner.run()\n<\/code><\/pre>\n<p>And the output:<\/p>\n<pre><code>feature_fraction, val_score: 0.327411:  43%|###################2      | 3\/7 [00:00&lt;00:00, 13.84it\/s]\n[1] cv_agg's binary_logloss: 0.609496 + 0.009315\n[2] cv_agg's binary_logloss: 0.554522 + 0.00607596\n[3] cv_agg's binary_logloss: 0.512217 + 0.0132959\n[4] cv_agg's binary_logloss: 0.479142 + 0.0168108\n[5] cv_agg's binary_logloss: 0.440044 + 0.0166129\n[6] cv_agg's binary_logloss: 0.40653 + 0.0200005\n[7] cv_agg's binary_logloss: 0.382273 + 0.0242429\n[8] cv_agg's binary_logloss: 0.363559 + 0.03312\n<\/code><\/pre>\n<p>Is there any way to get rid of this output?<\/p>\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1638229134663,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637743478823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the LightGBMTunerCV model where the cv_agg's binary_logloss output is slowing down the optimization process, especially with larger datasets. The user is seeking a way to suppress this output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70093026",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":134.9044,
        "Challenge_title":"Supressing optunas cv_agg's binary_logloss output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":200.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398509643447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dortmund, Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can pass verbose_eval parameter with value None in LightGBMTunerCV().<\/p>\n<p>Example:<\/p>\n<pre><code>tuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True, verbose_eval=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":4.25,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1428499037932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leipzig, Deutschland",
        "Answerer_reputation_count":2124.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":1.1770155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1658393400156,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658389162900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Weights & Biases sweeps for hyperparameter search for their NER model. They have done a grid search with about 100 runs and want to create a graph that shows the best 10 runs in terms of f-score, but they are unable to figure out how to do it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1770155556,
        "Challenge_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643710211767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.93,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":459.2678666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an experiment to create recommendations (using the Movie Ratings sample database), but without using the ratings. I simply consider that if a user has rated certain movies, then he would be interested by other movies that have been rated by users that have also rated his movies.<\/p>\n\n<p>I can consider, for instance, that ratings are 1 (exists in the database) or 0 (does not exist), but in that case, how do I transform the initial data to reflect this?<\/p>\n\n<p>I couldn't find any kind of examples or tutorials about this kind of scenario, and I don't really know how to proceed. Should I transform the data before injecting it into an algorithm? And\/or is there any kind of specific algorithm that I should use?<\/p>",
        "Challenge_closed_time":1471354453808,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469698571557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create recommendations using the Movie Ratings sample database in Azure ML without using ratings. They want to consider if a user has rated certain movies, then they would be interested in other movies that have been rated by users that have also rated their movies. The user is unsure how to transform the initial data to reflect this and is seeking advice on whether to transform the data before injecting it into an algorithm and if there is a specific algorithm to use.",
        "Challenge_last_edit_time":1469701089488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38632533",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":459.9672919444,
        "Challenge_title":"Recommendations without ratings (Azure ML)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":910.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461857379436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lille, France",
        "Poster_reputation_count":133.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are<\/em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)<\/p>\n\n<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:<\/p>\n\n<pre><code>library(dplyr)\ndf &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs\nall_movies &lt;- unique(df[['movie']])\nall_users &lt;- unique(df[['user']])\nn &lt;- 30  # number of random pairs to start with\n\nnegative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),\n                                    user = sample(all_users, n, replace=TRUE),\n                                    rating = rep(0, n))          \nacceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))\ndf &lt;- rbind(df, acceptable_negative_observations)\nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>\n\n<p>Alternatively, you could try a method like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning\" rel=\"nofollow\">association rule learning<\/a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Frequently-bought-together-market-basket-analyses-using-ARULES-1\" rel=\"nofollow\">nice example<\/a> of how to do this in R\/AML in the Cortana Intelligence Gallery.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1486312694643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Germany",
        "Answerer_reputation_count":668.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":7705.2959355556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Challenge_closed_time":1601639323903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601638106830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is experiencing slow query performance when using PyAthena to query data from Athena compared to running the same query directly from the Athena console. The user is unsure if PyAthena is causing the slowdown or if the data transfer from Athena to Sagemaker is the issue. The user is seeking advice on how to speed up the query performance.",
        "Challenge_last_edit_time":1601641259400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3380758334,
        "Challenge_title":"Pyathena is super slow compared to querying from Athena",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3188.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486312694643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Germany",
        "Poster_reputation_count":668.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1629380324768,
        "Solution_link_count":2.0,
        "Solution_readability":16.7,
        "Solution_reading_time":10.36,
        "Solution_score_count":13.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":81.7717394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I resume a sweep with <code>method<\/code> set to <code>bayes<\/code> (in its configuration), i.e. with <code> wandb sweep --resume<\/code>, does the Bayesian optimization process keep into consideration the parameter values already explored before the sweep was interrupted? Or is the previous history of the sweep ignored?<\/p>",
        "Challenge_closed_time":1685036073776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684741695514,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether the Bayesian optimization process considers the parameter values explored in previous runs when resuming a sweep with the \"bayes\" method using \"wandb sweep --resume\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/when-resuming-a-sweep-with-bayesian-optimization-are-the-previous-runs-kept-into-consideration\/4440",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":81.7717394444,
        "Challenge_title":"When resuming a sweep with Bayesian optimization, are the previous runs kept into consideration?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fantauzzi\">@fantauzzi<\/a> , whenever you pause a sweep and then resume it, all methods of a sweep retain history of the parameter values already explored. Resuming will pick up where the sweep left off and run to completion.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.28,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.1534905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Challenge_closed_time":1636391832512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636160879946,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to integrate HParams with Wandb to perform hyperparameter tuning for a custom model using TensorFlow. However, when initializing Wandb with wandb.init(), it seems to initialize it for the whole process and doesn't change when it is a new parameter set. The user is calling wandb.init() and logging after each parameter run, but it doesn't create a unique job. The user is seeking a solution to record each call to train_model in a new run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":64.1534905556,
        "Challenge_title":"Using Wandb with HParams on TF",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":579.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just had a chat with the support and figured out how to fix the problem with over-writing.<\/p>\n<p>Issue was with the init function and there is a flag for reinitializing (<code>reinit=True<\/code>)<\/p>\n<p><code>wandb.init(project=\"newjob1\", entity=\"demou\",reinit=True)<\/code>  this fixed this issue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.018835,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I run several agents in one sweep.<br>\nI want to stop a specific agent among them, but I don\u2019t know how to stop it.<\/p>",
        "Challenge_closed_time":1658470957452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658330489646,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is running multiple agents and wants to know how to stop a specific agent using a command in the terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-kill-a-specific-agent-using-a-command-in-terminal\/2782",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.2,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":39.018835,
        "Challenge_title":"How to kill a specific agent using a command in terminal?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jeongwhanchoi\">@jeongwhanchoi<\/a> , please see this <a href=\"https:\/\/community.wandb.ai\/t\/hp-sweep-correct-way-to-stop-a-specific-agent-and-not-the-entire-sweep\/1173\">post<\/a> for stopping agents. Please let me know if you have additional questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":3.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7982433333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any way to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer)?I know there is an integration between CI and Azure Machine Learning studio (classic). Please help to integrate these two services.<\/p>",
        "Challenge_closed_time":1656632594976,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656618921300,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer) as there is already an integration between CI and Azure Machine Learning studio (classic).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909965\/azure-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.7982433333,
        "Challenge_title":"Azure machine learning",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @Yasuo-9899     <\/p>\n<p>Thanks for reaching out to us for this question. Are you looking for this document? <a href=\"https:\/\/learn.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\">https:\/\/learn.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments<\/a>    <\/p>\n<p>I have found one pic which is described the structure well:    <br \/>\n<img src=\"https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/raw\/main\/images\/workshop-playbook\/media\/image2.png\" alt=\"image2.png\" \/>    <\/p>\n<p>And also a repo you may want to refer to: <a href=\"https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\">https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md<\/a>    <\/p>\n<p>Please let us know more details you are interested in so that we can help. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":20.9,
        "Solution_reading_time":12.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5792555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Challenge_closed_time":1610631020420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610628627223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to visualize real-time losses and metrics for a TensorFlow model on an AWS Sagemaker instance using Tensorboard in a Jupyter notebook, but is encountering issues and is seeking guidance on how to get it working.",
        "Challenge_last_edit_time":1610628935100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":4.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6647769444,
        "Challenge_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":715.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512023194592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":547.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1383535151876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":502.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":70.0674963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mlflow question regarding to serve R models, below is my code to train and serialize the R model.<\/p>\n<pre><code>library(mlflow)\nlibrary(caret)\nlibrary(carrier)\n\nset.seed(101)\nsample &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)\niris_train &lt;- iris[sample,]\niris_test &lt;- iris[-sample,]\n\n\nwith(mlflow_start_run()  , {\n  control &lt;- trainControl(method='cv', number=10)\n  metric &lt;- 'Accuracy'\n\n  # Linear Discriminant Analysis (LDA)\n  model &lt;- train(Species~., data=iris_train, method='lda', trControl=control, metric=metric,\n                preProcess=c(&quot;center&quot;, &quot;scale&quot;))\n  fn &lt;- crate(~ caret::predict.train(model, .x), model = model)\n  # fn &lt;- crate(~ stats::predict(model, .x), model = model)\n  iris_prediction &lt;- predict(model, iris_test)\n\n  mlflow_log_model(model = fn, artifact_path=&quot;model&quot;)\n})\n\n<\/code><\/pre>\n<p>However, when I serve it with the API request,<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.With&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>\n<p>I have the following error:\n<code>Invalid Request.  object 'Sepal.Width' not found<\/code><\/p>\n<p>Note, if I change the training to use linear regression model,<\/p>\n<pre><code>with(mlflow_start_run(), {\n  model &lt;- lm(Sepal.Width ~ Sepal.Length, iris)\n  mlflow_log_model(\n    crate(~ stats::predict(model, .x), model=model), &quot;model&quot;)\n})\n<\/code><\/pre>\n<p>The same API request, everything works fine.<\/p>\n<p>Can anyone help me find any clue? I am not that familiar with R.<\/p>\n<p>Thanks a lot!<\/p>",
        "Challenge_closed_time":1641228071910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640975828923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while serving R models using mlflow. The API request fails with an error message stating that the object 'Sepal.Width' is not found when using LDA instead of linear regression for training the model. However, the API request works fine when using linear regression. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70544873",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":24.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":70.0674963889,
        "Challenge_title":"mlflow serving r models failed if use LDA instead of linear regression",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":194,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383535151876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":502.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>it turns out the issue is a typo in curl script. it's Sepal.Width not Sepal.With.<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":12.9035880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Challenge_closed_time":1649156563120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649110110203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where wandb is getting logged without initiating it. The user does not want to use wandb and is following a notebook for finetuning. However, when the user runs trainer.train(), they get an error related to wandb.log being called. The user has tried disabling wandb but still encounters the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":12.9035880556,
        "Challenge_title":"wandb getting logged without initiating",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499867951607,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":307.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":133.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1526732158023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130stanbul, T\u00fcrkiye",
        "Answerer_reputation_count":888.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":178.8386513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting an error on my modeling of lightgbm searching for optimal auc. Any help would be appreciated.<\/p>\n<pre><code>import optuna  \nfrom sklearn.model_selection import StratifiedKFold\nfrom optuna.integration import LightGBMPruningCallback\ndef objective(trial, X, y):\n    param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;auc&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),\n        &quot;lambda_l2&quot;: trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n    }\n\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, &quot;auc&quot;)\n        \n        model = lgb.LGBMClassifier(**param)\n        \n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            early_stopping_rounds=100,\n            callbacks=[pruning_callback])\n        \n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n        auc_scores[idx] = roc_auc_score(y_test, preds)\n        \n    return np.mean(cv_scores), np.mean(auc_scores)\n    \n\n\nstudy = optuna.create_study(direction=&quot;minimize&quot;, study_name=&quot;LGBM Classifier&quot;)\nfunc = lambda trial: objective(trial, sample_df[cols_to_keep], sample_df[target])\n\nstudy.optimize(func, n_trials=1)\n<\/code><\/pre>\n<blockquote>\n<p>Trial 0 failed because of the following error: ValueError('The\nintermediate values are inconsistent with the objective values in\nterms of study directions. Please specify a metric to be minimized for\nLightGBMPruningCallback.',)*<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1649404346652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648760527507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while modeling LightGBM to search for optimal AUC. The error message suggests that the intermediate values are inconsistent with the objective values in terms of study directions, and it recommends specifying a metric to be minimized for LightGBMPruningCallback.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71699098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":30.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":178.8386513889,
        "Challenge_title":"Optuna LightGBM LightGBMPruningCallback",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380052343127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":419.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>Your objective function returns two values but you specify only one direction when creating the study. Try this:<\/p>\n<pre><code>study = optuna.create_study(directions=[&quot;minimize&quot;, &quot;maximize&quot;], study_name=&quot;LGBM Classifier&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1225669466307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, United Kingdom",
        "Answerer_reputation_count":236107.0,
        "Answerer_view_count":18730.0,
        "Challenge_adjusted_solved_time":0.1705583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1410733358503,
        "Challenge_comment_count":3,
        "Challenge_created_time":1410732744493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to translate a C# code sample that invokes a web service in Azure ML to F#. The user has everything except the async and await done.",
        "Challenge_last_edit_time":1446025307110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":17.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1705583334,
        "Challenge_title":"C# async\/await to F# using Azure ML example",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":607.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349689794400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation_count":4174.0,
        "Poster_view_count":396.0,
        "Solution_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":18.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":156.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.0155555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Challenge_closed_time":1650034426000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649933570000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the full episode data logging in a random agent script using wandb, where some steps are being skipped. The problem is due to wandb counting the episode reward logging steps made before the full data logging. A potential solution suggested is to add another metric to log the timestep and day proportionally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":102.0,
        "Challenge_repo_star_count":20.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.0155555556,
        "Challenge_title":"In random agent script wandb full episode data logging skips a few steps",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This has been implemented and will be shipped with v0.4.4 \ud83d\ude80",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.72,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":1.0135986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Machine Learning Services and when I am trying to implement Deep Neural Network, I am getting CV2 issue. The CV2 library is being bothering the code block. The following is the error I am getting when I am trying to use CV2 for DNN_BACKEND_CUDA.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hTSXM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hTSXM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Any help is appreciable.<\/p>",
        "Challenge_closed_time":1652071883272,
        "Challenge_comment_count":4,
        "Challenge_created_time":1652068234317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while implementing Deep Neural Network using Machine Learning Services due to the CV2 library causing errors in the code block. The error occurs when using CV2 for DNN_BACKEND_CUDA. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1652092278248,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72166808",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0135986111,
        "Challenge_title":"OpenCV issue while using DNN implementation with any version in Machine Learning Services",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651094469216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>The issue raised is very rare and there are less chances of getting the success rate even after the proper installation of libraries. When the code was deployed in Azure Machine Learning some of the issues might be resolved. Checkout the following steps to be taken care of:<\/p>\n<ol>\n<li>Check with the version of Open CV<\/li>\n<\/ol>\n<p><code>import cv2<\/code><\/p>\n<p><code>cv2.__version__<\/code><\/p>\n<ol start=\"2\">\n<li>After installation, implement the following steps<\/li>\n<\/ol>\n<p>these steps are very much time taking.<\/p>\n<pre><code>%cd \/content\n!git clone https:\/\/github.com\/opencv\/opencv\n!git clone https:\/\/github.com\/opencv_contrib\n!mkdir \/content\/build\n%cd \/content\/build\n!cmake -DOPENCV_EXTRA_MODULES_PATH=\/content\/opencv_contrib\/modules  -DBUILD_SHARED_LIBS=OFF  -DBUILD_TESTS=OFF  -DBUILD_PERF_TESTS=OFF -DBUILD_EXAMPLES=OFF -DWITH_OPENEXR=OFF -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUDNN=ON -DOPENCV_DNN_CUDA=ON \/content\/opencv\n!make -j8 install\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the version of Open CV again.<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.9,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":14.8662555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Challenge_closed_time":1641974198663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641920680143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about how parallel trials work in GCP Vertex AI when running hyperparameter tuning jobs. They are unsure how multiple trials can run in parallel with only one worker and how each task can occupy 100% of the CPU's resources. The user is questioning if GCP provisions more than one machine to handle parallel trials.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.8662555556,
        "Challenge_title":"How do parallel trials in GCP Vertex AI work?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579801831103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":13.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":113.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":4.1729647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Challenge_closed_time":1570725963836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570710941163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a local Azure ML model using the provided code. The error message \"tarfile.ReadError: file could not be opened successfully\" is displayed in the terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1729647222,
        "Challenge_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570704481987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.0937622222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I've created an azure for students account wiht $100 free credit and started using Azure Notebooks to train some ML models. I've created a GPU instance which costs $1.20\/hr. I've been using it for at least 1.5h now and what's weird is that no usage is being shown on my dashboard, and on the sponsorship page it's showing that it is not active and that I haven't used any of my credit:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239817-image.png?platform=QnA\" alt=\"239817-image.png\" \/>    <\/p>\n<p>On the other hand when I go to my subscriptions it says it's active:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239809-image.png?platform=QnA\" alt=\"239809-image.png\" \/>    <\/p>\n<p>Is something wrong or does it take a while to see usage statistics\/credit spending?    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1662904704207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662893566663,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created an Azure for Students account with $100 free credit and has been using Azure Notebooks to train ML models on a GPU instance costing $1.20\/hr. However, despite using it for at least 1.5 hours, no usage is being shown on the dashboard and the sponsorship page shows that it is not active and no credit has been used. The subscriptions page shows that it is active. The user is unsure if something is wrong or if it takes time to see usage statistics\/credit spending.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1002201\/azure-for-students-showing-no-usage-despite-using",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.0937622222,
        "Challenge_title":"Azure for students showing no usage despite using it",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":125,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>Usually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/understand\/download-azure-daily-usage\">download-azure-daily-usage<\/a>    <\/p>\n<p>==    <br \/>\nPlease &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":27.9929452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Challenge_closed_time":1653620941596,
        "Challenge_comment_count":2,
        "Challenge_created_time":1653520166993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the VertexAI class AutoMLtabularTrainingJob, specifically with two parameters listed in the documentation: column_specs and export_evaluated_data_items. The user has created a function to create a prediction model, but the class is not accepting these parameters, resulting in a TypeError. The user is questioning whether the documentation is updated and accurate.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72385022",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":23.0,
        "Challenge_reading_time":39.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.9929452778,
        "Challenge_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1563027261236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":315.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":11.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":78.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":4.3897661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Challenge_closed_time":1559781766688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559765963530,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained and deployed a PyTorch model in Sagemaker and is able to get predictions using the default input_fn() function. However, they are facing challenges in serving the model with REST API and HTTP POST using lambda and API gateway. They are not sure what to send in the body for PyTorch and believe they need to write a custom input_fn to accept and process the data sent through invoke_client. They are seeking guidance on how to write the input_fn to accept a CSV from invoke_endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4.3897661111,
        "Challenge_title":"Making a Prediction Sagemaker Pytorch",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2346.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":20.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":172.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.1527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Challenge_closed_time":1661948109000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661861159000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running train_model from examples after installing graphnet from scratch and signing up to WandB. The error occurred due to the absence of a directory called \"wandb\" and can be fixed by creating the folder manually. The user suggests automatically creating the folder if it is not present.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.9,
        "Challenge_reading_time":59.04,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":24.1527777778,
        "Challenge_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":330,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8082.0786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"It fails at the \"apply patch\" stage",
        "Challenge_closed_time":1624956881000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1595861398000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a warning message in Enchanter v0.7.0 stating that the function `log_asset_data(..., file_name=...)` is deprecated and should be replaced with `log_asset_data(..., name=...)` when using Context API. The user did not provide any error messages, stack traces, or logs, nor did they provide steps to reproduce the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.94,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":12.0,
        "Challenge_repo_issue_count":219.0,
        "Challenge_repo_star_count":42.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8082.0786111111,
        "Challenge_title":"Comet \"Reproduce\" feature doesn't work",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":11,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"is this still an issue @51N84D ? Yeah, this still doesn't work. I don't think anyone has tried to resolve it yet Ok ; should we in your opinion?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.7,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.1641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644307000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599546516000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue when using Hydra and MLFlow together, where the parameters passed to the LightningModule is a `DictConfig`, causing the condition in the `logger\/base.py` to not be met. This results in an error message when trying to log hyperparameters with MLFlow. The expected behavior is to check whether the instance is `dict` or `DictConfig` in the given line.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":27.1641666667,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":532,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":null,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":0.4063402778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Challenge_closed_time":1464685093652,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464683630827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to test multiple algorithms at once in Azure Machine Learning Studio to quickly determine which one has the lowest error rate.",
        "Challenge_last_edit_time":1465977920520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37540703",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.2,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4063402778,
        "Challenge_title":"Test multiple algorithms in one experiment",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is able to log and fetch metrics to AzureML using Run.log, but is unable to log run parameters like Learning Rate or Momentum. The user has tried to find a solution in the AzureML Python SDK documentation but has been unsuccessful. However, the user has found a way to log parameters using MLflow's mlflow.log_param, which shows up on the AzureML Studio Dashboard. The user is looking for a way to log parameters directly using azureml.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":688.1130463889,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1043.1541666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Challenge_closed_time":1654689077000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1650933722000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where wandb is not compatible with PL 1.6.1 while using Hyperparameter Search, resulting in a FileNotFoundError. The error occurs while trying to access the debug-internal.log file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.0,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1043.1541666667,
        "Challenge_title":"Wandb is not compatible with PL 1.6.1",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I have met the same problem. > I have met the same problem.\r\n\r\nInstall PL=1.5.10 for me it's working with 1.6.3 \r\nonly update wandb 0.12.16",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.62,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1403804251956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":609.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":0.3923111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Challenge_closed_time":1603941358963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603939946643,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up a hyperparameter tuning job for the xgboost algorithm in Sagemaker. The error is related to the \"scale_pos_weight\" parameter, which cannot be tuned for the algorithm. The user has tried upgrading the Sagemaker package and restarting the kernel and instance, but the problem persists. The user has shared the code used for the hyperparameter tuning job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":31.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.3923111111,
        "Challenge_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":827.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524603494496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.4,
        "Solution_reading_time":4.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1249258805300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":909.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":1.4120652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Challenge_closed_time":1533929944952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533924861517,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to initialize a hyperparameter tuning job for DeepAR on SageMaker. The error message indicates that the objective metric type specified for the job is not valid for the algorithm. The user has tried upgrading the sagemaker package and restarting the instance and kernel, but the problem persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.4,
        "Challenge_reading_time":47.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1.4120652778,
        "Challenge_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1355.0,
        "Challenge_word_count":285,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378935265347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":11.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":25.8737591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a neural-network where I'm using Optuna to find some optimal hyper-parameters e.g batch-size etc.<\/p>\n<p>I want to save the nets-parameters when Optuna finds a new best parameter-combination.<\/p>\n<p>I have tried the following two approahces:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>SCORE = 0\ndef objective(trial):\n    BATCH_SIZE = trial.suggest_int(&quot;BATCH_SIZE&quot;,20,100)\n    LEARNING_RATE = trial.suggest_float(&quot;LEARNING_RATE&quot;,0.05,1)\n    DROPOUT = trial.suggest_float(&quot;DROPOUT&quot;,0.1,0.9)\n\n    Y_SCORE,Y_VAL = train_NN(X,y,word_model,BATCH_SIZE,250,LEARNING_RATE,DROPOUT)\n    y_val_pred = Y_SCORE.argmax(axis=1)\n    labels = encode.inverse_transform(np.arange(6))\n    a = classification_report(Y_VAL, y_val_pred,zero_division=0,target_names=labels,output_dict=True)\n    score = a.get(&quot;macro avg&quot;).get(&quot;f1-score&quot;)\n    if score&gt;SCORE: #New best weights found - save the net-parameters\n        SCORE = score\n        torch.save(net,&quot;..\/model_weights.pt&quot;)\n    return score\n<\/code><\/pre>\n<p>which fails with <code>UnboundLocalError: local variable 'SCORE' referenced before assignment<\/code> but if I move <code>SCORE=0<\/code> inside the function at the top, it resets at each trial.<\/p>\n<p>The reason I want to save the weights right away, and not just run another training with <code>study.best_params<\/code> at the end, is that sometimes the random initialization of the weights has an impact and gives a higher score (although if the training is robust, it should not make a difference) - but that is not the point\/the issue.<\/p>",
        "Challenge_closed_time":1628000129956,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627906984423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save the neural network's parameters when Optuna finds a new best parameter combination. They have tried two approaches, but both have failed. The first approach resulted in an \"UnboundLocalError\" while the second approach resets the score at each trial. The user wants to save the weights right away instead of running another training with \"study.best_params\" at the end.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68621547",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":25.8737591667,
        "Challenge_title":"\"do something\" when Optuna finds new best parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":483.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461069934027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3961.0,
        "Poster_view_count":616.0,
        "Solution_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/62144904\/python-how-to-retrive-the-best-model-from-optuna-lightgbm-study\/62164601#62164601\">This answer<\/a> is helpful to save the neural network's weights; We can use the callback function to save the model's checkpoint.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":3.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.1277094445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>I\u2019m implementing W&amp;B into an existing project in which Agent, Model creation and Environment are constructed in classes. The code structure in the Python file (<code>AIAgent.py<\/code>) looks like this:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nconfig = {\n    'layer_sizes': [17, 16, 12, 4],\n    'batch_minsize': 32,\n    'max_memory': 100_000,\n    'episodes': 2,\n    'epsilon': 1.0,\n    'epsilon_decay': 0.998,\n    'epsilon_min': 0.01,\n    'gamma': 0.9,\n    'learning_rate': 0.001,\n    'weight_decay': 0,\n    'optimizer': 'sgd',\n    'activation': 'relu',\n    'loss_function': 'mse'\n}\n\nclass AIAgent:\n    def __init__(self):\n        self.config = config\n        self.pipeline(self.config)\n\n\n    def pipeline(self, config):\n        wandb.init()\n        config = wandb.config\n\n        model, criterion, optimizer = self.make(config)\n        self.train(model, criterion, optimizer, config) \n\n\n    def make(self, config):\n        model = LinearQNet(config).to(device)\n\n        if config['loss_function'] == 'mse':\n            criterion = nn.MSELoss()\n\n        if config['optimizer'] == 'adam':\n            optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=config['weight_decay'], amsgrad=False)\n \n        wandb.watch(model, criterion, log='all', log_freq=1)\n        summary(model)\n\n        return model, criterion, optimizer\n\n\n    def train(self, model, criterion, optimizer, config):\n        for episode in range(1, config['episodes'] + 1):\n            while True:\n                # Where the training is performed\n\n                if done:\n                    if (episode % 1) == 0:\n                        wandb.log({'episode': episode, 'epsilon': epsilon, 'score': score, 'loss': loss_mean, 'reward': reward_mean, 'score_mean': score_mean, 'images': [wandb.Image(img) for img in env_images]}, step=episode})\n                    break\n\n            if episode &lt; config['episodes']:\n                game.game_reset()\n            else:\n                wandb.finish()\n                break\n\n\nclass LinearQNet(nn.Module):\n    def __init__(self, config):\n        super(LinearQNet, self).__init__()\n        self.config = config\n        # Where the NN is configured\n\n\nif __name__ == '__main__':\n    AIAgent.__init__(AIAgent())\n<\/code><\/pre>\n<p>I\u2019m currently initializing the sweep configuration via a .yaml file calling  <code>wandb sweep sweep.yaml<\/code>. The sweep.yaml file looks like this:<\/p>\n<pre><code class=\"lang-auto\">program: AIAgent.py\nproject: evaluation-sweep-1\nmethod: random\nmetric:\n  name: score_mean\n  goal: maximize\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\nparameters:\n  layer_sizes:\n    distribution: constant\n    value: [17, 16, 512, 4]\n  batch_minsize:\n    distribution: int_uniform\n    max: 1024\n    min: 32\n  max_memory:\n    distribution: constant\n    value: 100_000\n  episodes:\n    distribution: constant\n    value: 50\n  epsilon:\n    distribution: constant\n    value: 1.0\n  epsilon_decay:\n    distribution: constant\n    value: 0.995\n  epsilon_min:\n    distribution: constant\n    value: 0.01\n  gamma:\n    distribution: uniform\n    max: 0.99\n    min: 0.8\n  learning_rate:\n    distribution: uniform\n    max: 0.1\n    min: 0.0001  \n  weight_decay:\n    distribution: constant\n    value: 0\n  optimizer:\n    distribution: categorical\n    values: ['sgd', 'adam', 'adamw']\n  activation:\n    distribution: categorical\n    values: ['relu', 'sigmoid', 'tanh', 'leakyrelu']\n  loss_function:\n    distribution: constant\n    value: 'mse'\nearly_terminate:\n  type: hyperband\n  min_iter: 5\n<\/code><\/pre>\n<p>Besides general feedback on the implementation I\u2019m a bit dumbfounded with a current bug. The sweeps run fine and show up in the W&amp;B interface but every sweep is performed twice under the same name of which only the loffing of the first is displayed and the second runs \u2018silently\u2019 in the environment without update of wandb.log. Does anybody have an idea what the reason for this might be?<\/p>\n<p>Thanks,<br>\nTobias<\/p>",
        "Challenge_closed_time":1652885547134,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652719487380,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is implementing W&B into an existing project using classes and sweep configuration. The sweep runs fine, but every sweep is performed twice under the same name, and only the logging of the first is displayed, while the second runs silently without updating wandb.log. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-integration-using-class-and-sweep-running-twice-under-the-same-name\/2433",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":45.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":46.1277094445,
        "Challenge_title":"Wandb integration using class and sweep running twice under the same name",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":320.0,
        "Challenge_word_count":392,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Tobias,<\/p>\n<p>Looks like the source of this bug is this line: <code>AIAgent.__init__(AIAgent())<\/code> which is calling 2 constructors: 1 from <code>AIAgent.__init__()<\/code> and 1 from <code>AIAgent()<\/code>. This, in turn calls <code>pipeline<\/code> twice, which ends up meaning 2 calls to <code>wandb.init()<\/code> and therefore you see 2 runs.<\/p>\n<p>I would suggest changing that line to just <code>AIAgent<\/code> to prevent this error.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":6.14,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":60.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.9051222222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing run data keys in several of my runs. Specifically, I have logged a metric in my code, the metric is tracked in the online wandb UI, but when I try accessing the data using the following code<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nrun = api.run(\"xxxxxx\")\nrun.history()[['_step', 'metric_name']]\n<\/code><\/pre>\n<p>It throws a <code>KeyError: \"['metric_name'] not in index\"<\/code>.<\/p>\n<p>When I print out <code>run.history()<\/code> in table format, it does show \u2018metric_name\u2019 as one of the columns; \u2018metric_name\u2019 also appears as a key in <code>run.summary<\/code>. I wonder what is the issue here?<\/p>\n<p>Would appreciate any help. Thank you!<\/p>",
        "Challenge_closed_time":1670943419675,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670785361235,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to access run data keys in several of their runs despite logging a metric in their code and tracking it in the online wandb UI. When attempting to access the data using the provided code, a KeyError is thrown. The metric_name appears as a column in the run history table and as a key in run.summary, leaving the user unsure of the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-access-run-data-via-run-history\/3530",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.9051222222,
        "Challenge_title":"Cannot access run data via run.history()",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/toruowo\">@toruowo<\/a> thank you for writing in! Can you please change your last line to:<\/p>\n<pre><code class=\"lang-auto\">run.history(keys=['_step', 'metric_name'])\n<\/code><\/pre>\n<p>Would this work for you?  Please let me know if you have any further issues or questions. You may also find some more <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#public-api-examples\">API examples here<\/a> if that helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1663710134840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":231.2612944445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Challenge_closed_time":1663711220203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662878679543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether the Amazon SageMaker built-in LightGBM algorithm supports distributed training, as they currently use Databricks for this purpose and would consider migrating to SageMaker if it does. The documentation on SageMaker's website does not provide clear information on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":231.2612944445,
        "Challenge_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":9.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":4.4438166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Challenge_closed_time":1660907847087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660891849347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble understanding the meaning of \"config=wandb.config\" in the tutorials of wandb while setting up a sweep for their logistic regression model. They have provided the code they have used for the sweep and training, and are seeking an explanation of the steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":33.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.4438166667,
        "Challenge_title":"What is the meaning of 'config = wandb.config'?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":209,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557474244067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":626.0,
        "Poster_view_count":140.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":28.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":33.0,
        "Solution_word_count":255.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.4618902778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Challenge_closed_time":1568630636312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568567773507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train the DeepAR AWS SageMaker built-in algorithm with specific hyper-parameters using the sagemaker SDK. They want to train the model again with a smaller learning rate using the incremental training method, but it does not work as only two built-in models support incremental learning. The user is seeking a workaround to train a built-in algorithm with a scheduled learning rate.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":18.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":17.4618902778,
        "Challenge_title":"Incremental learning with a built-in sagemaker algorithm",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445974115332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington, United States",
        "Poster_reputation_count":5939.0,
        "Poster_view_count":324.0,
        "Solution_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":19.0,
        "Solution_reading_time":10.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":257.7152338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm completely new to Azure ML, but I wanted to try out their automated ML UX. So I've followed the instructions to finally deploy my app (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model<\/a>). Now I've got my \"Scoring URI\", but I don't know how to use it? <strong>How can I test an input and get an output - can I do it with Postman?<\/strong><\/p>\n\n<ul>\n<li>the tutorial doesn't tell me what to do with this \"Scoring URI\", and so I am stuck<\/li>\n<\/ul>",
        "Challenge_closed_time":1565163075267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565145529467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and has followed the instructions to deploy their app. They have obtained the \"Scoring URI\" but are unsure how to use it to test input and get output. The tutorial does not provide guidance on how to use the \"Scoring URI\".",
        "Challenge_last_edit_time":1565163086470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57386269",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":9.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4.8738333334,
        "Challenge_title":"How to use Azure ML Scoring URI?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":826.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526863814910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>On the bottom of the page that you have linked above, there is a link:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">Learn how to consume a web service.<\/a><\/p>\n\n<p>This is exactly on that topic on how to use the deployed web service for scoring (sending an input and getting an output).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1566090861312,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9439969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, While trying authentication in AzureML SDK v2 the DefaultAzureCredential failed to retrieve a token from the included credentials. Attempted credentials: \tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured. <\/p>",
        "Challenge_closed_time":1671083438316,
        "Challenge_comment_count":1,
        "Challenge_created_time":1671076439927,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with AzureML SDK v2 where the DefaultAzureCredential failed to retrieve a token from the included credentials. The attempted credentials, EnvironmentCredential, were unavailable due to incomplete configuration of environment variables.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1129620\/defaultazurecredential-failed-to-retrieve-a-token",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.7,
        "Challenge_reading_time":4.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.9439969444,
        "Challenge_title":"DefaultAzureCredential failed to retrieve a token from the included credentials.",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a>  Thanks for the question. Hers is the <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/main\/sdk\/identity\/azure-identity\/TROUBLESHOOTING.md#troubleshoot-environmentcredential-authentication-issues\">Troubleshooting guide<\/a> for Default Azure credential errors.    <\/p>\n<p> from azure.core.exceptions import ClientAuthenticationError    <br \/>\n    from azure.identity import DefaultAzureCredential  <br \/>\n    from azure.keyvault.secrets import SecretClient  <\/p>\n<pre><code># Create a secret client using the DefaultAzureCredential  \nclient = SecretClient(&quot;https:\/\/myvault.vault.azure.net\/&quot;, DefaultAzureCredential())  \ntry:  \n    secret = client.get_secret(&quot;secret1&quot;)  \nexcept ClientAuthenticationError as ex:  \n    print(ex.message)  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.8,
        "Solution_reading_time":11.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":29.32571,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Challenge_closed_time":1578755333996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578649761440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to host two different models, each trained on a different dataset, using a single endpoint in AWS Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":29.32571,
        "Challenge_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":411.0,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.5,
        "Solution_reading_time":10.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":176.3419125,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I got this on win10,it\u2019s stucked<br>\nthe enviornment is<\/p>\n<ul>\n<li>python3.7.10<\/li>\n<li>wandb 0.12.9<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" data-download-href=\"\/uploads\/short-url\/pvNysR6Ps6qxY0fl0Y5gjzfovBK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" alt=\"image\" data-base62-sha1=\"pvNysR6Ps6qxY0fl0Y5gjzfovBK\" width=\"547\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">686\u00d7626 26.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1641690199763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641055368878,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error with wandb on Windows 10 while using Python 3.7.10 and wandb version 0.12.9. The user has provided a screenshot of the error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/error-with-wandb-on-win10\/1656",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":25.9,
        "Challenge_reading_time":15.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":176.3419125,
        "Challenge_title":"Error with wandb on win10",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019ve deleted wandb in docker and pip ,then I reinstalled them.<br>\nAnd I got the right page after waiting about 5 or 6 minutes.<br>\nBut I don\u2019t know whtether the reason is the versions are different or something.<br>\nThis time I didn\u2019t set the LOCAL_RESOTRE var, I don\u2019t know whether the time will decrease.<br>\nAnd I notice that once I get the right page, the next time I can get in immediately.<br>\nThanks a lot.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1492331396980,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":197.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":4174.8991611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started to develop machine learning models on The Microsoft Azure Machine Learning Studio service. The tutorials and information related to this service are rather clear but I am looking for some information that I did not find concerning the deployment of the service.<\/p>\n\n<p>I would like to understand why the input schema requires the definition of the variable to predict and why the output returns all variable fields given in entry. In this response\/request exchange a part of information transmitted is useless. I wondering if it is possible to modify manually this schema.<\/p>\n\n<p>I searched in the configuration tab of the web service panel but I did not find any information to modify the schema passed to the model.<\/p>\n\n<p>The code below is the input schema that the model requires and the value to predict is <code>WallArea<\/code>. It is not really useful to pass this variable because it is the one we try to predict. (except if we want to compare the actual value and the predicted one for test purpose).<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"WallArea\",\n        \"RoofArea\",\n        \"OverallHeight\",\n        \"GlazingArea\",\n        \"HeatingLoad\"\n      ],\n      \"Values\": [\n        [\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>The json returned by the model with the predicted value sent all data. It is much more info to what we really need (\"Scored Label Mean\" and \"Scored Label Standard Deviation\")<\/p>\n\n<pre><code>{\n  \"Results\": {\n    \"output1\": {\n      \"type\": \"DataTable\",\n      \"value\": {\n        \"ColumnNames\": [\n          \"WallArea\",\n          \"RoofArea\",\n          \"OverallHeight\",\n          \"GlazingArea\",\n          \"HeatingLoad\",\n          \"Scored Label Mean\",\n          \"Scored Label Standard Deviation\"\n        ],\n        \"ColumnTypes\": [\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\"\n        ],\n        \"Values\": [\n          [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\"\n          ]\n        ]\n      }\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>My question is how to reduce\/synthesize the input\/output schema if it is possible and why the variable to predict must be sent with the input schema?<\/p>",
        "Challenge_closed_time":1568622249776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568375681780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing machine learning models on Microsoft Azure Machine Learning Studio service and is looking for information on modifying the input\/output schema for deployment. They are wondering why the input schema requires the definition of the variable to predict and if it is possible to modify the schema manually. The user has searched in the configuration tab of the web service panel but did not find any information to modify the schema passed to the model. They are also questioning why the variable to predict must be sent with the input schema and how to reduce\/synthesize the input\/output schema if possible.",
        "Challenge_last_edit_time":1568376011443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57923187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":68.49111,
        "Challenge_title":"How to modify the input\/output schema for an Azure deployment service?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":471.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492331396980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":197.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>I found the solution.<\/p>\n\n<p>For those who have the same problem, it is pretty simple in fact. You need to add two <strong>Select Columns in Dataset<\/strong> box in your <strong>Predictive experiment<\/strong> schema.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 2020:<\/strong> Following some updates done on the service, the solution proposed is partially broken. Indeed, if you decide to not include the outcome in the first Select columns box, you well not be able to retrieve it in the second <strong><em>Select Column box<\/em><\/strong> leading to an error. To solve that, you have to remove the first Select Column box and take all features. For the second <strong><em>Select Column box<\/em><\/strong> nothing change, you select the features you want for your predictive response.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583405648423,
        "Solution_link_count":2.0,
        "Solution_readability":9.8,
        "Solution_reading_time":11.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":2.15403,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>I am referring to the links below to use Tensorboard in Sagemaker Script Mode method.<\/strong><\/p>\n<p><a href=\"https:\/\/www.tensorflow.org\/tensorboard\/get_started\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tensorboard\/get_started<\/a><\/p>\n<p><a href=\"https:\/\/levelup.gitconnected.com\/how-to-use-tensorboard-in-an-amazon-sagemaker-notebook-instance-a41ce2fd973f\" rel=\"nofollow noreferrer\">https:\/\/levelup.gitconnected.com\/how-to-use-tensorboard-in-an-amazon-sagemaker-notebook-instance-a41ce2fd973f<\/a><\/p>\n<p><a href=\"https:\/\/towardsdatascience.com\/using-tensorboard-in-an-amazon-sagemaker-pytorch-training-job-a-step-by-step-tutorial-19b2b9eb4d1c\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/using-tensorboard-in-an-amazon-sagemaker-pytorch-training-job-a-step-by-step-tutorial-19b2b9eb4d1c<\/a><\/p>\n<p><strong>Below is my tensorboard callback in my training script which is a .py file<\/strong><\/p>\n<pre><code>model = create_model()\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nlog_dir = &quot;logs\/fit\/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nmodel.fit(x=x_train, \n          y=y_train, \n          epochs=5, \n          validation_data=(x_test, y_test), \n          callbacks=[tensorboard_callback])\n<\/code><\/pre>\n<p><strong>In a notebook, I am creating the below Tensorflow Estimator where I am passing the above Script file name as entry_point.<\/strong><\/p>\n<pre><code>estimator = TensorFlow(\n    entry_point='Script_File.py',\n    train_instance_type=train_instance_type,\n    train_instance_count=1,\n    model_dir=model_dir,\n    hyperparameters=hyperparameters,\n    role=sagemaker.get_execution_role(),\n    base_job_name='tf-fashion-mnist',\n    framework_version='1.12.0', \n    py_version='py3',\n    output_path=&lt;S3 Path&gt;,\n    script_mode=True,\n)\n<\/code><\/pre>\n<p><strong>I am using the below code in my notebook to start the training.<\/strong><\/p>\n<pre><code>estimator.fit(inputs)\n<\/code><\/pre>\n<p><strong>Once training is done, I am using the below code in a Terminal(have tried in my Notebook cell as well) to launch tensorboard.<\/strong><\/p>\n<pre><code>tensorboard --logdir logs\/fit\n<\/code><\/pre>\n<p>But in the tensorboard I am not able to view any graphs. It is showing the message &quot;Failed to fetch runs&quot;.\nIs there something that I am missing? Or do I have to do any extra setting in my script to see my logs in Tensorboard?<\/p>",
        "Challenge_closed_time":1606409118743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605782220677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use Tensorboard in AWS Sagemaker Script Mode by referring to some links. They have created a Tensorflow Estimator in a notebook and passed a script file name as an entry point. After starting the training, they are trying to launch Tensorboard using a code in a terminal but are not able to view any graphs and are getting the message \"Failed to fetch runs\". They are seeking help to understand if they are missing anything or if they need to do any extra settings in their script to see the logs in Tensorboard.",
        "Challenge_last_edit_time":1607679075168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64909903",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":19.5,
        "Challenge_reading_time":33.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":174.1383516667,
        "Challenge_title":"How to use Tensorboard in AWS Sagemaker",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605774298436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":5.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Your tensorboard <code>logdir<\/code> is not <code>logs\/fit<\/code>.. but there is the current date appended. Try using a <code>logs\/fit<\/code> as <code>log_dir<\/code> and see if it's working.<\/p>\n<p>EDIT<\/p>\n<p>If you want to use tensorboard locally you have to send tensorboard logs to S3 and read from there. In order to do this you have to do what your third linked example does, so include sagemaker debugger:<\/p>\n<blockquote>\n<p>from sagemaker.debugger import TensorBoardOutputConfig<\/p>\n<p>tensorboard_output_config = TensorBoardOutputConfig(\ns3_output_path='s3:\/\/path\/for\/tensorboard\/data\/emission',\ncontainer_local_output_path='\/local\/path\/for\/tensorboard\/data\/emission'\n)<\/p>\n<\/blockquote>\n<p>then your tensorboard command will be something like:<\/p>\n<blockquote>\n<p>AWS_REGION= &lt;your-region&gt; AWS_LOG_LEVEL=3 tensorboard --logdir\ns3:\/\/path\/for\/tensorboard\/data\/emission<\/p>\n<\/blockquote>\n<p>Alternatively if you want to use tensorboard in the notebook you have to do what the second linked example does, so simply install in a cell and run tensorboard with something like:<\/p>\n<p>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1607686829676,
        "Solution_link_count":1.0,
        "Solution_readability":16.1,
        "Solution_reading_time":15.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":125.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":579.0058813889,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I\u2019ve been trying to find some documentation, I don\u2019t want to save all the hyperparameters each epoch, just the learning rate.<br>\nWould be so great if you can help me out.<\/p>\n<p>Cheers,<\/p>\n<p>Oli<\/p>",
        "Challenge_closed_time":1679602846363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677518425190,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help on how to log the learning rate with PyTorch Lightning when using a scheduler, without saving all the hyperparameters each epoch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-the-learning-rate-with-pytorch-lightning-when-using-a-scheduler\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":579.0058813889,
        "Challenge_title":"How to log the learning rate with pytorch lightning when using a scheduler?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019m also wondering how this is done! Whether within a sweep configuration or not - when using a lr scheduler, I am trying to track the lr at epoch during training, as it is now dynamic. Even within a sweep, you will have some initial lr  determined during the sweep, but it will not stay constant for the duration of training.<\/p>\n<p>edit:<\/p>\n<p>The example on the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/1.2.10\/api\/pytorch_lightning.callbacks.lr_monitor.html#learning-rate-monitor\" rel=\"noopener nofollow ugc\">lightning site here<\/a> worked for me:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateMonitor\n&gt;&gt;&gt; lr_monitor = LearningRateMonitor(logging_interval='step')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_monitor])\n<\/code><\/pre>\n<p>Passing the <code>WandBLogger<\/code> to the trainer I see my lr is logged on the <code>wandb<\/code> dashboard.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":12.09,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":15.3127302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>A bit confused with automatisation of Sagemaker retraining the model.<\/p>\n<p>Currently I have a notebook instance with Sagemaker <code>LinearLerner<\/code> model making the classification task. So using <code>Estimator<\/code> I'm making training, then deploying the model creating <code>Endpoint<\/code>. Afterwards using <code>Lambda<\/code> function for invoke this endpoint, I add it to the <code>API Gateway<\/code> receiving the api endpoint which can be used for POST requests and sending back response with class.<\/p>\n<p>Now I'm facing with the problem of retraining. For that I use <code>serverless<\/code> approach and <code>lambda<\/code> function getting environment variables for training_jobs. But the problem that Sagemaker not allow to rewrite training job and you can only create new one. My goal is to automatise the part when the new training job and the new endpoint config will apply to the existing endpoint that I don't need to change anything in API gateway. Is that somehow possible to automatically attach new endpoint config with existing endpoint?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1594485516256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594430390427,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with automating the retraining of a Sagemaker LinearLerner model. They have created an endpoint using an Estimator and deployed it, and are using a Lambda function to invoke the endpoint and add it to an API Gateway. However, they are unable to rewrite the training job and can only create a new one, which is causing issues with automating the process. The user is seeking a solution to automatically attach a new endpoint configuration with the existing endpoint without having to change anything in the API Gateway.",
        "Challenge_last_edit_time":1594502576883,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62844211",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":15.3127302778,
        "Challenge_title":"Updating Sagemaker Endpoint with new Endpoint Configuration",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3443.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386491614716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2778.0,
        "Poster_view_count":352.0,
        "Solution_body":"<p>If I am understanding the question correctly, you should be able to use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> near the end of the training job, then use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>:<\/p>\n<p><code>Deploys the new EndpointConfig specified in the request, switches to using newly created endpoint, and then deletes resources provisioned for the endpoint using the previous EndpointConfig (there is no availability loss).<\/code><\/p>\n<p>If the API Gateway \/ Lambda is routed via the endpoint ARN, that should not change after using <code>UpdateEndpoint<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":10.31,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":18.9362980555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Challenge_closed_time":1655191745992,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655130099670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build an API using an MLflow model, but the load_model function fails with an error message stating that it is unable to infer schema for Parquet. The model artifacts are already downloaded to the folder \/model, and the load model call is in the main.py file. The user has ensured that the package references are identical and that the final resulting folder structure is the same. However, the parquet files in the itemFactors folder are not getting copied over to the image, even though the copy command is present. The user is seeking help to understand why the load_model function is throwing an exception and why the parquet files are not getting copied over to the image.",
        "Challenge_last_edit_time":1655134670387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":50.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":17.1239783333,
        "Challenge_title":"MLflow load model fails Python",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":417,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655202841060,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    }
]