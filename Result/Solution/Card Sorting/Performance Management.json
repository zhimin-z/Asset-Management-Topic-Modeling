[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":131.5469241667,
        "Challenge_answer_count":10,
        "Challenge_body":"<p>Hi,<br>\nI\u2019m currently working on a self-supervised representation learning project, and to evaluate the quality of my models I train a linear classifier on the outputs of my (frozen) trained encoder and look at the downstream classification accuracy.<\/p>\n<p>This evaluation procedure is done separately from the training of the encoder, however is there still a way to add the metrics computed during this evaluation phase to the standard metrics I log during the training phase, in the same run panel?<\/p>\n<p>More generally, can I add metrics to a run that is already finished?<\/p>\n<p>Thanks a lot!<\/p>",
        "Challenge_closed_time":1674087539911,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673613970984,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is working on a self-supervised representation learning project and evaluates the quality of the models by training a linear classifier on the outputs of the trained encoder. The evaluation is done separately from the training of the encoder, and the user wants to know if there is a way to add the metrics computed during this evaluation phase to the standard metrics logged during the training phase in the same run panel. The user also wants to know if it is possible to add metrics to a run that is already finished.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/log-custom-metrics-for-a-run-outside-of-the-training-loop\/3696",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":131.5469241667,
        "Challenge_title":"Log custom metrics for a run outside of the training loop",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":249.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a> , appreciate your your additional feedback.<\/p>\n<p>This approach of first logging , <code>loss<\/code>, to a run, then revisiting\/resuming a run to log different metric, <code>accuracy<\/code>, starting from <strong>step zero<\/strong> again is not supported. The wandb logging step must be monotonically increasing in each call, otherwise the <code>step<\/code> value is ignored during your call to <code>log()<\/code>. Now if you are not interested in logging accuracy at step 0, you <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#resuming-guidance\">could resume<\/a> the previously finished run using its un id and log additional metrics to the run. this however is problematic as the new metric is logged starting at the last known\/registered step for the run.<\/p>\n<p>One approach to get around the issue you are running into  is to assign each of the runs to a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\">specific group<\/a>. Example set <code>group = version_0<\/code> for any runs that logs metrics for this specific version of the model. You could then set grouping in the workspace to help with tracking  the different metrics for each experiment, <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Group-Viz-Test\/groups\/L2\/workspace?workspace=user-mohammadbakir\">see this example workspace<\/a>.<\/p>\n<p>Hope this helps and please let us know if you have additional questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.63,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":183.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":49.2027833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As far as I can tell there is no way to use the Excel add in for Azure ML using the new Azure ML service, it only works for the Classic. Is there any plan to provide a replacement add in that brings this functionality to the new Azure ML before Classic stops being supported in 2024?<\/p>",
        "Challenge_closed_time":1647857647663,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647680517643,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in using the Excel add-in for Azure ML with the new Azure ML service as it only works for the Classic version. They are inquiring about any plans to provide a replacement add-in before the Classic version stops being supported in 2024.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/778717\/replacement-for-azure-ml-classic-excel-add-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":4.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":49.2027833334,
        "Challenge_title":"Replacement for Azure ML Classic Excel Add In",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=325bba53-0a8f-4bf1-ab22-527f5cbac10d\">@Tim Cahill  <\/a>  Thanks for the question. Currently it's on roadmap to support in the near  future.  Excel add in feature similar to studio classic, it will be built on top on v2 online endpoints.    <br \/>\nCurrently, managed endpoints are not integrated with Designer, we need to first provide capability to do a no code designer deployment on v2 online endpoints and integrating excel add in for v2 endpoints.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":5.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.6854797222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Challenge_closed_time":1584005785480,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583933260433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created a score.py file and want to pass an argument to it using argparse, but they are unable to find a way to pass arguments. They have shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Challenge_last_edit_time":1584005920356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":20.1458463889,
        "Challenge_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":196,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1584011988083,
        "Solution_link_count":3.0,
        "Solution_readability":35.1,
        "Solution_reading_time":25.25,
        "Solution_score_count":-2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334851920780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":158.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":278.8402786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I write some training job on AWS-SageMaker framework.<\/p>\n\n<p>For some it's requirements, it needs know the job-name of which current running on.<\/p>\n\n<p>I know this code works for it ...<\/p>\n\n<pre><code>import sagemaker_containers\nenv = sagemaker_containers.training_env()\njob_name = env['job_name']\n<\/code><\/pre>\n\n<p>But <code>sagemaker_containers<\/code> package has been deprecated. (I read that on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">it's GitHub<\/a>)<\/p>\n\n<p>What should i do?<\/p>\n\n<p>I just started learning about this platform last month. I would appreciate any advice. Thank you.<\/p>",
        "Challenge_closed_time":1593541151550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592537326547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is writing a training job on AWS-SageMaker framework and needs to know the job-name of the current running job. They have been using the sagemaker_containers package to get the job-name, but it has been deprecated. The user is seeking advice on what to do next.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62462790",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":278.8402786111,
        "Challenge_title":"How can I get current job-name in SageMaker training job script?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":753.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401925028590,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":113.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>For older containers using the deprecated <code>sagemaker_containers<\/code>, the approach you described is correct.<\/p>\n<p>For newer containers that use <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\"><code>sagemaker-training-toolkit<\/code><\/a>, this is how you retrieve information about the environment: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker_training import environment\n\nenv = environment.Environment()\n\njob_name = env[&quot;job_name&quot;]\n<\/code><\/pre>\n<p>You can check the <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/dlc-release-notes.html\" rel=\"nofollow noreferrer\">DLC Release Notes<\/a> to see what's installed in each version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.9,
        "Solution_reading_time":13.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369252294280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":324.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":527.1234527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Challenge_closed_time":1655934198727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654036554297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while loading a xgboost model from the MLFlow registry. Although the model loads successfully, the model type is different, and the predictions are in numpy.float32 instead of int64. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":527.1234527778,
        "Challenge_title":"Problem when loading a xgboost model from mlflow registry",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369252294280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":324.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":34.0954786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>1) According to <a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html<\/a> Amazon ML uses SGD. However I can't find how many hidden layers are used in the neural network?<\/p>\n\n<p>2) Can someone confirm that SageMaker would be able to do what Amazon ML does? i.e. SageMaker is more powerful than Amazon ML?<\/p>",
        "Challenge_closed_time":1512471655643,
        "Challenge_comment_count":1,
        "Challenge_created_time":1512348911920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is having difficulty finding information on the number of hidden layers used in the neural network for Amazon Machine Learning's SGD algorithm. They are also seeking confirmation that SageMaker is more powerful than Amazon ML and can perform the same tasks.",
        "Challenge_last_edit_time":1513142314292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47625056",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":34.0954786111,
        "Challenge_title":"Amazon Machine Learning and SageMaker algorithms",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2794.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_created_time":1264406140363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2211.0,
        "Poster_view_count":176.0,
        "Solution_body":"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n\n<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).<\/p>\n\n<p>You can check out each of those containers to dive deep on how it all works.<\/p>\n\n<p>You can find an exhaustive list of available algorithms in SageMaker here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For now, as of December 2017, these algorithms are all available:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html\" rel=\"noreferrer\">Linear Learner<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines.html\" rel=\"noreferrer\">Factorization Machines<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"noreferrer\">XGBoost Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"noreferrer\">Image Classification Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq.html\" rel=\"noreferrer\">Amazon SageMaker Sequence2Sequence<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/k-means.html\" rel=\"noreferrer\">K-Means Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pca.html\" rel=\"noreferrer\">Principal Component Analysis (PCA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda.html\" rel=\"noreferrer\">Latent Dirichlet Allocation (LDA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"noreferrer\">Neural Topic Model (NTM)<\/a><\/li>\n<\/ul>\n\n<p>The general SageMaker SDK interface to these algorithms looks something like this:<\/p>\n\n<pre><code>from sagemaker import KMeans\nkmeans = KMeans(role=\"SageMakerRole\",\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                data_location=\"s3:\/\/training_data\/\",\n                output_path=\"s3:\/\/model_artifacts\/\",\n                k=10)\n<\/code><\/pre>\n\n<p>The libraries here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples<\/a>\nand here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a> are particularly useful for playing with SageMaker.<\/p>\n\n<p>You can also make use of Spark with SageMaker the Spark library here: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1512575702208,
        "Solution_link_count":19.0,
        "Solution_readability":25.2,
        "Solution_reading_time":39.62,
        "Solution_score_count":7.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":194.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1410333105327,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Turin, Metropolitan City of Turin, Italy",
        "Answerer_reputation_count":477.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":0.0448425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Challenge_closed_time":1636732235590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636727119830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to change the default directory of mlflow logs from the current directory to a new directory. They are unsure of how to do this and are using MLflow to log metrics. The code provided shows the implementation of the Model class, which includes the fit() and evaluation() methods.",
        "Challenge_last_edit_time":1636755379243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.3,
        "Challenge_reading_time":121.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":124,
        "Challenge_solved_time":1.4210444445,
        "Challenge_title":"How to change the directory of mlflow logs?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":636,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410333105327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Turin, Metropolitan City of Turin, Italy",
        "Poster_reputation_count":477.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636755540676,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.0502777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, \r\n\r\nI'm using `wandb` logger (and `csv` as well), I found recently `hydra` config no longer save to `wandb` 's`config.yaml` file.\r\nBefore:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.EarlyStopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.ModelCheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichModelSummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichProgressBar\r\nckpt_path:\r\n  desc: null\r\n  value: None\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.MNISTDataModule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.MNISTLitModule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.SimpleDenseNet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.Adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.ReduceLROnPlateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.Trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nNow:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nThis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#L172\r\nAny idea how to restore to previous state?",
        "Challenge_closed_time":1670781696000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670626715000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the full episode data logging in a random agent script using wandb, where some steps are being skipped. The problem is due to wandb counting the episode reward logging steps made before the full data logging. A potential solution suggested is to add another metric to log the timestep and day proportionally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/478",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":16.9,
        "Challenge_reading_time":71.23,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":43.0502777778,
        "Challenge_title":"Question: How to save hydra config to wandb config.yaml",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":551,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1428499037932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leipzig, Deutschland",
        "Answerer_reputation_count":2124.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":1.1770155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1658393400156,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658389162900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Weights & Biases sweeps for hyperparameter search for their NER model. They have done a grid search with about 100 runs and want to create a graph that shows the best 10 runs in terms of f-score, but they are unable to figure out how to do it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1770155556,
        "Challenge_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643710211767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.93,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":82.4554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.<\/p>\n<p><strong>training code (whentrain.ipynb) :<\/strong><\/p>\n<pre><code>#import libs and packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom azureml.core import Workspace, Dataset\n\n# get existing workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n\n# load the dataset which is placed in the data folder\ndataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\ndataset = dataset.to_pandas_dataframe()\n\n# Create the outputs directories to save the model and images\nos.makedirs('outputs\/model', exist_ok=True)\nos.makedirs('outputs\/output', exist_ok=True)\ndataset['Date'] = pd.to_datetime(dataset['Date'])\ndataset = dataset.set_index('Date')\n###\nscaler = MinMaxScaler()\n\n#inputs\nX = dataset.iloc[:, 1:]\n#output\ny = dataset.iloc[:, :1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n###\n\nmodel1 = RandomForestRegressor(n_estimators = 6,\n                                   max_depth = 10,\n                                   min_samples_leaf= 1,\n                                   oob_score = 'True',\n                                   random_state=42)\nmodel1.fit(X_train, y_train.values.ravel())\n\ny_pred2 = model1.predict(X_test)\n<\/code><\/pre>\n<p><strong>And here is the code on the estimator part (estimator.ipynb):<\/strong><\/p>\n<pre><code>from azureml.core import Experiment\nfrom azureml.core import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import TensorFlow\nfrom azureml.widgets import RunDetails\n\nimport os\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\nexp = Experiment(workspace=workspace, name='azure-exp')\ncluster_name = &quot;gpucluster&quot;\n\ntry:\n    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                           max_nodes=1)\n\n    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\nfrom azureml.core import ScriptRunConfig\nsource_directory = os.getcwd()\n\nfrom azureml.core import Environment\n\nmyenv = Environment(&quot;user-managed-env&quot;)\nmyenv.python.user_managed_dependencies =True\nfrom azureml.core import Dataset\ntest_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n\nsrc = ScriptRunConfig(source_directory=source_directory,\n                      script='whentrain.ipynb',\n\n                      arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                      compute_target=compute_target,\n                      environment=myenv)\nrun = exp.submit(src)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>The error that happens in <strong>run.wait_for_completion<\/strong> states :<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 107, in &lt;module&gt;\n[stderr]    &quot;notebookHasBeenCompleted&quot;: true\n[stderr]NameError: name 'true' is not defined\n[stderr]\n<\/code><\/pre>\n<p>As you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?<\/p>\n<p>I'm running the Notebook on Python 3.<\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>Okay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 115, in &lt;module&gt;\n[stderr]    &quot;source_hidden&quot;: false,\n[stderr]NameError: name 'false' is not defined\n[stderr]\n<\/code><\/pre>",
        "Challenge_closed_time":1640628230640,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640331391010,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a NameError when trying to run a ScriptRunConfig in Azure Machine Learning. The error occurs in the whentrain.ipynb file and states that the name 'true' or 'false' is not defined. The user is unsure of the source of the error and is seeking assistance in resolving it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":61.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":82.4554527778,
        "Challenge_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":413,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b72317d8-d212-487c-8510-7d965e8d135f\">@Ash  <\/a> Ok, I think the issue is here.     <\/p>\n<pre><code>src = ScriptRunConfig(source_directory=source_directory,  \n                       script='whentrain.ipynb',  \n                            \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],  \n                       compute_target=compute_target,  \n                       environment=myenv)  \n<\/code><\/pre>\n<p>The script parameter is set to the notebook &quot;whentrain.ipynb&quot;, This should be a python script *.py which can train your model. Since you are using the notebook filename the entire source of jupyter notebook is loaded and it fails with these errors. You can lookup samples on azure ml notebook github repo for <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train.py\">reference<\/a>. I think if you can convert your whentrain.ipynb file to a python script whentrain.py and save it the current folder structure you should be able to use it in this step.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":12.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":26.3555658333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Challenge_closed_time":1656329528480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655994282873,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use wandb for hyperparameter tuning on a random forest regressor using their own dataframe. However, when trying to initialize the sweep, they receive a \"Sweep user not valid\" error. The end goal is to successfully initialize the sweep.",
        "Challenge_last_edit_time":1656234648443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":20.6,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":93.1237797222,
        "Challenge_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3288888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?\nI'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Challenge_closed_time":1607969193000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607968009000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether anyone has successfully run multi-node gradient boosting on Amazon SageMaker, specifically with Catboost, LightGBM, or XGBoost. They are interested in finding a SageMaker-compatible multi-node training solution and would appreciate a public demo link if available.",
        "Challenge_last_edit_time":1667925925012,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3288888889,
        "Challenge_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":51,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612842503288,
        "Solution_link_count":1.0,
        "Solution_readability":26.1,
        "Solution_reading_time":3.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"When pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard. This happens only with [full] installation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.2693258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?  <\/p>\n<p>azureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle&lt;2.0.0 and &gt;=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle&lt;=2.0.0  <\/p>\n<p>Intro to new cloudpickle:  <br \/>\n<a href=\"https:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\">https:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs<\/a>  <br \/>\nPR:  <br \/>\n<a href=\"https:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\">https:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417<\/a>  <br \/>\nGithub issue:  <br \/>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637<\/a>  <\/p>",
        "Challenge_closed_time":1637290125060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637242355487,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting that the azureml-dataprep dependency for azureml-dataset-runtime should allow cloudpickle<=2.0.0, as there are no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0 and the latter introduces useful tools for serializing helper scripts. The user has provided links to the new cloudpickle and related PR and Github issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":16.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":13.2693258333,
        "Challenge_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=a10bf22e-b97c-4af2-89b9-23142e132503\">@Thomas H  <\/a>     <\/p>\n<p>Thank you so much for the contribute, I have sent an email to the author for the PR review and merge.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.4,
        "Solution_reading_time":17.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":86.4928975,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>How do I create a separate API so that I can log metrics from test pipelines? It doesn\u2019t make sense to use a personal key for that.<\/p>",
        "Challenge_closed_time":1675687046564,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675375672133,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to create a separate API key for logging metrics from test pipelines in MLOps, as using a personal key for this purpose does not seem appropriate.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/if-im-logging-metrics-for-mlops-from-a-test-pipeline-how-do-i-create-a-separate-api-key-for-that\/3803",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":86.4928975,
        "Challenge_title":"If I'm logging metrics for MLOps from a test pipeline, how do I create a separate api key for that?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":4.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":260.4842072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Challenge_closed_time":1656410088380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656241190023,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use Tensorboard to monitor their model's performance within a Jupyter notebook running on Amazon SageMaker Studio Lab, but the commands they have used to set up Tensorboard are not working and nothing shows up in the output of the cell where the commands are executed. The user is seeking suggestions to solve this problem.",
        "Challenge_last_edit_time":1656243378470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":12.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":46.9162102778,
        "Challenge_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420286650807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Trondheim, Norway",
        "Poster_reputation_count":720.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1657181121616,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":11.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7800052778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi! The following error happens while trying to create an endpoint from a successful trained model:\n\n* In the web console: \n> The customer:primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.\n * CloudWatch logs: \n> exec: \"serve\": executable file not found in $PATH\n\nIm deploying the model using a Lambda step, just as in this [notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb). The Lambda step is successful, and I can see in the AWS web console that the model configuration is created with success. \n\nThe exact same error happens when I  create an endpoint for the registered model in the AWS web console, under Inference -> Models. In the console I can see that an inference container was created for the model, with the following characteristics:\n* Image: 763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39\n* Mode: single model\n* Environment variables (Key Value): \n> SAGEMAKER_CONTAINER_LOG_LEVEL\t20\n\n> SAGEMAKER_PROGRAM\tinference.py\n\n> SAGEMAKER_REGION\teu-west-3\n\n> SAGEMAKER_SUBMIT_DIRECTORY\t\/opt\/ml\/model\/code\n \nI absolutely have no clue what is wrong and I could not find anything relevant online about this problem. Is it necessary to provide an custom docker image for inference or something?\n\nFor more details, please find below the pipeline model steps code. Any help would be much appreciated!\n```\nmodel = Model(\n    image_uri=estimator.training_image_uri(),\n    model_data=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=sagemaker_session,\n    role=sagemaker_role,\n    source_dir='code',\n    entry_point='inference.py'\n)\nstep_model_create = ModelStep(\n        name=\"CreateModelStep\",\n        step_args=model.create(instance_type=\"ml.m5.large\")\n )\n\nregister_args = model.register(\n        content_types=[\"*\"],\n        response_types=[\"application\/json\"],\n        inference_instances=[\"ml.m5.large\"],\n        transform_instances=[\"ml.m5.large\"],\n        model_package_group_name=\"test\",\n        approval_status=\"Approved\"\n)\nstep_model_register = ModelStep(name=\"RegisterModelStep\", step_args=register_args)\n```",
        "Challenge_closed_time":1670290340636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670280332617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create an endpoint from a trained model. The error message states that the customer:primary container for production variant AllTraffic did not pass the ping health check and to check CloudWatch logs for this endpoint. The CloudWatch logs show an error message \"exec: \"serve\": executable file not found in $PATH\". The user is deploying the model using a Lambda step and has created the model configuration successfully. The same error occurs when creating an endpoint for the registered model in the AWS web console. The user is unsure of what is wrong and is seeking help.",
        "Challenge_last_edit_time":1670628195195,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0JgbfwUoS5m6VH4TvtZmkg\/error-creating-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":29.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":2.7800052778,
        "Challenge_title":"Error Creating Endpoint",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":218,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, the problem here is that your inference model's container URI `763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39` is using a **training** image, not an **inference** image for TensorFlow. Because the images are each optimized for their own function, the serving executable is not available in the training container in this case.\n\nUsually, the framework-specific SDK classes will handle this lookup for you (for example `TensorFlowModel(...)` as used in the notebook you linked, or when calling `sagemaker.tensorflow.TensorFlow.deploy(...)` from the Estimator class.\n\nI see here though that you're using the generic `Model`, so guess you don't know (or don't want to commit to) the framework and version at the point the Lambda function runs?\n\nMy suggestions would be:\n\n- Can you use the Pipelines `ModelStep` to create your model before calling the Lambda deployment function? Similarly to how your linked notebook uses `CreateModelStep`. This would build your framework & version into the pipeline definition itself, but should mean that the selection of inference container image gets handled properly & automatically.\n- If you really need to be dynamic, I think you might need to find a way of looking up at least the *framework* from the training job. From my testing, you can use `estimator = sagemaker.tensorflow.TensorFlow.attach(\"training-job-name\")` and then `model = estimator.create_model(...)` to correctly infer the specific inference container *version* from a training job, but it still relies on knowing that TensorFlow is the correct framework. I'm not aware of a framework-agnostic equivalent? So could e.g. try describing the training job, manually inferring which framework it uses from that information, and then using the relevant framework estimator class' [attach()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.attach) method to figure out the specifics and create your model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1670290340638,
        "Solution_link_count":1.0,
        "Solution_readability":11.4,
        "Solution_reading_time":25.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":266.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":22.0598044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use optuna lib in Python to optimise parameters for recommender systems' models. Those models are custom and look like standard fit-predict sklearn models (with methods get\/set params). <\/p>\n\n<p>What I do: simple objective function that selects two parameters from uniform int distribution, set these params to model, predicts the model (there no fit stage as it simple model that uses params only in predict stage) and calculates some metric. <\/p>\n\n<p>What I get: the first trial runs normal, it samples params and prints results to log. But on the second and next trial I have some strange errors (look code below) that I can't solve or google. When I run study on just 1 trial everything is okay.<\/p>\n\n<p>What I tried: to rearrange parts of objective function, put fit stage inside, try to calculate more simpler metrics - nothing helps. <\/p>\n\n<p>Here is my objective function: <\/p>\n\n<pre><code># getting train, test\n# fitting model\nself.model = SomeRecommender()\nself.model.fit(train, some_other_params)\n\ndef objective(trial: optuna.Trial):\n    # save study\n    if path is not None:\n        joblib.dump(study, some_path)\n\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # setting params to model\n    params = {'alpha': alpha,\n              'beta': beta}\n    self.model.set_params(**params)\n\n    # getting predict\n    recs = self.model.predict(some_other_params)\n\n    # metric computing\n    metric_result = Metrics.hit_rate_at_k(recs, test, k=k)\n\n    return metric_result\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>That's what I get on three trials:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>[I 2019-10-01 12:53:59,019] Finished trial#0 resulted in value: 0.1. Current best value is 0.1 with parameters: {'alpha': 59.6135986324444, 'beta': 40.714559720597585}.\n[W 2019-10-01 13:39:58,140] Setting status of trial#1 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n[W 2019-10-01 13:39:58,206] Setting status of trial#2 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n<\/code><\/pre>\n\n<p>I can't understand where is the problem and why the first trial is working. Please, help. <\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1570006754996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569926663223,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the optuna library in Python to optimize parameters for recommender systems' models. The first trial runs normally, but on the second and subsequent trials, the user encounters an error that they cannot solve or find on Google. The error message is \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\". The user has tried rearranging parts of the objective function, putting the fit stage inside, and calculating simpler metrics, but nothing has helped.",
        "Challenge_last_edit_time":1569927339700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58183158",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":61.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.2477147222,
        "Challenge_title":"How to fix error \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\" - strange behaviour in optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":666.0,
        "Challenge_word_count":446,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Your code seems to have no problems.<\/p>\n\n<p>I ran a simplified version of your code (see below), and it worked well in my environment:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # evaluating params\n    return alpha + beta\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>Could you tell me about your environment in order to investigate the problem? (e.g., OS, Python version, Python interpreter (CPython, PyPy, IronPython or Jython), Optuna version)<\/p>\n\n<blockquote>\n  <p>why the first trial is working.<\/p>\n<\/blockquote>\n\n<p>This error is raised by <a href=\"https:\/\/github.com\/pfnet\/optuna\/blob\/389a176c8cd1c860001a7a4562670006643e5e11\/optuna\/samplers\/tpe\/sampler.py#L558\" rel=\"noreferrer\">optuna\/samplers\/tpe\/sampler.py#558<\/a>, and this line is only executed when the number of completed trials in the study is greater than zero.<\/p>\n\n<p>BTW, you might be able to avoid this problem by using <code>RandomSampler<\/code> as follows:<\/p>\n\n<pre><code>sampler = optuna.samplers.RandomSampler()\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\n<\/code><\/pre>\n\n<p>Notice that the optimization performance of <code>RandomSampler<\/code> tends to be worse than <code>TPESampler<\/code> that is the default sampler of Optuna.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":18.86,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":153.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1384322523967,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":574.0,
        "Answerer_view_count":66.0,
        "Challenge_adjusted_solved_time":42.996675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Challenge_closed_time":1655933087183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655778299153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while logging a model using mlflow and sklearn. When the user tries to log the model, two folders are created - one named 'model' and the other named 'logger'. The user is unsure if they are missing something or if there is a configuration for this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":42.996675,
        "Challenge_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":122.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544467691223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Zacatecas, Mexico",
        "Poster_reputation_count":131.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":53.8060766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using aws sagemaker to invoke the endpoint : <\/p>\n\n<pre><code>payload = pd.read_csv('payload.csv', header=None)\n\n&gt;&gt; payload\n\n\n    0   1   2   3   4\n0   setosa  5.1     3.5     1.4     0.2\n1   setosa  5.1     3.5     1.4     0.2\n<\/code><\/pre>\n\n<p>with this code :<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>But I got this problem : <\/p>\n\n<pre><code>ParamValidationError                      Traceback (most recent call last)\n&lt;ipython-input-304-f79f5cf7e0e0&gt; in &lt;module&gt;()\n      1 response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n      2                                    ContentType='text\/csv',\n----&gt; 3                                    Body=payload)\n      4 \n      5 result = json.loads(response['Body'].read().decode())\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    584         }\n    585         request_dict = self._convert_to_request_dict(\n--&gt; 586             api_params, operation_model, context=request_context)\n    587 \n    588         handler, event_response = self.meta.events.emit_until_response(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _convert_to_request_dict(self, api_params, operation_model, context)\n    619             api_params, operation_model, context)\n    620         request_dict = self._serializer.serialize_to_request(\n--&gt; 621             api_params, operation_model)\n    622         prepare_request_dict(request_dict, endpoint_url=self._endpoint.host,\n    623                              user_agent=self._client_config.user_agent,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/validate.py in serialize_to_request(self, parameters, operation_model)\n    289                                                     operation_model.input_shape)\n    290             if report.has_errors():\n--&gt; 291                 raise ParamValidationError(report=report.generate_report())\n    292         return self._serializer.serialize_to_request(parameters,\n    293                                                      operation_model)\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value:         0    1    2    3    4\n0  setosa  5.1  3.5  1.4  0.2\n1  setosa  5.1  3.5  1.4  0.2, type: &lt;class 'pandas.core.frame.DataFrame'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I am just using the same code\/step like in the aws tutorial .  <\/p>\n\n<p>Can you help me to resolve this problem please?<\/p>\n\n<p>thank you<\/p>",
        "Challenge_closed_time":1536693194576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536499492700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to invoke an endpoint with Sagemaker. They are using AWS Sagemaker to invoke the endpoint with a specific code, but they are getting a \"ParamValidationError\" error. The error message indicates that the \"Body\" parameter is invalid, and the user is passing a Pandas DataFrame instead of a valid type like bytes or a file-like object. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52244963",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":34.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":53.8060766667,
        "Challenge_title":"Impossible to invoke endpoint with sagemaker",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4668.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>The payload variable is a Pandas' DataFrame, while <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">invoke_endpoint()<\/a> expects  <code>Body=b'bytes'|file<\/code>.<\/p>\n\n<p>Try something like this (coding blind):<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=open('payload.csv'))\n<\/code><\/pre>\n\n<p>More on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-inference.html\" rel=\"nofollow noreferrer\">expected formats here<\/a>. \nMake sure the file doesn't include a header.<\/p>\n\n<p>Alternatively, convert your DataFrame to bytes, <a href=\"https:\/\/stackoverflow.com\/questions\/34666860\/converting-pandas-dataframe-to-bytes\">like in this example<\/a>, and pass those bytes instead of passing a DataFrame.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.25,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":10,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug in the logger behavior of PyTorchLightning after a recent update. The logger starts using `COMET_EXPERIMENT_KEY` but does not respect it if it is already set. The logger overwrites the user's value, deletes the variable, and ignores the set variable in the version function. The user plans to create a pull request to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1530826195963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":202.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":4541.5040575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Challenge_closed_time":1660920166467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644569855830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information about the mechanism behind hyperparameter tuning job in AWS Sagemaker. They are specifically trying to bring their own container and minimize cross entropy loss. They are questioning whether the hyperparameters defined in the HyperParameterTuner class get copied into \/opt\/ml\/input\/config\/hyperparameters.json and whether the training image should be adjusted to use the hyperparameters from that file. The user is also confused about how hyperparameters are passed into the training code, as some sample HPO notebooks use argparser to pass in the hyperparameters.",
        "Challenge_last_edit_time":1644570751860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4541.7529547222,
        "Challenge_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446577693503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11.8299888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Challenge_closed_time":1579719419647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579686060327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in throwing an exception from within an MLflow project. They are executing a function using mlflow.run but are getting an ExecutionException. The user is seeking a way to get the exception being raised where they are executing mlflow.run or to send an mlflow.exceptions.ExecutionException with a custom message set from within the project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.2664777778,
        "Challenge_title":"How can I throw an exception from within an MLflow project?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472932425400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1579728648287,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":0,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a ResourceLimitExceeded error when running code cell [17] to create an endpoint to host the model while walking through the SageMaker Studio tour for the first time in a new AWS account. The error occurred due to the account-level service limit 'ml.m4.xlarge for endpoint usage' being 0 Instances. The user suggests that the Prerequisites section could address this proactively with a link to the service limit increase page or the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of 0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221528724667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"West Coast, North America",
        "Answerer_reputation_count":11340.0,
        "Answerer_view_count":737.0,
        "Challenge_adjusted_solved_time":182.4722333334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1587598650403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586941750363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to delete worker task templates in AWS Augmented AI. They have created a test template but are unsure of how to remove it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61225077",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":182.4722333334,
        "Challenge_title":"How to delete worker task templates in AWS Augmented AI?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5792555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Challenge_closed_time":1610631020420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610628627223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to visualize real-time losses and metrics for a TensorFlow model on an AWS Sagemaker instance using Tensorboard in a Jupyter notebook, but is encountering issues and is seeking guidance on how to get it working.",
        "Challenge_last_edit_time":1610628935100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":4.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6647769444,
        "Challenge_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":715.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512023194592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":547.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MlflowMetricsDataSet ignores the specified run_id when the prefix is not specified in the catalog, and instead uses the name in the catalog. This results in the current run_id overriding the specified run_id, causing the metric to be logged in a new run instead of the expected run. The bug also occurs in the latest version on develop.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.7779194445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Bonjour    <br \/>\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning    <br \/>\nA l'\u00e9tape Create compute resources    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute<\/a>    <\/p>\n<p>On me demande Search for and select Standard_DS11_v2    <\/p>\n<p>Hors, l'interface me dit que je n'ai pas les quotas disponibles.    <br \/>\nJ'utilise l'offre d'essai \u00e0 200 USD.    <br \/>\nComment faire pour que cela fonctionne ?    <br \/>\nCordialement    <br \/>\nThibaut<\/p>",
        "Challenge_closed_time":1638432598510,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638368598000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is taking an online course on machine learning and is facing issues while creating compute resources. The interface is not allowing the user to select Standard_DS11_v2 as it shows that the user does not have the required quotas available. The user is using the trial offer of $200 and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/647767\/comment-s-lectionner-standard-ds11-v2",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.5,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.7779194445,
        "Challenge_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=e0b4def2-2525-4e3c-954a-129251c1bdb4\">@Thibaut Jacquin  <\/a> For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":10.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630081759000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a CUDA error 46 while trying to run onnxruntime-gpu on an Azure Machine Learning instance with the openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04 base image. The error message suggests that all CUDA-capable devices are busy or unavailable. The user had previously run similar environments with earlier versions of CUDA and onnxruntime, which makes them think that the issue is related to the compatibility between CUDA versions and onnxruntime. The user is unsure if the issue is related to the difference between cudnn 8.4 used in the docker image and the compatibility list for onnxruntime, which suggests cudnn 8.2.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":270.0,
        "Challenge_repo_star_count":13.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2554944445,
        "Challenge_answer_count":6,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/105827-image.png?platform=QnA\" alt=\"105827-image.png\" \/>    <\/p>\n<p>I tried to deploy a VM to Azure Machine Learning, but I get the error message &quot;You do not have enough quota for the following VM sizes. Click here to view and request quota.&quot; And the VM cannot be deployed.    <\/p>\n<p>But I have enough quota (24 CPUs).    <\/p>\n<p>What is causing the problem?    <\/p>\n<p>I'm using Azure's Free trial plan.<\/p>",
        "Challenge_closed_time":1623772934643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to deploy a VM on Azure Machine Learning due to an error message stating that they do not have enough quota for certain VM sizes, despite having enough quota (24 CPUs). The user is using Azure's Free trial plan and is seeking to understand the cause of the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437136\/cant-deploy-a-vm-on-the-azure-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2554944445,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=80eeb45c-8aa8-49ab-81df-1bb291fc79a5\">@ShoM  <\/a> ,    <\/p>\n<p>there are different quotas in Azure:    <\/p>\n<ul>\n<li> There are quotas for <code>vCPUs per Azure Region<\/code>    <\/li>\n<li> In addition there are quotas for <code>vCPUs per VM Series<\/code>    <\/li>\n<\/ul>\n<p>Both quotas (for Azure Region and VM Series) must fit the requirements.    <\/p>\n<p>It seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.    <br \/>\nYou can check your quotas by the link you marked with the red line in your screenshot.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":9.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565697423932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Uzbekistan",
        "Answerer_reputation_count":602.0,
        "Answerer_view_count":117.0,
        "Challenge_adjusted_solved_time":8037.6370116666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Challenge_closed_time":1630917852487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630917852487,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to optimize for multiple metrics simultaneously inside the objective function of Optuna, specifically for a scenario where they are training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.",
        "Challenge_last_edit_time":1630917952870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to optimize for multiple metrics in Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1887.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565697423932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Uzbekistan",
        "Poster_reputation_count":602.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659853446112,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":18.47,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":113.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1565215898703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":7.4180927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to deploy an AML Notebook VM via an ARM template? If so, is there an example or documentation somewhere?<\/p>",
        "Challenge_closed_time":1565216213847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565189508713,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to deploy an AML Notebook VM through an ARM template and if there are any examples or documentation available.",
        "Challenge_last_edit_time":1565217649487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57397150",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.4180927778,
        "Challenge_title":"Deploy Notebook VM via ARM Template?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":27,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529439461716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":392.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Unfortunately this is not supported today, but ARM support is in our roadmap<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":1.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":6386.0465466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1607293074048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607263332140,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a trained sklearn model to an endpoint in AWS SageMaker and serve it as an API for predictions. However, the user is facing challenges as every blog and documentation they have read suggests that the sklearn model has to be trained on SageMaker in order to be deployed. The user is looking for a way to specify the S3 location of their serialized model and have SageMaker load it from S3 for inference. The user also encountered an error when trying to deploy the model from the notebook of SageMaker studio.",
        "Challenge_last_edit_time":1607399645212,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":51.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.2616411111,
        "Challenge_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3221.0,
        "Challenge_word_count":393,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1630389412780,
        "Solution_link_count":19.0,
        "Solution_readability":16.5,
        "Solution_reading_time":122.15,
        "Solution_score_count":7.0,
        "Solution_sentence_count":93.0,
        "Solution_word_count":900.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2751.6177777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Challenge_closed_time":1623275505000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1613369681000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using wandb as it shows steps instead of episodes, making it difficult to compare runs with longer durations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.88,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2751.6177777778,
        "Challenge_title":"Getting Errors with wandb sweeps ",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I got this error when I tried to run wandb sweeps for a regressionn classifcation. It complained when I included the required num_labels. After removing it, the error is what I get. Are there some additional settings required besides setting regression=True. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.8672141667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Challenge_closed_time":1640309961152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640245639181,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a sweep from existing runs using ray's wandb_mixin, but the panel Parallel Coordinates is empty. The user is unsure if this is intended behavior or a bug. The sweep only contains one run instead of the expected 42 runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":13.8,
        "Challenge_reading_time":10.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8672141667,
        "Challenge_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.08,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1679.0761111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Challenge_closed_time":1632957644000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626912970000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing issues with Neptune_catalyst.ipynb failing and suspects that there may be a typo or missing `run` object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1679.0761111111,
        "Challenge_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @Roshin29, thank you for the bug report! \r\n\r\nThe machine learning sample notebooks received substantial revisions in [Release 3.0.1](https:\/\/github.com\/aws\/graph-notebook\/releases\/tag\/v3.0.1). This release also included a number of changes under the hood to support the general availability release of Amazon Neptune ML.\r\n\r\nThe Gremlin query listed is only seen in older versions of the `Neptune-ML-01-Introduction-to-Node-Classification-Gremlin` sample notebook, and is now replaced by the one below:\r\n```\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"${endpoint}\").\r\n  with(\"Neptune#ml.limit\",3).\r\n  V().has('title', 'Apollo 13 (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\n```\r\nI am not able to reproduce the listed exception when running this query using graph-notebook v3.0.6, so the issue appears to have been resolved with the latest changes.\r\n\r\nClosing this issue out, as there are no further action items at this time. Please feel free to re-open if you have any further questions.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.78,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.8108333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n## How to reproduce\r\n\r\nReproduce by starting a non-dry run via a notebook\r\n\r\n1. Start the run\r\n2. Look at files on the wandb interface. There are no checkpoints\r\n\r\n## Expected\r\n\r\nCheckpoints should be uploaded to wandb whenever there is a better one available during training.\r\n\r\n## Additional context\r\n\r\nI thought I fixed wandb, but it seems that I don't understand the symlinking model of wandb. Apparently you need to have checkpoints under the project root? But this would mean that you can't run multiple experiements at the same time. ",
        "Challenge_closed_time":1624957138000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1624867819000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Wandb sweep on their primary notebook as it stalls after the first part of the sweep completes, causing problems.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/9",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":24.8108333333,
        "Challenge_title":"Fix writing of checkpoints to wandb",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":104,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed. See https:\/\/github.com\/feldberlin\/wavenet\/commit\/1125dcc5ce5004386160f3dfe0e1d1dc1e5aed98.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":44.6,
        "Solution_reading_time":1.4,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1509012479112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belo Horizonte, MG, Brasil",
        "Answerer_reputation_count":97.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":11.9663136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1568727043932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568683965203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up hyperparameters for a classification training job using AWS SageMaker Training Jobs console with H2o.AutoMl. Specifically, the user is unsure how to set up the 'training' field and is seeking assistance. The user's data is stored on S3 as a CSV file with 250000 columns, 4 categorical columns, and one target column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.9663136111,
        "Challenge_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.3,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":134.9044,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>if I tune a model with the LightGBMTunerCV I always get this massive result of the cv_agg's binary_logloss. If I do this with a bigger dataset, this (unnecessary) io slows down the performance of the optimization process.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nimport optuna\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nbreast_cancer = load_breast_cancer()\n\nX_train, X_test, Y_train, Y_test = train_test_split(breast_cancer.data, breast_cancer.target)\n\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=breast_cancer.feature_names.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=breast_cancer.feature_names.tolist())\ncallbacks = [lgb.log_evaluation(period=0)]\ntuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True)\n\n\ntuner.run()\n<\/code><\/pre>\n<p>And the output:<\/p>\n<pre><code>feature_fraction, val_score: 0.327411:  43%|###################2      | 3\/7 [00:00&lt;00:00, 13.84it\/s]\n[1] cv_agg's binary_logloss: 0.609496 + 0.009315\n[2] cv_agg's binary_logloss: 0.554522 + 0.00607596\n[3] cv_agg's binary_logloss: 0.512217 + 0.0132959\n[4] cv_agg's binary_logloss: 0.479142 + 0.0168108\n[5] cv_agg's binary_logloss: 0.440044 + 0.0166129\n[6] cv_agg's binary_logloss: 0.40653 + 0.0200005\n[7] cv_agg's binary_logloss: 0.382273 + 0.0242429\n[8] cv_agg's binary_logloss: 0.363559 + 0.03312\n<\/code><\/pre>\n<p>Is there any way to get rid of this output?<\/p>\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1638229134663,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637743478823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the LightGBMTunerCV model where the cv_agg's binary_logloss output is slowing down the optimization process, especially with larger datasets. The user is seeking a way to suppress this output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70093026",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":134.9044,
        "Challenge_title":"Supressing optunas cv_agg's binary_logloss output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":200.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398509643447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dortmund, Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can pass verbose_eval parameter with value None in LightGBMTunerCV().<\/p>\n<p>Example:<\/p>\n<pre><code>tuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True, verbose_eval=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":4.25,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7235097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We are experimenting with programmatic report generation with WandB.<br>\nI would like to be able to add a Confusion Matrix to a report, but this is not one of the base types (as far as I can tell). Is there a good way to do this?<br>\nI could generate a PNG\/Image and insert it, but I can\u2019t figure out how to add an Image to a report yet (see recent question).  Are there other ways?<br>\nThanks.<\/p>",
        "Challenge_closed_time":1666015569168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665987764533,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to generate a report programmatically using WandB and wants to add a Confusion Matrix to the report. However, they are unable to find a way to add it as it is not one of the base types. They are considering generating a PNG\/Image and inserting it, but are unsure how to add an image to the report.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/adding-confusion-matrix-to-report-programatically\/3267",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.7235097222,
        "Challenge_title":"Adding Confusion Matrix to report programatically",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> I am also posting here <a href=\"https:\/\/colab.research.google.com\/drive\/1Fepp-JLFvK-wLL2BZ_BnkCG_fAC6HFbo#scrollTo=An_example_with_all_of_the_blocks_and_panels\" rel=\"noopener nofollow ugc\">this Colab<\/a> and the Python SDK commands of our <a href=\"https:\/\/docs.wandb.ai\/guides\/reports\/edit-a-report#add-plots\">Reports reference docs<\/a> which may be helpful.<\/p>\n<p>The confusion matrix isn\u2019t <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/main\/wandb\/apis\/reports\/panels.py\" rel=\"noopener nofollow ugc\">currently exposed<\/a> but I have increased this feature requests for our engineering team. We will reach out to you once this is implemented. I hope this helps, please let me know if you have any further questions or issues with the Reports API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":10.75,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1393576024047,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":879.0,
        "Answerer_view_count":138.0,
        "Challenge_adjusted_solved_time":5394.9012733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i have started a sagemaker job:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server'{'enabled':False}})\n\ntraining_data_uri ='s3:\/\/path\/to\/my\/data'\nmytraining.fit(training_data_uri,run_tensorboard_locally=True)\n<\/code><\/pre>\n\n<p>using <code>run_tesorboard_locally=True<\/code> gave me<\/p>\n\n<pre><code>Tensorboard is not supported with script mode. You can run the following command: tensorboard --logdir None --host localhost --port 6006 This can be run from anywhere with access to the S3 URI used as the logdir.\n<\/code><\/pre>\n\n<p>It seems like i cant use it script mode, but I can access the logs of tensorboard in s3? But where are the logs in s3?<\/p>\n\n<pre><code>def _parse_args():\n    parser = argparse.ArgumentParser()\n\n    # Data, model, and output directories\n    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n    parser.add_argument('--model_dir', type=str)\n    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n\n    return parser.parse_known_args()\n\nif __name__ == \"__main__\":\n    args, unknown = _parse_args()\n\n    train_data, train_labels = load_training_data(args.train)\n    eval_data, eval_labels = load_testing_data(args.train)\n\n    mymodel= model(train_data, train_labels, eval_data, eval_labels)\n\n    if args.current_host == args.hosts[0]:\n        mymodel.save(os.path.join(args.sm_model_dir, '000000002\/model.h5'))\n<\/code><\/pre>\n\n<p>similiar question is here :<a href=\"https:\/\/stackoverflow.com\/questions\/53713660\/tensorboard-without-callbacks-for-keras-docker-image-in-sagemaker\">stack<\/a><\/p>\n\n<p>EDIT i tried this new config but it doesnt work.<\/p>\n\n<pre><code> tensorboard_output_config = TensorBoardOutputConfig( s3_output_path='s3:\/\/PATH\/to\/my\/bucket')\n\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server': {'enabled':False}},\n                        tensorboard_output_config=tensorboard_output_config)\n<\/code><\/pre>\n\n<p>i added the callback in my model.py script that is actually what i use without sagemaker. As logdir i defined the default dir, where the TensoboardOutputConfig writes the data.. but it doesnt work. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_TensorBoardOutputConfig.html\" rel=\"nofollow noreferrer\">docs<\/a> I also used it without the callback.<\/p>\n\n<pre><code> tensorboardCallback = tf.keras.callbacks.TensorBoard(\n        log_dir='\/opt\/ml\/output\/tensorboard',\n        histogram_freq=0,\n        # batch_size=32,ignored tf.2.0\n        write_graph=True,\n        write_grads=False,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None,\n        embeddings_data=None,\n        update_freq='batch') \n<\/code><\/pre>",
        "Challenge_closed_time":1604514854867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585083712870,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use TensorBoard with AWS SageMaker TensorFlow but is encountering issues. They have tried using `run_tensorboard_locally=True` but received an error message stating that TensorBoard is not supported with script mode. The user is now trying to access the logs of TensorBoard in S3 but is unsure where they are located. They have also tried using `TensorBoardOutputConfig` and adding a callback in their `model.py` script, but it still does not work.",
        "Challenge_last_edit_time":1585093210283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60839279",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":43.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":5397.5394436111,
        "Challenge_title":"how can i use tensorboard with aws sagemaker tensorflow?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1011.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455058326760,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1337.0,
        "Poster_view_count":214.0,
        "Solution_body":"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.<\/p>\n<ol>\n<li><p>Followed guide on <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook\" rel=\"noreferrer\">sagemaker debugging<\/a> to configure the <code>S3<\/code> output path for tensorboard logs.<\/p>\n<pre><code>from sagemaker.debugger import TensorBoardOutputConfig\n\ntensorboard_output_config = TensorBoardOutputConfig(\n       s3_output_path = 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n)\n\nestimator = TensorFlow(entry_point='train.py',\n               source_dir='.\/',\n               model_dir=model_dir,\n               output_path= output_dir,\n               train_instance_type=train_instance_type,\n               train_instance_count=1,\n               hyperparameters=hyperparameters,\n               role=sagemaker.get_execution_role(),\n               base_job_name='Testing-TrainingJob',\n               framework_version='2.2',\n               py_version='py37',\n               script_mode=True,\n               tensorboard_output_config=tensorboard_output_config)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<\/li>\n<li><p>Start the tensorboard with the <code>S3<\/code> location provided above via a terminal on the notebook instance.<\/p>\n<pre><code>$ tensorboard --logdir 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n<\/code><\/pre>\n<\/li>\n<li><p>Access the board via URL with <code>\/proxy\/6006\/<\/code>. You need to update the notebook instance details in the following URL.<\/p>\n<pre><code>https:\/\/myinstance.notebook.us-east-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":20.74,
        "Solution_score_count":5.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":316.8641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539227000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1660398516000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MLFlow while running hyperparameter tuning as it expects an mlruns folder which they have not created. This can be resolved by sticking to the standard and avoiding the need to run `mlflow ui` with the backend store argument.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":316.8641666667,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":77.33,
        "Solution_score_count":null,
        "Solution_sentence_count":61.0,
        "Solution_word_count":510.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":3.7756897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Challenge_closed_time":1653626541543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653612949060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to query a list of frequently used compute instance sizes under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. However, the list of resources that can be queried does not include compute under microsoft.machinelearningservices\/ (not classic studio) and Microsoft.Databricks\/workspaces. The user has tried a Kusto query to get VM instance size but it does not show what they have under Azure Machine Learning\/Azure Databricks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.7756897222,
        "Challenge_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":183.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1076.4220325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Challenge_closed_time":1617006916700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611574816563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an event rule in Cloudwatch to monitor Sagemaker training job state changes and trigger a lambda function to send messages in a telegram group. However, the events are fired multiple times with the same payload, resulting in duplicate messages. Enabling the Sagemaker profiler causes the number of identical rule invocations to increase. The user suspects a bug in AWS and is looking for a simple solution to avoid duplicate messages.",
        "Challenge_last_edit_time":1613131797383,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1508.9167047222,
        "Challenge_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":780.0,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":0.5757988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use <code>[OPTUNA][1]<\/code> with <code>sklearn<\/code> <code>[MLPRegressor][1]<\/code> model.<\/p>\n<p>For almost all hyperparameters it is quite straightforward how to set OPTUNA for them.\nFor example, to set the learning rate:\n<code>learning_rate_init = trial.suggest_float('learning_rate_init ',0.0001, 0.1001, step=0.005)<\/code><\/p>\n<p>My problem is how to set it for <code>hidden_layer_sizes<\/code> since it is a tuple. So let's say I would like to have two hidden layers where the first will have 100 neurons and the second will have 50 neurons. Without OPTUNA I would do:<\/p>\n<p><code>MLPRegressor( hidden_layer_sizes =(100,50))<\/code><\/p>\n<p>But what if I want OPTUNA to try different neurons in each layer? e.g., from 100 to 500, how can I set it? the <code>MLPRegressor<\/code> expects a tuple<\/p>",
        "Challenge_closed_time":1636650321043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636648248167,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in setting the hidden_layer_sizes hyperparameter in the MLPRegressor model of sklearn using OPTUNA trial, as it is a tuple and they want to try different numbers of neurons in each layer.",
        "Challenge_last_edit_time":1660311769156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931757",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":11.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5757988889,
        "Challenge_title":"How to set hidden_layer_sizes in sklearn MLPRegressor using optuna trial",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517513644076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":995.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You could set up your objective function as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport warnings\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nwarnings.filterwarnings('ignore')\n\nX, y = make_regression(random_state=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n\ndef objective(trial):\n\n    params = {\n        'learning_rate_init': trial.suggest_float('learning_rate_init ', 0.0001, 0.1, step=0.005),\n        'first_layer_neurons': trial.suggest_int('first_layer_neurons', 10, 100, step=10),\n        'second_layer_neurons': trial.suggest_int('second_layer_neurons', 10, 100, step=10),\n        'activation': trial.suggest_categorical('activation', ['identity', 'tanh', 'relu']),\n    }\n\n    model = MLPRegressor(\n        hidden_layer_sizes=(params['first_layer_neurons'], params['second_layer_neurons']),\n        learning_rate_init=params['learning_rate_init'],\n        activation=params['activation'],\n        random_state=1,\n        max_iter=100\n    )\n\n    model.fit(X_train, y_train)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=3)\n# [I 2021-11-11 18:04:02,216] A new study created in memory with name: no-name-14c92e38-b8cd-4b8d-8a95-77158d996f20\n# [I 2021-11-11 18:04:02,283] Trial 0 finished with value: 161.8347337123744 and parameters: {'learning_rate_init ': 0.0651, 'first_layer_neurons': 20, 'second_layer_neurons': 40, 'activation': 'tanh'}. Best is trial 0 with value: 161.8347337123744.\n# [I 2021-11-11 18:04:02,368] Trial 1 finished with value: 159.55535852658082 and parameters: {'learning_rate_init ': 0.0551, 'first_layer_neurons': 90, 'second_layer_neurons': 70, 'activation': 'relu'}. Best is trial 1 with value: 159.55535852658082.\n# [I 2021-11-11 18:04:02,440] Trial 2 finished with value: 161.73980822730888 and parameters: {'learning_rate_init ': 0.0051, 'first_layer_neurons': 100, 'second_layer_neurons': 30, 'activation': 'identity'}. Best is trial 1 with value: 159.55535852658082.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":28.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":174.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.9341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nStep Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers\nOlivier",
        "Challenge_closed_time":1549478317000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549456954000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best practice for versioning the code used to create ML workflows using Step Functions, specifically whether to use boto3 code in CodeCommit or another method.",
        "Challenge_last_edit_time":1668624402176,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.9341666667,
        "Challenge_title":"how to version step functions for ML?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":555.0,
        "Challenge_word_count":36,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601285800631,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":936.1641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1619815518000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1616445327000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the MLflow logger where the log_param() function requires a 'run_id' argument, which is not consistent with the behavior of the mlflow API. The user has provided a code sample and the environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.86,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":936.1641666667,
        "Challenge_title":"External MLFlow logging failures cause training job to fail",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":224,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":360.0336111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943713000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1634647592000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running benchmark.py with WandB due to the config being too large, resulting in a 400 error. The error message suggests that the train_selection array may be too big. The user suggests that the data selections may not need to be uploaded to WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":360.0336111111,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":97,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1631000506000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running train_model from examples after installing graphnet from scratch and signing up to WandB. The error occurred due to the absence of a directory called \"wandb\" and can be fixed by creating the folder manually. The user suggests automatically creating the folder if it is not present.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":10471.2408333333,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1532464254552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":310.004845,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Challenge_closed_time":1547752819392,
        "Challenge_comment_count":1,
        "Challenge_created_time":1544396689430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to run hyper-parameter auto tuning jobs using the image classification algorithm in AWS Sagemaker due to a validation error in the input data configuration. The error message indicates that additional properties are not allowed and the user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":1546636801950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53697587",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":19.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":932.2583227778,
        "Challenge_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1697.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421238326280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Poster_reputation_count":1951.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.44,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1540309037063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":72.6951308334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Challenge_closed_time":1561274854928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561274387650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to copy or upload a Google Colab notebook onto an AWS Sagemaker instance but is unable to do so. They have tried selecting all cells and using copy-paste icons and keyboard shortcuts but it did not work.",
        "Challenge_last_edit_time":1561274899232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1297994444,
        "Challenge_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1621.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540309037063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1561536601703,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":5.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1547395160296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":30365.0,
        "Answerer_view_count":5514.0,
        "Challenge_adjusted_solved_time":0.2671719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Challenge_closed_time":1594554603092,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594553641273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a MemoryError while training a model using Keras on SageMaker. The error message indicates that there is not enough memory to allocate for an array with shape (25000, 2000) and data type float64. The user is seeking advice on how to clean the memory or use SageMaker's memory to resolve the issue.",
        "Challenge_last_edit_time":1594581031423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2671719444,
        "Challenge_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1210.0,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.84,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7982433333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any way to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer)?I know there is an integration between CI and Azure Machine Learning studio (classic). Please help to integrate these two services.<\/p>",
        "Challenge_closed_time":1656632594976,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656618921300,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer) as there is already an integration between CI and Azure Machine Learning studio (classic).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909965\/azure-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.7982433333,
        "Challenge_title":"Azure machine learning",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @Yasuo-9899     <\/p>\n<p>Thanks for reaching out to us for this question. Are you looking for this document? <a href=\"https:\/\/learn.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\">https:\/\/learn.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments<\/a>    <\/p>\n<p>I have found one pic which is described the structure well:    <br \/>\n<img src=\"https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/raw\/main\/images\/workshop-playbook\/media\/image2.png\" alt=\"image2.png\" \/>    <\/p>\n<p>And also a repo you may want to refer to: <a href=\"https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\">https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md<\/a>    <\/p>\n<p>Please let us know more details you are interested in so that we can help. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":20.9,
        "Solution_reading_time":12.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1609843354000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the LED model in SageMaker SMP training. They have tried several fixes, including matching the python, transformers, and pytorch versions, but are still facing issues. The error is in the \"modeling_led\" within the transformers module, which is expecting a different input_ids shape. The user tried to unsqueeze input tensors to the \"modeling_led\" to solve the above error, which helped move forward in the process, but they got another error further down in the code. The error message is \"Tensors must have the same number of dimensions: got 4 and 3.\" The user is seeking feedback and assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":571.4619775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running the following code to create an endpoint with a preexisting model:<\/p>\n<pre><code>from sagemaker.tensorflow import serving\nsagemaker_session = sagemaker.Session()\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',\n                                     entry_point=&quot;inference.py&quot;,\n                                     source_dir=&quot;inf_source_dir&quot;,\n                                     role=get_execution_role(),\n                                     framework_version='1.14',\n                                     sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<p>However this create a copy of the model into the default sagemaker bucket. How can I pass a custom path? I've tried model_dir, and output_path but neither are accepted as parameters<\/p>",
        "Challenge_closed_time":1645727041932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643669778813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an endpoint with a preexisting model using SageMaker TensorFlow. However, the code creates a copy of the model into the default SageMaker bucket, and the user is unable to pass a custom path using model_dir or output_path parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70933814",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":9.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":571.4619775,
        "Challenge_title":"SageMaker custom model output path for tensorflow when creating from s3 artifacts",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point<\/code> and <code>source_dir<\/code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.<\/p>\n<p>You can change this behavior by setting the <code>default_bucket<\/code> in your <code>sagemaker_session<\/code> as follows:<\/p>\n<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)\n\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',         \n                    .\n                    .\n                    sagemaker_session=sagemaker_session)\n                    .\n                    )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.9,
        "Solution_reading_time":7.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":305.2888888889,
        "Challenge_answer_count":0,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017877000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1642918837000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user suggests that the `config` parameter in WandBLogger should be more flexible to allow passing configurations directly as a dictionary, instead of being restricted to `args.namespace` type. The user proposes that the conversion should be done outside the logger to make it more general in terms of config input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":305.2888888889,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":null,
        "Solution_sentence_count":23.0,
        "Solution_word_count":376.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1319019150600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3073.0,
        "Answerer_view_count":341.0,
        "Challenge_adjusted_solved_time":13.1913916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run XGBoost on AWS Sagemaker and trying to call the container for XGBoost.<\/p>\n<pre><code>\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'}\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;binary:logistic&quot;,\n        &quot;num_round&quot;:50\n        }\n\nestimator = sagemaker.estimator.Estimator(image_name=containers['us-east-1'], \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\n\n<\/code><\/pre>\n<p>However, running the following throws an error:<\/p>\n<pre><code>estimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid DNS suffix 'amazonaws.com' for region 'us-east-1' in training image. Please provide the valid &lt;region&gt;.&lt;dns-suffix&gt;: 'ap-south-1.amazonaws.com'\n<\/code><\/pre>\n<p>Can someone help on how to fix this error? Thank you.<\/p>",
        "Challenge_closed_time":1603876056100,
        "Challenge_comment_count":5,
        "Challenge_created_time":1603828567090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run XGBoost on AWS SageMaker by calling the container for XGBoost. However, when trying to fit the estimator, an error is thrown stating that there is an invalid DNS suffix for the region 'us-east-1' in the training image. The user is seeking help to fix this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64561968",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":21.1,
        "Challenge_reading_time":21.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.1913916667,
        "Challenge_title":"Building XGBoost on SageMaker",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>The notebook instance was created in ap-south-1 and the S3 bucket was in us-east-1. Creating another notebook instance from the same region as the S3 bucket resolved the issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":2.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1423640080283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":457.0,
        "Answerer_view_count":125.0,
        "Challenge_adjusted_solved_time":18.3632913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to import mlflow after having launched a log with opencensus Azure.\nThe MLFlow import runs forever.<\/p>\n<p>My environment is the following:<\/p>\n<ul>\n<li>Python 3.7<\/li>\n<li>opencensus-ext-azure 1.0.7<\/li>\n<li>opencensus-ext-logging 0.1.0<\/li>\n<li>mlflow 1.15.0<\/li>\n<\/ul>\n<p>Here is the code to repoduce the bug:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Challenge_closed_time":1619077377892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619011270043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing MLFlow after launching a log with opencensus Azure. The MLFlow import runs indefinitely. The user's environment includes Python 3.7, opencensus-ext-azure 1.0.7, opencensus-ext-logging 0.1.0, and mlflow 1.15.0. The provided code reproduces the bug.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67196775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.3632913889,
        "Challenge_title":"Is there a workaround to make opencensus work with MLFlow?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>I found a workaround, not the cleanest one though.<\/p>\n<p>I import mlflow at the beginning even if it's not useful this way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":6.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.8905811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi community,   <br \/>\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants.  I haven't built any models yet, just wondering how feasible the below is.  A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.  <\/p>\n<p>A client uploads a set of standard documents (usually scanned PDF's)  using a file upload in our .net application.    <br \/>\nCan we:  <\/p>\n<ol>\n<li> Use form recogniser to extract key value pairs, after training a custom model.  <\/li>\n<li> Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.  <\/li>\n<li> Report back to the user the missing information so they can correct the document and reupload them?  <br \/>\nPreferably in real time?  So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?  <\/li>\n<\/ol>",
        "Challenge_closed_time":1647513011952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647481005860,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether Azure Form Recognizer or another tool can be used to screen the correctness of uploaded applications, such as funding grant applications. They want to know if it is possible to use form recognizer to extract key value pairs, find missing information, and report back to the user in real-time so they can correct the document and re-upload it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.8905811111,
        "Challenge_title":"Form recognizer to report on missing information in (near) real-time",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d4ec46af-03e5-46ab-ace4-d0dd3b8d93ba\">@Andrew Robertson  <\/a> Yes, you can use Azure form recognizer to analyze a document that is passed to the API and use the result of the analyze operation to report any missing fields in the form back to the user. This is the most widely used use case by most of the customers.     <\/p>\n<p>Form recognizer comes with a set of prebuilt APIs where it can extract common information from invoices, business cards, receipts etc. If you have a form that does not conform to the prebuilt API standards you need to create a custom model to extract the text in the form of a tags and their key:value pairs. The custom models require some basic training with some test forms and if all the forms that need extraction follow the same layout or guidelines the extraction results will be good.     <\/p>\n<p>In the case of custom forms the results are provided in almost real time where the form is submitted or <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/AnalyzeDocument\">POST<\/a> request is sent to the API and an operation id is returned to retrieve the results using <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/GetAnalyzeDocumentResult\">GET<\/a>.  Depending on your pricing tier of your resource if you intend to perform these actions synchronously you might have to limit the rate of requests sent to the API to avoid any TPS errors. If you are using async operations with a slight delay to fetch the results then you can design an application that can take large number of documents and provide results to the users within a short span of time.     <\/p>\n<p>I hope the above information is helpful.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.4,
        "Solution_reading_time":27.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":292.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1376999872723,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Malaysia",
        "Answerer_reputation_count":998.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":0.0276297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create an forecasting experiment using R engine. My data source is pivoted, hence I need to pass row by row.\nThe output works great with single row prediction. But when I try to populate multiple lines, it still gives single row output - for the first record only.<\/p>\n\n<p>I'm trying to loop my result as follows :<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(forecast)\nlibrary(reshape)\nlibrary(dplyr)\nlibrary(zoo)\n#exclude non required columns\nmy.ds &lt;- dataset1[, -c(4,5,6)]\n# set the CIs we want to use here, so we can reuse this vector\ncis &lt;- c(80, 95)\n\nfor (i in 1:nrow(my.ds)) {\nmy.start &lt;- my.ds[i,c(3)]\nmy.product &lt;- my.ds[i, \"Product\"]\nmy.location &lt;- my.ds[i, \"Location\"]\nmy.result &lt;- melt(my.ds[i,], id = c(\"Product\",\"Location\"))\nmy.ts &lt;- ts(my.result$value, frequency=52, start=c(my.start,1))\n# generate the forecast using those ci levels\nf &lt;- forecast(na.interp(my.ts), h=52, level=cis)\n# make a data frame containing the forecast information, including the index\nz &lt;- as.data.frame(cbind(seq(1:52),\n                       f$mean,\n                       Reduce(cbind, lapply(seq_along(cis), function(i) cbind(f$lower[,i], f$upper[,i])))))\n# give the columns better names\nnames(z) &lt;- c(\"index\", \"mean\", paste(rep(c(\"lower\", \"upper\"), times = length(cis)), rep(cis, each = 2), sep = \".\"))\n# manipulate the results as you describe\nzw &lt;- z %&gt;%\n# keep only the variable you want and its index\nmutate(sssf = upper.95 - mean) %&gt;%\nselect(index, mean, sssf) %&gt;%\n# add product and location info\nmutate(product = my.product,\n       location = my.location) %&gt;%\n# rearrange columns so it's easier to read\nselect(product, location, index, mean, sssf)\nzw &lt;- melt(zw, id.vars = c(\"product\", \"location\", \"index\"), measure.vars = c(\"mean\",\"sssf\"))\ndata.set &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n}\n<\/code><\/pre>\n\n<p>This is design of my experiment :\n<a href=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" alt=\"experiment\"><\/a><\/p>\n\n<p>And this is how sample <a href=\"https:\/\/www.dropbox.com\/s\/xgfc7pnyy29frid\/dhf-00009E850%20-%20Copy.csv?dl=0\" rel=\"nofollow noreferrer\" title=\"input file\">input<\/a> looks like :<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" alt=\"Sample input\"><\/a><\/p>\n\n<p>I'm testing using the Excel test workbook downloaded from experiment site.<\/p>",
        "Challenge_closed_time":1480086125487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1480051198437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Batch Run - Single Output. The output works fine with single row prediction, but when the user tries to populate multiple lines, it still gives a single row output for the first record only. The user is trying to loop the result, but it is not working as expected. The user has shared the code and experiment design for reference.",
        "Challenge_last_edit_time":1480086026020,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40798184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":33.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":9.7019583334,
        "Challenge_title":"Azure ML Batch Run - Single Output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376999872723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Malaysia",
        "Poster_reputation_count":998.0,
        "Poster_view_count":136.0,
        "Solution_body":"<p>I figured out the problem :<\/p>\n\n<pre><code>{\n...\nds &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\ndata.set &lt;- rbind(data.set, ds)\n}\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>I should be merging the rows and then output outside of the loop.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1397589101936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dominican Republic",
        "Answerer_reputation_count":563.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":10.2489016667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im creating a model using optuna lightgbm integration, My training set has some categorical features and i pass those features to the model using the <code>lgb.Dataset<\/code> class, here is the code im using ( NOTE: X_train, X_val, y_train, y_val are all pandas dataframes ).<\/p>\n<pre><code>\nimport lightgbm as lgb \n\n        grid = {\n            \n       \n            'boosting': 'gbdt',\n            'metric': ['huber', 'rmse' , 'mape'],\n            'verbose':1\n\n        }\n        \n        X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n        cat_features = [ col for col in X_train if col.startswith('cat') ]\n\n        dval = Dataset(X_val, label=y_val, categorical_feature=cat_features)\n        dtrain = Dataset(X_train, label=y_train,  categorical_feature=cat_features)\n        \n        model = lgb.train(      \n                                    grid,\n                                    dtrain,\n                                    valid_sets=[dval],\n                                    early_stopping_rounds=100)\n                                    \n\n<\/code><\/pre>\n<p>Every time the <code>lgb.train<\/code> function is called, i get the following user warning<\/p>\n<pre><code>\n UserWarning: categorical_column in param dict is overridden.\n\n<\/code><\/pre>\n<p>I believe that lighgbm is not treating my categorical features the way it should, someone knows how to fix this issue? Am i using the parameter correctly?<\/p>",
        "Challenge_closed_time":1613830364163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613793468117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating a model using Optuna lightgbm integration. The training set has categorical features, and the user is passing those features to the model using the lgb.Dataset class. However, every time the lgb.train function is called, the user gets a UserWarning that the categorical_column in the param dict is overridden. The user is seeking help to fix this issue and wants to know if they are using the parameter correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66287854",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":10.2489016667,
        "Challenge_title":"Optuna lightgbm integration giving categorical features error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586625057632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>In case of picking the name (not indexes) of those columns, add as well the <code>feature_name<\/code> parameters as the <a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Dataset.html#lightgbm.Dataset.__init__\" rel=\"nofollow noreferrer\">documentation states<\/a><\/p>\n<p>That said, your <code>dval<\/code> and <code>dtrain<\/code> will be initialized as follow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>dval = Dataset(X_val, label=y_val, feature_name=cat_features, categorical_feature=cat_features)\ndtrain = Dataset(X_train, label=y_train, feature_name=cat_features, categorical_feature=cat_features)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.8,
        "Solution_reading_time":8.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1396913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi team,     <br \/>\nI am learning about the quota for machine learning service and I have a general doubt.    <\/p>\n<p>I can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.    <br \/>\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.     <\/p>\n<p>so that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.    <\/p>\n<p>I am able to find out  the-  get details query but this one doesnt give me details specific to each resource-group or the workspace.    <\/p>\n<p>HTTP query -&gt; <a href=\"https:\/\/management.azure.com\/subscriptions\/%7Bsubs_id%7D\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01\">https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01<\/a><\/p>",
        "Challenge_closed_time":1667069947336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667069444447,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to set quotas for CPU cores at the resource group level in Azure Machine Learning Service. They have a subscription level total CPU cores quota of 10 and want to assign 5 cores each to two resource groups under that subscription to ensure that if all the cores are taken up by the resources under one resource group, the other resource group or the ML workspace under it should not suffer. The user is unable to find specific details for each resource group or workspace using the provided HTTP query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":13.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1396913889,
        "Challenge_title":"how to set Quota at resource group level?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7bafa6a3-9285-4b1c-a003-4490a74d05b5\">@JA  <\/a> ,    <\/p>\n<p>quotas can be set on Azure Subscription level only.    <br \/>\nThere is no option to apply quotas for different Azure Resource Groups.    <br \/>\nThere are 2 options I can see for your requirement:    <br \/>\nUse 2 Azure Subscriptions for each Resource Group    <br \/>\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1558310526776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":70.5894952778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Challenge_closed_time":1651509693110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651238636737,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a pre-trained sklearn model on AWS Sagemaker using a serverless deployment method. They have saved the model using joblib.dump and uploaded it to S3, along with an inference script. They have also created a tar file with the model data and inference code and uploaded it to S3. However, the endpoint creation is stuck on \"creating\" and the user is receiving an error message in Cloudwatch. The user has tried different folder structures and accounts, but the issue persists. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":1651255570927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.9,
        "Challenge_reading_time":128.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":60,
        "Challenge_solved_time":75.2934369445,
        "Challenge_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":397.0,
        "Challenge_word_count":816,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558310526776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":21.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.3688888889,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522860000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1588992332000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running a custom image using the sagemaker training operator. The error occurred when the user used boto3 to manually download an object from s3, resulting in an \"Unable to locate credentials\" error. Although the boto credentials were found in environment variables, they did not make their way to the boto3 client instantiated inside the custom image. The user expected the credentials to be passed to the image that the training operator is running, but it did not happen.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":147.3688888889,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":141,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2082036111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have loaded <a href=\"https:\/\/automlsamplenotebookdata.blob.core.windows.net\/automl-sample-notebook-data\/bankmarketing_train.csv\">bankmarketing_train.csv<\/a> to get a dataset and auto generated a model to predict &quot;y&quot; field value with AutoML.    <br \/>\nVoting Ensemble model was generated as the best model and tested its behavior after deployed to the endpoint.    <\/p>\n<p>Schema is generated like this for the endpoint.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/113929-schema-2021-07-13-124905.png?platform=QnA\" alt=\"113929-schema-2021-07-13-124905.png\" \/>    <\/p>\n<p>Tried with the endpoint test feature in ML Studio. It worked and responded an expected output (left side in the fig below).    <br \/>\nBut my python REST call fails with 502 Bad Gateway(right side)    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114082-screenshot-2021-07-12-232443.png?platform=QnA\" alt=\"114082-screenshot-2021-07-12-232443.png\" \/>    <\/p>\n<p>Using the REST plug-in for VSCode, I have requested as below. This also failed with the same response status code.    <\/p>\n<pre><code>POST http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score  \nContent-Type: application\/json  \nAuthorization: Bearer === My correct key here ===  \n  \n{&quot;data&quot;: [{&quot;age&quot;: 87, &quot;campaign&quot;: 1, &quot;cons.conf.idx&quot;: -46.2, &quot;cons.price.idx&quot;: 92.893, &quot;contact&quot;: &quot;cellular&quot;, &quot;day_of_week&quot;: &quot;mon&quot;, &quot;default&quot;: &quot;no&quot;, &quot;duration&quot;: 471, &quot;education&quot;: &quot;university.degree&quot;, &quot;emp.var.rate&quot;: -1.8, &quot;euribor3m&quot;: 1.299, &quot;housing&quot;: &quot;yes&quot;, &quot;job&quot;: &quot;blue-collar&quot;, &quot;loan&quot;: &quot;yes&quot;, &quot;marital&quot;: &quot;married&quot;, &quot;month&quot;: &quot;may&quot;, &quot;nr.employed&quot;: 5099.1, &quot;pdays&quot;: 999, &quot;poutcome&quot;: &quot;failure&quot;, &quot;previous&quot;: 1}]}  \n<\/code><\/pre>\n<p>Investigated in the App Insight and queried the exceptions.    <br \/>\nI found this end point tries to convert 'yes' to int value. Of course it fails.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png?platform=QnA\" alt=\"114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png\" \/>    <\/p>\n<p>The value 'yes' is set to 'loan' and 'housing&quot;. Both are defined string value in the swagger.json for this endpoint.    <\/p>\n<p>What do you think?    <br \/>\nAm I missing something?    <br \/>\nIs this a bug with the endpoint?    <\/p>",
        "Challenge_closed_time":1626168091356,
        "Challenge_comment_count":3,
        "Challenge_created_time":1626149341823,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user generated a model using AutoML to predict a field value and deployed it to an endpoint. While testing the endpoint, the user found that the REST call fails with a 502 Bad Gateway error. The user investigated the issue and found that the endpoint tries to convert 'yes' to an int value, which fails. The user suspects that this may be a bug with the endpoint and seeks advice.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/473223\/endpoint-fails-with-the-model-generated-by-automat",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":5.2082036111,
        "Challenge_title":"Endpoint fails with the model generated by Automated ML",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, I tried that. Following is the code coming from the consume, the values are set accordingly.    <\/p>\n<pre><code>import urllib.request  \nimport json  \nimport os  \nimport ssl  \n  \ndef allowSelfSignedHttps(allowed):  \n    # bypass the server certificate verification on client side  \n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):  \n        ssl._create_default_https_context = ssl._create_unverified_context  \n  \nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.  \n  \n# Request data goes here  \n  \ndata = {&quot;data&quot;:  \n        [  \n          {  \n            &quot;age&quot;: &quot;17&quot;,  \n            &quot;campaign&quot;: &quot;1&quot;,  \n            &quot;cons.conf.idx&quot;: &quot;-46.2&quot;,  \n            &quot;cons.price.idx&quot;: &quot;92.893&quot;,  \n            &quot;contact&quot;: &quot;cellular&quot;,  \n            &quot;day_of_week&quot;: &quot;mon&quot;,  \n            &quot;default&quot;: &quot;no&quot;,  \n            &quot;duration&quot;: &quot;971&quot;,  \n            &quot;education&quot;: &quot;university.degree&quot;,  \n            &quot;emp.var.rate&quot;: &quot;-1.8&quot;,  \n            &quot;euribor3m&quot;: &quot;1.299&quot;,  \n            &quot;housing&quot;: &quot;yes&quot;,  \n            &quot;job&quot;: &quot;blue-collar&quot;,  \n            &quot;loan&quot;: &quot;yes&quot;,  \n            &quot;marital&quot;: &quot;married&quot;,  \n            &quot;month&quot;: &quot;may&quot;,  \n            &quot;nr.employed&quot;: &quot;5099.1&quot;,  \n            &quot;pdays&quot;: &quot;999&quot;,  \n            &quot;poutcome&quot;: &quot;failure&quot;,  \n            &quot;previous&quot;: &quot;1&quot;  \n          }  \n      ]  \n    }  \n  \n  \nbody = str.encode(json.dumps(data))  \n  \nurl = 'http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score'  \napi_key = '&lt;key&gt;' # Replace this with the API key for the web service  \nheaders = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}  \n  \nreq = urllib.request.Request(url, body, headers)  \n  \ntry:  \n    response = urllib.request.urlopen(req)  \n  \n    result = response.read()  \n    print(result)  \nexcept urllib.error.HTTPError as error:  \n    print(&quot;The request failed with status code: &quot; + str(error.code))  \n  \n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure  \n    print(error.info())  \n    print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))  \n  \n<\/code><\/pre>\n<p>The result was the same. 'yes' was tried to cast to int and failed.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114222-consume-2021-07-13-175924.png?platform=QnA\" alt=\"114222-consume-2021-07-13-175924.png\" \/>    <\/p>\n<p>In the deployment log, following exception observed. Something is happening inside the server call, which I cannot see.    <\/p>\n<pre><code>2021-07-13 08:56:49,684 | root | ERROR | Encountered Exception: Traceback (most recent call last):  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 64, in run_scoring  \n    response = invoke_user_with_timer(service_input, request_headers)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 97, in invoke_user_with_timer  \n    result = user_main.run(**params)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/wrapt\/wrappers.py&quot;, line 567, in __call__  \n    args, kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 57, in decorator_input  \n    kwargs[param_name] = _deserialize_input_argument(kwargs[param_name], param_type, param_name)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 285, in _deserialize_input_argument  \n    input_data = param_type.deserialize_input(input_data)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/parameter_types\/pandas_parameter_type.py&quot;, line 79, in deserialize_input  \n    data_frame = data_frame.astype(dtype=converted_types)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5865, in astype  \n    dtype=dtype[col_name], copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5882, in astype  \n    dtype=dtype, copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 581, in astype  \n    return self.apply(&quot;astype&quot;, dtype=dtype, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 438, in apply  \n    applied = getattr(b, f)(**kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 559, in astype  \n    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 643, in _astype  \n    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/dtypes\/cast.py&quot;, line 707, in astype_nansafe  \n    return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)  \n  File &quot;pandas\/_libs\/lib.pyx&quot;, line 547, in pandas._libs.lib.astype_intsafe  \nValueError: invalid literal for int() with base 10: 'yes'  \n  \nDuring handling of the above exception, another exception occurred:  \n  \nTraceback (most recent call last):  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1832, in full_dispatch_request  \n    rv = self.dispatch_request()  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1818, in dispatch_request  \n    return self.view_functions[rule.endpoint](**req.view_args)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 43, in score_realtime  \n    return run_scoring(service_input, request.headers, request.environ.get('REQUEST_ID', '00000000-0000-0000-0000-000000000000'))  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 77, in run_scoring  \n    raise RunFunctionException(str(exc))  \nrun_function_exception.RunFunctionException  \n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":86.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":52.0,
        "Solution_word_count":408.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":11.9435663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1651171215176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651128218337,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully initialized a ModelQualityMonitor object and created a monitoring schedule using the CreateMonitoringSchedule API in AWS Sagemaker. However, the user is missing the MAPE (Mean Absolute Percentage Error) in the predefined regression metrics and wants to create it in the future. The user is looking for ways to include custom regression metrics, such as MAPE, in ModelQualityMonitor and CloudWatch using the provided functionalities of Preprocessing and Postprocessing, Bring Your Own Containers, and CloudWatch Metrics for Bring Your Own Containers. The user has shared a code snippet of their current implementation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":43.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":11.9435663889,
        "Challenge_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.93,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":5.6297925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Microsoft Azure Machine Learning and was wondering if anyone had done some experiments on date time features. Doe sit automatically derive additional features like \"day of week\", \"day of month\", \"hour of day\" from them, or do I have to provide these?<\/p>\n\n<p>I could not find any info in the official documentation (and a lack of a Microsoft support forum =)<\/p>",
        "Challenge_closed_time":1434736380420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1434716113167,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Microsoft Azure Machine Learning and is seeking information on whether the platform automatically derives additional features like \"day of week\", \"day of month\", \"hour of day\" from date time features or if they need to provide these features themselves. The user was unable to find any information in the official documentation or a Microsoft support forum.",
        "Challenge_last_edit_time":1445833326870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30937903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6297925,
        "Challenge_title":"How are date features utilized in Microsoft Azure Machine Studio",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1221999894423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delaware",
        "Poster_reputation_count":2603.0,
        "Poster_view_count":225.0,
        "Solution_body":"<p>Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples below<\/p>\n\n<p>x&lt;-as.Date(\"12\/3\/2009\", \"%m\/%d\/%Y\")<\/p>\n\n<blockquote>\n  <p>months.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"December\"<\/p>\n\n<blockquote>\n  <p>weekdays.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Thursday\"<\/p>\n\n<blockquote>\n  <p>quarters(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Q4\"<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.0155555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Challenge_closed_time":1650034426000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649933570000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the full episode data logging in a random agent script using wandb, where some steps are being skipped. The problem is due to wandb counting the episode reward logging steps made before the full data logging. A potential solution suggested is to add another metric to log the timestep and day proportionally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":102.0,
        "Challenge_repo_star_count":20.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.0155555556,
        "Challenge_title":"In random agent script wandb full episode data logging skips a few steps",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This has been implemented and will be shipped with v0.4.4 \ud83d\ude80",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.72,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1486312694643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Germany",
        "Answerer_reputation_count":668.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":7705.2959355556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Challenge_closed_time":1601639323903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601638106830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is experiencing slow query performance when using PyAthena to query data from Athena compared to running the same query directly from the Athena console. The user is unsure if PyAthena is causing the slowdown or if the data transfer from Athena to Sagemaker is the issue. The user is seeking advice on how to speed up the query performance.",
        "Challenge_last_edit_time":1601641259400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3380758334,
        "Challenge_title":"Pyathena is super slow compared to querying from Athena",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3188.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486312694643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Germany",
        "Poster_reputation_count":668.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1629380324768,
        "Solution_link_count":2.0,
        "Solution_readability":16.7,
        "Solution_reading_time":10.36,
        "Solution_score_count":13.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1243547542743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6438.0,
        "Answerer_view_count":922.0,
        "Challenge_adjusted_solved_time":193.0081575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Challenge_closed_time":1449768300887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449073471520,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the Jupyter notebook kernel dies when attempting to create a dummy column for a column with 5,196 unique values. The resulting dataframe should have a shape of (647054, 5196), but the kernel dies almost every time the user runs the code. The user is unsure if this is a Jupyter notebook or pandas bug and has tried running the code on a machine with >100 GB of RAM with no success.",
        "Challenge_last_edit_time":1454300528203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":193.0081575,
        "Challenge_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5063.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373475615300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":2037.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":8.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.88302,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Challenge_closed_time":1676070775536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676042396664,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to rerun a few runs in a wandb sweep, but after deleting them from the dashboard, wandb is not able to rerun those runs even after running the command again. The user suggests adding a feature to rerun the deleted runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.8,
        "Challenge_reading_time":5.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.88302,
        "Challenge_title":"Rerun a deleted run in wandb sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1310482059760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":10753.0,
        "Answerer_view_count":895.0,
        "Challenge_adjusted_solved_time":265.4097633333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Challenge_closed_time":1588575738648,
        "Challenge_comment_count":3,
        "Challenge_created_time":1587620263500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble finding the EC2 instances being used by their Sagemaker endpoint and is looking for a way to monitor their usage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":265.4097633333,
        "Challenge_title":"Sagemaker usage of EC2 instances",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":615.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.9,
        "Solution_reading_time":6.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":0.4981455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a recommender system using sagemaker's built-in factorisation machine model.<\/p>\n\n<p>My desired result is to have a <strong>rating matrix<\/strong> where I can look up a predicted score by a <strong>user id<\/strong> and an <strong>item id<\/strong>.<\/p>\n\n<p>I understand that there is a <strong>predict API<\/strong> provided by the model: <\/p>\n\n<pre><code>result = fm_predictor.predict(X_test[1000:1010].toarray())\n<\/code><\/pre>\n\n<p>But I am not sure how I can use it to achieve the desired purpose. If I want to know, say, if user#123 is interested in movie#456, how can I use the above API?<\/p>\n\n<p>Reference: \n<a href=\"https:\/\/medium.com\/@julsimon\/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8<\/a><\/p>\n\n<p><a href=\"https:\/\/www.slideshare.net\/AmazonWebServices\/building-a-recommender-system-on-aws-aws-summit-sydney-2018\" rel=\"nofollow noreferrer\">https:\/\/www.slideshare.net\/AmazonWebServices\/building-a-recommender-system-on-aws-aws-summit-sydney-2018<\/a> (p.41,43)<\/p>\n\n<hr>\n\n<p>Updated:<\/p>\n\n<p>I think I understand how to use the API now, you have to build another one-hot encoded dataset as input, for example:<\/p>\n\n<pre><code>X_new = lil_matrix((1, nbFeatures)).astype('float32')\nX_new[0, 935] = 1\nX_new[0, 1600] = 1\n\nprediction2 = X_new[0].toarray()\nresult2 = fm_predictor.predict(prediction2)\n\nprint(result2)\n<\/code><\/pre>\n\n<p>But it seems it would be quite inefficient to fill out the recommendation matrix this way. What would be the best practice?<\/p>",
        "Challenge_closed_time":1548238102360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548212721507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a recommender system using Amazon SageMaker's built-in factorisation machine model and wants to create a rating matrix to predict scores based on user and item IDs. They are unsure how to use the predict API provided by the model to achieve this. The user has since figured out how to use the API by building another one-hot encoded dataset as input, but is concerned about the inefficiency of filling out the recommendation matrix this way and is seeking best practices.",
        "Challenge_last_edit_time":1548236309036,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54319471",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.0502369444,
        "Challenge_title":"Amazon SageMaker factorisation machine rating matrix and endpoint",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":199.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394122990683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I think one could think about 2 scenarios:<\/p>\n\n<p>1) if you need very low latency, you can fill up the matrix indeed, i.e. compute all recos for all users, and store it in a key\/value backend queried by your app. You can definitely predict multiple users at a time, using the one-hot encoded technique above.<\/p>\n\n<p>2) predict on-demand by invoking the endpoint directly from the app. This is quite simpler, at the cost of a little latency.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6481894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From here: <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-training-vnet?view=azureml-api-2&amp;tabs=cli%2Crequired#compute-instancecluster-with-no-public-ip\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-training-vnet?view=azureml-api-2&amp;tabs=cli%2Crequired#compute-instancecluster-with-no-public-ip<\/a><\/p>\n<p>The command to create either a AMLCompute or Compute Instance contains the flag option using the (az ml command) for <code>--set enable_node_public_ip=False<\/code><\/p>\n<p>This option isn't available in ARM, Bicep or Terraform (AzAPI provider) templates - and it really needs to be for Infrastructure-as-Code provisioning<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-terraform\">https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-terraform<\/a><\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1684750280332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684744346850,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using ARM\/Bicep templates to create either AMLCompute or Compute Instance as the flag option to set no public IP address is not available in these templates. This option is only available in the az ml command and needs to be added to the templates for Infrastructure-as-Code provisioning.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1289333\/no-options-for-setting-no-public-ip-address-flag-f",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":28.3,
        "Challenge_reading_time":15.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.6481894444,
        "Challenge_title":"No options for setting no public IP address flag for Compute options in ARM\/Bicep templates",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The second link you provided in the question shows how to set this flag in Bicep\/Arm\/Terraform<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-bicep#amlcomputeproperties\">https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-bicep#amlcomputeproperties<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/652a27eb-d7f4-41e6-bd28-0750b1ac5ced?platform=QnA\" alt=\"User's image\" \/><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":35.8,
        "Solution_reading_time":8.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1495636394672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":1006.7836063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Challenge_closed_time":1613677698376,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610053277393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while using AWS SageMaker Hyperparameter tuning job as they are unable to use P2 or P3 instances due to a resource limit exceeded error. The error message suggests that the account-level service limit for 'ml.p3.2xlarge for training job usage' is 2 instances, with current utilization of 0 instances and a request delta of 5 instances. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65619881",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1006.7836063889,
        "Challenge_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495636394672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":127.0472208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Challenge_closed_time":1517706762272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517249392277,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in taking advantage of the speedup from distributing the job to many instances by increasing the instance count argument of the training algorithm when using a custom algorithm in AWS Sagemaker. Increasing the instance count duplicates the training on every instance, leading to no speedup. The user suspects that there is something special they need to do to control how the training is handled differently for a particular instance inside their custom train() function. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":14.0,
        "Challenge_reading_time":29.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":127.0472208333,
        "Challenge_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":295,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368093601223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":249.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":20.5317583333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using optuna for parameter optimisation of my custom models. <\/p>\n\n<p>Is there any way to sample parameters until current params set was not tested before? I mean, do try sample another params if there were some trial in the past with the same set of parameters. <\/p>\n\n<p>In some cases it is impossible, for example, when there is categorial distribution and <code>n_trials<\/code> is greater than number os possible unique sampled values. <\/p>\n\n<p>What I want: have some config param like <code>num_attempts<\/code> in order to sample parameters up to <code>num_attempts<\/code> in for-loop until there is a set that was not tested before, else - to run trial on the last sampled set. <\/p>\n\n<p>Why I need this: just because it costs too much to run heavy models several times on the same parameters. <\/p>\n\n<p>What I do now: just make this \"for-loop\" thing but it's messy.<\/p>\n\n<p>If there is another smart way to do it - will be very grateful for information.  <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1573636947556,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573568558157,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is using optuna for parameter optimization of custom models and is looking for a way to sample parameters without duplicates. They want to avoid running heavy models several times on the same parameters and are currently using a messy for-loop to achieve this. They are looking for a more efficient solution, such as a config parameter like \"num_attempts\" to sample parameters until a set that was not tested before is found.",
        "Challenge_last_edit_time":1573569572550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58820574",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.9970552778,
        "Challenge_title":"How to sample parameters without duplicates in optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2121.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>To the best of my knowledge, there is no direct way to handle your case for now.\nAs a workaround, you can check for parameter duplication and skip the evaluation as follows:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # Sample parameters.\n    x = trial.suggest_int('x', 0, 10)\n    y = trial.suggest_categorical('y', [-10, -5, 0, 5, 10])\n\n    # Check duplication and skip if it's detected.\n    for t in trial.study.trials:\n        if t.state != optuna.structs.TrialState.COMPLETE:\n            continue\n\n        if t.params == trial.params:\n            return t.value  # Return the previous value without re-evaluating it.\n\n            # # Note that if duplicate parameter sets are suggested too frequently,\n            # # you can use the pruning mechanism of Optuna to mitigate the problem.\n            # # By raising `TrialPruned` instead of just returning the previous value,\n            # # the sampler is more likely to avoid sampling the parameters in the succeeding trials.\n            #\n            # raise optuna.structs.TrialPruned('Duplicate parameter set')\n\n    # Evaluate parameters.\n    return x + y\n\n# Start study.\nstudy = optuna.create_study()\n\nunique_trials = 20\nwhile unique_trials &gt; len(set(str(t.params) for t in study.trials)):\n    study.optimize(objective, n_trials=1)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1573643486880,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":14.88,
        "Solution_score_count":10.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":149.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":5.6782083333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Challenge_closed_time":1561838537720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561818096170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, they encountered an error stating that the SKLearn NearestNeighbors learner does not have a predict method. The user is now trying to find a way to call this endpoint within Sagemaker or determine if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":5.6782083333,
        "Challenge_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":694.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511812140112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":169.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.3012636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've a azure synapse analytics workspace in region North Europe, as the region has hardware Accelerated pools, GPU base pools so to say. But i don't see the packages setting.     <br \/>\nhere is the comparison for 2 workspace, 1 in north Europe and other one in West Europe.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209418-screenshot-2022-06-08-at-120209.png?platform=QnA\" alt=\"209418-screenshot-2022-06-08-at-120209.png\" \/> vs <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209494-screenshot-2022-06-08-at-120550.png?platform=QnA\" alt=\"209494-screenshot-2022-06-08-at-120550.png\" \/>    <\/p>\n<p>Even the package setting in the Workspace itself is disabled for me: here is the screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209434-screenshot-2022-06-08-at-120143.png?platform=QnA\" alt=\"209434-screenshot-2022-06-08-at-120143.png\" \/>    <\/p>\n<p>I've 2 questions in this reagrd:     <\/p>\n<ul>\n<li> Am I missing any configuration for the GPU pool or this feature is not released?    <\/li>\n<li> Is there any alternate way to install a package? <code>pip install<\/code> or <code>pip3 install<\/code> are not working.      <\/li>\n<\/ul>",
        "Challenge_closed_time":1654770659816,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654683175267,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing a Python package in a hardware accelerated GPU Spark pool in their Azure Synapse Analytics workspace located in North Europe. They are unable to find the package setting and it is disabled in the workspace. The user has two questions regarding this issue: whether they are missing any configuration for the GPU pool or if this feature is not yet released, and if there is an alternate way to install the package as pip install or pip3 install are not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/881432\/how-to-install-python-package-in-hardware-accelera",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":24.3012636111,
        "Challenge_title":"How to install python package in hardware accelerated GPU spark pool ?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=f236076b-e057-4c60-b0fd-0068a6492053\">@Prateek Narula  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>(UPDATE:6\/10\/2022): Unfortunately, we do not have Library Management (Package) support for GPU spark pools in Azure Synapse Analytics.    <\/p>\n<\/blockquote>\n<p>---------------------------------------------------    <\/p>\n<p>As per the repro, I had noticed similar behaviour.     <\/p>\n<blockquote>\n<p>Looks like packages are only supported for Node size family: &quot;Memory Optimized&quot; - let me get a confirmation from the product team.    <\/p>\n<\/blockquote>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209808-synape-gpu.gif?platform=QnA\" alt=\"209808-synape-gpu.gif\" \/>    <\/p>\n<p>We are reaching out to internal team to get more information related to this issue and will get back to you as soon as we have an update.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.6,
        "Solution_reading_time":26.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":213.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":11.6196091666,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I tried to deploy example from Amazon SageMaker<\/p>\n\n<pre><code>xgb_predictor = xgb.deploy(initial_instance_count=1,\n                           instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/p>\n\n<p>Any idea how to fix this? <\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1556224263716,
        "Challenge_comment_count":1,
        "Challenge_created_time":1556178339113,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a \"Resource Limit Exceeded\" error while trying to deploy an example from Amazon SageMaker. The error message indicates that the account-level service limit for endpoint usage is 0 instances, with a request delta of 1 instance. The user is seeking advice on how to fix this issue.",
        "Challenge_last_edit_time":1556182433123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55844248",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":12.7568341667,
        "Challenge_title":"Resource Limit Exceeded- xgb.deploy",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1093.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556178046967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>AWS is using soft limits to prevent customers from making a mistake that might cause them more money than they expected. When you are starting to use a new service, such as Amazon SageMaker, you will hit these soft limits and you need to ask specifically to raise them using the \"Support\" link on the top right side of your AWS management console. <\/p>\n\n<p>Here is a link to guide on how to do that: <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/<\/a><\/p>\n\n<p>You will usually get the limit increased within a couple of days. In the meanwhile, you can choose a smaller instance (such as t2) that are often available.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1320517748836,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37777.0,
        "Answerer_view_count":5307.0,
        "Challenge_adjusted_solved_time":1739.8018944444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-inference-on-kubernetes-with-amazon-sagemaker-operators\/\" rel=\"nofollow noreferrer\">Kubernetes SageMaker Operations<\/a> with the XGBoost MNIST AWS's <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_mnist\/xgboost_mnist.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Before I am enabling Kubernetes SageMaker Ops, I have deployed the XGBoost MNIST example via SageMaker WebUI itself and tried to access the endpoint via awscli:<\/p>\n<pre><code>$ aws sagemaker-runtime invoke-endpoint \\\n    --region eu-west-1 \\\n    --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n    --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n    &gt;(cat) \\\n    --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>\n<p>However, I am encountering the following decoding error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;Loading csv data failed with Exception, please ensure data is in csv format:\n &lt;class 'UnicodeDecodeError'&gt;\n 'utf-8' codec can't decode byte 0xd7 in position 0: invalid continuation byte&quot;. See https:\/\/xxxx.console.aws.amazon.com\/cloudwatch\/home?region=xxxxx#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2020-11-20-06-26-30 in account XXX for more information.\n<\/code><\/pre>\n<p>And in the log I can see:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data\n    decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\nTraceback (most recent call last): File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>When I go to the source code of <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/master\/src\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py#L102\" rel=\"nofollow noreferrer\">sagemaker_xgboost_container<\/a> I can see that they expect UTF-8 format:<\/p>\n<pre><code>        decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>My <code>locale<\/code> seems fine and I am really not sure what else could go wrong:<\/p>\n<pre><code>$ locale\nLANG=C.UTF-8\nLANGUAGE=\nLC_CTYPE=&quot;C.UTF-8&quot;\nLC_NUMERIC=&quot;C.UTF-8&quot;\nLC_TIME=&quot;C.UTF-8&quot;\nLC_COLLATE=&quot;C.UTF-8&quot;\nLC_MONETARY=&quot;C.UTF-8&quot;\nLC_MESSAGES=&quot;C.UTF-8&quot;\nLC_PAPER=&quot;C.UTF-8&quot;\nLC_NAME=&quot;C.UTF-8&quot;\nLC_ADDRESS=&quot;C.UTF-8&quot;\nLC_TELEPHONE=&quot;C.UTF-8&quot;\nLC_MEASUREMENT=&quot;C.UTF-8&quot;\nLC_IDENTIFICATION=&quot;C.UTF-8&quot;\nLC_ALL=\n<\/code><\/pre>",
        "Challenge_closed_time":1606454756256,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605856943127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a decoding error while trying to access the endpoint of AWS's XGBoost MNIST example via awscli. The error message suggests that the payload is not in UTF-8 format, which is expected by the sagemaker_xgboost_container. The user has checked their locale and is unsure of what else could be causing the issue.",
        "Challenge_last_edit_time":1606193820232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64925353",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":22.3,
        "Challenge_reading_time":40.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":166.0592025,
        "Challenge_title":"How to run and deploy AWS's XGBoost MNIST sample notebook on SageMaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":231,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320517748836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":37777.0,
        "Poster_view_count":5307.0,
        "Solution_body":"<p>I have contacted AWS support and apparently this is a bug in <code>awscli<\/code>. Here is a revised excerpt from their long and detailed answer.<\/p>\n<blockquote>\n<p>This is an encoding issue with AWSCLI v2. For now, you can proceed\nwith AWSCLI v1.18 as a temporary solution.<\/p>\n<\/blockquote>\n<p>I also verified it works with aws-cli\/1.18.185:<\/p>\n<pre><code>$ aws --version\naws-cli\/1.18.185 Python\/3.8.3 Linux\/4.19.104-microsoft-standard botocore\/1.19.25\n\n$ aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region eu-west-1 \\\n&gt;     --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n8.0%\n<\/code><\/pre>\n<p>In AWS cli v2.1.21 amazon added the <code>--cli-binary-format raw-in-base64-out<\/code> option and this should work as it worked with AWS cli v1.18:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region &lt;aws-region&gt; \\\n&gt;     --endpoint-name &lt;you-endpoint-name&gt; \\\n&gt;     --cli-binary-format raw-in-base64-out \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612457107052,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":15.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":137.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.5462247223,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Challenge_closed_time":1652695869278,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652672302869,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using Sweeps to run through different configuration models and wants to know if there is an easier way to query the best model configuration and run predictions off it, instead of creating a new sweep with the best performing parameter set.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.5,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.5462247223,
        "Challenge_title":"Run best model off sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":8.17,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.0455230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1660860269032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to store weights locally and use the sweep id and\/or run id for naming. They are seeking guidance on how to access these within the train function they are using for the agent.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":39.0455230556,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":755.2722222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221269000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1612502289000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error with CometLogger when using an API key without a save directory. The error occurs because the train loop tries to read the save directory, which is not set. The issue can be resolved by setting the save directory to None.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":755.2722222222,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":21.92,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":326.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":602.3895386111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Challenge_closed_time":1524806701632,
        "Challenge_comment_count":1,
        "Challenge_created_time":1522638099293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the differences between Azure Machine Learning Studio and Azure Machine Learning Workbench, including their intended differences and whether Workbench is being phased out in favor of Studio. The user has found some differences, such as Studio having a hard limit on input data and a more user-friendly GUI, while Workbench has more powerful deployment tools. However, there are also scattered references claiming that Studio is simply a renamed and updated version of Workbench. The user is seeking guidance on which offering to choose as a fresh data scientist looking to adopt the Microsoft stack for enterprise-scale use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":15.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":602.3895386111,
        "Challenge_title":"Azure Machine Learning Studio vs. Workbench",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3387.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434736108840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dallas, TX, United States",
        "Poster_reputation_count":2045.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":10.52,
        "Solution_score_count":6.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":247.7520736111,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Challenge_closed_time":1656556961635,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655665054170,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble associating their sweeps with a project in wandb. They have tried setting the project name in the sweep configuration and using wandb.init with the project name, but the sweeps are still showing up as uncategorized in the projects interface. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.6,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":247.7520736111,
        "Challenge_title":"Can't associate sweeps with project",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1045.5080555556,
        "Challenge_answer_count":0,
        "Challenge_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_closed_time":1582730951000,
        "Challenge_comment_count":14,
        "Challenge_created_time":1578967122000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to import the ML library in an Azure Notebook VM. The error is related to the attribute error of the TensorFlow logging module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Challenge_link_count":0,
        "Challenge_participation_count":14,
        "Challenge_readability":15.3,
        "Challenge_reading_time":1.92,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1045.5080555556,
        "Challenge_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@alla15747 Hi, thanks for reaching out to us. Could you please share your environment file so that we can know the details of this issue? @alla15747  please make sure that azureml-train-automl-runtime is installed in your environment if you using sdk>=1.0.76 or azureml-train-automl if using older version I'm running the code on the compute target and not my local machine. SDK 1.0.72\r\nHow to install packages in Azure Devops environment like azureml-train-automl? (base) C:\\Users\\aabdel137>pip freeze\r\nabsl-py==0.8.1\r\nadal==1.2.2\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.7\r\nanaconda-project==0.8.3\r\nansiwrap==0.8.4\r\napplicationinsights==0.11.9\r\nasn1crypto==0.24.0\r\nastor==0.8.0\r\nastroid==2.3.1\r\nastropy==3.2.1\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nazure-common==1.1.23\r\nazure-graphrbac==0.61.1\r\nazure-mgmt-authorization==0.60.0\r\nazure-mgmt-containerregistry==2.8.0\r\nazure-mgmt-keyvault==2.0.0\r\nazure-mgmt-resource==5.1.0\r\nazure-mgmt-storage==6.0.0\r\nazureml-contrib-interpret==1.0.72\r\nazureml-contrib-notebook==1.0.72\r\nazureml-core==1.0.72\r\nazureml-dataprep==1.1.29\r\nazureml-dataprep-native==13.1.0\r\nazureml-explain-model==1.0.72\r\nazureml-interpret==1.0.72.1\r\nazureml-pipeline==1.0.72\r\nazureml-pipeline-core==1.0.72\r\nazureml-pipeline-steps==1.0.72\r\nazureml-sdk==1.0.72\r\nazureml-telemetry==1.0.72\r\nazureml-train==1.0.72\r\nazureml-train-core==1.0.72\r\nazureml-train-restclients-hyperdrive==1.0.72\r\nazureml-widgets==1.0.72\r\nBabel==2.7.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.5\r\nbackports.os==0.1.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.7.1\r\nbitarray==0.9.3\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nbokeh==1.2.0\r\nboto==2.49.0\r\nBottleneck==1.2.1\r\ncertifi==2019.6.16\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.2.1\r\nclyent==1.2.2\r\ncolorama==0.4.1\r\ncomtypes==1.1.7\r\nconda==4.7.10\r\nconda-build==3.18.8\r\nconda-package-handling==1.3.11\r\nconda-verify==3.4.2\r\ncontextlib2==0.5.5\r\ncoverage==4.5.4\r\ncryptography==2.7\r\ncycler==0.10.0\r\nCython==0.29.12\r\ncytoolz==0.10.0\r\ndask==2.1.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\ndistributed==2.1.0\r\ndistro==1.4.0\r\ndocker==4.1.0\r\ndocutils==0.14\r\ndotnetcore2==2.1.10\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nflake8-formatter-junit-xml==0.0.6\r\nFlask==1.1.1\r\nfusepy==3.0.1\r\nfuture==0.17.1\r\ngast==0.3.2\r\ngevent==1.4.0\r\nglob2==0.7\r\ngoogle-pasta==0.1.7\r\ngreenlet==0.4.15\r\ngrpcio==1.24.3\r\nh5py==2.9.0\r\nheapdict==1.0.0\r\nhtml5lib==1.0.1\r\nidna==2.8\r\nimageio==2.5.0\r\nimagesize==1.1.0\r\nimportlib-metadata==0.23\r\ninterpret-community==0.1.0.3.3\r\ninterpret-core==0.1.18\r\nipykernel==5.1.1\r\nipython==7.6.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.0\r\nisodate==0.6.0\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.13.3\r\njeepney==0.4.1\r\nJinja2==2.10.1\r\njmespath==0.9.4\r\njoblib==0.13.2\r\njson5==0.8.4\r\njsonpickle==1.2\r\njsonschema==3.0.1\r\njunit-xml==1.8\r\njupyter==1.0.0\r\njupyter-client==5.3.1\r\njupyter-console==6.0.0\r\njupyter-core==4.5.0\r\njupyterlab==1.0.2\r\njupyterlab-server==1.0.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==18.0.0\r\nkiwisolver==1.1.0\r\nkmodes==0.10.1\r\nlazy-object-proxy==1.4.2\r\nlibarchive-c==2.8\r\nllvmlite==0.29.0\r\nlocket==0.2.0\r\nlxml==4.3.4\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.0\r\nmccabe==0.6.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmkl-service==2.0.2\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmsrest==0.6.10\r\nmsrestazure==0.6.2\r\nmultipledispatch==0.6.0\r\nnavigator-updater==0.2.1\r\nnbconvert==5.5.0\r\nnbformat==4.4.0\r\nndg-httpsclient==0.5.1\r\nnetworkx==2.3\r\nnltk==3.4.4\r\nnose==1.3.7\r\nnotebook==6.0.0\r\nnumba==0.44.1\r\nnumexpr==2.6.9\r\nnumpy==1.16.4\r\nnumpydoc==0.9.1\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopenpyxl==2.6.2\r\npackaging==19.2\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\npapermill==1.2.1\r\nparso==0.5.0\r\npartd==1.0.0\r\npath.py==12.0.1\r\npathlib2==2.3.4\r\npathspec==0.6.0\r\npatsy==0.5.1\r\npep8==1.7.1\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.0\r\nply==3.11\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.10.0\r\npsutil==5.6.3\r\npy==1.8.0\r\npy4j==0.10.7\r\npyasn1==0.4.7\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.3\r\npyflakes==2.1.1\r\nPygments==2.4.2\r\nPyJWT==1.7.1\r\npylint==2.4.2\r\npyodbc==4.0.26\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npypiwin32==223\r\npyreadline==2.1\r\npyrsistent==0.14.11\r\nPySocks==1.7.0\r\npyspark==2.4.4\r\npytest==5.2.2\r\npytest-arraydiff==0.3\r\npytest-astropy==0.5.0\r\npytest-cov==2.7.1\r\npytest-doctestplus==0.3.0\r\npytest-openfiles==0.3.2\r\npytest-remotedata==0.3.1\r\npython-dateutil==2.8.0\r\npython-dotenv==0.10.3\r\npytz==2019.1\r\nPyWavelets==1.0.3\r\npywin32==223\r\npywinpty==0.5.5\r\nPyYAML==5.1.1\r\npyzmq==18.0.0\r\nQtAwesome==0.5.7\r\nqtconsole==4.5.1\r\nQtPy==1.8.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.2.0\r\nrope==0.14.0\r\nruamel-yaml==0.15.46\r\nruamel.yaml==0.15.89\r\nscikit-image==0.15.0\r\nscikit-learn==0.21.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nSecretStorage==3.1.1\r\nSend2Trash==1.5.0\r\nshap==0.29.3\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.12.0\r\nsklearn==0.0\r\nsnowballstemmer==1.9.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.8\r\nSphinx==2.1.2\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.1.2\r\nspyder==3.3.6\r\nspyder-kernels==0.5.1\r\nSQLAlchemy==1.3.5\r\nstatsmodels==0.10.0\r\nsympy==1.4\r\ntables==3.5.2\r\ntblib==1.4.0\r\ntenacity==5.1.5\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-gpu==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntextwrap3==0.9.2\r\ntf-estimator-nightly==1.14.0.dev2019031401\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.37.0\r\ntraitlets==4.3.2\r\ntyped-ast==1.4.0\r\nunicodecsv==0.14.1\r\nunittest-xml-reporting==2.5.2\r\nurllib3==1.24.2\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.15.4\r\nwidgetsnbextension==3.5.0\r\nwin-inet-pton==1.1.0\r\nwin-unicode-console==0.5\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nxlrd==1.2.0\r\nXlsxWriter==1.1.8\r\nxlwings==0.15.8\r\nxlwt==1.3.0\r\nzict==1.0.0\r\nzipp==0.6.0\r\n AutoML became a part of default distribution (azureml-sdk) since 1.0.83\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2020-01-06\r\n\r\nif your client, that I believe pins the version of azureml sdk packages for the remote environment is 1.0.83, you will have automl on remote. \r\n\r\nIf you want to stay with 1.0.72 you can either reference automl extras azureml-sdk[automl] or explicitly reference azureml-train-automl (prefered).\r\n\r\nI would recommend to do both, upgrade client to the latest version and explicitly reference packages you need for your particular scenario not relying on metapackages like azureml-sdk\r\n\r\nOur reference doc will help you to get a set of the packages needed for your scenario\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py\r\n\r\nBy design every AzureML Python SDK package will bring necessary internal dependencies (of course except some corner cases :) )\r\n Thanks Vizhur, do you mind showing an example on how to reference azureml-train-automl in Azure Devops or Portal? \r\nThank you for the links! Not sure about your particular scenario, would you mind to share your ADO scenario so I can think of how to update it? MY scenario is implementing MLOPs example with automl step. Let me try to pull some relevant folks into the thread For AutoML, all the remote dependencies will get taken care of and will match whatever local dependencies are installed, e.g. if you have azureml-train-automl==1.0.72 installed, that version will be installed remotely for the training job.\r\nWe provide 2 clients to submitting these remote jobs currently, a thin client for submitting some types of remote jobs which is included as part of azureml-sdk, and a fuller client which enables more experiences such as Pipeline runs as part of azureml-train-automl. Since it looks like you are trying to use Pipelines, you will need to install the full azureml-train-automl client.\r\n\r\nFurthermore, the namespace for AutoMLStep changed recently, if you are using <1.0.76 the namespace would be \"from azureml.train.automl import AutoMLStep\", for >=1.0.76, you'll want to use \"from azureml.train.automl.runtime import AutoMLStep\" instead. I'm have sdk 1.0.72 installed. And I'm using from azureml.train.automl import AutoMLStep. Is there anyway to check the sdk version on the compute target machine? From your pip freeze, it doesn't look like you have the AutoML SDK installed. For the pipelines experience, you will need to have the SDK installed locally, not just on the target compute. Could you run \"pip install azureml-train-automl\"? @alla15747 \r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion. @SKrupa - Are you running your own code or a particular notebook sample from this repo?\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":117.81,
        "Solution_score_count":null,
        "Solution_sentence_count":38.0,
        "Solution_word_count":786.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":53.7180736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Challenge_closed_time":1569671934552,
        "Challenge_comment_count":4,
        "Challenge_created_time":1569478549487,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a mobile application using React Native and needs to access a machine learning model to send pictures for segmentation. They want to use Amazon Sagemaker to receive a segmented picture back, but are unsure how to create an endpoint that they can access through React Native without running a separate server.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":16.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":53.7180736111,
        "Challenge_title":"How do I access Amazon Sagemaker through React Native?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":717.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488334447848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":17.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1336887489390,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":758.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":2.2057469444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I am considering to use amazon sagemaker to train my deep learning model because I'm having memory issues due to the large dataset and neural network that I have to train. I'm confused whether to store my data in my notebook instance or in S3? If I store it in my s3 would I be able to access it to train on my notebook instance? I'm confused on the concepts. Can anyone explain the use of S3 in machine learning in AWS?<\/p>",
        "Challenge_closed_time":1619062178832,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619054238143,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is new to AWS and is considering using Amazon SageMaker to train a deep learning model due to memory issues caused by a large dataset and neural network. They are unsure whether to store their data in their notebook instance or in S3 and are seeking clarification on the use of S3 in machine learning in AWS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67205469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":6.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.2057469444,
        "Challenge_title":"Is it a good idea to store my dataset in my notebook instance in sagemaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>Yes you can use S3 as storage for your training datasets.<\/p>\n<p>Refer diagram in this link describing how everything works together: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html<\/a><\/p>\n<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I\/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.<\/p>\n<\/blockquote>\n<p>The blog also contains python code snippets using Pipe input mode for reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":15.6,
        "Solution_reading_time":25.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":201.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3566.0353452778,
        "Challenge_answer_count":2,
        "Challenge_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas \n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a [request for a service quota increase](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#service-limit-increase-request-procedure).",
        "Challenge_closed_time":1655317929862,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642480202619,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to check their current service quotas for Amazon SageMaker, but there is no link provided for region-specific quotas, which may have changed after a request for a service quota increase.",
        "Challenge_last_edit_time":1668602220822,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sagemaker-service-quotas",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":14.7,
        "Challenge_reading_time":8.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3566.0353452778,
        "Challenge_title":"How do I check my current SageMaker service quotas?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1212.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655365526940,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1297805556,
        "Challenge_answer_count":1,
        "Challenge_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n```\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n```\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Challenge_closed_time":1669736547972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669725280762,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in passing environment variables in Sagemaker tuner job as there is no equivalent option available for tuner jobs. The SagemakerTuner in the python SDK is also ignoring the environment variables set in the passed estimator. The user is seeking a solution to pass environment variables to the training jobs started by a tuner job programmatically.",
        "Challenge_last_edit_time":1670071482024,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":9.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.1297805556,
        "Challenge_title":"How to pass environment variables in sagemaker tuner job",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":553.0,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for raising this. Yes, as you point out the `Environment` collection is not supported in the underlying `CreateHyperparameterTuningJob` API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the [SM Py SDK GitHub issue here](https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2934), you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore [setting the variables from your Python script](https:\/\/stackoverflow.com\/a\/5971326) (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the [ENV command](https:\/\/docs.docker.com\/engine\/reference\/builder\/#env)? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\n- Use `sagemaker.image_uris.retrieve(...)` to find the base image URI for your given framework, version, region, etc. You'll need to [authenticate Docker to this registry](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/getting-started-cli.html#cli-authenticate-registry) as well as your own Amazon ECR account.\n- Create a Dockerfile that takes this base image URI as an arg and builds `FROM` it, something like [this example](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/b95b62cd2abd304ee347cacdd3eaf2a76e8b5953\/notebooks\/custom-containers\/train-inf\/Dockerfile)\n- Add the required `ENV` commands to bake in the (static) environment variables you need\n- `docker build` your custom container (passing in the base image URI as a `--build-arg`), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669736547974,
        "Solution_link_count":5.0,
        "Solution_readability":13.1,
        "Solution_reading_time":22.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":211.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":27.9929452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Challenge_closed_time":1653620941596,
        "Challenge_comment_count":2,
        "Challenge_created_time":1653520166993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the VertexAI class AutoMLtabularTrainingJob, specifically with two parameters listed in the documentation: column_specs and export_evaluated_data_items. The user has created a function to create a prediction model, but the class is not accepting these parameters, resulting in a TypeError. The user is questioning whether the documentation is updated and accurate.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72385022",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":23.0,
        "Challenge_reading_time":39.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.9929452778,
        "Challenge_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1563027261236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":315.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":11.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":78.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1870847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used to use Azure Machine Learning Studio (classic).  <br \/>\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.  <br \/>\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).  <br \/>\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.  <\/p>\n<p>How can I process at the same speed as classic?<\/p>",
        "Challenge_closed_time":1635458665072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635432791567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing a significant difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic). The virtual machine size is Standard_DS3_v2, and steps that have been changed even slightly take 20 times longer to process than in classic. The user is seeking advice on how to process at the same speed as classic.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.1870847222,
        "Challenge_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\">Optimize Data Processing<\/a>. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/170450\/\">(1)<\/a>, <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html\">(2)<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":188.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1527682322812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":31.9027575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Challenge_closed_time":1587720289803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587603987047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to log a SageMaker DeepAR model with MLFlow for version tracking, but the mlflow.sagemaker module does not have a \"log_model\" function. The user tried to use the mlflow.pyfunc module, but it did not work. The user is seeking help to log the SageMaker model and generate cloudpickle and yaml files with MLFlow.",
        "Challenge_last_edit_time":1587605439876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61377643",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":32.3063211111,
        "Challenge_title":"Tracking SageMaker Estimator with MLFlow",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548023586667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":63.8910877778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Related: <a href=\"https:\/\/community.wandb.ai\/t\/unable-to-manage-columns-in-project-run-table\/3551\/4\" class=\"inline-onebox\">Unable to manage columns in project run table - #4 by artsiom<\/a><\/p>\n<p>I was unable to make step metric columns visible in the Table view. I tried logging metrics both via <code>run.log<\/code> and <code>wandb.log<\/code>, as well as refreshing the page in my browser. When attempting to drag and drop a column name from \u201cHidden Columns\u201d to \u201cVisible Columns\u201d (see the screenshot), a gap is created, but on mouse release the column name returns to \u201cHidden Columns\u201d. Clicking on column names to move them to \u201cVisible\u201d does not work either. The logged values appear in the web interface elsewhere. Manipulation with non-metric columns (e.g. config values, name, state etc) worked flawlessly as expected.<\/p>\n<p>The problem remained <em>for a fraction of a minute<\/em> after I logged a summary metric using <code>wandb.summary[...] = ...<\/code>. In particular, I tried moving all columns by pressing \u201cShow all\u201d, but without any visible result, and I closed the pop-up (on the screenshot). Suddenly, after 10 or so seconds, all columns became visible.<\/p>\n<p>The problem is similar to the one in the linked post. Unlike there, in my case, refreshing the web-page did not seem to help. I\u2019ll take a wild guess and suggest possible reasons for the bug:<\/p>\n<ol>\n<li>Something was going on in your back-end, and I had to wait till all necessary data validation or calculations are completed that would enable adding metric columns. This is unacceptably long time (several minutes), within which I was able to read relevant reference, search issues, and do a couple of empty test runs to see what\u2019s going on.<\/li>\n<li>There is a bug which prevents conversion step metrics to summary metrics unless at least one summary metric is explicitly added via <code>wandb.summary<\/code>.<\/li>\n<\/ol>\n<p>I hope you will be able to get to the bottom of it and fix it.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" data-download-href=\"\/uploads\/short-url\/6GUslld1E38x1uAv9m6acBIMSwH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" alt=\"image\" data-base62-sha1=\"6GUslld1E38x1uAv9m6acBIMSwH\" width=\"518\" height=\"500\" data-dominant-color=\"F7F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">667\u00d7643 8.92 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676062281055,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675832273139,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue where they were unable to add step metric columns to the Table view in the web interface. They tried logging metrics using both `run.log` and `wandb.log`, but were unable to move the column names from \"Hidden Columns\" to \"Visible Columns\". The problem persisted for a short time even after logging a summary metric using `wandb.summary[...] = ...`. The user suggests two possible reasons for the bug: either something was going on in the back-end that required several minutes to complete, or there is a bug preventing the conversion of step metrics to summary metrics unless at least one summary metric is explicitly added via `wandb.summary`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/could-not-add-summary-columns-for-display-in-table\/3841",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":38.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":63.8910877778,
        "Challenge_title":"Could not add summary columns for display in Table",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":348,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a> , I\u2019ve been able to to consistently  reproduce this behavior on my end and flagged it as a bug. I will update you on a timeline for a fix once I have additional info. Thanks again for the insight!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.07,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1043.1541666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Challenge_closed_time":1654689077000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1650933722000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where wandb is not compatible with PL 1.6.1 while using Hyperparameter Search, resulting in a FileNotFoundError. The error occurs while trying to access the debug-internal.log file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.0,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1043.1541666667,
        "Challenge_title":"Wandb is not compatible with PL 1.6.1",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I have met the same problem. > I have met the same problem.\r\n\r\nInstall PL=1.5.10 for me it's working with 1.6.3 \r\nonly update wandb 0.12.16",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.62,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.8982075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to perform the random sampling to accomplish the hyper parameter tuning and tuning parameter version 1 (v1). I would like to get the chance to define the algorithm as sampling algorithm explicitly.<\/p>\n<p>Currently using the below code block and is there any chance of implementing explicitly defining sampling in V1? If not, any specific procedure to solve the issue is helpful.<\/p>\n<pre><code>from azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import normal, uniform, choice\nparam_sampling = RandomParameterSampling( {\n        &quot;learning_rate&quot;: normal(10, 3),\n        &quot;keep_probability&quot;: uniform(0.05, 0.1),\n        &quot;batch_size&quot;: choice(16, 32, 64, 128)\n    }\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1659100027590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659096794043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to define a sampling algorithm explicitly for hyper parameter tuning using random sampling in version V1. They have provided a code block and are seeking a solution to the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73166561",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.8982075,
        "Challenge_title":"Error while defining sampling algorithm in hyper parameter tuning using random sampling - Version V1",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651094469216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>There is an explicit procedure called a <strong>sweep job<\/strong>. This sweep job in <strong>hyperparameter value<\/strong>. We can mention the random sampling using the sweep job explicitly.<\/p>\n<p>From azure.ai.ml.sweep import Normal, Uniform, RandomParameterSampling<\/p>\n<pre><code>Command_job_for_sweep = command_job(\n    learning_rate = Normal(mu=value, sigma=value),\n    keep_probability=Uniform(min_value= your value, max_value= value),\n    batch_size = Choice(value=[.your values in list]),\n)\n\nSweep_job = command_job_sweep.sweep(\n    Computer =\u201dcluster\u201d,\n    sampling_algorithm=\u201drandom\u201d,\n    ....\n)\n<\/code><\/pre>\n<p>This will be available in <strong>version 2 (v2)<\/strong> of hyperparameter tuning in random sampling.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":9.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0222222222,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi everyone. Is there a way to save display preferences in the job runs search UI? My main interest is in saving (1) which columns are displayed, and (2) the order in which they are displayed. Thank you.",
        "Challenge_closed_time":1650636762000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650636682000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to save display preferences in the job runs search UI, specifically which columns are displayed and the order in which they are displayed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1499",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":4.2,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0222222222,
        "Challenge_title":"How to persist custom job runs table",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There\u2019s a save button next to the query search, it persist everything configured in the dashboard: https:\/\/polyaxon.com\/docs\/management\/organizations\/searches\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":2.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.8680555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can I train models in parallel? Is is possible to train model in parallel on like hyperdrive?<\/p>",
        "Challenge_closed_time":1653922130807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653904605807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training models in parallel, similar to using hyperdrive.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869619\/parallel-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.8680555556,
        "Challenge_title":"Parallel training",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3d6a9d61-6cf9-45d8-870d-2fbbf147f56d\">@Chungsun  <\/a>  Thanks for the question. The max number of parallel tasks is limited by number of cores in the cluster (excluding master node).    <br \/>\nThe demand for parallelism comes from two sources: 1. The cross validation which address multiple combination of train-val datasets &amp; parameters 2. The training algorithm itself which can be parallelized.    <\/p>\n<p>\u2022\tYou can run multiple runs in a distributed fashion across AML clusters, meaning that each cluster node can be running a run in parallel to other nodes running other runs. For instance, that\u2019s what we also do with Pipeline steps, HyperParameter Tunning child runs and for Azure AutoML child runs.    <\/p>\n<p> <a href=\"https:\/\/github.com\/microsoft\/solution-accelerator-many-models\"> https:\/\/aka.ms\/many-models<\/a> is a solution accelerator that will help you walk through to run many models.     <br \/>\nIn the HyperDriveConfig there is AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run. So there would be 1 execution per node.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":80.3919036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a model quality monitor job, using the class ModelQualityMonitor from Sagemaker model_monitor, and i think i have all the import statements defined yet i get the message cannot import name error<\/p>\n<pre><code>from sagemaker import get_execution_role, session, Session\nfrom sagemaker.model_monitor import ModelQualityMonitor\n                \nrole = get_execution_role()\nsession = Session()\n\nmodel_quality_monitor = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=session\n)\n<\/code><\/pre>\n<p>Any pointers are appreciated<\/p>",
        "Challenge_closed_time":1608016613856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607727203003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to create a model quality monitor job using the class ModelQualityMonitor from Sagemaker model_monitor. The user has imported all the necessary statements, but is receiving an ImportError message stating that the name 'ModelQualityMonitor' cannot be imported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65259702",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":80.3919036111,
        "Challenge_title":"AWS sagemaker model monitor- ImportError: cannot import name 'ModelQualityMonitor'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":544.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546959992036,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3<\/code> Amazon SageMaker notebook, I don't get any errors at all.<\/p>\n<p>Example screenshot output showing no errors:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined<\/code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker<\/code> and then see if this resolves your error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424791933163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2483.0,
        "Answerer_view_count":192.0,
        "Challenge_adjusted_solved_time":1.1469897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to optimize a custom model (no fancy ML whatsoever) that has 13 parameters, 12 of which I know to be normally distributed. I've gotten decent results using the <code>hyperopt<\/code> library:<\/p>\n<pre><code>space = {\n    'B1': hp.normal('B1', B1['mean'], B1['std']),\n    'B2': hp.normal('B2', B2['mean'], B2['std']),\n    'C1': hp.normal('C1', C1['mean'], C1['std']),\n    'C2': hp.normal('C2', C2['mean'], C2['std']),\n    'D1': hp.normal('D1', D1['mean'], D1['std']),\n    'D2': hp.normal('D2', D2['mean'], D2['std']),\n    'E1': hp.normal('E1', E1['mean'], E1['std']),\n    'E2': hp.normal('E2', E2['mean'], E2['std']),\n    'F1': hp.normal('F1', F1['mean'], F1['std']),\n    'F2': hp.normal('F2', F2['mean'], F2['std'])\n}\n<\/code><\/pre>\n<p>where I can specify the shape of the search space per parameter to be normally distributed.<\/p>\n<p>I've got 32 cores and the default <code>Trials()<\/code> object only uses one of them.  <code>Hyperopt<\/code> suggests two ways to parallelize the search process, both of which I could not get to work on my windows machine for the life of me, so I've given up and want to try a different framework.<\/p>\n<p>Even though Bayesian Hyper Parameter Optimization as far as I know is based on the idea that values are distributed according to a distribution, and the normal distribution is so prevalent that it is literally called normal. I cannot find a way to specify to <code>Optuna<\/code> that my parameters have a <code>mean<\/code> and a <code>standard deviation<\/code>.<\/p>\n<p>How do i tell <code>Optuna<\/code> the <code>mean<\/code> and <code>standard deviation<\/code> of my parameters?<\/p>\n<p>The only distributions I can find in the documentation are: <code>suggest_uniform()<\/code>, <code>suggest_loguniform()<\/code> and <code>suggest_discrete_uniform()<\/code>.<\/p>\n<p>Please tell me if I am somehow misunderstanding loguniform distrubution (It looks somewhat similar, but I can't specify a standard deviation?) or the pruning process.<\/p>\n<p>As you might be able to tell from my text, I've spent a frustrating amount of time trying to figure this out and gotten exactly nowhere, any help will be highly appreciated!<\/p>\n<p>Special thanks to dankal444 for this elegant solution (i will replace the mean and std with my own values):<\/p>\n<pre><code>from scipy.special import erfinv\nspace = {\n    'B1': (erfinv(trial.suggest_float('B1', -1, 1))-mean)*std,\n    'B2': ...\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1636670797220,
        "Challenge_comment_count":2,
        "Challenge_created_time":1636666668057,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to optimize a custom model with 13 parameters, 12 of which are normally distributed. They have used the hyperopt library to specify the shape of the search space per parameter to be normally distributed. However, they could not get the parallelization process to work and want to try a different framework, Optuna. The user is struggling to find a way to specify the mean and standard deviation of their parameters in Optuna, as the only distributions available in the documentation are suggest_uniform(), suggest_loguniform(), and suggest_discrete_uniform(). They are seeking help to understand if they are misunderstanding the loguniform distribution or the pruning process.",
        "Challenge_last_edit_time":1636679968800,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69935219",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":31.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":1.1469897222,
        "Challenge_title":"How to search a set of normally distributed parameters using optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":322,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349369044408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":380.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>You can cheat <code>optuna<\/code> by using uniform distribution and transforming it into normal distribution. To do that one of the method is <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.erfinv.html\" rel=\"nofollow noreferrer\">inversed error function<\/a> implemented in <code>scipy<\/code>.<\/p>\n<p>Function takes uniform distribution from in range &lt;-1, 1&gt; and converts it to standard normal distribution<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import special\n\n\nx = np.linspace(-1, 1)\nplt.plot(x, special.erfinv(x))\nplt.xlabel('$x$')\nplt.ylabel('$erf(x)$')\n\nmean = 2\nstd = 3\nrandom_uniform_data = np.random.uniform(-1 + 0.00001, 1-0.00001, 1000)\nrandom_gaussianized_data = (special.erfinv(random_uniform_data) - mean) * std\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\naxes[0].hist(random_uniform_data, 30)\naxes[1].hist(random_gaussianized_data, 30)\naxes[0].set_title('uniform distribution samples')\naxes[1].set_title('erfinv(uniform distribution samples)')\nplt.show()\n<\/code><\/pre>\n<p>This is how the function looks like:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/E363H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E363H.png\" alt=\"inverse error function\" \/><\/a><\/p>\n<p>And below example of transforming of uniform distribution into normal with custom mean and standard deviation (see code above)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rspUX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rspUX.png\" alt=\"transforming uniform to normal distribution\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.7,
        "Solution_reading_time":20.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":143.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1586647457000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a warning message related to the use of the deprecated Comet API logger, comet_ml.papi, instead of the newer comet_ml.api. The warning suggests using the updated API and provides a link for more information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":88.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1587507179987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":0.0376611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Challenge_closed_time":1601906782043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601650547150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user successfully loaded data, trained and deployed a model using AWS SageMaker, and is now trying to connect it to AWS Lambda. The user added data to the IAM policy JSON to give Lambda IAM execution role permission to invoke a model endpoint, but the Lambda function is unable to see the endpoint even with the permission. The error message indicates that the endpoint is not found.",
        "Challenge_last_edit_time":1601906646463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":71.1763591667,
        "Challenge_title":"AWS Sagemaker + AWS Lambda",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":623.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587507179987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2725,
        "Challenge_answer_count":0,
        "Challenge_body":"If an ERT subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. This will then lead to a \"successful\" run in mlflow, whereas it should be registered as a failed run.",
        "Challenge_closed_time":1606475795000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606471214000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where an invalid metric is being logged, which is breaking the Mlflow UI. They are trying to create a custom metric indicator, but it is not being displayed when they use the Mlflow UI. They are using Kedro and Kedro-mlflow version 0.10.0, Python version 3.9.0, and operating system Windows 10.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/equinor\/flownet\/issues\/269",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":28.0,
        "Challenge_repo_issue_count":455.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.2725,
        "Challenge_title":"Failed ERT runs are not registered correctly in mlflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.2721991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am considering using Personalizer for project and have found limited third party metrics for this service. This one article indicates needing TENS of thousands of hits to get good results.  <\/p>\n<p><a href=\"https:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e\">https:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e<\/a>  <\/p>\n<p>Can any one provide any other data?   <\/p>\n<p>Obviously, over time it will get better, but does it have to get to 10K+ to get good?<\/p>",
        "Challenge_closed_time":1606893531207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606863751290,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is considering using Personalizer for a project but is facing limited third-party metrics for the service. They have come across an article that suggests needing tens of thousands of hits to get good results. The user is seeking additional data and wondering if it is necessary to reach 10K+ hits to achieve good results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/182318\/training-personalizer",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.2721991667,
        "Challenge_title":"Training Personalizer",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=bac6bba0-0ba2-419e-b1dd-254f191be319\">@Gregorio Rojas  <\/a> The minimum requirements to have an effective recommendation is to have a minimum of ~1k\/day content-related events. Higher rate of events do help you to provide faster and better recommendations. All the requirements are documented in the official documentation <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/cognitive-services\/personalizer\/what-is-personalizer#content-requirements\">page<\/a> of the service. The samples <a href=\"https:\/\/github.com\/Azure-Samples\/cognitive-services-personalizer-samples\">repo<\/a> provides some data along with the quickstart's from the documentation to get started. The service now provides an E0 tier or apprentice mode that helps you test the service and gain confidence to move to a higher tier with production level recommendations.     <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":11.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":95.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.4847938889,
        "Challenge_answer_count":1,
        "Challenge_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Challenge_closed_time":1649246917948,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649205572690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure the ideal value for MaxConcurrentTransforms in setting up a SageMaker batch transform. The documentation suggests that the ideal value is equal to the number of compute workers in the batch transform job. However, the user is unsure how to determine the number of compute workers, which may depend on the instance type, and whether the instance count parameter needs to be taken into account.",
        "Challenge_last_edit_time":1668580983582,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-maxconcurrenttransforms-in-setting-up-a-sagemaker-batch-transform",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":6.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.4847938889,
        "Challenge_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":368.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model. \n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the **memory** utilisation by your model. \n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1649246917948,
        "Solution_link_count":0.0,
        "Solution_readability":15.7,
        "Solution_reading_time":6.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1550713581256,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santiago, Chile",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":72.0,
        "Challenge_adjusted_solved_time":0.7812944445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before I realized that changes do not get applied unless you also delete the corresponding Model and Endpoint configuration so that new ones can be created with that name. <\/p>\n\n<p>Is there a way with the sagemaker python api to delete all three instead of just the endpoint?<\/p>",
        "Challenge_closed_time":1550714082127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550711269467,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while deleting and recreating endpoints with the same name using SageMaker python API. They realized that changes do not get applied unless they also delete the corresponding Model and Endpoint configuration. The user is seeking a way to delete all three using the SageMaker python API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54797698",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.7812944445,
        "Challenge_title":"SageMaker delete Models and Endpoint configurations with python API",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4760.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/647\" rel=\"nofollow noreferrer\" title=\"sagemaker-python-sdk\/pull\/647\">this<\/a> pull request. <\/p>\n\n<p>For the time being Amazon's only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\" rel=\"nofollow noreferrer\" title=\"docs.aws.amazon.com\/sagemaker\">recommendation<\/a> is to delete everything via the console. <\/p>\n\n<p>If this is critical to your system you can probably manage everything via Cloud Formation and create\/delete services containing your Sagemaker models and endpoints.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":8.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443482042487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":1995.0,
        "Answerer_view_count":150.0,
        "Challenge_adjusted_solved_time":56.7013711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using an Amazon SageMaker Notebook that has 72 cores and 144 GB RAM, and I carried out 2 tests with a sample of the whole data to check if the Dask cluster was working.<\/p>\n\n<p>The sample has 4500 rows and 735 columns from 5 different \"assets\" (I mean 147 columns for each asset). The code is filtering the columns and creating a feature matrix for each filtered Dataframe.<\/p>\n\n<p>First, I initialized the cluster as follows, I received 72 workers, and got 17 minutes of running. (I assume I created 72 workers with one core each.)<\/p>\n\n<pre><code>    from dask.distributed import Client, LocalCluster\n    cluster = LocalCluster(processes=True,n_workers=72,threads_per_worker=72)\n\n    def main():\n      import featuretools as ft\n      list_columns = list(df_concat_02.columns)\n\n      list_df_features=[]\n      from tqdm.notebook import tqdm\n\n      for asset in tqdm(list_columns,total=len(list_columns)):\n        dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n        es = ft.EntitySet()  \n        es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                      index = 'index', \n                                      time_index = 'Date')\n        fm, features = ft.dfs(entityset=es, \n                              target_entity='MARKET',\n                              trans_primitives = ['divide_numeric'],\n                              agg_primitives = [],\n                              max_depth=1,\n                              verbose=True,\n                              dask_kwargs={'cluster': client.scheduler.address}\n\n                              )\n        list_df_features.append(fm)\n      return list_df_features\n\n    if __name__ == \"__main__\":\n        list_df = main()\n<\/code><\/pre>\n\n<p>Second, I initialized the cluster as follows, I received 9 workers, and got 3,5 minutes of running. (I assume I created 9 workers with 8 cores each.)<\/p>\n\n<pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(processes=True)\n\ndef main():\n  import featuretools as ft\n  list_columns = list(df_concat_02.columns)\n\n  list_df_features=[]\n  from tqdm.notebook import tqdm\n\n  for asset in tqdm(list_columns,total=len(list_columns)):\n    dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n    es = ft.EntitySet()  \n    es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                  index = 'index', \n                                  time_index = 'Date')\n    fm, features = ft.dfs(entityset=es, \n                          target_entity='MARKET',\n                          trans_primitives = ['divide_numeric'],\n                          agg_primitives = [],\n                          max_depth=1,\n                          verbose=True,\n                          dask_kwargs={'cluster': client.scheduler.address}\n\n                          )\n    list_df_features.append(fm)\n  return list_df_features\n\nif __name__ == \"__main__\":\n    list_df = main()\n<\/code><\/pre>\n\n<p>For me, it's mind-blowing because I thought that 72 workers could carry the work out faster! Once I'm not a specialist neither in Dask nor in FeatureTools I guess that I'm setting something wrong.<\/p>\n\n<p>I would appreciate any kind of help and advice!<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1583793190592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583540635953,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Featuretools slowing down when increasing the number of Dask workers. They conducted two tests with a sample of the whole data and found that initializing the cluster with 9 workers with 8 cores each resulted in faster running time (3.5 minutes) compared to initializing the cluster with 72 workers with one core each (17 minutes). The user is seeking advice on how to optimize the process.",
        "Challenge_last_edit_time":1583589065656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60573260",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":34.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":70.1540663889,
        "Challenge_title":"Why does Featuretools slows down when I increase the number of Dask workers?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":297,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>You are correctly setting <code>dask_kwargs<\/code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":4.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":4.4438166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Challenge_closed_time":1660907847087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660891849347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble understanding the meaning of \"config=wandb.config\" in the tutorials of wandb while setting up a sweep for their logistic regression model. They have provided the code they have used for the sweep and training, and are seeking an explanation of the steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":33.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.4438166667,
        "Challenge_title":"What is the meaning of 'config = wandb.config'?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":209,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557474244067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":626.0,
        "Poster_view_count":140.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":28.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":33.0,
        "Solution_word_count":255.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1624867238152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":94.6778030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For big ML models with many parameters, it is helpful if one can interrupt and resume the hyperparameter optimization search.\nOptuna allows doing that with the RDB backend, which stores the study in a SQlite database (<a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py<\/a>).<\/p>\n<p>However, when interrupting and resuming a study, the results are not the same as that of an uninterrupted study.<\/p>\n<p><strong>Expect<\/strong>: For a fixed seed, the results from an optimization run with <code>n_trials = x<\/code> are identical to a study with <code>n_trials = x\/5<\/code>, that is resumed 5 times and a study, that is interrupted with <code>KeyboardInterrupt<\/code> 5 times and resumed 5 times until <code>n_trials = x<\/code>.<\/p>\n<p><strong>Actual<\/strong>: The results are equal up to the point of the first interruption. From then on, they differ.<\/p>\n<p>The <a href=\"https:\/\/i.stack.imgur.com\/ijIzl.png\" rel=\"nofollow noreferrer\">figures<\/a> show the optimization history of all trials in a study. The left-most figure (A) shows the uninterrupted run, the center one shows a run interrupted by keyboard (B), the right-most figure shows the run interrupted by <code>n_iter<\/code> (C). In B and C, the red dotted line shows the point where the first study was first interrupted. Left of the line, the results are equal to the uninterrupted study, to the right they differ.<\/p>\n<p>Is it possible to interrupt and resume a study, so that another study with the same seed that has not been interrupted generates exactly the same result?\n(Obviously assuming that the objective function behaves in a non-deterministic way.)<\/p>\n<p>Minimal working example to reproduce:<\/p>\n<pre><code>import optuna\nimport logging\nimport sys\nimport numpy as np\n\ndef objective(trial):\n    x = trial.suggest_float(&quot;x&quot;, -10, 10)\n    return (x - 4) ** 2\n\ndef set_study(db_name, \n                study_name, \n                seed, \n                direction=&quot;minimize&quot;):\n    '''\n    Creates a new study in a sqlite database located in results\/ .\n    The study can be resumed after keyboard interrupt by simple creating it\n    using the same command used for the initial creation.\n    '''\n\n    # Add stream handler of stdout to show the messages\n    optuna.logging.get_logger(&quot;optuna&quot;).addHandler(logging.StreamHandler(sys.stdout))\n    sampler = optuna.samplers.TPESampler(seed = seed, n_startup_trials = 0)\n    storage_name = f&quot;sqlite:\/\/\/{db_name}.db&quot;\n    storage = optuna.storages.RDBStorage(storage_name, heartbeat_interval=1)\n\n    study = optuna.create_study(storage=storage, \n                                study_name=study_name, \n                                sampler=sampler, \n                                direction=direction, \n                                load_if_exists=True)\n    return study\n\n\nstudy = set_study('optuna_test', 'optuna_test_study', 1)\n\ntry:\n    # Press CTRL+C to stop the optimization.\n    study.optimize(objective, n_trials=100)  \nexcept KeyboardInterrupt:\n    pass\n\n\ndf = study.trials_dataframe(attrs=(&quot;number&quot;, &quot;value&quot;, &quot;params&quot;, &quot;state&quot;))\n\nprint(df)\n\nprint(&quot;Best params: &quot;, study.best_params)\nprint(&quot;Best value: &quot;, study.best_value)\nprint(&quot;Best Trial: &quot;, study.best_trial)\n# print(&quot;Trials: &quot;, study.trials)\n\n\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n<\/code><\/pre>",
        "Challenge_closed_time":1661932276243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660225615907,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Optuna hyperparameter search not being reproducible with interrupted\/resumed studies. Although Optuna allows interrupting and resuming the hyperparameter optimization search with the RDB backend, the results are not the same as that of an uninterrupted study. The results are equal up to the point of the first interruption, but from then on, they differ. The user is looking for a solution to interrupt and resume a study so that another study with the same seed that has not been interrupted generates exactly the same result.",
        "Challenge_last_edit_time":1661591436152,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73321808",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":44.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":474.0723155556,
        "Challenge_title":"Optuna hyperparameter search not reproducible with interrupted \/ resumed studies",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":376,
        "Platform":"Stack Overflow",
        "Poster_created_time":1624867238152,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Found whats causing the problem: The random number generator in the sampler is initialized using the seed, but of course it returns a different number if the study is interrupted and resumed (it is then reinitialised)\nThis is especially bad using random search with fixed seed, as the search then basically starts from new.<\/p>\n<p>If one really needs reproducible runs, one can simply extract the rng into a binary file after a run or keyboard interrupt, and resume by overwriting the newly generated rng of the sampler with the saved one.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.9,
        "Solution_reading_time":6.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":91.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1549041651583,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":3106.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":52.7292055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#workspace-level-quota\" rel=\"nofollow noreferrer\">link<\/a>), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?<\/p>",
        "Challenge_closed_time":1597341970283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597248542537,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to access compute quotas for each Azure ML workspace in each Azure location for their organization's Azure subscription. They are looking for a way to do this programmatically using the Azure CLI or Python SDK, as it would be more efficient than doing it manually through the Azure Portal.",
        "Challenge_last_edit_time":1597249191852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63380531",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":25.9521516667,
        "Challenge_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369863777596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation_count":335.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.<\/p>\n<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest<\/code> command in the CLI.<\/p>\n<p>There are a few commands that may interest you:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspacesandcomputes\/usages\/list\" rel=\"nofollow noreferrer\">Usage<\/a> and Quotas for a region:\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/usages?api-version=2019-05-01<\/code>\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/quotas?api-version=2020-04-01<\/code><\/p>\n<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1597439016992,
        "Solution_link_count":3.0,
        "Solution_readability":18.8,
        "Solution_reading_time":20.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":131.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.6325675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?<\/p>",
        "Challenge_closed_time":1607117314596,
        "Challenge_comment_count":1,
        "Challenge_created_time":1607093437353,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether AutoML can optimize convolutional neural networks based on the number of layers and pool layer parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.4,
        "Challenge_reading_time":2.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.6325675,
        "Challenge_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":31,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.213155,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Within my optuna study, I want that each trial is separately logged by wandb. Currently, the study is run and the end result is tracked in my wandb dashboard. Instead of showing each trial run separately, the end result over all epochs is shown. So, wandb makes one run out of multiple runs.<\/p>\n<p>I found the following <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/_modules\/optuna\/integration\/wandb.html\" rel=\"noopener nofollow ugc\">docs<\/a> in optuna:<\/p>\n<pre><code>Weights &amp; Biases logging in multirun mode.\n\n    .. code::\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">            import optuna\n            from optuna.integration.wandb import WeightsAndBiasesCallback\n\n            wandb_kwargs = {\"project\": \"my-project\"}\n            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n\n            @wandbc.track_in_wandb()\n            def objective(trial):\n                x = trial.suggest_float(\"x\", -10, 10)\n                return (x - 2) ** 2\n\n\n            study = optuna.create_study()\n            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n\n<\/code><\/pre>\n<p>I implemented this line of code yet it produces the following error:<\/p>\n<p><code>ConfigError: Attempted to change value of key \"learning_rate\" from 5e-05 to     0.0005657929921495451 If you really want to do this, pass allow_val_change=True to config.update()    wandb: Waiting for W&amp;B process to finish... (failed 1).<\/code><\/p>\n<p>Did anyone succeed in implementing logging per trial in a multi-trial study?<\/p>",
        "Challenge_closed_time":1679650682812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679646315454,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to enable logging of each trial separately in their optuna study using wandb. However, currently, wandb is tracking the end result over all epochs instead of showing each trial run separately. The user tried implementing a line of code from the optuna documentation but encountered an error. They are seeking help to successfully implement logging per trial in a multi-trial study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-enable-logging-of-each-trial-separately\/4115",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.213155,
        "Challenge_title":"How to enable logging of each trial separately?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":167,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I actually solved it now:<br>\nIt seems that the optimizer that i used caused errors in the generation of a value for the learning rate when starting a new trial. Once I took the optimizer back out, the follwing implementation worked and generated separate logs in my wandb dashboard:<\/p>\n<pre><code class=\"lang-auto\">wandb_kwargs = {\"project\": \"my-project\"}\nwandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n@wandbc.track_in_wandb()\ndef objective(trial):\n    \n    training_args = Seq2SeqTrainingArguments( \n        \"tuning\", \n        num_train_epochs=1,            \n        # num_train_epochs = trial.suggest_categorical('num_epochs', [3, 5, 8]),\n        per_device_eval_batch_size=3, \n        per_device_train_batch_size=3, \n        learning_rate=  trial.suggest_float('learning_rate', low=0.00004, high=0.0001, step=0.0005, log=False),             \n        # per_device_train_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),       \n        # per_device_eval_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),  \n        disable_tqdm=True, \n        predict_with_generate=True,\n        gradient_accumulation_steps=4,\n        # gradient_checkpointing=True,\n        # weight_decay= False\n        seed = 12, \n        warmup_steps=5,\n        # evaluation and logging\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        save_total_limit=1,\n        logging_strategy=\"epoch\",\n        logging_steps = 1, \n        load_best_model_at_end=True,\n        metric_for_best_model = \"eval_loss\",\n        # use_cache=False,\n        push_to_hub=False,\n        fp16=False,\n        remove_unused_columns=True\n    )\n    # optimizer = Adafactor(\n    #     t5dmodel.parameters(),\n    #     lr=trial.suggest_float('learning_rate', low=4e-5, high=0.0001),  #   ('learning_rate', 1e-6, 1e-3),\n    #     # weight_decay=trial.suggest_float('weight_decay', WD_MIN, WD_CEIL),   \n    #     # lr=1e-3,\n    #     eps=(1e-30, 1e-3),\n    #     clip_threshold=1.0,\n    #     decay_rate=-0.8,\n    #     beta1=None,\n    #     # weight_decay= False\n    #     weight_decay=0.1,\n    #     relative_step=False,\n    #     scale_parameter=False,\n    #     warmup_init=False,\n    # )\n    \n    # lr_scheduler = AdafactorSchedule(optimizer)\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=t5dmodel)\n    trainer = Seq2SeqTrainer(model=t5dmodel,\n                            args=training_args,\n                            train_dataset=tokenized_train_dataset['train'],\n                            eval_dataset=tokenized_val_dataset['validation'],\n                            data_collator=data_collator,\n                            tokenizer=tokenizer,\n                           #  optimizers=(optimizer, lr_scheduler)\n                            )       \n    \n    trainer.train()\n    scores = trainer.evaluate() \n    return scores['eval_loss']\n\nif __name__ == '__main__':\n    t5dmodel = AutoModelForSeq2SeqLM.from_pretrained(\"yhavinga\/t5-base-dutch\",  use_cache=False) \n    tokenizer = AutoTokenizer.from_pretrained(\"yhavinga\/t5-base-dutch\", additional_special_tokens=None)\n    \n    features = {\n    'WordRatioFeature': {'target_ratio': 0.8},\n    'CharRatioFeature': {'target_ratio': 0.8},\n    'LevenshteinRatioFeature': {'target_ratio': 0.8},\n    'WordRankRatioFeature': {'target_ratio': 0.8},\n    'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n    }\n    \n    trainset_processed = get_train_data(WIKILARGE_PROCESSED, 0, 10)  \n    print(trainset_processed)\n    valset_processed = get_validation_data(WIKILARGE_PROCESSED, 0,7)\n    print(valset_processed)\n    tokenized_train_dataset = trainset_processed.map((tokenize_train), batched=True, batch_size=1)\n    tokenized_val_dataset =  valset_processed.map((tokenize_train), batched=True, batch_size=1)   \n    print('Triggering Optuna study')\n    study = optuna.create_study( direction='minimize', pruner=optuna.pruners.MedianPruner()) \n    study.optimize(objective, n_trials=4,callbacks=[wandbc],  gc_after_trial=True)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.3,
        "Solution_reading_time":45.02,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":211.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":6.8807786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Challenge_closed_time":1629122761020,
        "Challenge_comment_count":1,
        "Challenge_created_time":1629097990217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to display configurable hyperparameters such as epochs and learning rate in the UI configuration panel of ClearML, allowing for easy modification of these values within the UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":2.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.8807786111,
        "Challenge_title":"ClearML How to get configurable hyperparameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616008398583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Islamabad, Pakistan",
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.4,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":113.09082,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Dears, \n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : \nError : \n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n-------------------\nCode:\nfrom sagemaker.tuner import (\n    IntegerParameter,\n    CategoricalParameter,\n    ContinuousParameter,\n    HyperparameterTuner,\n)\nfrom sagemaker import image_uris\n\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    use_spot_instances=True,  # use spot instances\n    max_run=1800,  # max training time in seconds\n    max_wait=1800,  # seconds to wait for spot instance\n    output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path),\n    sagemaker_session=sess,\n)\nfreq = \"D\"\ncontext_length = 300\n\ndeepar.set_hyperparameters(\n    time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length)\n)\n\nCan you please help in solving the error? \nI have to do that in af-south-1 region. \n\nThanks \nBasem\n\nhyperparameter_ranges = {\n    \"mini_batch_size\": IntegerParameter(100, 400),\n    \"epochs\": IntegerParameter(200, 400),\n    \"num_cells\": IntegerParameter(30, 100),\n    \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]),\n    \"learning_rate\": ContinuousParameter(0.0001, 0.1),\n}\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner(\n    deepar,\n    objective_metric_name,\n    hyperparameter_ranges,\n    max_jobs=10,\n    strategy=\"Bayesian\",\n    objective_type=\"Minimize\",\n    max_parallel_jobs=10,\n    early_stopping_type=\"Auto\",\n)\n\ns3_input_train = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\"\n)\ns3_input_test = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\"\n)\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False)\ntuner.wait()",
        "Challenge_closed_time":1653062956983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652655830031,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a \"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region\" error while building an ML model using DeepAR Algorithm in the af-south-1 region. The error occurred when the user tried to create a HyperparameterTuner.",
        "Challenge_last_edit_time":1668587554704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/clienterror-an-error-occurred-unknownoperationexception-when-calling-the-createhyperparametertuningjob-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":113.09082,
        "Challenge_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The error message indicates that `CreateHyperParameterTuningJob` operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653062956983,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":5.0310547222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When performing a single-objective optimization with <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Optuna<\/a>, the best parameters of the study are accessible using:<\/p>\n<pre><code>import optuna\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    return (x - 2) ** 2\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2.002108042}\n<\/code><\/pre>\n<p>If I want to perform a multi-objective optimization, this would be become for example :<\/p>\n<pre><code>import optuna\ndef multi_objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    f1 = (x - 2) ** 2\n    f2 = -f1\n    return f1, f2\n\nstudy = optuna.create_study(directions=['minimize', 'maximize'])\nstudy.optimize(multi_objective, n_trials=100)\n<\/code><\/pre>\n<p>This works, but the command <code>study.best_params<\/code> fails with <code>RuntimeError: The best trial of a 'study' is only supported for single-objective optimization.<\/code><\/p>\n<p>How can I get the best parameters for a multi-objective optimization ?<\/p>",
        "Challenge_closed_time":1611273369707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611255257910,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in obtaining the best parameters for a multi-objective optimization using Optuna. While the best parameters can be easily accessed for a single-objective optimization, the command fails for multi-objective optimization, resulting in a runtime error. The user is seeking a solution to obtain the best parameters for multi-objective optimization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65833998",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.0310547222,
        "Challenge_title":"Best parameters of an Optuna multi-objective optimization",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2571.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588846413276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":108.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>In multi-objective optimization, you often end up with more than one best trial, but rather a set of trials. This set if often referred to as the Pareto front. You can get this Pareto front, or the list of trials, via <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.Study.html#optuna.study.Study.best_trials\" rel=\"noreferrer\"><code>study.best_trials<\/code><\/a>, then look at the parameters from each individual trial i.e. <code>study.best_trials[some_index].params<\/code>.<\/p>\n<p>For instance, given your directions of minimizing <code>f1<\/code> and maximizing <code>f2<\/code>, you might end up with a trial that has a small value for <code>f1<\/code> (good) but at the same time small value for <code>f2<\/code> (bad) while another trial might have a large value for both <code>f1<\/code> (bad) and <code>f2<\/code> (good). Both of these trials could be returned from <code>study.best_trials<\/code>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":12.09,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":115.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":100.1738869445,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have an OS stable diffusion fine tuner and use Tensorboard locally and am trying to integrate wandb with existing code that is largely just calling writer.log_scalar(\u2026).  I setup my SummaryWriter then call wandb.init, but I\u2019m having all sorts of odd behavior where most of the time only system monitors (gpu temp, memory etc) are logged to wandb and my calls to writer.log_scalar simply never get recorded to wandb.<\/p>\n<p>Everything seems to be failing silently and I don\u2019t know why nothing gets recorded.  The other day testing on two machines it works from one but not the other, and it is also now working from Colab notebook instances or docker container runs.<\/p>\n<p>The runs on <a href=\"http:\/\/wandb.com\" rel=\"noopener nofollow ugc\">wandb.com<\/a> are there and created, console output shows it fires up and links me to the run and the run URL works, etc.  But, only system monitors are showing up, none of my items logged with summarywriter, at least a vast majority of instances.<\/p>\n<p>At one point it was working fine, then started to stop working.  I had thought it was an issue with trying to pass in a dict of dicts to config={main: args, opt_cfg: optimizer_cfg} but even passing in dummy objects or simply config=args it fails.  At one point wanb.init was done before writer instantiation, and that was fixed, so I\u2019m not sure at what point things went sideways as I mostly run locally but many users use Colab\/Vast, etc and wandb is a significantly better solution for those cases.<\/p>\n<p>Is there any log file or debugging I can use to troubleshoot this?  Unfortunately it is just not working and doing so silently without any feedback.<\/p>",
        "Challenge_closed_time":1679780487814,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679419861821,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while integrating wandb with their existing code that uses SummaryWriter to log scalar values. They are experiencing odd behavior where only system monitors are logged to wandb and their calls to writer.log_scalar are not recorded. The issue is occurring on different machines and environments, and the user is unable to troubleshoot it as it fails silently without any feedback. The user is seeking help to debug the issue and find a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/with-tb-summarywriter-only-getting-sys-logs-no-log-scalar-shows-up\/4089",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":21.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":100.1738869445,
        "Challenge_title":"With TB SummaryWriter only getting sys logs, no log_scalar shows up",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":287,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ah I think I found the magic combination to work for my training script.  For posterity in case anyone else has the issues and stumbles on this post.  This is a raw torch trainer.<\/p>\n<p>(ex log_folder = \u201clogs\/projectname20230325_124523\u201d and contains the events.out.tfevents\u2026 file)<\/p>\n<pre><code class=\"lang-auto\">        wandb.tensorboard.patch(root_logdir=log_folder, pytorch=False, tensorboard_x=False, save=False)\n        wandb_run = wandb.init(\n            project=args.project_name,\n            config={\"main_cfg\": vars(args), \"optimizer_cfg\": optimizer_config},\n            name=args.run_name\n            )\n        log_writer = SummaryWriter(log_dir=log_folder...)\n\n        log_writer.add_scalar(...)\n<\/code><\/pre>\n<p>tensorboard 2.12.0<br>\nwandb 0.14.0<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.09,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605982845000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a warning message when calling \"kedro mlflow init\" which states that the project is not initialized yet and that the command must be called before any other command. However, this warning can be ignored as the command works as intended. The issue is due to the dynamic creation of the command.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1606374398152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":54.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":336.329535,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Am working on a project of consumer behaviour analysis on websites and predict the malicious activity of users in real-time.\nClick data is being collected for each click made by users.<\/p>\n<p>Am using multiple AWS services like kinesis stream, Lambda and sagemaker. I have created an autoencoder model and\ndeployed it as sagemaker endpoint which will be invoked using lambda when it receives new click data from the website through\nKinesis stream.<\/p>\n<p>Since sagemaker endpoint contains the only model but click data which lambda function receives is raw data with URLs, texts and\ndate. How can I pass raw data into required preprocessing steps and send processed data to sagemaker endpoint in the required format?<\/p>\n<p>Example of raw data:-<\/p>\n<p>{'URL':'www.amazon.com.au\/ref=nav_logo', 'Text':'Home', 'Information':'Computers'}<\/p>",
        "Challenge_closed_time":1624254151543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623043365217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a project to analyze consumer behavior on websites and predict malicious activity in real-time. They are using AWS services like Kinesis stream, Lambda, and SageMaker. They have created an autoencoder model and deployed it as a SageMaker endpoint, which will be invoked using Lambda when it receives new click data from the website through Kinesis stream. However, the raw data received by the Lambda function contains URLs, texts, and dates, and the user needs to preprocess this data before sending it to the SageMaker endpoint in the required format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67866286",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":11.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":336.329535,
        "Challenge_title":"Real-time Data Pre-processing in Lambda for SageMaker Endpoint",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":475.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604911047620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Victoria, Australia",
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.<\/p>\n<p>Reference:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a><\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":44.6,
        "Solution_reading_time":14.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":17.8840719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Challenge_closed_time":1572381894847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572325608637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy 50 NLP models on Azure Container Instances via the Azure Machine Learning service and is trying to write a generic score.py entry file to pass in the model name as a parameter. However, the interface method signature does not allow a parameter in the init() method of score.py, so the user moved the model loading into the run method. The user is looking for a way to pass in some value to the init() method of the container to tell it what model to load.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.6350583334,
        "Challenge_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":851.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572389991296,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":58.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":40.0,
        "Solution_word_count":476.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324351066392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Christchurch, New Zealand",
        "Answerer_reputation_count":1306.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":65.1553044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Challenge_closed_time":1618176409272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617943623520,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the Azure AutoML Python SDK to download and save a model, which warns about the version mismatch between the local and remote versions. The user is seeking guidance on how to match the local Azure AutoML Python SDK version to the remote version.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":64.6627088889,
        "Challenge_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324351066392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, New Zealand",
        "Poster_reputation_count":1306.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618178182616,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8082.0786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"It fails at the \"apply patch\" stage",
        "Challenge_closed_time":1624956881000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1595861398000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a warning message in Enchanter v0.7.0 stating that the function `log_asset_data(..., file_name=...)` is deprecated and should be replaced with `log_asset_data(..., name=...)` when using Context API. The user did not provide any error messages, stack traces, or logs, nor did they provide steps to reproduce the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.94,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":12.0,
        "Challenge_repo_issue_count":219.0,
        "Challenge_repo_star_count":42.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8082.0786111111,
        "Challenge_title":"Comet \"Reproduce\" feature doesn't work",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":11,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"is this still an issue @51N84D ? Yeah, this still doesn't work. I don't think anyone has tried to resolve it yet Ok ; should we in your opinion?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.7,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1620426047396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":386.5795511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Challenge_closed_time":1620426900467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619035214083,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using the library weights and biases to visualize a time series curve output by their model during training. They are looking for a way to create a slider that allows them to select an epoch and view the corresponding curve. They currently have code that logs the curve for each epoch using wandb.plot.line_series.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":386.5795511111,
        "Challenge_title":"How to get multiple lines exported to wandb",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":840.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1577734207070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":422.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":6.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1523298968403,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1754.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":140.9113833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Challenge_closed_time":1568804066440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1568296785460,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a model using the Sagemaker library in Python, but is encountering an error parsing the hyperparameters. They have identified that the missing parameter in their JSON configuration is the content_type parameter, which they have tried to add in various places with no success. The user is seeking guidance on how to properly specify the content_type in their training job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.0,
        "Challenge_reading_time":24.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":140.9113833333,
        "Challenge_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":644.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":51.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":388.0879419444,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Thanks for your good product.<\/p>\n<p>It would be good to add an archive feature for runs.<\/p>\n<p>In a project, we may try many ideas. But most of them result in no outcomes. It would be good to archive those runs to keep the workspace clean.<\/p>\n<p>It is not a good option to delete them, because we may check them in future for some cases, such as ablation study.<\/p>",
        "Challenge_closed_time":1676670702464,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675273585873,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user suggests adding an archive feature for runs in a project to keep the workspace clean. They explain that deleting runs is not a good option as they may need to refer to them in the future for certain cases.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/archive-runs\/3793",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":3.1,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":388.0879419444,
        "Challenge_title":"Archive runs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I believe currently wandb does support multiple selection. But not in the workspace view. In the table view I can select and tag multiple runs at once.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1662621266503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.6069663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Challenge_closed_time":1662652096572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662649911493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is familiar with SageMaker and has performed distributed training experiments. They are now seeking to know if it is possible to test and run SageMaker distributed training in local mode using SageMaker Notebook Instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6069663889,
        "Challenge_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662649653072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":3.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515259002820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":74.0,
        "Challenge_adjusted_solved_time":505.1086980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Challenge_closed_time":1515260139876,
        "Challenge_comment_count":1,
        "Challenge_created_time":1513441748563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty finding information on the underlying algorithm for AWS Sagemaker's Neural Topic Model, as the documentation does not provide any paper or details, and searching for \"neural topic model\" yields multiple results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47847736",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":505.1086980556,
        "Challenge_title":"AWS Sagemaker Neural Topic Model",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1374572439352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":2177.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.7,
        "Solution_reading_time":3.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.1641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644307000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599546516000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue when using Hydra and MLFlow together, where the parameters passed to the LightningModule is a `DictConfig`, causing the condition in the `logger\/base.py` to not be met. This results in an error message when trying to log hyperparameters with MLFlow. The expected behavior is to check whether the instance is `dict` or `DictConfig` in the given line.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":27.1641666667,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":532,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":null,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.55,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Challenge_closed_time":1653501835000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653399055000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MLFlow logging in the `compare_models` function of time series. While the parameters are logged, metrics and artifacts are not being logged, causing all runs to fail. However, the user is able to use the `create_model` function without any issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":21.61,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":28.55,
        "Challenge_title":"mlflow ui doesn't show any models",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Creating a clean env and installing pycaret again solved the issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":0.84,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":314.0,
        "Challenge_answer_count":40,
        "Challenge_body":"Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error:\u00a0\n\nQuota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
        "Challenge_closed_time":1684921140000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683790740000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a quota error while trying to use the bison chat model in Vertex AI. The error message indicates that the quota for online prediction requests per base model has been exceeded and the user needs to submit a quota increase request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-bison-chat-model-in\/m-p\/552421#M1857",
        "Challenge_link_count":0,
        "Challenge_participation_count":40,
        "Challenge_readability":9.2,
        "Challenge_reading_time":4.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":314.0,
        "Challenge_title":"Receiving quota error when trying to use bison chat model in Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":0.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"UPDATE: We have raised the default quotas for everyone.\u00a0 This roll out may take a day to reach everyone so.\u00a0 Thank you everyone for your patience and flagging this to us!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1565129860212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.0860186111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Challenge_closed_time":1569516992870,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569502183603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Service Pipeline and is invoking it externally using its rest endpoint. However, they need to monitor its run status periodically and are looking for a method inside the pipeline's rest endpoint to check its status. The user has tried the steps mentioned in a GitHub link but is still facing the issue.",
        "Challenge_last_edit_time":1569502283203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58117200",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1136852778,
        "Challenge_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":898.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508663110972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":54.5,
        "Solution_reading_time":14.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1592311727163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":138.6078019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Challenge_closed_time":1603707766790,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603208778703,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an UnexpectedStatusException while attempting to create a multi-model endpoint in AWS Sagemaker using Scikit-learn and a custom training script. The training script succeeds but Sagemaker throws the exception, and the user is unable to find any useful information in the Cloudwatch logs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":32.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":138.6078019445,
        "Challenge_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":4.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1545131500420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4473.0,
        "Answerer_view_count":745.0,
        "Challenge_adjusted_solved_time":213.3802761111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model in Azure ML and kept on getting the error 'model not found' from my score.py. So I decided to start from scratch again. I had my custom environment registered, and the Azure ML API for Environment class doesn't seem to have anything like 'delete' or 'unregister'. is there a way to work around this? Thanks<\/p>",
        "Challenge_closed_time":1604151289467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603383120473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a model in Azure ML but keeps getting an error message. They want to start from scratch but are unable to unregister their custom environment as there is no option available in the Azure ML API for Environment class. The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":1604274985247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64486262",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":213.3802761111,
        "Challenge_title":"Is there a way to un-register an environment in Azure ML studio",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595118700083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py&amp;preserve-view=true#delete--\" rel=\"nofollow noreferrer\">delete<\/a> method in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py\" rel=\"nofollow noreferrer\">Model<\/a> class to delete a registered model.<\/p>\n<p>This can also be done via the Azure CLI as:<\/p>\n<pre><code>az ml model delete &lt;model id&gt;\n<\/code><\/pre>\n<p>Other commands can be found here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest\" rel=\"nofollow noreferrer\">az ml model<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651909943000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where Wandb is only logging one run instead of three runs when using DDP and multirun in their `test.py` file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1666.9041666667,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":14.6592575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We want to tune a SageMaker <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/tuner.html?highlight=HyperparameterTuner\" rel=\"nofollow noreferrer\">HyperparameterTuner<\/a> (or something similar) where <em>several<\/em> components of the pipeline have associated hyperparameters. Both components in our case are realized via SageMaker containers for ML algorithms.<\/p>\n<pre><code>model = PipelineModel(..., models = [ our_model, xgb_model ])\ndeploy = Estimator(image_uri = model, ...)\n...\ntuner = HyperparameterTuner(deply, .... tune_parameters, ....)\ntuner.fit(...)\n<\/code><\/pre>\n<p>Now, there is of course the problem how to distribute the <code>tune_parameters<\/code> to the pipeline steps during the tuning.<\/p>\n<p>In scikit-learn this is achieved by specially naming the tuning parameters <code>&lt;StepName&gt;__&lt;ParameterName&gt;<\/code>.<\/p>\n<p>I don't see a way to achieve something similar with SageMaker, though. Also, search of the two keywords brings up the same question <a href=\"https:\/\/stackoverflow.com\/questions\/56308169\/creating-a-model-for-use-in-a-pipeline-from-a-hyperparameter-tuning-job\">here<\/a> but is not really what we want to do.<\/p>\n<p>Any suggestion how to achieve this?<\/p>",
        "Challenge_closed_time":1646418155780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646365382453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to tune a SageMaker PipelineModel with a HyperparameterTuner, but is facing the challenge of how to distribute the tuning parameters to the pipeline steps during the tuning process. The user is looking for suggestions on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71346372",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":14.6592575,
        "Challenge_title":"SageMaker: PipelineModel and Hyperparameter Tuning",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389092281248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ishikawa Prefecture",
        "Poster_reputation_count":227.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>If both the models need to be jointly optimized, you could run a SageMaker HPO job in script mode and define both the models in the script. Or you could run two HPO jobs, optimize each model, and then create the Pipeline Model. There is no native support for doing an HPO job on a PipelineModel.<\/p>\n<p>I work at AWS and my opinions are my own.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2063763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <br \/>\nI created custom Azure AI model what I would like to use in PowerBI.    <br \/>\nWhen I open a dataset in PowerBI and after select the &quot;Azure Machine learning&quot; after the pop-up window is empty but I suppose it should contain my custom model(s).    <br \/>\nI followed the below articles:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\">https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate<\/a>    <\/p>\n<p>Kind regards    <br \/>\nTom    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162671-powerbi-azure-ai.png?platform=QnA\" alt=\"162671-powerbi-azure-ai.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162606-azure-ai-model.png?platform=QnA\" alt=\"162606-azure-ai-model.png\" \/>    <\/p>",
        "Challenge_closed_time":1641419590032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641415247077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a custom Azure AI model and wants to use it in PowerBI. However, when they select \"Azure Machine learning\" in PowerBI, the pop-up window is empty and does not show their custom model. The user has followed the provided articles but is still facing this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/684845\/why-does-powerbi-not-see-my-custom-azure-ai-model",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":19.6,
        "Challenge_reading_time":14.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2063763889,
        "Challenge_title":"Why does PowerBI not see my custom Azure AI model?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The product group for Power Bi actively monitors questions over at    <br \/>\n<a href=\"https:\/\/community.powerbi.com\/\">https:\/\/community.powerbi.com\/<\/a>       <\/p>\n<p>--please don't forget to <code>upvote<\/code> and <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/145510-image.png?platform=QnA\" alt=\"145510-image.png\" \/> if the reply is helpful--    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1587712154000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an internal server error while deploying a container to AKS using Azure ML CLI. The error occurs sporadically and there is no clear pattern to it. The error message suggests creating a retry loop, but this would not address the underlying issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.8105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links.\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Challenge_closed_time":1611571671000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611507553000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for the API definitions and descriptions for the ModelBiasMonitor and ModelExplainabilityMonitor classes in Amazon SageMaker. They have found some mentions in the documentation but are unable to locate the actual reference and code implementation.",
        "Challenge_last_edit_time":1667925795096,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-modelbiasmonitor-and-modelexplainabilitymonitor",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":27.4,
        "Challenge_reading_time":8.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":17.8105555556,
        "Challenge_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The actual reference to the classes can be found here: \n[https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py ]()  \nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1613663015963,
        "Solution_link_count":1.0,
        "Solution_readability":24.7,
        "Solution_reading_time":3.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1527682322812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":31.9027575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Challenge_closed_time":1587720289803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587603987047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to log a SageMaker DeepAR model with MLFlow for version tracking, but the mlflow.sagemaker module does not have a \"log_model\" function. The user tried to use the mlflow.pyfunc module, but it did not work. The user is seeking help to log the SageMaker model and generate cloudpickle and yaml files with MLFlow.",
        "Challenge_last_edit_time":1587605439876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61377643",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":32.3063211111,
        "Challenge_title":"Tracking SageMaker Estimator with MLFlow",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548023586667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":145.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.1744444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1637601038000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637586010000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a comet logger when using pytorch lightning cli. The error message shows that the `self._kwargs` has an unexpected keyword argument 'agg_key_funcs'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.5,
        "Challenge_reading_time":56.44,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":4.1744444444,
        "Challenge_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":275,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@davidbaines, Did that fix it? Yes! Thanks so much.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-1.0,
        "Solution_reading_time":0.63,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":162.2517566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Challenge_closed_time":1611382397500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1610971789293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use Optuna for hyperparameter tuning of their model but is stuck on defining a search space with lognormal\/normal distribution, which is possible in hyperopt using hp.lognormal. The user is asking if it is possible to define such a space using a combination of the existing suggest_ api of Optuna.",
        "Challenge_last_edit_time":1610981620636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":114.0578352778,
        "Challenge_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445250318392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2164.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1611565726960,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":17.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":133.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1350376620496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colombo, Sri Lanka",
        "Answerer_reputation_count":207537.0,
        "Answerer_view_count":32114.0,
        "Challenge_adjusted_solved_time":0.0204122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a free trial account on MS Azure and I'm following this tutorial.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>I'm stuck when I try to \"submit the pipeline\".<\/p>\n\n<p>The reason seems to be that I can't create a compute instance or a training cluster on a free plan.\nI still have 200USDs of free credits. I guess there must be a solution?<\/p>\n\n<hr>\n\n<p>Error messages:<\/p>\n\n<pre><code>Invalid graph: The pipeline compute target is invalid.\n\n400: Compute Test3 in state Failed, which is not able to use\n\nCompute instance: creation failed\nThe specified subscription has a total vCPU quota of 0 and is less than the requested compute training cluster and\/or compute instance's min nodes of 1 which maps to 4 vCPUs\n<\/code><\/pre>",
        "Challenge_closed_time":1586681759587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586681686103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in submitting a pipeline while using a free trial account on MS Azure due to the inability to create a compute instance or a training cluster on a free plan. The error message indicates that the pipeline compute target is invalid and the specified subscription has a total vCPU quota of 0, which is less than the requested compute training cluster and\/or compute instance's min nodes of 1.",
        "Challenge_last_edit_time":1586690281396,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61168984",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":12.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0204122222,
        "Challenge_title":"Azure ML free trial: how to submit pipeline?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343682543648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":135.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Please check the announcement from MS Team regarding this:<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/<\/a><\/p>\n\n<p>All the free trials will not work as of now<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.4,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442334437952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":2272.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":0.1509027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use the azure ML designer (preview).<\/p>\n\n<p>referencing this - <a href=\"https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>using my own input sheet which has four columns and some decimal values. nothing fancy and identical to the sample datasets provided. <\/p>\n\n<p>I do this step (from the linked document above)<\/p>\n\n<p><em>Select the Train Model module.\nIn the module details pane to the right of the canvas, select Edit column selector.\nIn the Label column dialog box, expand the drop-down menu and select Column names.\nIn the text box, enter price to specify the value that your model is going to predict.<\/em><\/p>\n\n<p>and I get this (but there are no errors in the actual designer window.<\/p>\n\n<p>\"Failed to parse column picker rules\"<\/p>",
        "Challenge_closed_time":1582126496627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582125953377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while using the Azure ML designer preview. They are trying to use their own input sheet with four columns and decimal values, similar to the sample datasets provided in the documentation. However, when they try to select the Train Model module and edit the column selector, they receive an error message stating \"Failed to parse column picker rules.\"",
        "Challenge_last_edit_time":1582126556240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60303714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1509027778,
        "Challenge_title":"Failed to parse column picker rules - azure ML designer",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>Okay, I found an answer myself. Hope that is okay.<\/p>\n\n<p>In my input sheet, the title was something like this \"Interest Rate %\". Looks like azure was trying to say that it does not like special characters it the column names.<\/p>\n\n<p>I edited my original csv file in excel, and removed the % in all the titles. <\/p>\n\n<p>Then, created a new data store. problem solved. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":4.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.5540683334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:  <\/p>\n<p>best_run, fitted_model = remote_run.get_output()  <br \/>\nfitted_model  <\/p>\n<p>When runnning the code, I get the following error:   <\/p>\n<p>AttributeError: 'DataTransformer' object has no attribute 'enable_dnn'  <\/p>\n<p>Any help would be much appreciated.   <\/p>",
        "Challenge_closed_time":1613128985603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613083790957,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a remote run model built using the AutoML function. The code is producing an error message stating that the 'DataTransformer' object has no attribute 'enable_dnn'. The user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":12.5540683334,
        "Challenge_title":"Remote run model unable to be saved",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=f0b99777-9086-42ac-b2e2-2b069641e943\">@Bernardo Jaccoud  <\/a> Did your run configure enable_dnn i.e bert <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#bert-integration-in-automated-ml\">settings<\/a> of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.5933255556,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I was wondering if anybody from the W&amp;B team can confirm that there is an outage at the moment.<\/p>\n<p>I\u2019ve been having issues starting runs and it seems like other folks are having issues syncing runs with a network time out error (<a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4424\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: canno't sync my runs \u00b7 Issue #4424 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>). It\u2019s been ongoing for about 2 hours now.<\/p>\n<p>The status page is saying everything is fine - <a href=\"https:\/\/status.wandb.com\" rel=\"noopener nofollow ugc\">https:\/\/status.wandb.com<\/a><\/p>\n<p>All the best,<br>\nAlexey<\/p>",
        "Challenge_closed_time":1667343090214,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667333754242,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing issues with starting runs and syncing runs due to a network time out error. They are inquiring if there is an outage at W&B, but the status page indicates that everything is fine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/w-b-outage-11-1-2022\/3360",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.5933255556,
        "Challenge_title":"W&B Outage? 11\/1\/2022",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thank you for your patience! Our engineers were able to push a fix for this. There\u2019s still currently an issue regarding batch moving runs, but for the most part this issue has been resolved.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.41,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.0207547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Because of network problem. The local <code>debug-internal.log<\/code> files of some runs are too large (more than 500MB). To save the disk space, is there any way to avoid the generation of these log files?<\/p>",
        "Challenge_closed_time":1672190667020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671769392303,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the local debug-internal.log files of some runs are too large (more than 500MB) due to a network problem. They are looking for a way to avoid the generation of these log files to save disk space.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/the-debug-internal-log-file-is-too-large-500mb\/3589",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":117.0207547222,
        "Challenge_title":"The debug-internal.log file is too large (>500MB)",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , thank you for writing in and happy to look into this for you.  <code>debug-internal.log<\/code> files are automatically generated and cannot be disabled by the user.  Please see this github issue thread that was raised about this issue were a user provided <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4223#issuecomment-1236304565\" rel=\"noopener nofollow ugc\">workaround<\/a> solution to address this . Do let me know if this reference helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":6.39,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1278673735383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":242.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":669.9685427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Challenge_closed_time":1573131591227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570719704473,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is working with Deepar and is trying to understand the quantile values returned. They are confused as the quantiles returned for a single data point are linear and not similar to the distribution selected with the likelihood hyperparameter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":669.9685427778,
        "Challenge_title":"Deepar Prediction Quantiles Explained",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457123770467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver",
        "Poster_reputation_count":246.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":5.51,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":66.8869986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Challenge_closed_time":1579951518923,
        "Challenge_comment_count":2,
        "Challenge_created_time":1579709239420,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a throttling error while invoking an endpoint from a Spark job in SageMaker. They are unsure if there is a limit on the rate of inferences one can make for a SageMaker endpoint and if it is determined by the instance type or number of instances. They have checked AWS Service Quotas for SageMaker but couldn't find the information. They are also wondering if the number of concurrent tasks is a factor they should be taking care of when running inference.",
        "Challenge_last_edit_time":1579710725728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":47.7,
        "Challenge_reading_time":60.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":67.2998619445,
        "Challenge_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1716.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":11.6,
        "Solution_reading_time":27.59,
        "Solution_score_count":4.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":289.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2472222222,
        "Challenge_answer_count":1,
        "Challenge_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Challenge_closed_time":1603455348000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603454458000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. They are looking for a way to restrict the instance size options available through SageMaker, such as removing the ability to launch ml.p3.16xlarge instances, using IAM policies or another method.",
        "Challenge_last_edit_time":1668589944840,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2472222222,
        "Challenge_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":966.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\n**Note:** This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"EnforceInstanceType\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"sagemaker:CreateTrainingJob\",\n                    \"sagemaker:CreateHyperParameterTuningJob\"\n                ],\n                \"Resource\": \"*\",\n                \"Condition\": {\n                    \"ForAllValues:StringLike\": {\n                        \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                    }\n                }\n            }\n    \n         ]\n    }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925584228,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1303910479480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4196.0,
        "Answerer_view_count":67.0,
        "Challenge_adjusted_solved_time":3788.8637213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Challenge_closed_time":1631884865500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618244956103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running experiments on AWS spot instances and sometimes the experiments are stopped. They want to know how to continue logging to the same run and set the run-id of the active run. They have provided pseudocode but it is not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3788.8637213889,
        "Challenge_title":"Continue stopped run in MLflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1340284487940,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":151.2120097222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Challenge_closed_time":1544569877892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544027022437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a CapacityError while running two TrainingJob instances on AWS SageMaker, with one instance running fine and the other exiting with the error message \"CapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\" The user is unsure about the meaning of this error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":150.7931819444,
        "Challenge_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3875.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1544571385672,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.51,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":13.3565036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Challenge_closed_time":1655849768796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655801685383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy AWS using CDK and Sagemaker, but is encountering an error when running pytest. They are stuck on step 3 of the process and are unsure of what to add in the infrastructure code. They are also unsure if they should do the whole process on their local terminal or on Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":13.3565036111,
        "Challenge_title":"How to deploy AWS using CDK, sagemaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604312740476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.3760377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Challenge_closed_time":1573141908796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573137628147,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble disabling logs in Kedro despite trying to add \"disable_existing_loggers: True\" to the logging.yml file and \"disable:True\" to existing logs. They are seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1890691667,
        "Challenge_title":"How to disable logs in Kedro",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479159384132,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Illinois, United States",
        "Poster_reputation_count":513.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573142581883,
        "Solution_link_count":1.0,
        "Solution_readability":12.4,
        "Solution_reading_time":12.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1476780098123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":1672.0,
        "Answerer_view_count":159.0,
        "Challenge_adjusted_solved_time":13.8667347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Challenge_closed_time":1624416005592,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624366085347,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed a custom model on AWS Sagemaker inference endpoint but has observed that the CPU utilization metric is maxing out at 100% instead of #vCPU*100%. The inference endpoint is not using all cores, resulting in a limit of one API call per second. The user is seeking settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency or using the multiprocessing python package inside the inference.py file to distribute calculations\/prediction across all available cores.",
        "Challenge_last_edit_time":1628443120180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.8667347222,
        "Challenge_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":661.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473938483227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Poster_reputation_count":909.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1624470584452,
        "Solution_link_count":23.0,
        "Solution_readability":20.5,
        "Solution_reading_time":99.55,
        "Solution_score_count":5.0,
        "Solution_sentence_count":55.0,
        "Solution_word_count":599.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1308581761180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":7201.0,
        "Answerer_view_count":861.0,
        "Challenge_adjusted_solved_time":69.4949169444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I deployed a large 3D model to aws sagemaker. Inference will take 2 minutes or more. I get the following error while calling the predictor from Python:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;'\n<\/code><\/pre>\n<p>In Cloud Watch I also see some PING time outs while the container is processing:<\/p>\n<pre><code>2020-10-07T16:02:39.718+02:00 2020\/10\/07 14:02:39 https:\/\/forums.aws.amazon.com\/ 106#106: *251 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/unix:\/tmp\/gunicorn.sock\/ping&quot;, host: &quot;model.aws.local:8080&quot;\n<\/code><\/pre>\n<p>How do I increase the invocation time out?<\/p>\n<p>Or is there a way to make async invocations to an sagemaker endpoint?<\/p>",
        "Challenge_closed_time":1602331706568,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602081524867,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed a large 3D model to AWS Sagemaker and is facing an error while calling the predictor from Python due to invocation time out. The user also sees PING time outs in Cloud Watch while the container is processing. The user is seeking a way to increase the invocation time out or make async invocations to an Sagemaker endpoint.",
        "Challenge_last_edit_time":1602331765720,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64246437",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":15.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":10.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":69.4949169444,
        "Challenge_title":"How to increase AWS Sagemaker invocation time out while waiting for a response",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":8464.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>It\u2019s currently not possible to increase timeout\u2014this is an open issue in GitHub. Looking through the issue and similar questions on SO, it seems like you may be able to use batch transforms in conjunction with inference.<\/p>\n<h1>References<\/h1>\n<p><a href=\"https:\/\/stackoverflow.com\/a\/55642675\/806876\">https:\/\/stackoverflow.com\/a\/55642675\/806876<\/a><\/p>\n<p>Sagemaker Python SDK timeout issue: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":7.27,
        "Solution_score_count":6.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.5003305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure AI and initially had been using Machine Learning Studio Classic.<\/p>\n\n<p>It was working well but was slow training my models. From looking around, it seems that if I use Azure Machine Learning Studio, I can control the hardware used to run the experiments, so this is what I am trying.<\/p>\n\n<p>My issue is that Azure Machine Learning Studio is extremely slow in starting the experiments-it can take 10 minutes to even start.<\/p>\n\n<p>Is this as expected or am I missing something?<\/p>\n\n<p>Incidentally, NC24 was actually slower than NC6 - is this because of the configuration of my experiment?<\/p>\n\n<p>GPU Training    Whole run\nNC6 2m 36s  10m 48s\nNC24    2m 52s  16m 48s<\/p>",
        "Challenge_closed_time":1581956292927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581954491737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing slow start-up times while using Azure Machine Learning Studio, which is taking up to 10 minutes to start an experiment. They had previously used Machine Learning Studio Classic, which was slow in training models. The user is also unsure if the slower performance of NC24 compared to NC6 is due to the experiment configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60265992",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.5003305556,
        "Challenge_title":"Azure Machine Learning - slow to start",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1143.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420792743172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3531.0,
        "Poster_view_count":312.0,
        "Solution_body":"<p>I'm assuming that you're:<\/p>\n\n<ul>\n<li>using Azure ML Studio (i.e. <a href=\"https:\/\/ml.azure.com\/\" rel=\"nofollow noreferrer\"><code>ml.azure.com\/<\/code><\/a>)'s Pipeline Designer, and <\/li>\n<li>creating a new compute target before running?<\/li>\n<\/ul>\n\n<p>If so, then 10 minutes is normal for the first run, given that a cluster of VMs has to be created and provisioned with a Docker container and Conda environment. After the run first completes the compute target is configured to stay on and available for two hours so future runs should execute without the 10 minute delay (provided you don't change the Conda dependencies or choose a new compute target).<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#amlcompute\" rel=\"nofollow noreferrer\">more information about AMLCompute<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":34.3028483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1643782095707,
        "Challenge_comment_count":9,
        "Challenge_created_time":1643658605453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with running code on Google Cloud Platform VertexAI due to an unstable local internet connection. They are looking for a way to run the code on the backend without any output until it is written in the log file.",
        "Challenge_last_edit_time":1645284643350,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Challenge_link_count":2,
        "Challenge_participation_count":10,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":34.3028483334,
        "Challenge_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":514.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622195346030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":9.0,
        "Solution_reading_time":29.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":255.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1531218624572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":78.4838436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a function which looks like this:<\/p>\n<pre><code>def fine_tuning(x,y,model1,model2,model3,trial):\n   pred1 = model1.predict(x)\n   pred2 = model2.predict(x)\n   pred3 = model3.predict(x)\n   \n   h1 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h2 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h3 = trial.suggest_float('h1', 0.0001, 1, log = True)\n\n   pred = pred1 * h1 + pred2 * h2 + pred3 * h3\n\n   return mean_absolute_error(y, pred)\n<\/code><\/pre>\n<p>The problem with this function is that h1+h2+h3 != 1. How would I change this function in order to make the sum of the hyperparmaters = 1?<\/p>",
        "Challenge_closed_time":1627290244260,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627007702423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in fine-tuning a function using Optuna as the sum of the hyperparameters does not add up to 1. They are seeking guidance on how to modify the function to ensure that the sum of the hyperparameters equals 1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68493392",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":78.4838436111,
        "Challenge_title":"Making hyperparameters add up to 1 when fine tuning using Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575887707992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":196.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>Basically, you're looking for a dirichlet distribution for h1, 2, 3. Here's a guide on how to implement that for Optuna: <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":38.4,
        "Solution_reading_time":6.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.3609763889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Am using custom data generator as part of my image augmentation. Are you able to use sweeps to try different parameters for such augmentation?<\/p>\n<p>for example:<\/p>\n<pre><code class=\"lang-auto\">idg = CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=config.v_kernel_size,\n                              h_kernel_size=config.h_kernel_size,  \n                             height_shift_range = config.height_shift_range, \n                             width_shift_range = config.width_shift_range, \n                             rotation_range = config.rotation_range, \n                             shear_range = config.shear_range,\n                             zoom_range = config.zoom_range,)\n    return idg\n<\/code><\/pre>",
        "Challenge_closed_time":1677704879784,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677494780269,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using a custom data generator for image augmentation and is wondering if it is possible to use sweeps to try different parameters for the augmentation. They have provided an example of their current code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-sweeps-for-custom-data-generator-in-keras\/3957",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":58.3609763889,
        "Challenge_title":"Using sweeps for custom data generator in keras",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zookcx\">@zookcx<\/a> thanks for writing in! Is in your example the <code>CustomDataGenerator<\/code> a function that you could call from the main training function? Would something like the following work for you?<\/p>\n<pre><code class=\"lang-auto\">import wandb\ndef main():\n    wandb.init(project='custom-data-sweep')\n    data = CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=wandb.config.v_kernel_size,\n                              h_kernel_size=wandb.config.h_kernel_size)\n    wandb.log({'data': data})\n    wandb.finish()\n\n\ndef CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=0,\n                             h_kernel_size=0,\n                          ):\n  \n    # add your own custom data generator logic\n    idg = v_kernel_size + h_kernel_size\n\n    return idg\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">\nsweep_config = {\n    'method': 'grid',\n    'project': 'sweep-configs',\n    'parameters': {\n        'v_kernel_size': {\n            'values': [32, 64, 96, 128, 256]\n        },\n        'h_kernel_size': {\n            'values': [32, 64, 96, 128, 256]\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config)\nwandb.agent(sweep_id, function=main)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":14.43,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":102.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":9.3958480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Challenge_closed_time":1643080918688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643047926007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on GCP Vertex AI and deployed it on an endpoint. They are able to execute predictions from a sample within their GCP project, but they are unsure if it is possible to request the endpoint from another GCP project with the use of a service account and IAM role in both projects.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.1646336111,
        "Challenge_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":417.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598873976143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Versailles, France",
        "Poster_reputation_count":140.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1643081751060,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":163.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1600296752470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3586.0,
        "Answerer_view_count":684.0,
        "Challenge_adjusted_solved_time":88.4833777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>EDIT: Found a solution, see bottom of post.<\/p>\n<p>I have a pre-trained keras model (model.h5) which is a CNN for image classification. My goal is to deploy the model on sagemaker and use a lambda function to interface with the sagemaker endpoint and make predictions. When I predict with the model on my local machine using the following code, I get results I would expect:<\/p>\n<pre><code>model = load_model(r'model.h5')\nphoto_fp = r'\/path\/to\/photo.jpg'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\n\nprint(model.predict(img_batch))\n# [[9.9984562e-01 1.5430539e-04 2.2775747e-14 9.5851349e-16]]\n<\/code><\/pre>\n<p>However, when I deploy the model as an endpoint on sagemaker, I get different results. Below is my code to deploy the model as an endpoint:<\/p>\n<pre><code>model = load_model(r'model.h5')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sagemaker\nimport boto3, re\nfrom sagemaker import get_execution_role\ndef convert_h5_to_aws(loaded_model):\n    # Interpreted from 'Data Liam'\n    from tensorflow.python.saved_model import builder\n    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n    from tensorflow.python.saved_model import tag_constants\n    \n    model_version = '1'\n    export_dir = 'export\/Servo\/' + model_version\n    \n    # Build the Protocol Buffer SavedModel at 'export_dir'\n    builder = builder.SavedModelBuilder(export_dir)\n    \n    # Create prediction signature to be used by TensorFlow Serving Predict API\n    signature = predict_signature_def(\n        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\n\n    with tf.compat.v1.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        # Save the meta graph and variables\n        builder.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n        builder.save()\n    \n    #create a tarball\/tar file and zip it\n    import tarfile\n    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n        archive.add('export', recursive=True)\n        \nconvert_h5_to_aws(model)\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n\n!touch train.py # from notebook\n# the (default) IAM role\nrole = get_execution_role()\nframework_version = tf.__version__\n\n# Create Sagemaker model\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = framework_version,\n                                  entry_point = 'train.py')\n\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This deploys fine and saves as an endpoint. Then, I invoke the endpoint:<\/p>\n<pre><code>runtime = boto3.client('runtime.sagemaker')\nendpoint_name = 'endpoint-name-for-stackoverflow'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\npredictor = TensorFlowPredictor(endpoint_name)\nresult = predictor.predict(data=img_batch)\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>As you can see, the classifier is predicting all of the outputs as nearly equal probabilities, which is not what was predicted on the local machine. This leads me to believe that something is going wrong in my deployment.<\/p>\n<p>I have tried loading the model weights and json model structure to sagemaker rather than the entire h5 model but that yielded the same results. I also used invoke endpoint instead of the predictor API with the following code:<\/p>\n<pre><code>payload = json.dumps(img_batch.tolist())\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                                   ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>But yet again, the same results.<\/p>\n<p>Any ideas why I'm getting different results with the sagemaker than on my local machine with the same model?\nThanks!<\/p>\n<p>EDIT: Found a solution. The problem was with the TensorflowModel framework version argument. I changed the framework_version to '1.12' and installed version 1.12 in the Sagemaker Jupyter instance and retrained my model locally using TF 1.12. I'm not totally sure why this works but all of the blogs I found (e.g. <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this one<\/a>) used 1.12. Hope this helps.<\/p>",
        "Challenge_closed_time":1621926225083,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620830223953,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a pre-trained Keras model for image classification and wants to deploy it on Sagemaker to make predictions using a lambda function. While the local classification works fine, the Sagemaker classification produces different results, with all outputs having nearly equal probabilities. The user has tried loading the model weights and JSON model structure to Sagemaker, but it yielded the same results. The user is seeking help to understand why the Sagemaker classification is different from the local classification.",
        "Challenge_last_edit_time":1621607684923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67505781",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":62.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":55,
        "Challenge_solved_time":304.4447583334,
        "Challenge_title":"Loading Pretrained Keras to Sagemaker - local classification works but sagemaker classification changes",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":479,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606275044956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>For the benefit of community providing solution in answer section<\/p>\n<blockquote>\n<p>The problem was with the <code>TensorflowModel<\/code> framework version argument. After\nchanging the <code>framework_version<\/code> to <code>1.12<\/code> and installed version <code>TF 1.12<\/code> in\nthe <code>Sagemaker Jupyter<\/code> instance and retrained model locally using <code>TF 1.12<\/code> got same results. (paraphrased from Peter Van Katwyk)<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1231865173143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mountain View, CA",
        "Answerer_reputation_count":29068.0,
        "Answerer_view_count":2064.0,
        "Challenge_adjusted_solved_time":4.0522230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Challenge_closed_time":1591773861796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591767515617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue when using MLFlow and Hydra together. While both work fine individually, when used together, the user encounters an error that causes the program to crash. The error message suggests that the run is not found, and the user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":25.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.7628275,
        "Challenge_title":"MLFlow and Hydra causing crash when used together",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":718.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583072555672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1591782103620,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":22.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":200.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### Version\r\n\r\n23.01\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\nDocker\r\n\r\n### Describe the bug.\r\n\r\nUnable to start the mlflow server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\nDowngrading  mlflow version to `<1.29.0` works fine.\r\n\r\n\r\n### Minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/Morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up mlflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[+] Running 3\/3                                                                                                                                               \r\n \u283f Network production_backend      Created                                                                                                               0.0s \r\n \u283f Network production_frontend     Created                                                                                                               0.0s\r\n \u283f Container mlflow_server  Created                                                                                                               0.1s\r\nAttaching to mlflow_server\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Error initializing backend store\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server  | Traceback (most recent call last):\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 392, in server\r\nmlflow_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 265, in initialize_backend_stores\r\nmlflow_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 244, in _get_tracking_store\r\nmlflow_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\nmlflow_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\nmlflow_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\nmlflow_server  |     return SqlAlchemyStore(store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\nmlflow_server  |     mlflow.store.db.utils._verify_schema(self.engine)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 71, in _verify_schema\r\nmlflow_server  |     raise MlflowException(\r\nmlflow_server  | mlflow.exceptions.MlflowException: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server exited with code 1\r\n```\r\n\r\n\r\n### Full env printout\r\n\r\n```shell\r\n<details><summary>Click here to see environment details<\/summary><pre>\r\n     \r\n     **git***\r\n     commit 9619c0e3a5ddbdd476aba9331f288aac855da7cd (HEAD -> dfp-pipeline-module, origin\/dfp-pipeline-module)\r\n     Author: bsuryadevara <bhargavsuryadevara@gmail.com>\r\n     Date:   Wed Nov 30 17:13:05 2022 -0600\r\n     \r\n     used dill to persist source and preprocess schema\r\n     **git submodules***\r\n     -27efc4fd1c984332920db2a2d6ab1f84d3cb55cd external\/morpheus-visualizations\r\n     \r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU7140058-R1\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.6 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\n     SUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\n     BUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\n     PRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx04 4.15.0-162-generic #170-Ubuntu SMP Mon Oct 18 11:38:05 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\r\n     \r\n     ***GPU Information***\r\n     Thu Dec  1 17:37:05 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    56W \/ 300W |  11763MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    43W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N\/A   28C    P0    41W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N\/A   29C    P0    44W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N\/A   31C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     \r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N\/A  N\/A     31232      C   ...da\/envs\/rapids\/bin\/python      303MiB |\r\n     |    0   N\/A  N\/A     41206      C   ...da\/envs\/rapids\/bin\/python     7051MiB |\r\n     |    0   N\/A  N\/A     52497      C   ...nda3\/envs\/venv\/bin\/python     3137MiB |\r\n     |    0   N\/A  N\/A     55973      C   tritonserver                     1267MiB |\r\n     +-----------------------------------------------------------------------------+\r\n     \r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3267.078\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.17\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n     \r\n     ***CMake***\r\n     \/usr\/bin\/cmake\r\n     cmake version 3.10.2\r\n     \r\n     CMake suite maintained and supported by Kitware (kitware.com\/cmake).\r\n     \r\n     ***g++***\r\n     \/usr\/bin\/g++\r\n     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n     Copyright (C) 2017 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n     \r\n     \r\n     ***nvcc***\r\n     \/usr\/local\/cuda\/bin\/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5\/compiler.30672275_0\r\n     \r\n     ***Python***\r\n     \/usr\/bin\/python\r\n     Python 2.7.17\r\n     \r\n     ***Environment Variables***\r\n     PATH                            : \/usr\/local\/cuda\/bin:\/opt\/bin\/:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin:\/home\/nfs\/bsuryadevara:\/home\/nfs\/bsuryadevara\r\n     LD_LIBRARY_PATH                 :\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    :\r\n     PYTHON_PATH                     :\r\n     \r\n     conda not found\r\n     ***pip packages***\r\n     \/usr\/bin\/pip\r\n\/usr\/lib\/python2.7\/dist-packages\/OpenSSL\/crypto.py:12: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\r\n  from cryptography import x509\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\n     ansible (2.9.9)\r\n     asn1crypto (0.24.0)\r\n     backports.functools-lru-cache (1.6.4)\r\n     backports.shutil-get-terminal-size (1.0.0)\r\n     bcrypt (3.1.7)\r\n     beautifulsoup4 (4.9.3)\r\n     boto3 (1.17.112)\r\n     botocore (1.20.112)\r\n     bs4 (0.0.1)\r\n     certifi (2018.1.18)\r\n     cffi (1.11.5)\r\n     chardet (3.0.4)\r\n     click (7.1.2)\r\n     configparser (4.0.2)\r\n     contextlib2 (0.6.0.post1)\r\n     cryptography (3.3.2)\r\n     decorator (4.1.2)\r\n     defusedxml (0.6.0)\r\n     distro (1.6.0)\r\n     dnspython (1.15.0)\r\n     docker (4.4.4)\r\n     docopt (0.6.2)\r\n     enum34 (1.1.10)\r\n     fastrlock (0.8)\r\n     flake8 (3.9.2)\r\n     functools32 (3.2.3.post2)\r\n     futures (3.3.0)\r\n     gssapi (1.4.1)\r\n     gyp (0.1)\r\n     html-to-json (2.0.0)\r\n     html2text (2019.8.11)\r\n     html5lib (0.999999999)\r\n     http (0.2)\r\n     httplib2 (0.14.0)\r\n     httpserver (1.1.0)\r\n     idna (2.6)\r\n     importlib-metadata (2.1.3)\r\n     ipaclient (4.6.90rc1+git20180411)\r\n     ipaddress (1.0.17)\r\n     ipalib (4.6.90rc1+git20180411)\r\n     ipaplatform (4.6.90rc1+git20180411)\r\n     ipapython (4.6.90rc1+git20180411)\r\n     Jinja2 (2.10)\r\n     jmespath (0.10.0)\r\n     lxml (4.2.1)\r\n     MarkupSafe (1.0)\r\n     mccabe (0.6.1)\r\n     netaddr (0.7.19)\r\n     netifaces (0.10.4)\r\n     numpy (1.16.6)\r\n     ofed-le-utils (1.0.3)\r\n     olefile (0.45.1)\r\n     pandas (0.24.2)\r\n     paramiko (2.11.0)\r\n     pathlib2 (2.3.7.post1)\r\n     Pillow (5.1.0)\r\n     pip (9.0.1)\r\n     ply (3.11)\r\n     pyasn1 (0.4.2)\r\n     pyasn1-modules (0.2.1)\r\n     pycodestyle (2.7.0)\r\n     pycparser (2.18)\r\n     pycrypto (2.6.1)\r\n     pyflakes (2.3.1)\r\n     pygobject (3.26.1)\r\n     PyNaCl (1.4.0)\r\n     pyOpenSSL (17.5.0)\r\n     python-apt (1.6.5+ubuntu0.7)\r\n     python-augeas (0.5.0)\r\n     python-dateutil (2.8.2)\r\n     python-dotenv (0.18.0)\r\n     python-ldap (3.0.0)\r\n     python-yubico (1.3.2)\r\n     pytz (2022.4)\r\n     pyusb (1.0.0)\r\n     PyYAML (5.4.1)\r\n     qrcode (5.3)\r\n     requests (2.27.1)\r\n     s3fs (0.2.2)\r\n     s3transfer (0.4.2)\r\n     scandir (1.10.0)\r\n     setuptools (39.0.1)\r\n     six (1.16.0)\r\n     soupsieve (1.9.6)\r\n     splunk-sdk (1.7.2)\r\n     subprocess32 (3.5.4)\r\n     tqdm (4.60.0)\r\n     typing (3.10.0.0)\r\n     urllib3 (1.26.12)\r\n     webencodings (0.5)\r\n     yapf (0.32.0)\r\n     zipp (1.2.0)\r\n     \r\n<\/pre><\/details>\r\n```\r\n\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Challenge_closed_time":1669922495000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1669916494000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with logging modified parameters on Mlflow while using Hydra for parameter modification during experiment runs. The default parameters set with Hydra are logged correctly, but the modified parameters are not being logged on Mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/512",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":155.99,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":167,
        "Challenge_solved_time":1.6669444444,
        "Challenge_title":"[BUG]: Unable to Start DFP Production MLFlow Server",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1151,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"yeah, MLflow recently has started enforcing db schema checks at startup\r\n After removing docker volumes that are related to MLFlow and restarted `mlflow_server`. Now it's working as expected.\r\n```\r\ndocker volume rm production_mlflow_data\r\ndocker volume rm production_db_data\r\n```\r\n\r\n```\r\nmlflow_server        |   worker_int: <function WorkerInt.worker_int at 0x7faaa8f90dc0>\r\nmlflow_server        |   worker_abort: <function WorkerAbort.worker_abort at 0x7faaa8f90ee0>\r\nmlflow_server        |   pre_exec: <function PreExec.pre_exec at 0x7faaa8fa6040>\r\nmlflow_server        |   pre_request: <function PreRequest.pre_request at 0x7faaa8fa6160>\r\nmlflow_server        |   post_request: <function PostRequest.post_request at 0x7faaa8fa61f0>\r\nmlflow_server        |   child_exit: <function ChildExit.child_exit at 0x7faaa8fa6310>\r\nmlflow_server        |   worker_exit: <function WorkerExit.worker_exit at 0x7faaa8fa6430>\r\nmlflow_server        |   nworkers_changed: <function NumWorkersChanged.nworkers_changed at 0x7faaa8fa6550>\r\nmlflow_server        |   on_exit: <function OnExit.on_exit at 0x7faaa8fa6670>\r\nmlflow_server        |   proxy_protocol: False\r\nmlflow_server        |   proxy_allow_ips: ['127.0.0.1']\r\nmlflow_server        |   keyfile: None\r\nmlflow_server        |   certfile: None\r\nmlflow_server        |   ssl_version: 2\r\nmlflow_server        |   cert_reqs: 0\r\nmlflow_server        |   ca_certs: None\r\nmlflow_server        |   suppress_ragged_eofs: True\r\nmlflow_server        |   do_handshake_on_connect: False\r\nmlflow_server        |   ciphers: None\r\nmlflow_server        |   raw_paste_global_conf: []\r\nmlflow_server        |   strip_header_spaces: False\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Starting gunicorn 20.1.0\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [DEBUG] Arbiter booted\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Listening at: http:\/\/0.0.0.0:5000 (30)\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Using worker: sync\r\n```",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":23.22,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065545000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581064035000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the Flask application startup fails due to an import error related to NeptuneConfig. The possible solution suggested is to make NeptuneConfig discoverable by the service. The user has followed the guide to set up the service to use Neptune and has deployed a container based on the amundsen-metadata image but is unable to start the app. The user is unable to connect to the Neptune cluster and is seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4194444444,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1501040260887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":2802.1180147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1625508026043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to use hyperparameters tuning on AWS Sagemaker. All 5 failed training jobs have the same error related to KeyError: 'SM_CHANNEL_TRAINING'. The issue is occurring at Step 4 of the project. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2802.1180147222,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588429621780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation_count":21.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1663710134840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":231.2612944445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Challenge_closed_time":1663711220203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662878679543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether the Amazon SageMaker built-in LightGBM algorithm supports distributed training, as they currently use Databricks for this purpose and would consider migrating to SageMaker if it does. The documentation on SageMaker's website does not provide clear information on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":231.2612944445,
        "Challenge_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":9.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":31.6572594445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Challenge_closed_time":1453832406127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453718439993,
        "Challenge_favorite_count":5.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Experiment that works fine, but when deployed and accessed through the front-end of their application, the API-call takes up to 30 seconds to calculate a simple linear regression. The user suspects that it may be due to the experiment still being in a development slot or being run on a slow machine. They are looking for a way to speed up the execution.",
        "Challenge_last_edit_time":1453911336527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":31.6572594445,
        "Challenge_title":"Azure Machine Learning Request Response latency",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1128.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446116840792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":311.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Solution_comment_count":11.0,
        "Solution_last_edit_time":1453911048927,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":16.44,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":218.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":248.2981122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1638931046247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a managed notebook on Vertex AI due to a quota limit error related to 'Create Runtime API requests per minute' of the 'notebooks.googleapis.com' service. The user has tried to edit the quota limit but is unable to increase it. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":248.2981122222,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":20.1381897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Challenge_closed_time":1526179930343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526107432860,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Sagemaker where they are unable to create region-specific security credentials for their endpoint. They have created an S3 bucket and trained and published the Sagemaker endpoint in Ireland server, but when they try to test the endpoint in Postman, they receive a region-specific error. As the user is using a root user account, they are unable to generate region-specific security credentials for Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":20.1381897222,
        "Challenge_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":750.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":14.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":377.9588433334,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Challenge_closed_time":1663850808295,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662490156459,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking advice on the appropriate workflow to run multiple experiments with different initial conditions and collect the results as a group. They are considering using a combination of sweeps and run groups, but are unsure of the best approach and how to collect the results and assets for a group. They are also interested in using the WandB platform to visualize the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.0,
        "Challenge_reading_time":24.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":377.9588433334,
        "Challenge_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":304,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jlperla\">@jlperla<\/a> thank you for the detailed information, and great to hear that the grouping issue has been now resolved. Regarding your question using the API to filter runs, you could do that indeed with the following command:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"sweep\": \"sweep_id\"})<\/code><br>\nAlternatively you can use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/tags#how-to-add-tags\">API to tag all your runs<\/a> based on <code>my_sweep_name<\/code> identifier and then query runs as follows:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"tags\": \"my_sweep_name\"})<\/code><br>\nIs my_sweep_name defined in your config? In that case you could do <code>filters={\"config.sweep_name\": \"my_sweep_name\"}<\/code>.<\/p>\n<p>Would any of these work for you? Please let me know if you have any further questions or issues with this!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":11.58,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":18.2316369445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Challenge_closed_time":1651629955790,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651564321897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use cross validation in Sagemaker hyperparameter tuning jobs to avoid relying too much on the train validation split, but is unsure if it is possible and has not found any information through extensive googling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.2316369445,
        "Challenge_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1572812561640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3502.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":0.1742652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1660922530248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660921902893,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using LightGBM in Azure ML Jupyter notebooks. The error message indicates that the Graphviz executables are not on the system's PATH, causing the \"ExecutableNotFound\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1742652778,
        "Challenge_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":5.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1590520808976,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1231.9309869445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Challenge_closed_time":1608656242863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604221291310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a hyper-parameter tuning job using AWS SageMaker python SDK, which runs multiple jobs in parallel to find the optimal HP values. The user wants to know if there is a way to describe a specific training job using the SageMaker SDK instead of making an authenticated request to AWS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64630198",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1231.9309869445,
        "Challenge_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":686.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":41.9,
        "Solution_reading_time":6.98,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":81.7717394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I resume a sweep with <code>method<\/code> set to <code>bayes<\/code> (in its configuration), i.e. with <code> wandb sweep --resume<\/code>, does the Bayesian optimization process keep into consideration the parameter values already explored before the sweep was interrupted? Or is the previous history of the sweep ignored?<\/p>",
        "Challenge_closed_time":1685036073776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684741695514,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether the Bayesian optimization process considers the parameter values explored in previous runs when resuming a sweep with the \"bayes\" method using \"wandb sweep --resume\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/when-resuming-a-sweep-with-bayesian-optimization-are-the-previous-runs-kept-into-consideration\/4440",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":81.7717394444,
        "Challenge_title":"When resuming a sweep with Bayesian optimization, are the previous runs kept into consideration?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fantauzzi\">@fantauzzi<\/a> , whenever you pause a sweep and then resume it, all methods of a sweep retain history of the parameter values already explored. Resuming will pick up where the sweep left off and run to completion.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.28,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1308769948883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":35204.0,
        "Answerer_view_count":4971.0,
        "Challenge_adjusted_solved_time":30.0725488889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1660669810152,
        "Challenge_comment_count":7,
        "Challenge_created_time":1658770653850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with hyperparameter data types and scales not being validated in a spaCy model while tuning hyperparameters using Vertex AI. The user is unsure if Vertex AI validates the inserted values and only triggers an error if the values do not make any sense. The user also has questions regarding the selection of values for the hyperparameters 'dropout' and 'batch_size'.",
        "Challenge_last_edit_time":1660741967556,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":10.5,
        "Challenge_reading_time":31.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":527.5434172222,
        "Challenge_title":"Hyperparameter data types and scales not being validated",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":326,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1660850228732,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.5499419444,
        "Challenge_answer_count":1,
        "Challenge_body":"I had made my custom training image so It can be conducted through CreateTrainingJob, not sagemaker training took kit (requiring \"ContainerEntrypoint\" option).\n\nBut when I'm trying to run HyperParameter Tuning Job, it is not allowed to add \"ContainerEntrypoint\" option in \"AlgorithmSpecification\" field.\n\nIs it impossible to run HyperParameter Tuning Job with training images that can not be run without sagemaker training toolkit?\n\nThanks!",
        "Challenge_closed_time":1668561111672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668501531881,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a custom training image that can be run through CreateTrainingJob without the sagemaker training toolkit. However, they are unable to add the \"ContainerEntrypoint\" option in the \"AlgorithmSpecification\" field when running a HyperParameter Tuning Job. The user is questioning if it is possible to run a HyperParameter Tuning Job with training images that cannot be run without the sagemaker training toolkit.",
        "Challenge_last_edit_time":1668848150584,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrrvbV_wJRuqG9TX1KyoZnQ\/is-it-possible-sagemaker-hyperparameter-tuning-job-without-sagemaker-training-tool-kit",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":16.5499419444,
        "Challenge_title":"Is it possible SageMaker HyperParameter Tuning Job without sagemaker-training tool kit",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there!\n\nFor custom training image, you can specify the entrypoint in your Dockerfile.\n\nBelow are some code snippet as well as links you can use as reference:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/rapids_bring_your_own\/code\/Dockerfile\n```\nENTRYPOINT [\".\/entrypoint.sh\"]\n```\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/keras_bring_your_own\/Dockerfile\n```\nENTRYPOINT [\"python\", \"-m\", \"trainer.start\"]\n```\n\nFurthermore, SageMaker Training Toolkit is a nicely wrapped up python package for you to use and eases the process of creating a custom training image, it's no different from implementing the logic yourself.\n\nSo it is definitely possible to run HyperParameter Tuning Job using custom containers without using our SageMaker Training Toolkit.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668561111672,
        "Solution_link_count":2.0,
        "Solution_readability":18.4,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0288888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi team, I am trying to reconfigure queues for one of our agents, and one queue (used by several projects) is full and has several runs queued. Is there a feature to drain a queue, like a button or an API to stop all runs for a specific queue in all projects? (edited)",
        "Challenge_closed_time":1650719577000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650719473000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to reconfigure queues for an agent but is facing an issue with a queue that is full and has several runs queued. They are looking for a feature or API to stop all runs for a specific queue in all projects.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1500",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.1,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0288888889,
        "Challenge_title":"How to stop all runs attached to a specific queue",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Under all runs, you can just filter the table by that queue and then stop the runs using the selection and multi-run action:\n\nIf you use the same name for your queues in all agents you can additionally select the specific agent using the filter:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.5,
        "Solution_reading_time":2.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":45.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.5084697222,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>To back up the code, I want to upload my training code to Wandb every time I run it. Is this possible?<\/p>",
        "Challenge_closed_time":1644912819264,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892988773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to upload their training code to WandB every time they run it to back up the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-upload-code-to-wandb-every-time-i-run-it\/1922",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.5084697222,
        "Challenge_title":"How do I upload code to WandB every time I run it",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":211.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No, wandb does not have an option to store code. Why do you want to save the code?<\/p>\n<ol>\n<li>\n<p>Are you changing the hparams in your code in every run?  - Then you could try using wandb.sweep() instead, as it visualizes your model\u2019s performance for different hparams<\/p>\n<\/li>\n<li>\n<p>Are you using different architectures while training? -   Wandb artifacts logs datasets and model\/training data. There are functions that track all your parameters (wandb. watch() iirc). This leads to an ONNX format of your model being saved.  This ONNX model can be visualized, and you could use that to see what model was trained. Or you could even save a string in your config file with details of the model you\u2019re training<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":8.92,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":85.6601075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1619694485790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619386109403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AzureML ParallelRunStep where all mini batches are run on one node during the ParallelRunStep, no matter how many nodes are put in the node_count config argument. The logs show that all the workers except the one that is working are saying \"FileNotFoundError\" for a side argument created in a previous PythonScriptStep that is being passed to the ParallelRunStep. The user is seeking help to understand why the side argument seems to be available only in one worker while it fails in the others.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":85.6601075,
        "Challenge_title":"AzureML ParallelRunStep runs only on one node",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":31.7,
        "Solution_reading_time":13.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":579.0058813889,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I\u2019ve been trying to find some documentation, I don\u2019t want to save all the hyperparameters each epoch, just the learning rate.<br>\nWould be so great if you can help me out.<\/p>\n<p>Cheers,<\/p>\n<p>Oli<\/p>",
        "Challenge_closed_time":1679602846363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677518425190,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help on how to log the learning rate with PyTorch Lightning when using a scheduler, without saving all the hyperparameters each epoch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-the-learning-rate-with-pytorch-lightning-when-using-a-scheduler\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":579.0058813889,
        "Challenge_title":"How to log the learning rate with pytorch lightning when using a scheduler?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019m also wondering how this is done! Whether within a sweep configuration or not - when using a lr scheduler, I am trying to track the lr at epoch during training, as it is now dynamic. Even within a sweep, you will have some initial lr  determined during the sweep, but it will not stay constant for the duration of training.<\/p>\n<p>edit:<\/p>\n<p>The example on the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/1.2.10\/api\/pytorch_lightning.callbacks.lr_monitor.html#learning-rate-monitor\" rel=\"noopener nofollow ugc\">lightning site here<\/a> worked for me:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateMonitor\n&gt;&gt;&gt; lr_monitor = LearningRateMonitor(logging_interval='step')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_monitor])\n<\/code><\/pre>\n<p>Passing the <code>WandBLogger<\/code> to the trainer I see my lr is logged on the <code>wandb<\/code> dashboard.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":12.09,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1521856385820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":820.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":325.51976,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Challenge_closed_time":1663636318523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662464447387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a wrapper function to log models in MLflow, but the current function only logs models of type lightgbm. The user is seeking a solution to modify the function to log any type of model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73621446",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":325.51976,
        "Challenge_title":"Log Any Type of Model in MLflow",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521856385820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sri Lanka",
        "Poster_reputation_count":820.0,
        "Poster_view_count":165.0,
        "Solution_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.5032686111,
        "Challenge_answer_count":2,
        "Challenge_body":"When calculating the cost of SageMaker Studio Notebooks in [the AWS Pricing Calculator](https:\/\/calculator.aws\/#\/addService\/SageMaker), it asks you for the \"Number of Studio Notebook instances per data scientist per month.\"\n\nHow do you reason about this? What would be the use case for having multiple instances for one data scientist? Would that happen if an individual is working on multiple projects, which have different kernels and library dependencies?\n\nI imagine most of the time it will be 1 Studio Notebook instance per data scientist per month, instead of 2 or more instances per data scientist?",
        "Challenge_closed_time":1670809472327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670602460560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to determine the number of Studio Notebook instances per data scientist when calculating the cost of SageMaker Studio Notebooks in the AWS Pricing Calculator. They are unsure of the use case for having multiple instances for one data scientist and whether it would occur if an individual is working on multiple projects with different kernels and library dependencies. The user assumes that most of the time, it will be one Studio Notebook instance per data scientist per month.",
        "Challenge_last_edit_time":1670949831716,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5kNZTb_yRR6VUo1Reg6lxg\/how-do-you-choose-the-number-of-studio-notebook-instances-per-data-scientist",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":57.5032686111,
        "Challenge_title":"How do you choose the number of Studio Notebook Instances per Data Scientist?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @yann_stoneman, you're right. Up to 4 apps can run on the same instances, so different kernels could still be run on the same instance. For example, a data scientist could be working on a tabular use case, and an image processing use case - so they might have a CPU and GPU instance running. Or they might use a larger instance for data processing or data wrangler feature. \n\nDepending on your data scientists' projects and use cases, I'd account for at most 2 instances per data scientist running concurrently. If your users already use SageMaker Notebook Instances, you can use the commonly used resource type as the Studio instance resource type for estimates - that way you can get a closer estimate to the actual costs. \n\nIf you're allowing for shared spaces (real time collaboration), include additional instances in your estimate - the users will now be able to use a private space through their user profile (unique to one user) and a shared space (this instance can be accessed across profiles). \n\nI'd also recommend using a plugin to shut down idle instances as a best practice when your teams are onboarded to Studio, so these instances are shut down if there are no notebooks actively running (ref: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/)",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1670809472327,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":16.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8547805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I manually stop a run in a sweep so that the sweep agent will just continue with the next run?<\/p>",
        "Challenge_closed_time":1654829989272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654794512062,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to manually cancel a run in a sweep and proceed with the next run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/can-i-manually-cancel-a-run-in-a-sweep\/2583",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":1.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.8547805556,
        "Challenge_title":"Can I manually cancel a run in a sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fryderykkogl\">@fryderykkogl<\/a> ,<\/p>\n<p>Yes, you can manually stop runs directly from the webUI. In the sweeps run table, select the options menu for the run you want to stop, and select  <code>stop run<\/code>. The sweep will continue to the next run configuration. See this image for reference. Please let us know if you have any followup questions.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc.png\" data-download-href=\"\/uploads\/short-url\/bC6fK8EPrbIPohDGGY4pPBG24Ec.png?dl=1\" title=\"Screen Shot 2022-06-09 at 7.57.28 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png\" alt=\"Screen Shot 2022-06-09 at 7.57.28 PM\" data-base62-sha1=\"bC6fK8EPrbIPohDGGY4pPBG24Ec\" width=\"394\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_591x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_788x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-06-09 at 7.57.28 PM<\/span><span class=\"informations\">952\u00d71206 94.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":19.5,
        "Solution_reading_time":26.34,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":115.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":37.2757888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I found a similar posting [here](https:\/\/repost.aws\/questions\/QUEVxelof3TmimoLt1Kd1SBA\/how-to-configure-our-own-inference-py-for-two-different-py-torch-models-in-multi-data-model-to-build-single-endpoint-and-call-both-models-from-there) but I'm hoping my situation is a little simpler.\n\n---\n\n**Q: Is there a way to provide two separate inference scripts for each model in the multi-endpoint or does some dynamic\/custom inference script need to be made to handle both?**\n\nI have two model pipeline built using SageMaker Python SDK Scikit-learn processing\/models:\n* One is a clustering model to return cluster prediction and centroid distances when requesting inferencing\n* Other is simply PCA, returning 3-components when requesting inferencing\n---\n\n\nBecause of the odd formating of the data and the way that the output needs to be provided, both models are using custom inference scripts (e.g. predicting vs. transformation). \n\n\nFrom what I can see in the examples for MultiDataModel, it only accepts a single entry_point for inference.py when passing the model information and just the model artifacts are \"added\" later:\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n---\n\n\nDeployed as separate endpoints, both perform inference as expected, but I cannot get to function as one endpoint.\n\n\nBelow is the most recent error I receive, but it is hard to understand where is failing, seeing as the serialization should be handled properly in my inference script and when invoking the endpoint:\n\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from primary with message \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Unsupported model output data type.\"}\".\n```\n\nAny pointers or alternatives are appreciated!\nThanks",
        "Challenge_closed_time":1679382680431,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678900781216,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has two model pipelines built using SageMaker Python SDK Scikit-learn processing\/models, one is a clustering model and the other is PCA. Both models are using custom inference scripts due to the odd formatting of the data and the way that the output needs to be provided. The user is trying to deploy both models as a single endpoint using MultiDataModel, but is encountering an error related to unsupported model output data type. The user is seeking pointers or alternatives to resolve the issue.",
        "Challenge_last_edit_time":1679248487591,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUVF03ZNbBTL28_SaJ4RotQg\/multidatamodel-with-different-inference-scripts",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.3,
        "Challenge_reading_time":28.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":133.8608930556,
        "Challenge_title":"MultiDataModel with different inference scripts",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":251,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there!\n\nIn short, our SageMaker scikit-learn container do not currently support model specific inference script.\n\nThe entry_point script that you referenced in your MultiDataModel object will be the inference script used for all the models. If you've added logging in your script, you will be able to see them in CloudWatch logs.\n\nIf you have some pre\/post-processing script that needs to be performed on a specific model, you need to write them all in one universal inference.py . You can then add some extra attributes in your data when invoking the endpoint and have the same script read the extra attribute so it knows which pre\/post-processing script to perform.\n\nOne important thing to note is that although you reference a model object in MultiDataModel, i.e.\n\n```\n\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n\n\nThe only information fetched from the model object is the image_uri and entry_point, these information is needed for during endpoint deployment.\n\nAll the model.tar.gz sitting in 'model_data_prefix' should not have a inference.py as it confuses the container and forces it to go back to default handler, hence why you're probably receiving ModelError.\n\n\nTry something like below:\n\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\t#make sure the directory of this prefix is empty, i.e. no models in this location\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n\n\nlist(mme.list_models()) # this should be empty\n\nmme.add_model(model_data_source=cluster_artifact, model_data_path='cluster_artifact.tar.gz')\t#make sure model artifact doesn't contain inference.py\nmme.add_model(model_data_source=pca_artifact, model_data_path='pca_artifact.tar.gz') #make sure model artifact doesn't contain inference.py\n\nlist(mme.list_models()) # there should be two models listed now, if you look at the location of model_data_prefix, there should also be two model artifact\n\noutput_cluster = predictor.predict(data='<your-data>', target_model='cluster_artifact.tar.gz')\nprint(output_cluster) #this should work since it's using the inference.py from cluster_inference.py\n\noutput_pca = predictor.predict(data='<your-data>', target_model='pca_artifact.tar.gz')\nprint(output_pca) \t#this might fail since it's using cluster_inference.py, add this model's inference script into cluster_inference.py to make it work \n\n```\n\nI know this approach is not ideal since if you have a new model with new pre\/post-processing script, you'd have to redeploy your endpoint for the new script to come into effect.\n\nWe actually just added support in our tensorflow container to allow model specific inference script: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/2680\n\nYou can request for the same feature for our scikit container here: https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/issues",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679382680431,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":41.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":365.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535382420716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.9537672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Challenge_closed_time":1641216477460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641209554070,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has been charged for 6 days of AWS Sagemaker Data Wrangler usage, even though they only used it for 3 hours a week ago. The tool seems to have continued running in the background, and the user is unsure how to stop it as they have no endpoints to close. The user is not concerned about the cost but is frustrated with the lack of help from AWS support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9231638889,
        "Challenge_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535382420716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641216587632,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.5733333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Challenge_closed_time":1666184788000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1666114324000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The Neptune ML Export widget is throwing an error when the user tries to export data using a specific command from the Node Classification notebook. The error message states that the credential should be scoped to the correct service, 'execute-api'. The expected behavior is for the export to run to completion.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.5733333333,
        "Challenge_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for opening your first issue here!\n Any solution for this? > Any solution for this?\r\n\r\nThe error message was a bit of a red herring. The actual problem was amundsen-gremlin isn't installed as part of the base image creation `amundsendev\/amundsen-metadata`.\r\n\r\nSolution 1 - add the package to the requirements files and rebuild your own Amundsen image\r\n\r\nSolution 2 - build on the base image and add a `RUN pip install amundsen-gremlin` to your bespoke dockerfile. For my use case I've gone with the latter.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.16,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1655132901000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where the RichProgressBar does not display progress bar when using Comet logger. The issue has been verified to work correctly with tensorboard and wandb. The user has provided a code snippet and environment details for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":1943.8975,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":298,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score_count":null,
        "Solution_sentence_count":29.0,
        "Solution_word_count":312.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":2.2880941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Challenge_closed_time":1548960285576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548952048437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"Out of Memory\" error while using the predefined SageMaker Image Classification algorithm in a training job, even when using a p2.xlarge or p3.2xlarge instance with up to 1TB of memory. The user has tried resizing the images and adjusting hyperparameters, but the error persists.",
        "Challenge_last_edit_time":1549513217232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.2880941667,
        "Challenge_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2360.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361821819380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Puebla",
        "Poster_reputation_count":147.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1548962267732,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":13.54,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":135.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":998.4904036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training an image segmentation model on azure ML pipeline. During the testing step, I'm saving the output of the model to the associated blob storage. Then I want to find the IOU (Intersection over Union) between the calculated output and the ground truth. Both of these set of images lie on the blob storage. However, IOU calculation is extremely slow, and I think it's disk bound. In my IOU calculation code, I'm just loading the two images (commented out other code), still, it's taking close to 6 seconds per iteration, while training and testing were fast enough. <\/p>\n\n<p>Is this behavior normal? How do I debug this step?<\/p>",
        "Challenge_closed_time":1568735787296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568713801943,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is experiencing extremely slow disk I\/O while performing IOU calculation on image segmentation model output and ground truth images stored on Azure blob storage. The user is seeking advice on how to debug this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57971689",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.1070425,
        "Challenge_title":"Disk I\/O extremely slow on P100-NC6s-V2",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>A few notes on the drives that an AzureML remote run has available:<\/p>\n\n<p>Here is what I see when I run <code>df<\/code> on a remote run (in this one, I am using a blob <code>Datastore<\/code> via <code>as_mount()<\/code>):<\/p>\n\n<pre><code>Filesystem                             1K-blocks     Used  Available Use% Mounted on\noverlay                                103080160 11530364   86290588  12% \/\ntmpfs                                      65536        0      65536   0% \/dev\ntmpfs                                    3568556        0    3568556   0% \/sys\/fs\/cgroup\n\/dev\/sdb1                              103080160 11530364   86290588  12% \/etc\/hosts\nshm                                      2097152        0    2097152   0% \/dev\/shm\n\/\/danielscstorageezoh...-620830f140ab 5368709120  3702848 5365006272   1% \/mnt\/batch\/tasks\/...\/workspacefilestore\nblobfuse                               103080160 11530364   86290588  12% \/mnt\/batch\/tasks\/...\/workspaceblobstore\n<\/code><\/pre>\n\n<p>The interesting items are <code>overlay<\/code>, <code>\/dev\/sdb1<\/code>, <code>\/\/danielscstorageezoh...-620830f140ab<\/code> and <code>blobfuse<\/code>:<\/p>\n\n<ol>\n<li><code>overlay<\/code> and <code>\/dev\/sdb1<\/code> are both the mount of the <strong>local SSD<\/strong> on the machine (I am using a STANDARD_D2_V2 which has a 100GB SSD).<\/li>\n<li><code>\/\/danielscstorageezoh...-620830f140ab<\/code> is the mount of the <strong>Azure File Share<\/strong> that contains the project files (your script, etc.). It is also the <em>current working directory<\/em> for your run.<\/li>\n<li><strong><code>blobfuse<\/code><\/strong> is the blob store that I had requested to mount in the <code>Estimator<\/code> as I executed the run.<\/li>\n<\/ol>\n\n<p>I was curious about the performance differences between these 3 types of drives. My mini benchmark was to download and extract this file: <a href=\"http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz\" rel=\"nofollow noreferrer\">http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz<\/a> (it is a 220 MB tar file that contains about 3600 jpeg images of flowers).<\/p>\n\n<p>Here the results:<\/p>\n\n<pre><code>Filesystem\/Drive         Download_and_save       Extract\nLocal_SSD                               2s            2s  \nAzure File Share                        9s          386s\nPremium File Share                     10s          120s\nBlobfuse                               10s          133s\nBlobfuse w\/ Premium Blob                8s          121s\n<\/code><\/pre>\n\n<p>In summary, writing small files is much, much slower on the network drives, so it is highly recommended to use \/tmp or Python <code>tempfile<\/code> if you are writing smaller files. <\/p>\n\n<p>For reference, here the script I ran to measure: <a href=\"https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535<\/a><\/p>\n\n<p>And this is how I ran it: <a href=\"https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572308367396,
        "Solution_link_count":6.0,
        "Solution_readability":11.2,
        "Solution_reading_time":34.57,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.1061447222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with <code>wandb.sweep<\/code>. For now, in order to get the best group of hyper-params, I have to look for the best group on my own and record those params manually. I wonder whether there is a way to extract or collect reuslts of hyper-params automatically by <code>wandb<\/code>?<br>\nThanks a lot!<\/p>",
        "Challenge_closed_time":1682009719808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681937337687,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using wandb.sweep to tune hyper-parameters and is manually recording the best group of hyper-parameters. They are seeking a way to automatically extract or collect results of hyper-parameters using wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/collect-results-from-sweep\/4238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.1061447222,
        "Challenge_title":"Collect results from sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello!<\/p>\n<p>We have <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=G01IM4yVkc6u\" rel=\"noopener nofollow ugc\">Parallel Coordinate plots and Hyper Parameter Importance Plots<\/a> in the UI that can help with looking for the best group! In terms of collecting results of sweeps, the hyperparameters are automatically logged to the <code>config.yaml<\/code> file in your run\u2019s file tab.  However, if you want to collect the hyperparameters  yourself, you can also access individual hyperparameter values using <code>wandb.config['hyperparameter-name']<\/code> within the <code>main()<\/code> function you are running your sweep on. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/config\">Here<\/a> is our documentation on ways to use access and update the config file.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":11.56,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":336.0223277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Challenge_closed_time":1661377323147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660154954560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running a Sagemaker pipeline with two steps, tuning and training, and wants to use the best hyperparameters obtained from the tuning step in the next training step. They have tried to retrieve the hyperparameters using HyperparameterTuningJobAnalytics but are unable to pass them to the next training step's estimator due to an error message. The user is seeking suggestions to resolve this issue.",
        "Challenge_last_edit_time":1660167642767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":339.5468297222,
        "Challenge_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542606952710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    }
]