[
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.6854797222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Challenge_closed_time":1584005785480,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583933260433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created a score.py file and want to pass an argument to it using argparse, but they are unable to find a way to pass arguments. They have shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Challenge_last_edit_time":1584005920356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":20.1458463889,
        "Challenge_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":196,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1584011988083,
        "Solution_link_count":3.0,
        "Solution_readability":35.1,
        "Solution_reading_time":25.25,
        "Solution_score_count":-2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to pass arguments"
    },
    {
        "Answerer_created_time":1492331396980,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":197.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":4174.8991611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started to develop machine learning models on The Microsoft Azure Machine Learning Studio service. The tutorials and information related to this service are rather clear but I am looking for some information that I did not find concerning the deployment of the service.<\/p>\n\n<p>I would like to understand why the input schema requires the definition of the variable to predict and why the output returns all variable fields given in entry. In this response\/request exchange a part of information transmitted is useless. I wondering if it is possible to modify manually this schema.<\/p>\n\n<p>I searched in the configuration tab of the web service panel but I did not find any information to modify the schema passed to the model.<\/p>\n\n<p>The code below is the input schema that the model requires and the value to predict is <code>WallArea<\/code>. It is not really useful to pass this variable because it is the one we try to predict. (except if we want to compare the actual value and the predicted one for test purpose).<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"WallArea\",\n        \"RoofArea\",\n        \"OverallHeight\",\n        \"GlazingArea\",\n        \"HeatingLoad\"\n      ],\n      \"Values\": [\n        [\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>The json returned by the model with the predicted value sent all data. It is much more info to what we really need (\"Scored Label Mean\" and \"Scored Label Standard Deviation\")<\/p>\n\n<pre><code>{\n  \"Results\": {\n    \"output1\": {\n      \"type\": \"DataTable\",\n      \"value\": {\n        \"ColumnNames\": [\n          \"WallArea\",\n          \"RoofArea\",\n          \"OverallHeight\",\n          \"GlazingArea\",\n          \"HeatingLoad\",\n          \"Scored Label Mean\",\n          \"Scored Label Standard Deviation\"\n        ],\n        \"ColumnTypes\": [\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\"\n        ],\n        \"Values\": [\n          [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\"\n          ]\n        ]\n      }\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>My question is how to reduce\/synthesize the input\/output schema if it is possible and why the variable to predict must be sent with the input schema?<\/p>",
        "Challenge_closed_time":1568622249776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568375681780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing machine learning models on Microsoft Azure Machine Learning Studio service and is looking for information on modifying the input\/output schema for deployment. They are wondering why the input schema requires the definition of the variable to predict and if it is possible to modify the schema manually. The user has searched in the configuration tab of the web service panel but did not find any information to modify the schema passed to the model. They are also questioning why the variable to predict must be sent with the input schema and how to reduce\/synthesize the input\/output schema if possible.",
        "Challenge_last_edit_time":1568376011443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57923187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":68.49111,
        "Challenge_title":"How to modify the input\/output schema for an Azure deployment service?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":471.0,
        "Challenge_word_count":292,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492331396980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":197.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>I found the solution.<\/p>\n\n<p>For those who have the same problem, it is pretty simple in fact. You need to add two <strong>Select Columns in Dataset<\/strong> box in your <strong>Predictive experiment<\/strong> schema.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 2020:<\/strong> Following some updates done on the service, the solution proposed is partially broken. Indeed, if you decide to not include the outcome in the first Select columns box, you well not be able to retrieve it in the second <strong><em>Select Column box<\/em><\/strong> leading to an error. To solve that, you have to remove the first Select Column box and take all features. For the second <strong><em>Select Column box<\/em><\/strong> nothing change, you select the features you want for your predictive response.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583405648423,
        "Solution_link_count":2.0,
        "Solution_readability":9.8,
        "Solution_reading_time":11.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"modify input\/output schema"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2751.6177777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Challenge_closed_time":1623275505000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1613369681000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using wandb as it shows steps instead of episodes, making it difficult to compare runs with longer durations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.88,
        "Challenge_repo_contributor_count":89.0,
        "Challenge_repo_fork_count":707.0,
        "Challenge_repo_issue_count":1457.0,
        "Challenge_repo_star_count":3685.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2751.6177777778,
        "Challenge_title":"Getting Errors with wandb sweeps ",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Discussion_body":"I got this error when I tried to run wandb sweeps for a regressionn classifcation. It complained when I included the required num_labels. After removing it, the error is what I get. Are there some additional settings required besides setting regression=True. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty comparing runs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":624.1202777778,
        "Challenge_answer_count":0,
        "Challenge_body":"* IceNet version: 0.2.0.dev10\r\n\r\n`icenet\/model\/train.py` has the wandb.init entity hardcoded, oops\r\n\r\nMake this default to $USER, ICENET_WANDB_USER or be overridden by command line (whichever exists right to left... \ud83d\ude09 )\r\n\r\nDo the same for the project too",
        "Challenge_closed_time":1671706470000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669459637000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to use tensorboard as the default logger and have wandb as an optional feature within their project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/icenet-ai\/icenet\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":12.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":624.1202777778,
        "Challenge_title":"wandb entity is hardcoded",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use tensorboard and wandb"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.3086111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Challenge_closed_time":1634915290000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1634910579000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered two problems while running hyperparameter search using the hydra-optuna-sweeper and wandb logger. The first issue was due to a conflict between the versions of hydra-optuna-sweeper in the requirements.txt file and the latest version installed. The second issue was related to the wandb logger, where the first run was successful, but the second run had an error due to a problem communicating with the wandb process. The user is unsure about the parameters to be passed to the pytorch lighting wrapper to avoid this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.4,
        "Challenge_reading_time":3.53,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":95.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.3086111111,
        "Challenge_title":"wrong wandb dataset file name",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Discussion_body":"feature_68\uc5d0 \uc5c5\ub370\uc774\ud2b8\ub41c \uba54\uc778 \ube0c\ub79c\uce58\uac00 \uc801\uc6a9 \uc548\ub418\uc11c \uadf8\ub7f0\uac83.\r\nPR \uc0dd\uc131\ud574\uc11c \uc218\uc815\ud558\uc790",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"conflicts and communication error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.55,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Challenge_closed_time":1653501835000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653399055000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MLFlow logging in the `compare_models` function of time series. While the parameters are logged, metrics and artifacts are not being logged, causing all runs to fail. However, the user is able to use the `create_model` function without any issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":21.61,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":28.55,
        "Challenge_title":"mlflow ui doesn't show any models",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Discussion_body":"Creating a clean env and installing pycaret again solved the issue.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"metrics not logged"
    },
    {
        "Answerer_created_time":1565215898703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":7.4180927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to deploy an AML Notebook VM via an ARM template? If so, is there an example or documentation somewhere?<\/p>",
        "Challenge_closed_time":1565216213847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565189508713,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to deploy an AML Notebook VM through an ARM template and if there are any examples or documentation available.",
        "Challenge_last_edit_time":1565217649487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57397150",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.4180927778,
        "Challenge_title":"Deploy Notebook VM via ARM Template?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":27,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529439461716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":392.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Unfortunately this is not supported today, but ARM support is in our roadmap<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":1.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy AML Notebook VM"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.5233052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.  <br \/>\nI cannot find log\/user folder with &quot;Logs generated when loading and running user's scripts.&quot;<\/p>\n<p>readme.txt file states:  <br \/>\nParallelRunStep has two major parts:  <\/p>\n<ol>\n<li> Scheduling, progress tracking and file concatenation for append_row.  <\/li>\n<li> Processing mini batch by calling the entry script.  <br \/>\nThe agent manager on each node start agents.  <br \/>\nAn agent gets mini batch and calls the entry script against the mini batch.    The &quot;logs&quot; folder has user, sys and perf sub folders.\n    The user folder includes messages from the entry script in processing mini batches.\n    The sys folder includes messages from #1 and non-entry script log from #2.\n    The perf folder includes periodical checking result of resource usage.<\/li>\n<\/ol>\n<p>In majority case, users can find the processing messages from the user folder.  <br \/>\nUsers need to check sys folder for messages beyond processing mini batches.  <br \/>\nlogs\/  <br \/>\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep  <br \/>\nuser\/ : Logs generated when loading and running user's scripts.  <br \/>\nerror\/ : Logs of errors encountered while loading and running entry script.  <br \/>\nstderr\/ : stderr output of user's scripts.  <br \/>\nstdout\/ : stdout output of user's scripts.  <br \/>\nentry_script_log\/ : Logs generated by loggers of EntryScript()  <br \/>\n&lt;node seq&gt; :  <br \/>\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.<\/p>",
        "Challenge_closed_time":1645670223416,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645621539517,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to locate the user logs folder in Azure Machine Learning while running experiments using ParallelRunStep. The readme.txt file mentions that the user folder includes messages from the entry script in processing mini batches, while the sys folder includes messages from scheduling, progress tracking, and non-entry script logs. The user needs to check the sys folder for messages beyond processing mini batches.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiments-u",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":13.5233052778,
        "Challenge_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":251,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=60ea0bc6-9958-4a72-a07f-c7f2ff477569\">@Calabria Montero, Salvador (SGRE SE D FP&amp;DC WEF)  <\/a> Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to locate user logs"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":708.9712822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:<\/p>\n<pre><code>study0 = optuna.create_study(study_name=study_name,\n                             storage=storage_name,\n                             direction='minimize', \n                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),\n                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',\n                             reduction_factor=4, min_early_stopping_rate=0),\n                             load_if_exists=True)\n<\/code><\/pre>\n<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate<\/code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()<\/code> really do (specially <code>min_early_stopping_rate<\/code>)?\nThanks<\/p>",
        "Challenge_closed_time":1616058414416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613506117800,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Optuna 2.5 to optimize hyperparameters on a tf.keras CNN model and wants to use pruning to skip less promising corners of the hyperparameters space. However, the model sometimes stops after only a few epochs, and the user wants to ensure that the model always trains for at least 30 epochs before being pruned. The user has tried adjusting the \"min_early_stopping_rate\" parameter in the SuccessiveHalvingPruner() function but has not been successful and is seeking clarification on how this parameter works.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66231467",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":15.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":708.9712822222,
        "Challenge_title":"How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":148,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556636382232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p><code>min_resource<\/code>'s explanation on <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.pruners.SuccessiveHalvingPruner.html\" rel=\"nofollow noreferrer\">the documentation<\/a> says<\/p>\n<blockquote>\n<p>A trial is never pruned until it executes <code>min_resource * reduction_factor ** min_early_stopping_rate<\/code> steps.<\/p>\n<\/blockquote>\n<p>So, I suppose that we need to replace the value of <code>min_resource<\/code> with a specific number depending on <code>reduction_factor<\/code> and <code>min_early_stopping_rate<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.0,
        "Solution_reading_time":7.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":44.0,
        "Tool":"Optuna",
        "Challenge_type":"inquiry",
        "Challenge_summary":"adjust pruning parameter"
    },
    {
        "Answerer_created_time":1303910479480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4196.0,
        "Answerer_view_count":67.0,
        "Challenge_adjusted_solved_time":3788.8637213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Challenge_closed_time":1631884865500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618244956103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running experiments on AWS spot instances and sometimes the experiments are stopped. They want to know how to continue logging to the same run and set the run-id of the active run. They have provided pseudocode but it is not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3788.8637213889,
        "Challenge_title":"Continue stopped run in MLflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"continue logging on spot instances"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.1277094445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>I\u2019m implementing W&amp;B into an existing project in which Agent, Model creation and Environment are constructed in classes. The code structure in the Python file (<code>AIAgent.py<\/code>) looks like this:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nconfig = {\n    'layer_sizes': [17, 16, 12, 4],\n    'batch_minsize': 32,\n    'max_memory': 100_000,\n    'episodes': 2,\n    'epsilon': 1.0,\n    'epsilon_decay': 0.998,\n    'epsilon_min': 0.01,\n    'gamma': 0.9,\n    'learning_rate': 0.001,\n    'weight_decay': 0,\n    'optimizer': 'sgd',\n    'activation': 'relu',\n    'loss_function': 'mse'\n}\n\nclass AIAgent:\n    def __init__(self):\n        self.config = config\n        self.pipeline(self.config)\n\n\n    def pipeline(self, config):\n        wandb.init()\n        config = wandb.config\n\n        model, criterion, optimizer = self.make(config)\n        self.train(model, criterion, optimizer, config) \n\n\n    def make(self, config):\n        model = LinearQNet(config).to(device)\n\n        if config['loss_function'] == 'mse':\n            criterion = nn.MSELoss()\n\n        if config['optimizer'] == 'adam':\n            optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=config['weight_decay'], amsgrad=False)\n \n        wandb.watch(model, criterion, log='all', log_freq=1)\n        summary(model)\n\n        return model, criterion, optimizer\n\n\n    def train(self, model, criterion, optimizer, config):\n        for episode in range(1, config['episodes'] + 1):\n            while True:\n                # Where the training is performed\n\n                if done:\n                    if (episode % 1) == 0:\n                        wandb.log({'episode': episode, 'epsilon': epsilon, 'score': score, 'loss': loss_mean, 'reward': reward_mean, 'score_mean': score_mean, 'images': [wandb.Image(img) for img in env_images]}, step=episode})\n                    break\n\n            if episode &lt; config['episodes']:\n                game.game_reset()\n            else:\n                wandb.finish()\n                break\n\n\nclass LinearQNet(nn.Module):\n    def __init__(self, config):\n        super(LinearQNet, self).__init__()\n        self.config = config\n        # Where the NN is configured\n\n\nif __name__ == '__main__':\n    AIAgent.__init__(AIAgent())\n<\/code><\/pre>\n<p>I\u2019m currently initializing the sweep configuration via a .yaml file calling  <code>wandb sweep sweep.yaml<\/code>. The sweep.yaml file looks like this:<\/p>\n<pre><code class=\"lang-auto\">program: AIAgent.py\nproject: evaluation-sweep-1\nmethod: random\nmetric:\n  name: score_mean\n  goal: maximize\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\nparameters:\n  layer_sizes:\n    distribution: constant\n    value: [17, 16, 512, 4]\n  batch_minsize:\n    distribution: int_uniform\n    max: 1024\n    min: 32\n  max_memory:\n    distribution: constant\n    value: 100_000\n  episodes:\n    distribution: constant\n    value: 50\n  epsilon:\n    distribution: constant\n    value: 1.0\n  epsilon_decay:\n    distribution: constant\n    value: 0.995\n  epsilon_min:\n    distribution: constant\n    value: 0.01\n  gamma:\n    distribution: uniform\n    max: 0.99\n    min: 0.8\n  learning_rate:\n    distribution: uniform\n    max: 0.1\n    min: 0.0001  \n  weight_decay:\n    distribution: constant\n    value: 0\n  optimizer:\n    distribution: categorical\n    values: ['sgd', 'adam', 'adamw']\n  activation:\n    distribution: categorical\n    values: ['relu', 'sigmoid', 'tanh', 'leakyrelu']\n  loss_function:\n    distribution: constant\n    value: 'mse'\nearly_terminate:\n  type: hyperband\n  min_iter: 5\n<\/code><\/pre>\n<p>Besides general feedback on the implementation I\u2019m a bit dumbfounded with a current bug. The sweeps run fine and show up in the W&amp;B interface but every sweep is performed twice under the same name of which only the loffing of the first is displayed and the second runs \u2018silently\u2019 in the environment without update of wandb.log. Does anybody have an idea what the reason for this might be?<\/p>\n<p>Thanks,<br>\nTobias<\/p>",
        "Challenge_closed_time":1652885547134,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652719487380,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is implementing W&B into an existing project using classes and sweep configuration. The sweep runs fine, but every sweep is performed twice under the same name, and only the logging of the first is displayed, while the second runs silently without updating wandb.log. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-integration-using-class-and-sweep-running-twice-under-the-same-name\/2433",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":45.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":46.1277094445,
        "Challenge_title":"Wandb integration using class and sweep running twice under the same name",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":320.0,
        "Challenge_word_count":392,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Tobias,<\/p>\n<p>Looks like the source of this bug is this line: <code>AIAgent.__init__(AIAgent())<\/code> which is calling 2 constructors: 1 from <code>AIAgent.__init__()<\/code> and 1 from <code>AIAgent()<\/code>. This, in turn calls <code>pipeline<\/code> twice, which ends up meaning 2 calls to <code>wandb.init()<\/code> and therefore you see 2 runs.<\/p>\n<p>I would suggest changing that line to just <code>AIAgent<\/code> to prevent this error.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":6.14,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":60.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"sweep runs twice"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":49.0995572222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm building a Scikit-learn model on Sagemaker.<\/p>\n\n<p>I'd like to reference the data used in training in my <code>predict_fn<\/code>. (Instead of the indices returned from NNS, I'd like to return the names and data of each neighbor.)<\/p>\n\n<p>I know this can be done by writing\/reading from S3, as in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a> , but was wondering if there were more elegant solutions.<\/p>\n\n<p>Are there other ways to make the data used in the training job available to the prediction function?<\/p>\n\n<p>Edit: Using the advice from the accepted solution I was able to pass data as a dict.<\/p>\n\n<pre><code>model = nn.fit(train_data)\n\nmodel_dict = {\n   \"model\": model,\n   \"reference\": train_data\n}\n\njoblib.dump(model_dict, path)\n<\/code><\/pre>\n\n<p>predict_fn:<\/p>\n\n<pre><code>def predict_fn(input_data, model_dict):\n   model = model_dict[\"model\"]\n   reference = model_dict[\"reference\"]\n<\/code><\/pre>",
        "Challenge_closed_time":1581723595003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581546836597,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a Scikit-learn model on Sagemaker and wants to reference the data used in training in their predict function. They are looking for ways to make the data used in the training job available to the prediction function other than writing\/reading from S3.",
        "Challenge_last_edit_time":1582837925156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60197897",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":16.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":49.0995572222,
        "Challenge_title":"Does Sagemaker pass any data other than the model itself between training and prediction steps?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":130,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539701586583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":623.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz<\/code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn<\/code> or in the <code>output_fn<\/code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)<\/p>\n\n<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":113.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access training data"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":58.1877575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using this simple script, using the example blog post. However, it fails because of <code>wandb<\/code>. It was of no use to make <code>wandb<\/code> OFFLINE as well.<\/p>\n<pre><code>from datasets import load_dataset, load_metric\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          Trainer, TrainingArguments)\nimport wandb\n\n\nwandb.init()\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\ndataset = load_dataset('glue', 'mrpc')\nmetric = load_metric('glue', 'mrpc')\n\ndef encode(examples):\n    outputs = tokenizer(\n        examples['sentence1'], examples['sentence2'], truncation=True)\n    return outputs\n\nencoded_dataset = dataset.map(encode, batched=True)\n\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased', return_dict=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Evaluate during training and a bit more often\n# than the default to be able to prune bad trials early.\n# Disabling tqdm is a matter of preference.\ntraining_args = TrainingArguments(\n    &quot;test&quot;, eval_steps=500, disable_tqdm=True,\n    evaluation_strategy='steps',)\n\ntrainer = Trainer(\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=encoded_dataset[&quot;train&quot;],\n    eval_dataset=encoded_dataset[&quot;validation&quot;],\n    model_init=model_init,\n    compute_metrics=compute_metrics,\n)\n\ndef my_hp_space(trial):\n    return {\n        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True),\n        &quot;weight_decay&quot;: trial.suggest_float(&quot;weight_decay&quot;, 0.1, 0.3),\n        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 10),\n        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 20, 40),\n        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [32, 64]),\n    }\n\n\ntrainer.hyperparameter_search(\n    direction=&quot;maximize&quot;,\n    backend=&quot;optuna&quot;,\n    n_trials=10,\n    hp_space=my_hp_space\n)\n<\/code><\/pre>\n<p><code>Trail 0<\/code> finishes successfully, but next <code>Trail 1<\/code> crashes with following error:<\/p>\n<pre><code>  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 138, in _objective\n    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1376, in train\n    self.log(metrics)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1688, in log\n    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 371, in on_log\n    return self.call_event(&quot;on_log&quot;, args, state, control, logs=logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 378, in call_event\n    result = getattr(callback, event)(\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 754, in on_log\n    self._wandb.log({**logs, &quot;train\/global_step&quot;: state.global_step})\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/preinit.py&quot;, line 38, in preinit_wrapper\n    raise wandb.Error(&quot;You must call wandb.init() before {}()&quot;.format(name))\nwandb.errors.Error: You must call wandb.init() before wandb.log()\n<\/code><\/pre>\n<p>Any help is highly appreciated.<\/p>",
        "Challenge_closed_time":1627224502390,
        "Challenge_comment_count":8,
        "Challenge_created_time":1627015026463,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while performing hyperparameter search on Huggingface with Optuna due to a Wandb error. The error occurs during the second trial and states that Wandb must be initialized before logging. The user has attempted to make Wandb offline but it did not resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68494108",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":21.3,
        "Challenge_reading_time":50.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":58.1877575,
        "Challenge_title":"Hyperparam search on huggingface with optuna fails with wandb error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":532.0,
        "Challenge_word_count":252,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1335098927163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":860.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>Please check running the code on the latest versions of wandb and transformers. Works fine for me with <code>wandb 0.11.0<\/code> and <code>transformers 4.9.0<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Wandb error during hyperparameter search"
    },
    {
        "Answerer_created_time":1587507179987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":0.0376611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Challenge_closed_time":1601906782043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601650547150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user successfully loaded data, trained and deployed a model using AWS SageMaker, and is now trying to connect it to AWS Lambda. The user added data to the IAM policy JSON to give Lambda IAM execution role permission to invoke a model endpoint, but the Lambda function is unable to see the endpoint even with the permission. The error message indicates that the endpoint is not found.",
        "Challenge_last_edit_time":1601906646463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":71.1763591667,
        "Challenge_title":"AWS Sagemaker + AWS Lambda",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":623.0,
        "Challenge_word_count":131,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587507179987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint not found"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":31.6572594445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Challenge_closed_time":1453832406127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453718439993,
        "Challenge_favorite_count":5.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Experiment that works fine, but when deployed and accessed through the front-end of their application, the API-call takes up to 30 seconds to calculate a simple linear regression. The user suspects that it may be due to the experiment still being in a development slot or being run on a slow machine. They are looking for a way to speed up the execution.",
        "Challenge_last_edit_time":1453911336527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":31.6572594445,
        "Challenge_title":"Azure Machine Learning Request Response latency",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1128.0,
        "Challenge_word_count":222,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446116840792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":311.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Solution_comment_count":11.0,
        "Solution_last_edit_time":1453911048927,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":16.44,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":218.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"slow API-call"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":6.9159072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Challenge_closed_time":1472676812496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1472651915230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an \"out of memory\" issue while working with AzureML. They tried to split the code into three different codes, which partially worked. However, there is still a part of the code that is affected by memory issues. The user has created an experiment that works for a subset of their dataset, but when they try to run it for the whole dataset, it produces an error. The user is seeking help to understand where Azure crashes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":6.9159072222,
        "Challenge_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":102.0,
        "Challenge_word_count":165,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":18.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":211.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"out of memory issue"
    },
    {
        "Answerer_created_time":1243547542743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6438.0,
        "Answerer_view_count":922.0,
        "Challenge_adjusted_solved_time":193.0081575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Challenge_closed_time":1449768300887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449073471520,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the Jupyter notebook kernel dies when attempting to create a dummy column for a column with 5,196 unique values. The resulting dataframe should have a shape of (647054, 5196), but the kernel dies almost every time the user runs the code. The user is unsure if this is a Jupyter notebook or pandas bug and has tried running the code on a machine with >100 GB of RAM with no success.",
        "Challenge_last_edit_time":1454300528203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":193.0081575,
        "Challenge_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5063.0,
        "Challenge_word_count":177,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373475615300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":2037.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":8.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"kernel dies when creating dummy column"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2472222222,
        "Challenge_answer_count":1,
        "Challenge_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Challenge_closed_time":1603455348000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603454458000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. They are looking for a way to restrict the instance size options available through SageMaker, such as removing the ability to launch ml.p3.16xlarge instances, using IAM policies or another method.",
        "Challenge_last_edit_time":1668589944840,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2472222222,
        "Challenge_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":966.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\n**Note:** This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"EnforceInstanceType\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"sagemaker:CreateTrainingJob\",\n                    \"sagemaker:CreateHyperParameterTuningJob\"\n                ],\n                \"Resource\": \"*\",\n                \"Condition\": {\n                    \"ForAllValues:StringLike\": {\n                        \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                    }\n                }\n            }\n    \n         ]\n    }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925584228,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"restrict instance types"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":80.3919036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a model quality monitor job, using the class ModelQualityMonitor from Sagemaker model_monitor, and i think i have all the import statements defined yet i get the message cannot import name error<\/p>\n<pre><code>from sagemaker import get_execution_role, session, Session\nfrom sagemaker.model_monitor import ModelQualityMonitor\n                \nrole = get_execution_role()\nsession = Session()\n\nmodel_quality_monitor = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=session\n)\n<\/code><\/pre>\n<p>Any pointers are appreciated<\/p>",
        "Challenge_closed_time":1608016613856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607727203003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to create a model quality monitor job using the class ModelQualityMonitor from Sagemaker model_monitor. The user has imported all the necessary statements, but is receiving an ImportError message stating that the name 'ModelQualityMonitor' cannot be imported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65259702",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":80.3919036111,
        "Challenge_title":"AWS sagemaker model monitor- ImportError: cannot import name 'ModelQualityMonitor'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":544.0,
        "Challenge_word_count":71,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546959992036,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3<\/code> Amazon SageMaker notebook, I don't get any errors at all.<\/p>\n<p>Example screenshot output showing no errors:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined<\/code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker<\/code> and then see if this resolves your error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ImportError message"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":62.2546263889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong><em>Problem:<\/em><\/strong><\/p>\n\n<p>Jobs repeatedly fail after 5 minutes with the error<\/p>\n\n<blockquote>\n  <p>ClientError: .lst file missing in the train_lst channel.<\/p>\n<\/blockquote>\n\n<p><strong><em>Context:<\/em><\/strong><\/p>\n\n<p>Working within the AWS console, I have a binary classification task of images. I have labeled the classes in their filenames, per a guide.<\/p>\n\n<p>Eventually I started hitting errors that revealed that for this particular algorithm a <code>.lst<\/code> file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file.<\/p>\n\n<p><strong><em>Example Data:<\/em><\/strong><\/p>\n\n<p>I am trying to match the examples I see on <a href=\"https:\/\/stackoverflow.com\/questions\/51670563\/invalid-lst-file-in-sagemaker\">StackOverflow<\/a> and elsewhere online. The current iteration of <code>trn_list.lst<\/code> looks like this:<\/p>\n\n<pre><code>292 \\t 1 \\t dog-292.jpeg\n214 \\t 1 \\t dog-214.jpeg\n290 \\t 0 \\t cat-290.jpeg\n288 \\t 1 \\t dog-288.jpeg\n160 \\t 1 \\t dog-160.jpeg\n18 \\t 0 \\t cat-18.jpeg\n215 \\t 1 \\t dog-215.jpeg\n254 \\t 1 \\t dog-254.jpeg\n53 \\t 1 \\t dog-53.jpeg\n337 \\t 0 \\t cat-337.jpeg\n284 \\t 0 \\t cat-284.jpeg\n177 \\t 1 \\t dog-177.jpeg\n192 \\t 1 \\t dog-192.jpeg\n228 \\t 0 \\t cat-228.jpeg\n305 \\t 0 \\t cat-305.jpeg\n258 \\t 1 \\t dog-258.jpeg\n75 \\t 0 \\t cat-75.jpeg\n148 \\t 0 \\t cat-148.jpeg\n268 \\t 1 \\t dog-268.jpeg\n281 \\t 1 \\t dog-281.jpeg\n24 \\t 1 \\t dog-24.jpeg\n328 \\t 1 \\t dog-328.jpeg\n99 \\t 1 \\t dog-99.jpeg\n<\/code><\/pre>\n\n<p>The bucket has no sub-folders, so I just put the .lst on the <\/p>\n\n<p>In one iteration I allowed my R program that creates the .lst to replace the <code>\\t<\/code> with actual tabs when it writes it out. In other iterations I left the actual delimiters (<code>\\t<\/code>) in there. Didn't seem to affect it (?).<\/p>",
        "Challenge_closed_time":1559907535367,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559761618273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a problem with SageMaker where jobs repeatedly fail after 5 minutes with the error \"ClientError: .lst file missing in the train_lst channel.\" The user is working on a binary classification task of images and has labeled the classes in their filenames. However, the user discovered that for this particular algorithm a .lst file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file. The user has tried to match the examples they found online but is still encountering the same error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56466592",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":3.8,
        "Challenge_reading_time":23.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":40.5325261111,
        "Challenge_title":"SageMaker: ClientError: .lst file missing in the train_lst channel. (customized image classification)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":286,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399301338467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbia, MD, USA",
        "Poster_reputation_count":21413.0,
        "Poster_view_count":6465.0,
        "Solution_body":"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. <\/p>\n\n<p>The \"magic\" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.<\/p>\n\n<p>Here is the documentation of the definition of a channel in SageMaker: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html<\/a><\/p>\n\n<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train<\/code>, <code>validation<\/code>, <code>train_lst<\/code>, and <code>validation_lst<\/code> channels as values for the <code>InputDataConfig<\/code> parameter of the <code>CreateTrainingJob<\/code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application\/x-image<\/code>.<\/p>\n\n<p>See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1559985734928,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":24.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":240.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing .lst file"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.6668433334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi :  <\/p>\n<p>I am planing to use k-means to form algorithm to do project. However, I am aware that there are certain shortcomings to find the optimal groups using k-means.  <\/p>\n<p>Could you please tell the limitation and provide me with a detailed example?  <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1647898847903,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647856847267,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is planning to use k-means algorithm for a project but is aware of its limitations in finding optimal groups. They are seeking information on the limitations and a detailed example.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/780362\/machine-learning-algorithms-questions",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":3.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.6668433334,
        "Challenge_title":"machine learning algorithms questions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @hideonbush again,  <\/p>\n<p>Generally to think about k-means, please refer to below cons and pros. If you can provide more details and how you want to develop your project, I can share more:  <\/p>\n<p>Pros:  <\/p>\n<ul>\n<li> K-means is very simple, highly flexible, and efficient.   <\/li>\n<li> Easy to adjust and interpret the clustering results. Easy to explain the results in contrast to Neural Networks.  <\/li>\n<li> The efficiency of k-means implies that the algorithm is good at segmenting a dataset.  <\/li>\n<li> An instance can change cluster (move to another cluster) when the centroids are recomputed  <\/li>\n<\/ul>\n<p>Cons  <\/p>\n<ul>\n<li> It does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them.   <\/li>\n<li> When doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the center of the clusters is recalculated until an adequate &quot;center'' is found for the number of clusters requested.  <\/li>\n<li> The order of the data input has an impact on the final results.  <\/li>\n<\/ul>\n<p>Hope this helps!  <\/p>\n<p>Regards,  <br \/>\nYutong  <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":19.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":265.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"k-means limitations and example"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.8982075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to perform the random sampling to accomplish the hyper parameter tuning and tuning parameter version 1 (v1). I would like to get the chance to define the algorithm as sampling algorithm explicitly.<\/p>\n<p>Currently using the below code block and is there any chance of implementing explicitly defining sampling in V1? If not, any specific procedure to solve the issue is helpful.<\/p>\n<pre><code>from azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import normal, uniform, choice\nparam_sampling = RandomParameterSampling( {\n        &quot;learning_rate&quot;: normal(10, 3),\n        &quot;keep_probability&quot;: uniform(0.05, 0.1),\n        &quot;batch_size&quot;: choice(16, 32, 64, 128)\n    }\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1659100027590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659096794043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to define a sampling algorithm explicitly for hyper parameter tuning using random sampling in version V1. They have provided a code block and are seeking a solution to the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73166561",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.8982075,
        "Challenge_title":"Error while defining sampling algorithm in hyper parameter tuning using random sampling - Version V1",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651094469216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>There is an explicit procedure called a <strong>sweep job<\/strong>. This sweep job in <strong>hyperparameter value<\/strong>. We can mention the random sampling using the sweep job explicitly.<\/p>\n<p>From azure.ai.ml.sweep import Normal, Uniform, RandomParameterSampling<\/p>\n<pre><code>Command_job_for_sweep = command_job(\n    learning_rate = Normal(mu=value, sigma=value),\n    keep_probability=Uniform(min_value= your value, max_value= value),\n    batch_size = Choice(value=[.your values in list]),\n)\n\nSweep_job = command_job_sweep.sweep(\n    Computer =\u201dcluster\u201d,\n    sampling_algorithm=\u201drandom\u201d,\n    ....\n)\n<\/code><\/pre>\n<p>This will be available in <strong>version 2 (v2)<\/strong> of hyperparameter tuning in random sampling.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":9.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in hyperparameter tuning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1621374020000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with importing `CometLogger` due to a recent refactoring of logger imports. The `comet_ml` library needs to be imported before `torch` and `tensorboard` for it to work properly. However, since the refactoring, `torch` is now imported before `comet_ml` in `loggers\/comet.py`, which forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the `ImportError`. The expected behavior is for the `comet_ml` import inside `loggers\/comet.py` to come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_participation_count":7,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":2840.7811111111,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":325,
        "Discussion_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import order issue"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.4997222222,
        "Challenge_answer_count":0,
        "Challenge_body":"I have setup a community self-hosted polyaxon, and the config abourt ui is\n\nui:\n  enabled: true\n  offline: false\n  adminEnabled: true\n\n\naccording to the documentation, if I set adminEnabled to true, there should be an admin dashboard. But I did not find it, there is no difference to turn it on\/off",
        "Challenge_closed_time":1648196727000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648187728000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has set up a community self-hosted Polyaxon and enabled the admin dashboard in the UI configuration. However, they are unable to find the admin dashboard despite following the documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1460",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.4997222222,
        "Challenge_title":"Does Community UI has Admin dashboard?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The admin page is under http:\/\/localhost:8000\/_admin\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":0.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing admin dashboard"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.0410916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am encountering an issue of error 0138, while training the data, at the end it shows memory has been exhausted exception  <\/p>\n<p>I do not think my data has exceed the limit of azure ML studio, is there any way to solve this?<\/p>",
        "Challenge_closed_time":1653942581987,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653902834057,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an error 0138 while training data, which indicates a memory outage. They are unsure why this is happening as they believe their data has not exceeded the limit of Azure ML studio. They are seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869594\/memory-outage-while-running-module",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":3.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":11.0410916667,
        "Challenge_title":"memory outage while running module",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=d4454683-c6fe-4103-a1a8-d167ab8d04de\">@darya  <\/a>     <\/p>\n<p>Thanks for reaching out to us. This issue seldoms happen.  Could you please share your structure to us and how is your dataset size? Based on the error info, too many steps in your experiment may cause that.     <\/p>\n<p>I would suggest you try to remove some unnecessary one to try and see. If you believe your structure is reasonable, please share it to us. But it should be fine if you have not put too much.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer to help the community if you feel helpful, thanks.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"memory outage error"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":2.2880941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Challenge_closed_time":1548960285576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548952048437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"Out of Memory\" error while using the predefined SageMaker Image Classification algorithm in a training job, even when using a p2.xlarge or p3.2xlarge instance with up to 1TB of memory. The user has tried resizing the images and adjusting hyperparameters, but the error persists.",
        "Challenge_last_edit_time":1549513217232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.2880941667,
        "Challenge_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2360.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361821819380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Puebla",
        "Poster_reputation_count":147.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1548962267732,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":13.54,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":135.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"out of memory error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":0,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a ResourceLimitExceeded error when running code cell [17] to create an endpoint to host the model while walking through the SageMaker Studio tour for the first time in a new AWS account. The error occurred due to the account-level service limit 'ml.m4.xlarge for endpoint usage' being 0 Instances. The user suggests that the Prerequisites section could address this proactively with a link to the service limit increase page or the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of 0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":98.0,
        "Challenge_repo_fork_count":280.0,
        "Challenge_repo_issue_count":295.0,
        "Challenge_repo_star_count":242.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Discussion_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"service limit exceeded"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":328.67735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently deploying a model trained using AzureML to an AKS cluster as follows:<\/p>\n<pre><code>deployment_config_aks = AksWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1)\n\nservice = Model.deploy(ws, &quot;test&quot;, [model], inference_config, deployment_config_aks, aks_target)\n\n<\/code><\/pre>\n<p>I would like this service to be scheduled on a specific nodepool. With normal Kubernetes deployment, I can specify a <code>nodeSelector<\/code> like:<\/p>\n<pre><code>spec:\n  nodeSelector:\n    myNodeName: alpha\n<\/code><\/pre>\n<p>How do I specify a <code>nodeSelector<\/code> while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>",
        "Challenge_closed_time":1650522075520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649338837060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an Azure ML model to an AKS cluster and wants to schedule the service on a specific nodepool using a nodeSelector. They are unsure of how to specify a nodeSelector while deploying an Azure ML model to an AKS Cluster and are also looking for a way to merge their custom pod spec with the one generated by Azure ML library.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71783228",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.0,
        "Challenge_reading_time":10.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":328.67735,
        "Challenge_title":"How do I specify nodeSelector while deploying an Azure ML model to an AKS Cluster?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338974093052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beijing, China",
        "Poster_reputation_count":1830.0,
        "Poster_view_count":155.0,
        "Solution_body":"<blockquote>\n<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>\n<\/blockquote>\n<p>As per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\" rel=\"nofollow noreferrer\">Configure Kubernetes clusters for machine learning<\/a>:<\/p>\n<p><code>nodeSelector<\/code> : Set the node selector so the extension components and the training\/inference workloads will only be deployed to the nodes with all specified selectors.<\/p>\n<p>For example:<\/p>\n<p><code>nodeSelector.key=value<\/code> , <code>nodeSelector.node-purpose=worker<\/code> and <code>nodeSelector.node-region=eastus<\/code><\/p>\n<p>You can refer to <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/scheduling-eviction\/assign-pod-node\/#built-in-node-labels\" rel=\"nofollow noreferrer\">Assigning Pods to Nodes<\/a> and <a href=\"https:\/\/github.com\/Azure\/AKS\/issues\/2866\" rel=\"nofollow noreferrer\">Cannot create nodepool with node-restriction.kubernetes.io\/ prefix label<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":14.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy to specific nodepool"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.0968075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>ML studio is, by default picking up Python 3.6 kernel, even when I'm specifying use Python 3.8 AzureML kernel. In UI, it's changed but not actually.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/183312-image.png?platform=QnA\" alt=\"183312-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1647451118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647349970220,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Studio where it defaults to Python 3.6 kernel instead of the specified Python 3.8 AzureML kernel. The UI shows the change but it does not actually work. The user is unable to create a Microsoft ticket under MSDN and is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/772790\/azure-ml-studio-is-bugged-out-and-can-not-create-a",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":28.0968075,
        "Challenge_title":"Azure ML Studio is bugged out and can not create a Microsoft ticket under MSDN. Need a few suggestions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out.  It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:  <\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>Hope this helps!<\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect kernel default"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### Version\r\n\r\n23.01\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\nDocker\r\n\r\n### Describe the bug.\r\n\r\nUnable to start the mlflow server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\nDowngrading  mlflow version to `<1.29.0` works fine.\r\n\r\n\r\n### Minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/Morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up mlflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[+] Running 3\/3                                                                                                                                               \r\n \u283f Network production_backend      Created                                                                                                               0.0s \r\n \u283f Network production_frontend     Created                                                                                                               0.0s\r\n \u283f Container mlflow_server  Created                                                                                                               0.1s\r\nAttaching to mlflow_server\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Error initializing backend store\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server  | Traceback (most recent call last):\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 392, in server\r\nmlflow_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 265, in initialize_backend_stores\r\nmlflow_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 244, in _get_tracking_store\r\nmlflow_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\nmlflow_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\nmlflow_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\nmlflow_server  |     return SqlAlchemyStore(store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\nmlflow_server  |     mlflow.store.db.utils._verify_schema(self.engine)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 71, in _verify_schema\r\nmlflow_server  |     raise MlflowException(\r\nmlflow_server  | mlflow.exceptions.MlflowException: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server exited with code 1\r\n```\r\n\r\n\r\n### Full env printout\r\n\r\n```shell\r\n<details><summary>Click here to see environment details<\/summary><pre>\r\n     \r\n     **git***\r\n     commit 9619c0e3a5ddbdd476aba9331f288aac855da7cd (HEAD -> dfp-pipeline-module, origin\/dfp-pipeline-module)\r\n     Author: bsuryadevara <bhargavsuryadevara@gmail.com>\r\n     Date:   Wed Nov 30 17:13:05 2022 -0600\r\n     \r\n     used dill to persist source and preprocess schema\r\n     **git submodules***\r\n     -27efc4fd1c984332920db2a2d6ab1f84d3cb55cd external\/morpheus-visualizations\r\n     \r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU7140058-R1\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.6 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\n     SUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\n     BUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\n     PRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx04 4.15.0-162-generic #170-Ubuntu SMP Mon Oct 18 11:38:05 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\r\n     \r\n     ***GPU Information***\r\n     Thu Dec  1 17:37:05 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    56W \/ 300W |  11763MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    43W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N\/A   28C    P0    41W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N\/A   29C    P0    44W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N\/A   31C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     \r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N\/A  N\/A     31232      C   ...da\/envs\/rapids\/bin\/python      303MiB |\r\n     |    0   N\/A  N\/A     41206      C   ...da\/envs\/rapids\/bin\/python     7051MiB |\r\n     |    0   N\/A  N\/A     52497      C   ...nda3\/envs\/venv\/bin\/python     3137MiB |\r\n     |    0   N\/A  N\/A     55973      C   tritonserver                     1267MiB |\r\n     +-----------------------------------------------------------------------------+\r\n     \r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3267.078\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.17\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n     \r\n     ***CMake***\r\n     \/usr\/bin\/cmake\r\n     cmake version 3.10.2\r\n     \r\n     CMake suite maintained and supported by Kitware (kitware.com\/cmake).\r\n     \r\n     ***g++***\r\n     \/usr\/bin\/g++\r\n     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n     Copyright (C) 2017 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n     \r\n     \r\n     ***nvcc***\r\n     \/usr\/local\/cuda\/bin\/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5\/compiler.30672275_0\r\n     \r\n     ***Python***\r\n     \/usr\/bin\/python\r\n     Python 2.7.17\r\n     \r\n     ***Environment Variables***\r\n     PATH                            : \/usr\/local\/cuda\/bin:\/opt\/bin\/:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin:\/home\/nfs\/bsuryadevara:\/home\/nfs\/bsuryadevara\r\n     LD_LIBRARY_PATH                 :\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    :\r\n     PYTHON_PATH                     :\r\n     \r\n     conda not found\r\n     ***pip packages***\r\n     \/usr\/bin\/pip\r\n\/usr\/lib\/python2.7\/dist-packages\/OpenSSL\/crypto.py:12: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\r\n  from cryptography import x509\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\n     ansible (2.9.9)\r\n     asn1crypto (0.24.0)\r\n     backports.functools-lru-cache (1.6.4)\r\n     backports.shutil-get-terminal-size (1.0.0)\r\n     bcrypt (3.1.7)\r\n     beautifulsoup4 (4.9.3)\r\n     boto3 (1.17.112)\r\n     botocore (1.20.112)\r\n     bs4 (0.0.1)\r\n     certifi (2018.1.18)\r\n     cffi (1.11.5)\r\n     chardet (3.0.4)\r\n     click (7.1.2)\r\n     configparser (4.0.2)\r\n     contextlib2 (0.6.0.post1)\r\n     cryptography (3.3.2)\r\n     decorator (4.1.2)\r\n     defusedxml (0.6.0)\r\n     distro (1.6.0)\r\n     dnspython (1.15.0)\r\n     docker (4.4.4)\r\n     docopt (0.6.2)\r\n     enum34 (1.1.10)\r\n     fastrlock (0.8)\r\n     flake8 (3.9.2)\r\n     functools32 (3.2.3.post2)\r\n     futures (3.3.0)\r\n     gssapi (1.4.1)\r\n     gyp (0.1)\r\n     html-to-json (2.0.0)\r\n     html2text (2019.8.11)\r\n     html5lib (0.999999999)\r\n     http (0.2)\r\n     httplib2 (0.14.0)\r\n     httpserver (1.1.0)\r\n     idna (2.6)\r\n     importlib-metadata (2.1.3)\r\n     ipaclient (4.6.90rc1+git20180411)\r\n     ipaddress (1.0.17)\r\n     ipalib (4.6.90rc1+git20180411)\r\n     ipaplatform (4.6.90rc1+git20180411)\r\n     ipapython (4.6.90rc1+git20180411)\r\n     Jinja2 (2.10)\r\n     jmespath (0.10.0)\r\n     lxml (4.2.1)\r\n     MarkupSafe (1.0)\r\n     mccabe (0.6.1)\r\n     netaddr (0.7.19)\r\n     netifaces (0.10.4)\r\n     numpy (1.16.6)\r\n     ofed-le-utils (1.0.3)\r\n     olefile (0.45.1)\r\n     pandas (0.24.2)\r\n     paramiko (2.11.0)\r\n     pathlib2 (2.3.7.post1)\r\n     Pillow (5.1.0)\r\n     pip (9.0.1)\r\n     ply (3.11)\r\n     pyasn1 (0.4.2)\r\n     pyasn1-modules (0.2.1)\r\n     pycodestyle (2.7.0)\r\n     pycparser (2.18)\r\n     pycrypto (2.6.1)\r\n     pyflakes (2.3.1)\r\n     pygobject (3.26.1)\r\n     PyNaCl (1.4.0)\r\n     pyOpenSSL (17.5.0)\r\n     python-apt (1.6.5+ubuntu0.7)\r\n     python-augeas (0.5.0)\r\n     python-dateutil (2.8.2)\r\n     python-dotenv (0.18.0)\r\n     python-ldap (3.0.0)\r\n     python-yubico (1.3.2)\r\n     pytz (2022.4)\r\n     pyusb (1.0.0)\r\n     PyYAML (5.4.1)\r\n     qrcode (5.3)\r\n     requests (2.27.1)\r\n     s3fs (0.2.2)\r\n     s3transfer (0.4.2)\r\n     scandir (1.10.0)\r\n     setuptools (39.0.1)\r\n     six (1.16.0)\r\n     soupsieve (1.9.6)\r\n     splunk-sdk (1.7.2)\r\n     subprocess32 (3.5.4)\r\n     tqdm (4.60.0)\r\n     typing (3.10.0.0)\r\n     urllib3 (1.26.12)\r\n     webencodings (0.5)\r\n     yapf (0.32.0)\r\n     zipp (1.2.0)\r\n     \r\n<\/pre><\/details>\r\n```\r\n\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Challenge_closed_time":1669922495000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1669916494000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with logging modified parameters on Mlflow while using Hydra for parameter modification during experiment runs. The default parameters set with Hydra are logged correctly, but the modified parameters are not being logged on Mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/512",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":155.99,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":61.0,
        "Challenge_repo_issue_count":962.0,
        "Challenge_repo_star_count":176.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":167,
        "Challenge_solved_time":1.6669444444,
        "Challenge_title":"[BUG]: Unable to Start DFP Production MLFlow Server",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1151,
        "Discussion_body":"yeah, MLflow recently has started enforcing db schema checks at startup\r\n After removing docker volumes that are related to MLFlow and restarted `mlflow_server`. Now it's working as expected.\r\n```\r\ndocker volume rm production_mlflow_data\r\ndocker volume rm production_db_data\r\n```\r\n\r\n```\r\nmlflow_server        |   worker_int: <function WorkerInt.worker_int at 0x7faaa8f90dc0>\r\nmlflow_server        |   worker_abort: <function WorkerAbort.worker_abort at 0x7faaa8f90ee0>\r\nmlflow_server        |   pre_exec: <function PreExec.pre_exec at 0x7faaa8fa6040>\r\nmlflow_server        |   pre_request: <function PreRequest.pre_request at 0x7faaa8fa6160>\r\nmlflow_server        |   post_request: <function PostRequest.post_request at 0x7faaa8fa61f0>\r\nmlflow_server        |   child_exit: <function ChildExit.child_exit at 0x7faaa8fa6310>\r\nmlflow_server        |   worker_exit: <function WorkerExit.worker_exit at 0x7faaa8fa6430>\r\nmlflow_server        |   nworkers_changed: <function NumWorkersChanged.nworkers_changed at 0x7faaa8fa6550>\r\nmlflow_server        |   on_exit: <function OnExit.on_exit at 0x7faaa8fa6670>\r\nmlflow_server        |   proxy_protocol: False\r\nmlflow_server        |   proxy_allow_ips: ['127.0.0.1']\r\nmlflow_server        |   keyfile: None\r\nmlflow_server        |   certfile: None\r\nmlflow_server        |   ssl_version: 2\r\nmlflow_server        |   cert_reqs: 0\r\nmlflow_server        |   ca_certs: None\r\nmlflow_server        |   suppress_ragged_eofs: True\r\nmlflow_server        |   do_handshake_on_connect: False\r\nmlflow_server        |   ciphers: None\r\nmlflow_server        |   raw_paste_global_conf: []\r\nmlflow_server        |   strip_header_spaces: False\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Starting gunicorn 20.1.0\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [DEBUG] Arbiter booted\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Listening at: http:\/\/0.0.0.0:5000 (30)\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Using worker: sync\r\n```",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"modified parameters not logged"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":936.1641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1619815518000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1616445327000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the MLflow logger where the log_param() function requires a 'run_id' argument, which is not consistent with the behavior of the mlflow API. The user has provided a code sample and the environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.86,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":936.1641666667,
        "Challenge_title":"External MLFlow logging failures cause training job to fail",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":224,
        "Discussion_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Discussion_score_count":-1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"inconsistent behavior"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":243.16,
        "Challenge_answer_count":0,
        "Challenge_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Challenge_closed_time":1654438640000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1653563264000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that models with crossval_count greater than 1 automatically switch to train on AzureML even if the user overrides --azureml=False. This behavior is confusing and the runner should fail if there are contradicting parameters. Additionally, the histopathology.DeepSMILECrck model is not trainable because it does not have a default encoder type, and there is a question of whether base classes should be flagged as not trainable and throw an error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":19.4,
        "Challenge_reading_time":114.38,
        "Challenge_repo_contributor_count":69.0,
        "Challenge_repo_fork_count":439.0,
        "Challenge_repo_issue_count":1880.0,
        "Challenge_repo_star_count":588.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":243.16,
        "Challenge_title":"Can not add AzureML extention on openshift cluster ",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":533,
        "Discussion_body":"Hey friend! Thanks for opening this issue. We appreciate your contribution and welcome you to our community! We are glad to have you here and to have your input on the Azure Arc Jumpstart.<p><\/p> Hello Johan, thanks for submitting feedback. Have you checked the [prerequisites specific to ARO](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-kubernetes-anywhere?tabs=studio#prerequisites) prior to attempting the extension installation?  Hello @rataxe , have you checked the pre-reqs above. If you already had and still facing a problem, we recommend you open a support case as this is not strictly related to the Jumpstart project but to the product itself. ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"conflicting parameters"
    },
    {
        "Answerer_created_time":1310482059760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":10753.0,
        "Answerer_view_count":895.0,
        "Challenge_adjusted_solved_time":265.4097633333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Challenge_closed_time":1588575738648,
        "Challenge_comment_count":3,
        "Challenge_created_time":1587620263500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble finding the EC2 instances being used by their Sagemaker endpoint and is looking for a way to monitor their usage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":265.4097633333,
        "Challenge_title":"Sagemaker usage of EC2 instances",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":615.0,
        "Challenge_word_count":41,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.9,
        "Solution_reading_time":6.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"monitor EC2 usage"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.88302,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Challenge_closed_time":1676070775536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676042396664,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to rerun a few runs in a wandb sweep, but after deleting them from the dashboard, wandb is not able to rerun those runs even after running the command again. The user suggests adding a feature to rerun the deleted runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.8,
        "Challenge_reading_time":5.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.88302,
        "Challenge_title":"Rerun a deleted run in wandb sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"rerun deleted runs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.0937622222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I've created an azure for students account wiht $100 free credit and started using Azure Notebooks to train some ML models. I've created a GPU instance which costs $1.20\/hr. I've been using it for at least 1.5h now and what's weird is that no usage is being shown on my dashboard, and on the sponsorship page it's showing that it is not active and that I haven't used any of my credit:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239817-image.png?platform=QnA\" alt=\"239817-image.png\" \/>    <\/p>\n<p>On the other hand when I go to my subscriptions it says it's active:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239809-image.png?platform=QnA\" alt=\"239809-image.png\" \/>    <\/p>\n<p>Is something wrong or does it take a while to see usage statistics\/credit spending?    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1662904704207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662893566663,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created an Azure for Students account with $100 free credit and has been using Azure Notebooks to train ML models on a GPU instance costing $1.20\/hr. However, despite using it for at least 1.5 hours, no usage is being shown on the dashboard and the sponsorship page shows that it is not active and no credit has been used. The subscriptions page shows that it is active. The user is unsure if something is wrong or if it takes time to see usage statistics\/credit spending.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1002201\/azure-for-students-showing-no-usage-despite-using",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.0937622222,
        "Challenge_title":"Azure for students showing no usage despite using it",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":125,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>Usually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/understand\/download-azure-daily-usage\">download-azure-daily-usage<\/a>    <\/p>\n<p>==    <br \/>\nPlease &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no usage shown"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":17.8840719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Challenge_closed_time":1572381894847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572325608637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy 50 NLP models on Azure Container Instances via the Azure Machine Learning service and is trying to write a generic score.py entry file to pass in the model name as a parameter. However, the interface method signature does not allow a parameter in the init() method of score.py, so the user moved the model loading into the run method. The user is looking for a way to pass in some value to the init() method of the container to tell it what model to load.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.6350583334,
        "Challenge_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":851.0,
        "Challenge_word_count":195,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572389991296,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":58.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":40.0,
        "Solution_word_count":476.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to pass parameter in init() method"
    },
    {
        "Answerer_created_time":1350376620496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colombo, Sri Lanka",
        "Answerer_reputation_count":207537.0,
        "Answerer_view_count":32114.0,
        "Challenge_adjusted_solved_time":0.0204122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a free trial account on MS Azure and I'm following this tutorial.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>I'm stuck when I try to \"submit the pipeline\".<\/p>\n\n<p>The reason seems to be that I can't create a compute instance or a training cluster on a free plan.\nI still have 200USDs of free credits. I guess there must be a solution?<\/p>\n\n<hr>\n\n<p>Error messages:<\/p>\n\n<pre><code>Invalid graph: The pipeline compute target is invalid.\n\n400: Compute Test3 in state Failed, which is not able to use\n\nCompute instance: creation failed\nThe specified subscription has a total vCPU quota of 0 and is less than the requested compute training cluster and\/or compute instance's min nodes of 1 which maps to 4 vCPUs\n<\/code><\/pre>",
        "Challenge_closed_time":1586681759587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586681686103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in submitting a pipeline while using a free trial account on MS Azure due to the inability to create a compute instance or a training cluster on a free plan. The error message indicates that the pipeline compute target is invalid and the specified subscription has a total vCPU quota of 0, which is less than the requested compute training cluster and\/or compute instance's min nodes of 1.",
        "Challenge_last_edit_time":1586690281396,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61168984",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":12.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0204122222,
        "Challenge_title":"Azure ML free trial: how to submit pipeline?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343682543648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":135.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Please check the announcement from MS Team regarding this:<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/<\/a><\/p>\n\n<p>All the free trials will not work as of now<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.4,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid compute target"
    },
    {
        "Answerer_created_time":1458983415440,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cluj-Napoca, Romania",
        "Answerer_reputation_count":12439.0,
        "Answerer_view_count":1795.0,
        "Challenge_adjusted_solved_time":30.66359,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Challenge_closed_time":1659781694627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659671305703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up a trainer with arguments to report to wandb, but when they set up the trainer, it shows an error stating that \"w\" is not supported, and only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.",
        "Challenge_last_edit_time":1660033763876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.9,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":30.66359,
        "Challenge_title":"HuggingFace Trainer() cannot report to wandb",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587186468848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Taipei, Taiwan R.O.C",
        "Poster_reputation_count":163.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":5.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported reporting tool"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.5733333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Challenge_closed_time":1666184788000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1666114324000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The Neptune ML Export widget is throwing an error when the user tries to export data using a specific command from the Node Classification notebook. The error message states that the credential should be scoped to the correct service, 'execute-api'. The expected behavior is for the export to run to completion.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":209.0,
        "Challenge_repo_fork_count":928.0,
        "Challenge_repo_issue_count":2115.0,
        "Challenge_repo_star_count":3927.0,
        "Challenge_repo_watch_count":244.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.5733333333,
        "Challenge_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Discussion_body":"Thanks for opening your first issue here!\n Any solution for this? > Any solution for this?\r\n\r\nThe error message was a bit of a red herring. The actual problem was amundsen-gremlin isn't installed as part of the base image creation `amundsendev\/amundsen-metadata`.\r\n\r\nSolution 1 - add the package to the requirements files and rebuild your own Amundsen image\r\n\r\nSolution 2 - build on the base image and add a `RUN pip install amundsen-gremlin` to your bespoke dockerfile. For my use case I've gone with the latter.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_type":"anomaly",
        "Challenge_summary":"export error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.8672141667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Challenge_closed_time":1640309961152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640245639181,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a sweep from existing runs using ray's wandb_mixin, but the panel Parallel Coordinates is empty. The user is unsure if this is intended behavior or a bug. The sweep only contains one run instead of the expected 42 runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":13.8,
        "Challenge_reading_time":10.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8672141667,
        "Challenge_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.08,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty Parallel Coordinates panel"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Challenge_closed_time":1548817091420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548817091420,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while starting a SageMaker hyperparameter tuning job using the Python SDK for the built-in Image Classifier algorithm. The error message indicates that additional hyperparameters are not allowed, specifically the 'sagemaker_estimator_module' and 'sagemaker_estimator_class_name' properties. The user is unsure why this error is occurring as they are not explicitly passing these properties. The user is seeking guidance on the correct way to start the tuning job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":173,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1224733422316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":7707.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"additional hyperparameters not allowed"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":4.0573761111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying out <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-service\/\" rel=\"nofollow noreferrer\">Azure Machine Learning Service<\/a> for ML deployment.<\/p>\n\n<p>I have already trained a model on a compute VM and saved it as pickle, and now would like to deploy it (I am using Python on Azure notebooks for the purpose as of now).<\/p>\n\n<p>From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#register-model\" rel=\"nofollow noreferrer\">guide<\/a>, it looks like I need to I need a <code>run<\/code> object to be existing in my session to execute the \"model registration\" step:<\/p>\n\n<pre><code># register model \nmodel = run.register_model(model_name='my_model', model_path='outputs\/my_model.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')\n<\/code><\/pre>\n\n<p>However, I haven't created any <code>run<\/code> object as I haven't <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#submit-the-job-to-the-cluster\" rel=\"nofollow noreferrer\">executed<\/a> any experiment for training, I am just starting off with my pickled model.<\/p>\n\n<p>I also tried to register a model by uploading it via the Azure Portal (see screenshot below), but (as the model file is quite large, I assume) it fails with a <code>ajax error 413.<\/code> as in <a href=\"https:\/\/stackoverflow.com\/questions\/55064123\/unable-to-register-an-onnx-model-in-azure-machine-learning-service-workspace\">Unable to register an ONNX model in azure machine learning service workspace<\/a>.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" alt=\"model registering\"><\/a> <\/p>\n\n<p>Is there any way to register and then deploy a pretrained pickled mode (without the need of submitting a <code>run<\/code>, if that makes sense)?<\/p>",
        "Challenge_closed_time":1553175443707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553160837153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to register a machine learning model trained locally in Azure ML Service but is facing challenges as they do not have a run object and the model file is too large to upload via the Azure Portal. They are seeking a way to register and deploy the pretrained pickled model without submitting a run.",
        "Challenge_last_edit_time":1554642485923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55277334",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":25.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4.0573761111,
        "Challenge_title":"How can I register in Azure ML Service a machine learning model trained locally?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1466.0,
        "Challenge_word_count":212,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>Model registration can be done with <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">Model.register<\/a>, without the need of using a <code>run<\/code> object<\/p>\n\n<pre><code>model = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)\n<\/code><\/pre>\n\n<p>for the deployment one can follow steps as outlined in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-as-a-web-service\" rel=\"nofollow noreferrer\">Azure ML service doc<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.6,
        "Solution_reading_time":9.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"register large model without run object"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2754447222,
        "Challenge_answer_count":14,
        "Challenge_body":"<p>I followed the usual instructions:<\/p>\n<pre><code class=\"lang-auto\">pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code class=\"lang-auto\">(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<aside class=\"onebox stackexchange\" data-onebox-src=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Charlie Parker\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e9c0a0caedbda92f5ad9bc087e52e143936f9f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  <\/a>\n\n<h4>\n  <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">Wandb automatically logeed into the wrong user -- why?<\/a>\n<\/h4>\n\n<div class=\"tags\">\n  <strong>wand<\/strong>\n<\/div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Charlie Parker\n  <\/a>\n  on <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">02:32PM - 12 Aug 22 UTC<\/a>\n<\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1660319389712,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660314798111,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user installed Wandb and logged in, but the API key was automatically logged in as the wrong user, and the user is unsure how to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916",
        "Challenge_link_count":9,
        "Challenge_participation_count":14,
        "Challenge_readability":14.3,
        "Challenge_reading_time":34.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":1.2754447222,
        "Challenge_title":"Wandb automatically logeed into the wrong user -- why?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1783.0,
        "Challenge_word_count":225,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<aside class=\"quote no-group\" data-username=\"brando\" data-post=\"9\" data-topic=\"2916\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/brando\/40\/199_2.png\" class=\"avatar\"> brando:<\/div>\n<blockquote>\n<p>But that is weird, I\u2019ve ran it from pycharm debugger before\u2026<img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/confused.png?v=12\" title=\":confused:\" class=\"emoji\" alt=\":confused:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<\/blockquote>\n<\/aside>\n<p>First make sure your <code>.netrc<\/code> file looks right. Login in as instructed in wandb and its fine to relogin. Make sure it still looks fine. Make sure you update wandb with pip. You can set the env variable with your key too. Update pycharm. Remove all the .idea folders in your projects and start from scratch. Make sure your pycharm projs have the right path to the python interpreter form pycharm. Thats I think what worked\u2026<\/p>\n<pre><code class=\"lang-auto\">(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/ultimate-utils\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/iit-term-synthesis\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/pycoq\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/data\/.idea\n(iit_term_synthesis) brandomiranda~\/iit-term-synthesis \u276f rm -rf ..\/proverbot\/.idea\n<\/code><\/pre>\n<p>this was useful:<\/p>\n<pre><code class=\"lang-auto\">run_bash_command('pip install wandb --upgrade')\ncat_file('~\/.zshrc')\ncat_file('~\/.netrc')\n\nwandb.init(project=\"proof-term-synthesis\", entity=\"brando\", name='run_name', group='expt_name')\n\nprint('success!\\a')\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":23.42,
        "Solution_score_count":null,
        "Solution_sentence_count":18.0,
        "Solution_word_count":158.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"wrong API key login"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1870847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used to use Azure Machine Learning Studio (classic).  <br \/>\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.  <br \/>\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).  <br \/>\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.  <\/p>\n<p>How can I process at the same speed as classic?<\/p>",
        "Challenge_closed_time":1635458665072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635432791567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing a significant difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic). The virtual machine size is Standard_DS3_v2, and steps that have been changed even slightly take 20 times longer to process than in classic. The user is seeking advice on how to process at the same speed as classic.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.1870847222,
        "Challenge_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\">Optimize Data Processing<\/a>. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/170450\/\">(1)<\/a>, <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html\">(2)<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":188.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"slow processing time"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1225.01,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830449000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664420413000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where using mlflow without an mlflow writer configured fails silently. The expected behavior is for mlflow integration to write to mlflow by default and warn if missing or inconsistent config is set. The user suggests that whylogs should mention the missing mlflow writer in a warning and automatically add the mlflow writer (with a warning) to draw attention to where the behavior can be modified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":30.0,
        "Challenge_repo_issue_count":222.0,
        "Challenge_repo_star_count":57.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1225.01,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":222,
        "Discussion_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"mlflow fails silently"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":112.1258333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714116000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601310463000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an inconsistency issue with the MLFlowLogger.log_metrics method within steps of a LightningModule. The documentation states that the method should accept a dictionary of metric names and values, but the user found that using log_metrics or log_metric with the dictionary resulted in errors. The user was able to get the method to work by using log_metric with separate arguments for the key and value, but this behavior is not consistent with the documentation. The user has provided a minimum code example to reproduce the bug and has listed their environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":112.1258333333,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Discussion_body":"0332ede5",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"inconsistency with log_metrics method"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":2.9043861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Challenge_closed_time":1586317398727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586306942937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the most cost-effective and efficient way to deploy a forecasting model on AWS. They want to read data from S3 bucket monthly, run custom-built python files, and produce predicted results into a file back in S3 or push to other endpoints. The user is considering using AWS Sagemaker, but they have built and trained the model already and will be using a customized package. They are also unsure about creating a container image. Another option is to create multiple lambda functions to trigger the implementation of the python scripts and push the result using Sagemaker endpoint. The user is seeking guidance on the most efficient way to run the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":31.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.9043861111,
        "Challenge_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2675.0,
        "Challenge_word_count":425,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579188091243,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":19.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":246.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy forecasting model on AWS"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":272.3565766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>As in the documentation \/ tutorial mentioned, we can call <code>Estimator.fit()<\/code> to start Training Job. <\/p>\n\n<p>Required parameter for the method would be the <code>inputs<\/code> that is s3 \/ file reference to the Training File. Example:<\/p>\n\n<pre><code>estimator.fit({'train':'s3:\/\/my-bucket\/training_data})\n<\/code><\/pre>\n\n<p><strong>training-script.py<\/strong><\/p>\n\n<pre><code>parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n<\/code><\/pre>\n\n<p>I would expect <code>os.environ['SM_CHANNEL_TRAIN']<\/code> to be the S3 path. But instead, it returns <code>\/opt\/ml\/input\/data\/train<\/code>.<\/p>\n\n<p>Anyone know why?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>I also tried to call estimator.fit('s3:\/\/my-bucket\/training_data'). \nAnd somehow training instance didn't get the SM_CHANNEL_TRAIN Environment Variables. In fact, I didn't see the s3 URI in Environment Variables at all.<\/p>",
        "Challenge_closed_time":1567193332323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565949262600,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with SageMaker Estimator.fit() as it did not pass the 'train' input to the Training instance. The user expected os.environ['SM_CHANNEL_TRAIN'] to be the S3 path, but instead, it returned \/opt\/ml\/input\/data\/train. The user also tried calling estimator.fit('s3:\/\/my-bucket\/training_data'), but the training instance did not get the SM_CHANNEL_TRAIN Environment Variables.",
        "Challenge_last_edit_time":1566221662207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57522553",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":345.5749230556,
        "Challenge_title":"SageMaker Estimator.fit() didn't pass the 'train' input to the Training instance",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1411.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506308535436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Yogyakarta, Indonesia",
        "Poster_reputation_count":3575.0,
        "Poster_view_count":274.0,
        "Solution_body":"<p>When running training jobs in SageMaker the S3 URL containing your training data provided ends up being copied into the docker container (aka training job) from the specified url. Thus the environment variable SM_CHANNEL_TRAIN is pointing to the local path of the training data that was copied from the S3 URL provided.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567202145883,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"input not passed to training instance"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":50.4269444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Challenge_closed_time":1615408051000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615226514000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the hydra config is no longer being saved to the wandb logger's config.yaml file. The user has provided examples of the previous and current states of the file and suspects that it may be related to a specific line of code in their project. The user is seeking advice on how to restore the previous state.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":47.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":50.4269444444,
        "Challenge_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"config not saved"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.5483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## Instructions \r\nPage 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below:\r\n\r\n`cd Chapter07\/psystock-data-features-main`\r\n `mlflow run . --experiement-name=psystock_data_pipelines`\r\n\r\n## Problem\r\n\r\nThe following error message appears when running line of code specified above:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"feature_set_generation.py\", line 30, in <module>\r\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\r\nNameError: name 'x' is not defined\r\n```\r\n\r\n## Solution\r\nResolve this by deleting line 30 below in `feature_set_generation.py`\r\n\r\n`30         raise Exception('x should not exceed 5. The value of x was: {}'.format(x))`\r\nThe stray `raise` statement is referencing an undefined variable `x`.\r\n\r\nRemoving this line of code removed the reference to this point and lead to the successful deployment of the experiment. I would consider adding such assertions in the `check_verify_data.py` file instead.\r\n\r\n",
        "Challenge_closed_time":1637063606000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636550432000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MLFlowLogger fails to update the status of a `mlflow.entities.run_info.RunInfo` object to 'FAILED' when an error is raised during training, causing the MLFlow Tracking Server screen to show that the training is still in progress even though it has been terminated with an error. The user has provided a code snippet to reproduce the issue and expects the status of each MLFlow's run to be correctly updated when `pl.Trainer.fit` fails. The PyTorch Lightning version used is 1.4.9, and the MLFlow version is 1.12.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/PacktPublishing\/Machine-Learning-Engineering-with-MLflow\/issues\/7",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":64.0,
        "Challenge_repo_issue_count":19.0,
        "Challenge_repo_star_count":92.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":142.5483333333,
        "Challenge_title":"Chapter 7 `mlflow run . --experiement-name=psystock_data_pipelines` fails - BUGFIX",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Discussion_body":"Thanks, invaluable contributions. We will add this to the Errata!!!",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed status update"
    },
    {
        "Answerer_created_time":1410333105327,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Turin, Metropolitan City of Turin, Italy",
        "Answerer_reputation_count":477.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":0.0448425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Challenge_closed_time":1636732235590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636727119830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to change the default directory of mlflow logs from the current directory to a new directory. They are unsure of how to do this and are using MLflow to log metrics. The code provided shows the implementation of the Model class, which includes the fit() and evaluation() methods.",
        "Challenge_last_edit_time":1636755379243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.3,
        "Challenge_reading_time":121.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":124,
        "Challenge_solved_time":1.4210444445,
        "Challenge_title":"How to change the directory of mlflow logs?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":636,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410333105327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Turin, Metropolitan City of Turin, Italy",
        "Poster_reputation_count":477.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636755540676,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"change log directory"
    },
    {
        "Answerer_created_time":1463745452883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":248.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":2.2081222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently playing around with tensorboard on Sagemaker studio. According to aws, it is supposedly possible. However, I keep encountering error code 500 after launching tensorboard and changing the file path to .....\/proxy\/{port number}<\/p>\n<p>Great if somebody can assists on this topic :)<\/p>",
        "Challenge_closed_time":1657296138023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657288188783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use tensorboard on Sagemaker studio but keeps encountering error code 500 after launching tensorboard and changing the file path to ...\/proxy\/{port number}. They are seeking assistance to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72912418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.2081222222,
        "Challenge_title":"How do I get tensorboard to work with sagemaker studio?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":22.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=\"https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample\" rel=\"nofollow noreferrer\">https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample<\/a><\/p>\n<p>Did you follow those instructions?<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":3.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error code 500"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2082036111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have loaded <a href=\"https:\/\/automlsamplenotebookdata.blob.core.windows.net\/automl-sample-notebook-data\/bankmarketing_train.csv\">bankmarketing_train.csv<\/a> to get a dataset and auto generated a model to predict &quot;y&quot; field value with AutoML.    <br \/>\nVoting Ensemble model was generated as the best model and tested its behavior after deployed to the endpoint.    <\/p>\n<p>Schema is generated like this for the endpoint.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/113929-schema-2021-07-13-124905.png?platform=QnA\" alt=\"113929-schema-2021-07-13-124905.png\" \/>    <\/p>\n<p>Tried with the endpoint test feature in ML Studio. It worked and responded an expected output (left side in the fig below).    <br \/>\nBut my python REST call fails with 502 Bad Gateway(right side)    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114082-screenshot-2021-07-12-232443.png?platform=QnA\" alt=\"114082-screenshot-2021-07-12-232443.png\" \/>    <\/p>\n<p>Using the REST plug-in for VSCode, I have requested as below. This also failed with the same response status code.    <\/p>\n<pre><code>POST http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score  \nContent-Type: application\/json  \nAuthorization: Bearer === My correct key here ===  \n  \n{&quot;data&quot;: [{&quot;age&quot;: 87, &quot;campaign&quot;: 1, &quot;cons.conf.idx&quot;: -46.2, &quot;cons.price.idx&quot;: 92.893, &quot;contact&quot;: &quot;cellular&quot;, &quot;day_of_week&quot;: &quot;mon&quot;, &quot;default&quot;: &quot;no&quot;, &quot;duration&quot;: 471, &quot;education&quot;: &quot;university.degree&quot;, &quot;emp.var.rate&quot;: -1.8, &quot;euribor3m&quot;: 1.299, &quot;housing&quot;: &quot;yes&quot;, &quot;job&quot;: &quot;blue-collar&quot;, &quot;loan&quot;: &quot;yes&quot;, &quot;marital&quot;: &quot;married&quot;, &quot;month&quot;: &quot;may&quot;, &quot;nr.employed&quot;: 5099.1, &quot;pdays&quot;: 999, &quot;poutcome&quot;: &quot;failure&quot;, &quot;previous&quot;: 1}]}  \n<\/code><\/pre>\n<p>Investigated in the App Insight and queried the exceptions.    <br \/>\nI found this end point tries to convert 'yes' to int value. Of course it fails.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png?platform=QnA\" alt=\"114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png\" \/>    <\/p>\n<p>The value 'yes' is set to 'loan' and 'housing&quot;. Both are defined string value in the swagger.json for this endpoint.    <\/p>\n<p>What do you think?    <br \/>\nAm I missing something?    <br \/>\nIs this a bug with the endpoint?    <\/p>",
        "Challenge_closed_time":1626168091356,
        "Challenge_comment_count":3,
        "Challenge_created_time":1626149341823,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user generated a model using AutoML to predict a field value and deployed it to an endpoint. While testing the endpoint, the user found that the REST call fails with a 502 Bad Gateway error. The user investigated the issue and found that the endpoint tries to convert 'yes' to an int value, which fails. The user suspects that this may be a bug with the endpoint and seeks advice.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/473223\/endpoint-fails-with-the-model-generated-by-automat",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":5.2082036111,
        "Challenge_title":"Endpoint fails with the model generated by Automated ML",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, I tried that. Following is the code coming from the consume, the values are set accordingly.    <\/p>\n<pre><code>import urllib.request  \nimport json  \nimport os  \nimport ssl  \n  \ndef allowSelfSignedHttps(allowed):  \n    # bypass the server certificate verification on client side  \n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):  \n        ssl._create_default_https_context = ssl._create_unverified_context  \n  \nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.  \n  \n# Request data goes here  \n  \ndata = {&quot;data&quot;:  \n        [  \n          {  \n            &quot;age&quot;: &quot;17&quot;,  \n            &quot;campaign&quot;: &quot;1&quot;,  \n            &quot;cons.conf.idx&quot;: &quot;-46.2&quot;,  \n            &quot;cons.price.idx&quot;: &quot;92.893&quot;,  \n            &quot;contact&quot;: &quot;cellular&quot;,  \n            &quot;day_of_week&quot;: &quot;mon&quot;,  \n            &quot;default&quot;: &quot;no&quot;,  \n            &quot;duration&quot;: &quot;971&quot;,  \n            &quot;education&quot;: &quot;university.degree&quot;,  \n            &quot;emp.var.rate&quot;: &quot;-1.8&quot;,  \n            &quot;euribor3m&quot;: &quot;1.299&quot;,  \n            &quot;housing&quot;: &quot;yes&quot;,  \n            &quot;job&quot;: &quot;blue-collar&quot;,  \n            &quot;loan&quot;: &quot;yes&quot;,  \n            &quot;marital&quot;: &quot;married&quot;,  \n            &quot;month&quot;: &quot;may&quot;,  \n            &quot;nr.employed&quot;: &quot;5099.1&quot;,  \n            &quot;pdays&quot;: &quot;999&quot;,  \n            &quot;poutcome&quot;: &quot;failure&quot;,  \n            &quot;previous&quot;: &quot;1&quot;  \n          }  \n      ]  \n    }  \n  \n  \nbody = str.encode(json.dumps(data))  \n  \nurl = 'http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score'  \napi_key = '&lt;key&gt;' # Replace this with the API key for the web service  \nheaders = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}  \n  \nreq = urllib.request.Request(url, body, headers)  \n  \ntry:  \n    response = urllib.request.urlopen(req)  \n  \n    result = response.read()  \n    print(result)  \nexcept urllib.error.HTTPError as error:  \n    print(&quot;The request failed with status code: &quot; + str(error.code))  \n  \n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure  \n    print(error.info())  \n    print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))  \n  \n<\/code><\/pre>\n<p>The result was the same. 'yes' was tried to cast to int and failed.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114222-consume-2021-07-13-175924.png?platform=QnA\" alt=\"114222-consume-2021-07-13-175924.png\" \/>    <\/p>\n<p>In the deployment log, following exception observed. Something is happening inside the server call, which I cannot see.    <\/p>\n<pre><code>2021-07-13 08:56:49,684 | root | ERROR | Encountered Exception: Traceback (most recent call last):  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 64, in run_scoring  \n    response = invoke_user_with_timer(service_input, request_headers)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 97, in invoke_user_with_timer  \n    result = user_main.run(**params)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/wrapt\/wrappers.py&quot;, line 567, in __call__  \n    args, kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 57, in decorator_input  \n    kwargs[param_name] = _deserialize_input_argument(kwargs[param_name], param_type, param_name)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 285, in _deserialize_input_argument  \n    input_data = param_type.deserialize_input(input_data)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/parameter_types\/pandas_parameter_type.py&quot;, line 79, in deserialize_input  \n    data_frame = data_frame.astype(dtype=converted_types)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5865, in astype  \n    dtype=dtype[col_name], copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5882, in astype  \n    dtype=dtype, copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 581, in astype  \n    return self.apply(&quot;astype&quot;, dtype=dtype, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 438, in apply  \n    applied = getattr(b, f)(**kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 559, in astype  \n    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 643, in _astype  \n    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/dtypes\/cast.py&quot;, line 707, in astype_nansafe  \n    return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)  \n  File &quot;pandas\/_libs\/lib.pyx&quot;, line 547, in pandas._libs.lib.astype_intsafe  \nValueError: invalid literal for int() with base 10: 'yes'  \n  \nDuring handling of the above exception, another exception occurred:  \n  \nTraceback (most recent call last):  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1832, in full_dispatch_request  \n    rv = self.dispatch_request()  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1818, in dispatch_request  \n    return self.view_functions[rule.endpoint](**req.view_args)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 43, in score_realtime  \n    return run_scoring(service_input, request.headers, request.environ.get('REQUEST_ID', '00000000-0000-0000-0000-000000000000'))  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 77, in run_scoring  \n    raise RunFunctionException(str(exc))  \nrun_function_exception.RunFunctionException  \n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":86.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":52.0,
        "Solution_word_count":408.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"502 Bad Gateway error"
    },
    {
        "Answerer_created_time":1480087531307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.4062608334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Getting straight to it:\nWhy is my R code doing fine on my local CPU (under one minute), but tens of times slower on Azure Machine Learning, using one R script block (over 18 minutes)?<\/p>\n\n<p>I assume that it has to do with the resources allocated to the experiment, but how can I be sure? Can I obtain details about the resource allocated to the R script block from somwehere hidden in the Azure-ML Studio machinery?<\/p>\n\n<p>Thank you, Flo<\/p>\n\n<p>Later Edit:\nAs it often happens, I did finally find some information, which still doesn't solve my issue. According to <a href=\"https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes\" rel=\"nofollow noreferrer\">https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes<\/a> \"User-specified R code is run by a 64-bit R interpreter that runs in Azure using an A8 virtual machine with 56 GB of RAM.\"<\/p>\n\n<p>This is more than my local machine has, the R code is still much slower on the Azure-ML studio.<\/p>",
        "Challenge_closed_time":1480339657187,
        "Challenge_comment_count":5,
        "Challenge_created_time":1480336059453,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is experiencing slow execution time for their R code on Azure Machine Learning compared to their local CPU, despite Azure having more resources allocated to it. The user is seeking information on how to obtain details about the resource allocation to the R script block in Azure-ML Studio.",
        "Challenge_last_edit_time":1480338194648,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40844351",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.9993705556,
        "Challenge_title":"R code on Azure Machine Learning is slow compared to local execution time",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":733.0,
        "Challenge_word_count":158,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459503032960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vienna, Austria",
        "Poster_reputation_count":1.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Consider using rbenchmark or other benchmarking tools to get an idea of the runtime and complexity of your code. In general for loops tend to be slow.<\/p>\n\n<p>It's very possible that the server has less resources available (ram, cpu) or that you have to wait in a que before you get served. Without any more code it's hard to comment further on this issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"resource allocation details"
    },
    {
        "Answerer_created_time":1661172608640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":22.9405238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Challenge_closed_time":1661173080163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661090494277,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to control the version of experiment configuration files with Hydra and DVC without uploading the original config files to Git. However, Hydra does not specify which code version is needed to reproduce the experiment, and the user does not want to add git hash logging code in every experiment. The user is seeking a way to log git hash to Hydra log in default.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":22.9405238889,
        "Challenge_title":"Is there any way to log 'git hash' in hydra?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1603378831587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty logging git hash"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":285.6506852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Challenge_closed_time":1641201707430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640176354140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Vertex AI Endpoints where the number of replicas scales from 1 to 0 before increasing to 2, causing 504 errors in their API. The user has had to set the minimum replicas to a higher number to solve the issue, which has increased the monthly cost of the application. The user is seeking a solution to this problem and wondering if it is a known issue with Vertex AI\/GCP services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":284.8203583334,
        "Challenge_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400036379907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro - RJ, Brasil",
        "Poster_reputation_count":490.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1641204696607,
        "Solution_link_count":3.0,
        "Solution_readability":10.0,
        "Solution_reading_time":25.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"scaling issue with Vertex AI"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":34.3028483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1643782095707,
        "Challenge_comment_count":9,
        "Challenge_created_time":1643658605453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with running code on Google Cloud Platform VertexAI due to an unstable local internet connection. They are looking for a way to run the code on the backend without any output until it is written in the log file.",
        "Challenge_last_edit_time":1645284643350,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Challenge_link_count":2,
        "Challenge_participation_count":10,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":34.3028483334,
        "Challenge_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":514.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622195346030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":9.0,
        "Solution_reading_time":29.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":255.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unstable internet connection"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":280.5022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1589303399000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1588293591000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running a Kubeflow pipeline with AWS Sagemaker. The error is related to assigning hyperparameters, specifically 'k' and 'feature_dim', which are required but not defined in the pipeline parameters. The user is seeking assistance in resolving the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":47.45,
        "Challenge_repo_contributor_count":346.0,
        "Challenge_repo_fork_count":1432.0,
        "Challenge_repo_issue_count":9548.0,
        "Challenge_repo_star_count":3200.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":280.5022222222,
        "Challenge_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":390,
        "Discussion_body":"Thanks Marwan for trying out the component. \r\nI believe your script was buried inside your custom image, if so that custom image runs inside sagemaker which does not inherit the `use_aws_secret` values. So either you need to add permission to the role https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L10 or read it from AWS secret manager. \r\n\r\nWould you mind sharing your script or minimal reproducible code ?  @gautamkmr  - thank you for taking the time to respond to this issue\r\n\r\nPlease find below a very simplified version of the script I'd like to run but hopefully should be good enough to show where the issue is - please note the comments in the script\r\n```\r\nimport pathlib\r\nimport os\r\nimport boto3\r\nimport sys\r\n\r\n\r\ndef main():\r\n    try:\r\n        # Reading data that sagemaker has copied from s3\r\n        # works fine\r\n        prefix = pathlib.Path('\/opt\/ml\/')\r\n        input_path = prefix \/ 'input\/data\/train\/'\r\n\r\n        with open(input_path \/ 'test.txt', 'r') as f:\r\n            content = f.read()\r\n\r\n        assert 'hello world' in content\r\n\r\n        # the below portion is trying to read data from s3\r\n        # using boto3 but it fails\r\n        bucket_name = os.environ['AWS_BUCKET']\r\n        object_name = 'dummy_input\/test.txt'\r\n        file_name = 'test.txt'\r\n\r\n        s3 = boto3.client('s3')\r\n\r\n        # specifically the below line fails:\r\n        # botocore.exceptions.NoCredentialsError: Unable to locate credentials\r\n        s3.download_file(bucket_name, object_name, file_name)\r\n\r\n    except Exception:\r\n        sys.exit(255)\r\n```\r\n\r\nHere is the script for compiling the pipeline just in case you need it\r\n```\r\nimport kfp.compiler as compiler\r\nimport json\r\nimport os\r\nfrom kfp import components, dsl\r\nfrom kfp.aws import use_aws_secret\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"sm_kfp_example\",\r\n    description=\"sample sagemaker training job\"\r\n)\r\ndef sm_kfp_example():\r\n    bucket = os.environ.get('AWS_BUCKET')\r\n    role = os.environ.get('SAGEMAKER_ROLE_ARN')\r\n\r\n    train_channels = json.dumps([{\r\n        'ChannelName': 'train',\r\n        'DataSource': {\r\n            'S3DataSource': {\r\n                'S3Uri': f's3:\/\/{bucket}\/dummy_input\/',\r\n                'S3DataType': 'S3Prefix',\r\n                'S3DataDistributionType': 'FullyReplicated'\r\n            }\r\n        },\r\n        'ContentType': '',\r\n        'CompressionType': 'None',\r\n        'RecordWrapperType': 'None',\r\n        'InputMode': 'File'\r\n    }])\r\n\r\n    # use the sagemaker training operator defined by aws\r\n    # [a wrapper around Sagemaker CreateTrainingJob]\r\n    repo_path = 'https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines'\r\n    # commit hash of current version of kfp that we are using\r\n    commit = 'master'\r\n    suffix = 'components\/aws\/sagemaker\/train\/component.yaml'\r\n    path = f'{repo_path}\/{commit}\/{suffix}'\r\n\r\n    sagemaker_train_op = components.load_component_from_url(path)\r\n    output_path = f's3:\/\/{bucket}\/output'\r\n    account_id = os.environ.get('AWS_ACCOUNT_ID')\r\n    region = os.environ.get('AWS_REGION')\r\n    image = f'{account_id}.dkr.ecr.{region}.amazonaws.com\/sm_kfp_example:latest'\r\n\r\n    _ = sagemaker_train_op(\r\n        region=region,\r\n        endpoint_url='',\r\n        image=image,\r\n        training_input_mode='File',\r\n        hyperparameters='{}',\r\n        channels=train_channels,\r\n        instance_type='ml.m5.xlarge',\r\n        instance_count='1',\r\n        volume_size='20',\r\n        max_run_time='3600',\r\n        model_artifact_path=output_path,\r\n        output_encryption_key='',\r\n        network_isolation='True',\r\n        traffic_encryption='False',\r\n        spot_instance='False',\r\n        max_wait_time='3600',\r\n        checkpoint_config='{}',\r\n        role=role,\r\n    ).apply(\r\n        use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')\r\n    )\r\n\r\n\r\ndef compile(pipeline_func):\r\n    pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # compile the pipeline\r\n    compile(sm_kfp_example)\r\n```\r\n\r\nThe very strange thing is if I try to create the training job using the sagemaker python sdk (i.e. not using sagemaker's k8s training operator) - the script runs fine - i.e. the credentials are passed down to the container - below is the script in case you need it\r\n\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.estimator import Estimator\r\nimport os\r\n\r\naws_region = os.environ['AWS_REGION']\r\nalgorithm_name = \"sm_kfp_example\"\r\ns3_bucket = os.environ['AWS_BUCKET']\r\n\r\n# use the security token service to verify the account identity\r\nclient = boto3.client('sts')\r\naccount = client.get_caller_identity()['Account']\r\n\r\n# set the sagemaker role\r\nrole = os.environ['SAGEMAKER_ROLE_ARN']\r\n\r\n# create a boto_session\r\nboto_session = boto3.session.Session(\r\n    region_name=aws_region,\r\n)\r\n\r\n# get full training image url\r\ntraining_image = f\"{account}.dkr.ecr.{aws_region}.amazonaws.com\/{algorithm_name}:latest\"\r\n\r\n\r\n# specify location on s3_bucket to output the results\r\ns3_output_location = f's3:\/\/{s3_bucket}\/output'\r\n\r\n# create a sagemaker_session\r\nsagemaker_session = sagemaker.session.Session(boto_session=boto_session)\r\n\r\n# create an estimator\r\nestimator = Estimator(\r\n    training_image,\r\n    role,\r\n    train_instance_count=1,\r\n    train_instance_type='ml.m5.xlarge',\r\n    train_volume_size=10,  # 10 GB\r\n    train_max_run=600,  # 10 minutes = 600seconds\r\n    input_mode='File',\r\n    output_path=s3_output_location,\r\n    sagemaker_session=sagemaker_session,\r\n    hyperparameters={},\r\n    base_job_name=\"sagemaker-sample\",\r\n)\r\n\r\nestimator.fit(\r\n    inputs={\r\n        'train': f's3:\/\/{s3_bucket}\/dummy_input\/'\r\n    },\r\n    logs=True\r\n)\r\n```\r\n Note, to avoid opening other Sagemaker issues, I will list out some of the pain points I have faced trying to integrate sagemaker with Kubeflow here and let me know if there are any solutions to these - excuse me for not following protocol - but if these are deemed as valid issues -I would be glad to open up the relevant issues:\r\n\r\n- I can't seem to pass an image to sagemaker_training_op that is not hosted on ECR and if the image is hosted on ECR - it has to be in the same region as that specified for sagemaker_training_op ... \r\n\r\n- Currently, the sagemaker logs are being output to cloudwatch not to kubeflow (would be much easier if they can be forwarded to kubeflow)\r\n  There has been some undocumented change to the `load_component_*` functions. It used to return a `ContainerOp`, now it returns a `TaskSpec` instead.\r\n\r\nCurrently there are 2 possible workaround:\r\n\r\n1.  use the private func `_create_container_op_from_component_and_arguments` to generate ur containerop from taskspec\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\n\r\ncomponent_op = components.load_component_from_url(...)\r\ntaskspec = component_op(...)\r\ncontainerop = _create_container_op_from_component_and_arguments(\r\n  taskspec.component_ref.spec, \r\n  taskspec.arguments, \r\n  taskspec.component_ref\r\n)\r\n```\r\n\r\n2. overwrite the default `_default_container_task_constructor`\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\nimport kfp.components._components as _components\r\n\r\n_components._default_container_task_constructor = _create_container_op_from_component_and_arguments\r\n\r\n# now load_component will return a containerop\r\ncontainerop = components.load_component_from_url(...)\r\n```\r\n\r\nPS: @Ark-kun this is not the first time I seen this issue\/qns - what do u think? Hi @marwan116, I was able to reproduce the failure and the root cause is that the default value for `network_isolation` parameter is set to [False in python sdk](https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/bf48fb1219bd8ea22e78a913bfa091e544c57cc3\/src\/sagemaker\/estimator.py#L1112)  whereas in the pipeline definition you provided it is set to True, which is also the [default value](https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L73-L75) in training component\r\n\r\nCan you try set it to False and let us know if your issue has been resolved ?\r\n\r\n\r\n----\r\n\r\nHere are some clarifications based on posts on this thread: \r\n\r\n- The logs you posted in the issue initially [under whats happened section](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670#issue-610481941) (except the exception) is from the component pod and NOT from the training job itself. As you have already observed, for the training job logs you need to go to cloudwatch.\r\n  - the first log line which you see `INFO:botocore.credentials:Found credentials in environment variables.` is from boto session which is created by the component backend to call create_training_job API. It uses the credentials are from `aws-secret` that you would have created. These credentials are only used to invoke the job and are not passed to the instance in SageMaker\r\n\r\n- The SageMaker instance which runs in AWS assumes the credentials from the role ARN you provide in`SAGEMAKER_ROLE_ARN` not not from the secret\r\n\r\nLet us know if you have more questions.\r\n @surajkota  - thank you so much for taking the time to reproduce this - yes you are right it is because I had `network_isolation` set to `True` - (sorry I should have taken the time to understand what `network_isolation` does)\r\n\r\nAlso thank you for the clarifications!\r\n\r\nI saw @gautamkmr graciously took the time to open an issue concerning the logs - thank you @gautamkmr !\r\n\r\nI am closing this now as this particular issue is now resolved",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"undefined hyperparameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6481894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From here: <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-training-vnet?view=azureml-api-2&amp;tabs=cli%2Crequired#compute-instancecluster-with-no-public-ip\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-training-vnet?view=azureml-api-2&amp;tabs=cli%2Crequired#compute-instancecluster-with-no-public-ip<\/a><\/p>\n<p>The command to create either a AMLCompute or Compute Instance contains the flag option using the (az ml command) for <code>--set enable_node_public_ip=False<\/code><\/p>\n<p>This option isn't available in ARM, Bicep or Terraform (AzAPI provider) templates - and it really needs to be for Infrastructure-as-Code provisioning<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-terraform\">https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-terraform<\/a><\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1684750280332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684744346850,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using ARM\/Bicep templates to create either AMLCompute or Compute Instance as the flag option to set no public IP address is not available in these templates. This option is only available in the az ml command and needs to be added to the templates for Infrastructure-as-Code provisioning.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1289333\/no-options-for-setting-no-public-ip-address-flag-f",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":28.3,
        "Challenge_reading_time":15.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.6481894444,
        "Challenge_title":"No options for setting no public IP address flag for Compute options in ARM\/Bicep templates",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The second link you provided in the question shows how to set this flag in Bicep\/Arm\/Terraform<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-bicep#amlcomputeproperties\">https:\/\/learn.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/workspaces\/computes?pivots=deployment-language-bicep#amlcomputeproperties<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/652a27eb-d7f4-41e6-bd28-0750b1ac5ced?platform=QnA\" alt=\"User's image\" \/><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":35.8,
        "Solution_reading_time":8.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing flag option"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":170.435,
        "Challenge_answer_count":0,
        "Challenge_body":"I successfully trained and deployed a Tensorflow Recommender model on Vertex AI, Tensorflow 2.8.\r\n\r\nEverything is online and to predict the output. In the notebook I do:\r\n\r\n    loaded = tf.saved_model.load(path)\r\n    scores, titles = loaded([\"doctor\"])\r\n\r\nThat returns:\r\n\r\n    Recommendations: [b'Nelly & Monsieur Arnaud (1995)'\r\n     b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\r\n\r\nThat is, the payload (input for the neural network) must be `[\"doctor\"]`\r\n\r\nThen I generate the JSON for payload (the error is here):\r\n\r\n    !echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : {[\"\\\"\"doctor\"\\\"\"]}}]} > instances0.json\r\n\r\nAnd submit to the endpoint:\r\n\r\n    !curl -X POST  \\\r\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\r\n    -H \"Content-Type: application\/json\" \\\r\n    https:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\r\n    -d @instances0.json > results.json\r\n\r\n... as seen here: https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\r\n\r\nHowever, when I use this payload, I get error 400:\r\n\r\n    code: 400\r\n    message: \"Invalid JSON payload received. Expected an object key or }. s\" : [{\"input_1\" : {[\"doctor\"]}}]} ^\"\r\n    status: \"INVALID_ARGUMENT\"\r\n\r\nThis below don't work either:\r\n\r\n    !echo {\"inputs\": {\"input_1\": [\"doctor\"]}} > instances0.json\r\n\r\nEven with validated JSON Lint, it does not return the proper prediction.\r\n\r\nRunning:\r\n\r\n    !saved_model_cli show --dir \/home\/jupyter\/model --all\r\n\r\nI get:\r\n\r\n    MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n    \r\n    signature_def['__saved_model_init_op']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['__saved_model_init_op'] tensor_info:\r\n            dtype: DT_INVALID\r\n            shape: unknown_rank\r\n            name: NoOp\r\n      Method name is: \r\n    \r\n    signature_def['serving_default']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n        inputs['input_1'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1)\r\n            name: serving_default_input_1:0\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['output_1'] tensor_info:\r\n            dtype: DT_FLOAT\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:0\r\n        outputs['output_2'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:1\r\n      Method name is: tensorflow\/serving\/predict\r\n\r\n\r\n    Concrete Functions:\r\n      Function Name: '__call__'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n    \r\n      Function Name: '_default_save_signature'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n    \r\n      Function Name: 'call_and_return_all_conditional_losses'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n\r\nThe point is: I'm passing an array and I'm not sure if it must be in b64 format.\r\n\r\nThis Python code works, but returns a different result than expected:\r\n\r\n    import tensorflow as tf\r\n    import base64\r\n    from google.protobuf import json_format\r\n    from google.protobuf.struct_pb2 import Value\r\n    import numpy as np\r\n    from google.cloud import aiplatform\r\n    import os\r\n    vertex_model = tf.saved_model.load(\"gs:\/\/bucket\/model\")\r\n    \r\n    serving_input = list(\r\n        vertex_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\r\n    )[0]\r\n    \r\n    print(\"Serving input :\", serving_input)\r\n    \r\n    aip_endpoint_name = (\r\n        f\"projects\/my-project\/locations\/us-west1\/endpoints\/12345567\"\r\n    )\r\n    endpoint = aiplatform.Endpoint(aip_endpoint_name)\r\n    \r\n    def encode_input(input):\r\n        return base64.b64encode(np.array(input)).decode(\"utf-8\")\r\n    \r\n    instances_list = [{serving_input: {\"b64\": encode_input(np.array([\"doctor\"]))}}]\r\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\r\n    \r\n    results = endpoint.predict(instances=instances)\r\n    print(results.predictions[0][\"output_2\"])\r\n\r\n\r\n    ['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\r\n\r\nAny ideas on how to fix \/ encode the payload ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1659564318000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658950752000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering errors with wandb sweeps while running the astral-sweep-1 project. The error message indicates that there is a mismatch between the expected and actual data types for an argument in the code. The user has provided details of the agent's run configuration and the project's tracking and syncing status.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/749",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":72.41,
        "Challenge_repo_contributor_count":111.0,
        "Challenge_repo_fork_count":500.0,
        "Challenge_repo_issue_count":1925.0,
        "Challenge_repo_star_count":745.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":170.435,
        "Challenge_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":591,
        "Discussion_body":"This works perfectly:\r\n\r\n    def encode_64(input):\r\n        message = input\r\n        message_bytes = message.encode('ascii')\r\n        base64_bytes = base64.b64encode(message_bytes)\r\n        base64_message = base64_bytes.decode('ascii')\r\n        return base64_message\r\n    \r\n    \r\n    instances_list = [{serving_input: {\"b64\": encode_64(\"doctor\")}}]\r\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\r\n    \r\n    results = endpoint.predict(instances=instances)\r\n    print(results.predictions[0][\"output_2\"][:3])\r\n\r\n    ['Nelly & Monsieur Arnaud (1995)', 'Three Lives and Only One Death (1996)', 'Critical Care (1997)']\r\n\r\nBut I still have doubts regarding the CURL method. You have extra curly braces around your input data. Try something like:\r\n\r\n```\r\n!echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : [\"\\\"\"doctor\"\\\"\"]}]} > instances0.json\r\n```\r\n\r\nAssuming you are using a [GCP prebuilt container](https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers), this is endpoint launches the equivalent of a [TF Serving container](https:\/\/www.tensorflow.org\/tfx\/guide\/serving) and you can test your model locally with a container before pushing to an endpoint to make sure everything is working. Take a look at [this notebook](https:\/\/github.com\/northam-stp-team\/vertexai-apigee\/blob\/master\/notebooks\/Seq2SeqTranslation.ipynb).",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"data type mismatch"
    },
    {
        "Answerer_created_time":1498252453503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":298.7422783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Challenge_closed_time":1530057890672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529660522477,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to build a hyperparameter optimization job in Amazon Sagemaker using Python, but is encountering an error related to the 'tuner.py' file. The error message suggests that a 'str' object has no attribute 'keys', and the issue seems to be related to a 'list' object not having the required attribute.",
        "Challenge_last_edit_time":1530173488990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.8,
        "Challenge_reading_time":31.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":110.3800541667,
        "Challenge_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1531248961192,
        "Solution_link_count":1.0,
        "Solution_readability":25.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"attribute error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.2693258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?  <\/p>\n<p>azureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle&lt;2.0.0 and &gt;=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle&lt;=2.0.0  <\/p>\n<p>Intro to new cloudpickle:  <br \/>\n<a href=\"https:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\">https:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs<\/a>  <br \/>\nPR:  <br \/>\n<a href=\"https:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\">https:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417<\/a>  <br \/>\nGithub issue:  <br \/>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637<\/a>  <\/p>",
        "Challenge_closed_time":1637290125060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637242355487,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting that the azureml-dataprep dependency for azureml-dataset-runtime should allow cloudpickle<=2.0.0, as there are no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0 and the latter introduces useful tools for serializing helper scripts. The user has provided links to the new cloudpickle and related PR and Github issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":16.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":13.2693258333,
        "Challenge_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=a10bf22e-b97c-4af2-89b9-23142e132503\">@Thomas H  <\/a>     <\/p>\n<p>Thank you so much for the contribute, I have sent an email to the author for the PR review and merge.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.4,
        "Solution_reading_time":17.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"update dependency"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":248.2981122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1638931046247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a managed notebook on Vertex AI due to a quota limit error related to 'Create Runtime API requests per minute' of the 'notebooks.googleapis.com' service. The user has tried to edit the quota limit but is unable to increase it. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":248.2981122222,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"quota limit error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.7904747222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Adding tags to any project in my account will cause an internal server error. Is there an outage or it\u2019s something specific to my account?<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/cchi\/tags_test_project\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/7\/7e3d01bfbb9f37b8c9af3076e9688c9bef1b6347.png\" class=\"thumbnail onebox-avatar\" width=\"120\" height=\"120\">\n\n<h3><a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">cchi<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png\" data-download-href=\"\/uploads\/short-url\/o8iWO0KA8b01xdqNjqP8vCU1eUh.png?dl=1\" title=\"Screen Shot 2022-11-01 at 5.32.04 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png\" alt=\"Screen Shot 2022-11-01 at 5.32.04 PM\" data-base62-sha1=\"o8iWO0KA8b01xdqNjqP8vCU1eUh\" width=\"690\" height=\"258\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 2x\" data-dominant-color=\"FBF3F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-11-01 at 5.32.04 PM<\/span><span class=\"informations\">789\u00d7296 13.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1667362849064,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667338403355,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an internal server error when trying to add tags to any project in their account and is unsure if it's a general outage or specific to their account.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/adding-tags-cause-internal-server-error\/3363",
        "Challenge_link_count":10,
        "Challenge_participation_count":3,
        "Challenge_readability":21.7,
        "Challenge_reading_time":33.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":6.7904747222,
        "Challenge_title":"Adding tags cause internal server error",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cchi\">@cchi<\/a>,<\/p>\n<p>Could you try adding your tags once again? We were running into some site issues earlier today but they should be resolved now.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":2.73,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"internal server error"
    },
    {
        "Answerer_created_time":1324808381143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":9050.0,
        "Answerer_view_count":1750.0,
        "Challenge_adjusted_solved_time":2198.1168433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1623834809928,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to implement distributed training using SageMaker v2.29.2 and Tensorflow v2.3.2, following a blog post. However, they are having difficulties importing the smdistributed script and are receiving a \"ModuleNotFoundError\".",
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":2203.8220902778,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"module not found"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1396913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi team,     <br \/>\nI am learning about the quota for machine learning service and I have a general doubt.    <\/p>\n<p>I can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.    <br \/>\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.     <\/p>\n<p>so that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.    <\/p>\n<p>I am able to find out  the-  get details query but this one doesnt give me details specific to each resource-group or the workspace.    <\/p>\n<p>HTTP query -&gt; <a href=\"https:\/\/management.azure.com\/subscriptions\/%7Bsubs_id%7D\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01\">https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01<\/a><\/p>",
        "Challenge_closed_time":1667069947336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667069444447,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to set quotas for CPU cores at the resource group level in Azure Machine Learning Service. They have a subscription level total CPU cores quota of 10 and want to assign 5 cores each to two resource groups under that subscription to ensure that if all the cores are taken up by the resources under one resource group, the other resource group or the ML workspace under it should not suffer. The user is unable to find specific details for each resource group or workspace using the provided HTTP query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":13.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1396913889,
        "Challenge_title":"how to set Quota at resource group level?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7bafa6a3-9285-4b1c-a003-4490a74d05b5\">@JA  <\/a> ,    <\/p>\n<p>quotas can be set on Azure Subscription level only.    <br \/>\nThere is no option to apply quotas for different Azure Resource Groups.    <br \/>\nThere are 2 options I can see for your requirement:    <br \/>\nUse 2 Azure Subscriptions for each Resource Group    <br \/>\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to find resource details"
    },
    {
        "Answerer_created_time":1231865173143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mountain View, CA",
        "Answerer_reputation_count":29068.0,
        "Answerer_view_count":2064.0,
        "Challenge_adjusted_solved_time":4.0522230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Challenge_closed_time":1591773861796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591767515617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue when using MLFlow and Hydra together. While both work fine individually, when used together, the user encounters an error that causes the program to crash. The error message suggests that the run is not found, and the user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":25.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.7628275,
        "Challenge_title":"MLFlow and Hydra causing crash when used together",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":718.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583072555672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1591782103620,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":22.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":200.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"program crashes"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3288888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?\nI'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Challenge_closed_time":1607969193000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607968009000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether anyone has successfully run multi-node gradient boosting on Amazon SageMaker, specifically with Catboost, LightGBM, or XGBoost. They are interested in finding a SageMaker-compatible multi-node training solution and would appreciate a public demo link if available.",
        "Challenge_last_edit_time":1667925925012,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3288888889,
        "Challenge_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612842503288,
        "Solution_link_count":1.0,
        "Solution_readability":26.1,
        "Solution_reading_time":3.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"multi-node gradient boosting"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11971.2836111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take\r\n\r\nCode gets stuck in infinite loop is SageMaker training job gets stopped (unhandled use case)\r\n\r\n### What happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/sagemaker_training_component.py#L57-L66\r\n\r\nAbove code only caters for training job status `Completed` or `Failed`, so if the training job status is marked as `Stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/sagemaker\/common\/sagemaker_component.py#L197-L201\r\n\r\n### What did you expect to happen:\r\n\r\nTraining job status `stopped` to be catered for\r\n\r\n### Environment:\r\n\r\n### Anything else you would like to add:\r\n\r\n\r\n### Labels\r\n<!-- Please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Challenge_closed_time":1673301002000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1630204381000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a TypeError while trying to update an endpoint using Sagemaker Components for building Kubeflow pipelines. The error message indicates that the Sagemaker - Deploy Model() function received an unexpected keyword argument 'update_endpoint'. The user is using kfp 1.1.2 and sagemaker 2.1.0. The user expects to update the endpoint without any issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":11.5,
        "Challenge_reading_time":15.25,
        "Challenge_repo_contributor_count":346.0,
        "Challenge_repo_fork_count":1432.0,
        "Challenge_repo_issue_count":9548.0,
        "Challenge_repo_star_count":3200.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11971.2836111111,
        "Challenge_title":"[bug] Unhandled SageMaker training job status 'stopped' causing infinite loop",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":136,
        "Discussion_body":"Thanks for adding this issue. You are more than welcome to contribute a fix @benhyy.  \/area components\/aws\/sagemaker This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n This has been resolved in https:\/\/github.com\/kubeflow\/pipelines\/pull\/8336. Please use the latest version of components and let us know if you face any problems. Thanks \/close @surajkota: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465#issuecomment-1376375518):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unexpected keyword argument"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1586647457000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a warning message related to the use of the deprecated Comet API logger, comet_ml.papi, instead of the newer comet_ml.api. The warning suggests using the updated API and provides a link for more information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Discussion_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deprecated API"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":20.5317583333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using optuna for parameter optimisation of my custom models. <\/p>\n\n<p>Is there any way to sample parameters until current params set was not tested before? I mean, do try sample another params if there were some trial in the past with the same set of parameters. <\/p>\n\n<p>In some cases it is impossible, for example, when there is categorial distribution and <code>n_trials<\/code> is greater than number os possible unique sampled values. <\/p>\n\n<p>What I want: have some config param like <code>num_attempts<\/code> in order to sample parameters up to <code>num_attempts<\/code> in for-loop until there is a set that was not tested before, else - to run trial on the last sampled set. <\/p>\n\n<p>Why I need this: just because it costs too much to run heavy models several times on the same parameters. <\/p>\n\n<p>What I do now: just make this \"for-loop\" thing but it's messy.<\/p>\n\n<p>If there is another smart way to do it - will be very grateful for information.  <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1573636947556,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573568558157,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is using optuna for parameter optimization of custom models and is looking for a way to sample parameters without duplicates. They want to avoid running heavy models several times on the same parameters and are currently using a messy for-loop to achieve this. They are looking for a more efficient solution, such as a config parameter like \"num_attempts\" to sample parameters until a set that was not tested before is found.",
        "Challenge_last_edit_time":1573569572550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58820574",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.9970552778,
        "Challenge_title":"How to sample parameters without duplicates in optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2121.0,
        "Challenge_word_count":170,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>To the best of my knowledge, there is no direct way to handle your case for now.\nAs a workaround, you can check for parameter duplication and skip the evaluation as follows:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # Sample parameters.\n    x = trial.suggest_int('x', 0, 10)\n    y = trial.suggest_categorical('y', [-10, -5, 0, 5, 10])\n\n    # Check duplication and skip if it's detected.\n    for t in trial.study.trials:\n        if t.state != optuna.structs.TrialState.COMPLETE:\n            continue\n\n        if t.params == trial.params:\n            return t.value  # Return the previous value without re-evaluating it.\n\n            # # Note that if duplicate parameter sets are suggested too frequently,\n            # # you can use the pruning mechanism of Optuna to mitigate the problem.\n            # # By raising `TrialPruned` instead of just returning the previous value,\n            # # the sampler is more likely to avoid sampling the parameters in the succeeding trials.\n            #\n            # raise optuna.structs.TrialPruned('Duplicate parameter set')\n\n    # Evaluate parameters.\n    return x + y\n\n# Start study.\nstudy = optuna.create_study()\n\nunique_trials = 20\nwhile unique_trials &gt; len(set(str(t.params) for t in study.trials)):\n    study.optimize(objective, n_trials=1)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1573643486880,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":14.88,
        "Solution_score_count":10.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":149.0,
        "Tool":"Optuna",
        "Challenge_type":"inquiry",
        "Challenge_summary":"avoid duplicate parameter sampling"
    },
    {
        "Answerer_created_time":1532464254552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":310.004845,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Challenge_closed_time":1547752819392,
        "Challenge_comment_count":1,
        "Challenge_created_time":1544396689430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to run hyper-parameter auto tuning jobs using the image classification algorithm in AWS Sagemaker due to a validation error in the input data configuration. The error message indicates that additional properties are not allowed and the user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":1546636801950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53697587",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":19.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":932.2583227778,
        "Challenge_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1697.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421238326280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Poster_reputation_count":1951.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.44,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"validation error in input data"
    },
    {
        "Answerer_created_time":1566583092316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":479.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":8.3681258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Yesterday I have install tensorflow module from iPython notebook from Azure machine learning studio (classic) version. The import worked well after installing the module using (!pip install tensorflow). But today when tried to import this module got this \"missing module\" error and when I tried reinstalling the module it works well. Am I missing anything here? \nDo I need to install the module each and everyday, before using it? Can someone please explain?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1578608056696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578516766930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user installed the tensorflow module on iPython notebook from Azure machine learning studio (classic) version and it worked well. However, the next day when trying to import the module, they received a \"missing module\" error. Reinstalling the module fixed the issue, but the user is unsure if they need to reinstall the module every day before using it.",
        "Challenge_last_edit_time":1578577931443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59653641",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":8.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":25.3582683333,
        "Challenge_title":"Missing module tensorflow on iPython azure machine learning (Classic)",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>For Azure Machine Learning (Classic) Studio notebooks, you need to install Tensorflow. Furthermore, the notebook server session times out after a period of inactivity, hence, you need to re-install Tensorflow once the server shuts down or after starting a new session. Thanks.<\/p>\n\n<p>Here are some references:<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.2,
        "Solution_reading_time":9.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing module error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.0741666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Challenge_closed_time":1654420353000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1654326486000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the wandb logger while using a wandb-callbacks branch. After running the command `python train.py logger=wandb`, the user gets an error message stating that the job was cancelled by the user after 130 iterations because the wandb login does not appear. Changing `logger: wandb` in train.yaml does not work either. The user has tried different conda envs with different torch and pl versions but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.4,
        "Challenge_reading_time":61.14,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":458.0,
        "Challenge_repo_issue_count":506.0,
        "Challenge_repo_star_count":2692.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":210,
        "Challenge_solved_time":26.0741666667,
        "Challenge_title":"wandb logger not working",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":584,
        "Discussion_body":"`wandb-callbacks` haven't been maintained for a while and it might not work correctly with recent lightning and hydra releases. \r\n\r\nHave you trained using the `main` branch?\r\n\r\nI'm preparing new release and will fix the callbacks when it's ready https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/308\r\n So i managed to get it working using a fresh conda environment: \r\ntorch==1.10.0 with CUDA10.2\r\npytorch-lightning==1.6.4\r\nwandb == 0.12.17\r\n\r\nI doesnt check if all the callbacks work properly but my initial problem is solved. Thank you for your help! ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"wandb login issue"
    },
    {
        "Answerer_created_time":1393576024047,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":879.0,
        "Answerer_view_count":138.0,
        "Challenge_adjusted_solved_time":5394.9012733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i have started a sagemaker job:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server'{'enabled':False}})\n\ntraining_data_uri ='s3:\/\/path\/to\/my\/data'\nmytraining.fit(training_data_uri,run_tensorboard_locally=True)\n<\/code><\/pre>\n\n<p>using <code>run_tesorboard_locally=True<\/code> gave me<\/p>\n\n<pre><code>Tensorboard is not supported with script mode. You can run the following command: tensorboard --logdir None --host localhost --port 6006 This can be run from anywhere with access to the S3 URI used as the logdir.\n<\/code><\/pre>\n\n<p>It seems like i cant use it script mode, but I can access the logs of tensorboard in s3? But where are the logs in s3?<\/p>\n\n<pre><code>def _parse_args():\n    parser = argparse.ArgumentParser()\n\n    # Data, model, and output directories\n    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n    parser.add_argument('--model_dir', type=str)\n    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n\n    return parser.parse_known_args()\n\nif __name__ == \"__main__\":\n    args, unknown = _parse_args()\n\n    train_data, train_labels = load_training_data(args.train)\n    eval_data, eval_labels = load_testing_data(args.train)\n\n    mymodel= model(train_data, train_labels, eval_data, eval_labels)\n\n    if args.current_host == args.hosts[0]:\n        mymodel.save(os.path.join(args.sm_model_dir, '000000002\/model.h5'))\n<\/code><\/pre>\n\n<p>similiar question is here :<a href=\"https:\/\/stackoverflow.com\/questions\/53713660\/tensorboard-without-callbacks-for-keras-docker-image-in-sagemaker\">stack<\/a><\/p>\n\n<p>EDIT i tried this new config but it doesnt work.<\/p>\n\n<pre><code> tensorboard_output_config = TensorBoardOutputConfig( s3_output_path='s3:\/\/PATH\/to\/my\/bucket')\n\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server': {'enabled':False}},\n                        tensorboard_output_config=tensorboard_output_config)\n<\/code><\/pre>\n\n<p>i added the callback in my model.py script that is actually what i use without sagemaker. As logdir i defined the default dir, where the TensoboardOutputConfig writes the data.. but it doesnt work. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_TensorBoardOutputConfig.html\" rel=\"nofollow noreferrer\">docs<\/a> I also used it without the callback.<\/p>\n\n<pre><code> tensorboardCallback = tf.keras.callbacks.TensorBoard(\n        log_dir='\/opt\/ml\/output\/tensorboard',\n        histogram_freq=0,\n        # batch_size=32,ignored tf.2.0\n        write_graph=True,\n        write_grads=False,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None,\n        embeddings_data=None,\n        update_freq='batch') \n<\/code><\/pre>",
        "Challenge_closed_time":1604514854867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585083712870,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use TensorBoard with AWS SageMaker TensorFlow but is encountering issues. They have tried using `run_tensorboard_locally=True` but received an error message stating that TensorBoard is not supported with script mode. The user is now trying to access the logs of TensorBoard in S3 but is unsure where they are located. They have also tried using `TensorBoardOutputConfig` and adding a callback in their `model.py` script, but it still does not work.",
        "Challenge_last_edit_time":1585093210283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60839279",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":43.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":5397.5394436111,
        "Challenge_title":"how can i use tensorboard with aws sagemaker tensorflow?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1011.0,
        "Challenge_word_count":254,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455058326760,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1337.0,
        "Poster_view_count":214.0,
        "Solution_body":"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.<\/p>\n<ol>\n<li><p>Followed guide on <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook\" rel=\"noreferrer\">sagemaker debugging<\/a> to configure the <code>S3<\/code> output path for tensorboard logs.<\/p>\n<pre><code>from sagemaker.debugger import TensorBoardOutputConfig\n\ntensorboard_output_config = TensorBoardOutputConfig(\n       s3_output_path = 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n)\n\nestimator = TensorFlow(entry_point='train.py',\n               source_dir='.\/',\n               model_dir=model_dir,\n               output_path= output_dir,\n               train_instance_type=train_instance_type,\n               train_instance_count=1,\n               hyperparameters=hyperparameters,\n               role=sagemaker.get_execution_role(),\n               base_job_name='Testing-TrainingJob',\n               framework_version='2.2',\n               py_version='py37',\n               script_mode=True,\n               tensorboard_output_config=tensorboard_output_config)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<\/li>\n<li><p>Start the tensorboard with the <code>S3<\/code> location provided above via a terminal on the notebook instance.<\/p>\n<pre><code>$ tensorboard --logdir 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n<\/code><\/pre>\n<\/li>\n<li><p>Access the board via URL with <code>\/proxy\/6006\/<\/code>. You need to update the notebook instance details in the following URL.<\/p>\n<pre><code>https:\/\/myinstance.notebook.us-east-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":20.74,
        "Solution_score_count":5.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"TensorBoard not supported"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.1061447222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with <code>wandb.sweep<\/code>. For now, in order to get the best group of hyper-params, I have to look for the best group on my own and record those params manually. I wonder whether there is a way to extract or collect reuslts of hyper-params automatically by <code>wandb<\/code>?<br>\nThanks a lot!<\/p>",
        "Challenge_closed_time":1682009719808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681937337687,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using wandb.sweep to tune hyper-parameters and is manually recording the best group of hyper-parameters. They are seeking a way to automatically extract or collect results of hyper-parameters using wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/collect-results-from-sweep\/4238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.1061447222,
        "Challenge_title":"Collect results from sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello!<\/p>\n<p>We have <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=G01IM4yVkc6u\" rel=\"noopener nofollow ugc\">Parallel Coordinate plots and Hyper Parameter Importance Plots<\/a> in the UI that can help with looking for the best group! In terms of collecting results of sweeps, the hyperparameters are automatically logged to the <code>config.yaml<\/code> file in your run\u2019s file tab.  However, if you want to collect the hyperparameters  yourself, you can also access individual hyperparameter values using <code>wandb.config['hyperparameter-name']<\/code> within the <code>main()<\/code> function you are running your sweep on. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/config\">Here<\/a> is our documentation on ways to use access and update the config file.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":11.56,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"automate hyper-parameter results"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.8680555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can I train models in parallel? Is is possible to train model in parallel on like hyperdrive?<\/p>",
        "Challenge_closed_time":1653922130807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653904605807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training models in parallel, similar to using hyperdrive.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869619\/parallel-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.8680555556,
        "Challenge_title":"Parallel training",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3d6a9d61-6cf9-45d8-870d-2fbbf147f56d\">@Chungsun  <\/a>  Thanks for the question. The max number of parallel tasks is limited by number of cores in the cluster (excluding master node).    <br \/>\nThe demand for parallelism comes from two sources: 1. The cross validation which address multiple combination of train-val datasets &amp; parameters 2. The training algorithm itself which can be parallelized.    <\/p>\n<p>\u2022\tYou can run multiple runs in a distributed fashion across AML clusters, meaning that each cluster node can be running a run in parallel to other nodes running other runs. For instance, that\u2019s what we also do with Pipeline steps, HyperParameter Tunning child runs and for Azure AutoML child runs.    <\/p>\n<p> <a href=\"https:\/\/github.com\/microsoft\/solution-accelerator-many-models\"> https:\/\/aka.ms\/many-models<\/a> is a solution accelerator that will help you walk through to run many models.     <br \/>\nIn the HyperDriveConfig there is AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run. So there would be 1 execution per node.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"parallel model training"
    },
    {
        "Answerer_created_time":1221528724667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"West Coast, North America",
        "Answerer_reputation_count":11340.0,
        "Answerer_view_count":737.0,
        "Challenge_adjusted_solved_time":182.4722333334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1587598650403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586941750363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to delete worker task templates in AWS Augmented AI. They have created a test template but are unsure of how to remove it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61225077",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":182.4722333334,
        "Challenge_title":"How to delete worker task templates in AWS Augmented AI?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":41,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"delete worker task templates"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":235.0291777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using Inference Schema to autogenerate the swagger doc for my AzureML endpoint (as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">here<\/a>), I see that it creates a wrapper around my input_sample. Is there a way to\nnot wrap the input inside this &quot;data&quot; wrapper?<\/p>\n<p>Here is what my score.py looks like:<\/p>\n<pre><code>input_sample = {\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\noutput_sample = [{'prediction': 'true', 'predictionConfidence': 0.8279970776764844}]\n\n@input_schema('data', StandardPythonParameterType(input_sample))\n@output_schema(StandardPythonParameterType(output_sample))\ndef run(data):\n&quot;&quot;&quot;\n    {\n        data: { --&gt; DON'T WANT this &quot;data&quot; wrapper\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\n    }\n    &quot;&quot;&quot;\n    try:\n        id = data['id']\n        ...\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1601883755636,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600982458753,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Inference Schema to autogenerate the swagger doc for their AzureML endpoint. However, they are encountering a problem where Inference Schema creates a wrapper around their input_sample. The user is looking for a way to remove this \"data\" wrapper.",
        "Challenge_last_edit_time":1601039285472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64054587",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":16.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":250.3602452778,
        "Challenge_title":"How can I remove the wrapper around the input when using Inference Schema",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406298639460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":435.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>InferenceSchema used with Azure Machine Learning deployments, then the code for this package was recently published at <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/InferenceSchema<\/a> under an MIT license. So you could possibly use that to create a version specific to your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601885390512,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unwanted data wrapper"
    },
    {
        "Answerer_created_time":1619174589310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Challenge_adjusted_solved_time":254.5180547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1652647881720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651731616723,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use Golang to make predictions from a deployed Vertex AI model on an endpoint. They have created code based on a Google Cloud example, but when they run their test app, they receive an error message stating \"unexpected HTTP status code received from server: 404 (Not Found).\" The user has set the path to their service account JSON file on the GOOGLE_APPLICATION_CREDENTIALS environment variable.",
        "Challenge_last_edit_time":1654669807780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.0,
        "Challenge_reading_time":26.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":254.5180547222,
        "Challenge_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":177,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651730962056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1652712010567,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":8.78,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unexpected HTTP status code"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.2721991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am considering using Personalizer for project and have found limited third party metrics for this service. This one article indicates needing TENS of thousands of hits to get good results.  <\/p>\n<p><a href=\"https:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e\">https:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e<\/a>  <\/p>\n<p>Can any one provide any other data?   <\/p>\n<p>Obviously, over time it will get better, but does it have to get to 10K+ to get good?<\/p>",
        "Challenge_closed_time":1606893531207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606863751290,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is considering using Personalizer for a project but is facing limited third-party metrics for the service. They have come across an article that suggests needing tens of thousands of hits to get good results. The user is seeking additional data and wondering if it is necessary to reach 10K+ hits to achieve good results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/182318\/training-personalizer",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.2721991667,
        "Challenge_title":"Training Personalizer",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=bac6bba0-0ba2-419e-b1dd-254f191be319\">@Gregorio Rojas  <\/a> The minimum requirements to have an effective recommendation is to have a minimum of ~1k\/day content-related events. Higher rate of events do help you to provide faster and better recommendations. All the requirements are documented in the official documentation <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/cognitive-services\/personalizer\/what-is-personalizer#content-requirements\">page<\/a> of the service. The samples <a href=\"https:\/\/github.com\/Azure-Samples\/cognitive-services-personalizer-samples\">repo<\/a> provides some data along with the quickstart's from the documentation to get started. The service now provides an E0 tier or apprentice mode that helps you test the service and gain confidence to move to a higher tier with production level recommendations.     <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":11.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":95.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"Personalizer metrics"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1086111111,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI would like to compare the top 4 experiments based on a specific metrics.\n\nCurrently I query the top experiment using the cli:\n\npolyaxon ops ls -q \"name: GROUP_NAME, metrics.loss:<0.002\"  -s \"metrics.loss\" -l 5\n\nAnd then I copy\/paste the run UUIDs to:\n\npolyaxon run --hub tensorboard:mulit-run -P uuids=UUID1,UUID2,UUID3,UUID4,UUID5",
        "Challenge_closed_time":1649336377000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649335986000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to compare the top 4 experiments based on a specific metric and is currently using the CLI to query the top experiment. They then copy and paste the run UUIDs to start a Tensorboard for the top 5 experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1483",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1086111111,
        "Challenge_title":"How can I start a Tensorboard for the top 5 experiments",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"From the UI or using a YAML file, you can run:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"name: GROUP_NAME, metrics.loss:<0.002,  kind:job\"  \n  sort: \"metrics.loss\"\n  limit: 5\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nNote that in the UI if create a filter \/ sort configuration, you can automatically create a multi-run Tensorboard based on that query, for example:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Polyaxon",
        "Challenge_type":"inquiry",
        "Challenge_summary":"compare top experiments"
    },
    {
        "Answerer_created_time":1531218624572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":78.4838436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a function which looks like this:<\/p>\n<pre><code>def fine_tuning(x,y,model1,model2,model3,trial):\n   pred1 = model1.predict(x)\n   pred2 = model2.predict(x)\n   pred3 = model3.predict(x)\n   \n   h1 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h2 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h3 = trial.suggest_float('h1', 0.0001, 1, log = True)\n\n   pred = pred1 * h1 + pred2 * h2 + pred3 * h3\n\n   return mean_absolute_error(y, pred)\n<\/code><\/pre>\n<p>The problem with this function is that h1+h2+h3 != 1. How would I change this function in order to make the sum of the hyperparmaters = 1?<\/p>",
        "Challenge_closed_time":1627290244260,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627007702423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in fine-tuning a function using Optuna as the sum of the hyperparameters does not add up to 1. They are seeking guidance on how to modify the function to ensure that the sum of the hyperparameters equals 1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68493392",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":78.4838436111,
        "Challenge_title":"Making hyperparameters add up to 1 when fine tuning using Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575887707992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":196.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>Basically, you're looking for a dirichlet distribution for h1, 2, 3. Here's a guide on how to implement that for Optuna: <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":38.4,
        "Solution_reading_time":6.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"hyperparameters sum issue"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.9617827778,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1654627313182,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654584250764,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is exploring sweeps for a university project and wants to know if it is possible to stop bad runs after a few epochs to save time. They have provided a report showing a run that does not look promising and would like to cancel it and start a new one instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Challenge_link_count":10,
        "Challenge_participation_count":5,
        "Challenge_readability":17.8,
        "Challenge_reading_time":37.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":11.9617827778,
        "Challenge_title":"How to early stop bad runs in sweeps to save time",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":663.0,
        "Challenge_word_count":193,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":90.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"stop bad runs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.3012636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've a azure synapse analytics workspace in region North Europe, as the region has hardware Accelerated pools, GPU base pools so to say. But i don't see the packages setting.     <br \/>\nhere is the comparison for 2 workspace, 1 in north Europe and other one in West Europe.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209418-screenshot-2022-06-08-at-120209.png?platform=QnA\" alt=\"209418-screenshot-2022-06-08-at-120209.png\" \/> vs <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209494-screenshot-2022-06-08-at-120550.png?platform=QnA\" alt=\"209494-screenshot-2022-06-08-at-120550.png\" \/>    <\/p>\n<p>Even the package setting in the Workspace itself is disabled for me: here is the screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209434-screenshot-2022-06-08-at-120143.png?platform=QnA\" alt=\"209434-screenshot-2022-06-08-at-120143.png\" \/>    <\/p>\n<p>I've 2 questions in this reagrd:     <\/p>\n<ul>\n<li> Am I missing any configuration for the GPU pool or this feature is not released?    <\/li>\n<li> Is there any alternate way to install a package? <code>pip install<\/code> or <code>pip3 install<\/code> are not working.      <\/li>\n<\/ul>",
        "Challenge_closed_time":1654770659816,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654683175267,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing a Python package in a hardware accelerated GPU Spark pool in their Azure Synapse Analytics workspace located in North Europe. They are unable to find the package setting and it is disabled in the workspace. The user has two questions regarding this issue: whether they are missing any configuration for the GPU pool or if this feature is not yet released, and if there is an alternate way to install the package as pip install or pip3 install are not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/881432\/how-to-install-python-package-in-hardware-accelera",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":24.3012636111,
        "Challenge_title":"How to install python package in hardware accelerated GPU spark pool ?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=f236076b-e057-4c60-b0fd-0068a6492053\">@Prateek Narula  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>(UPDATE:6\/10\/2022): Unfortunately, we do not have Library Management (Package) support for GPU spark pools in Azure Synapse Analytics.    <\/p>\n<\/blockquote>\n<p>---------------------------------------------------    <\/p>\n<p>As per the repro, I had noticed similar behaviour.     <\/p>\n<blockquote>\n<p>Looks like packages are only supported for Node size family: &quot;Memory Optimized&quot; - let me get a confirmation from the product team.    <\/p>\n<\/blockquote>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209808-synape-gpu.gif?platform=QnA\" alt=\"209808-synape-gpu.gif\" \/>    <\/p>\n<p>We are reaching out to internal team to get more information related to this issue and will get back to you as soon as we have an update.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.6,
        "Solution_reading_time":26.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":213.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"disabled package installation"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1076.4220325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Challenge_closed_time":1617006916700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611574816563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an event rule in Cloudwatch to monitor Sagemaker training job state changes and trigger a lambda function to send messages in a telegram group. However, the events are fired multiple times with the same payload, resulting in duplicate messages. Enabling the Sagemaker profiler causes the number of identical rule invocations to increase. The user suspects a bug in AWS and is looking for a simple solution to avoid duplicate messages.",
        "Challenge_last_edit_time":1613131797383,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1508.9167047222,
        "Challenge_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":780.0,
        "Challenge_word_count":336,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"duplicate event rule firing"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":247.7520736111,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Challenge_closed_time":1656556961635,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655665054170,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble associating their sweeps with a project in wandb. They have tried setting the project name in the sweep configuration and using wandb.init with the project name, but the sweeps are still showing up as uncategorized in the projects interface. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.6,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":247.7520736111,
        "Challenge_title":"Can't associate sweeps with project",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"uncategorized sweeps"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":20.4471097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1507014239332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506940629737,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a machine learning model as a web service using Azure Machine Learning Studio. They trained a LogisticRegressionCV model from sklearn.linear_model locally on their machine and tried to publish it as a web service. However, they encountered an error stating that 'module' object has no attribute 'LogisticRegressionCV'. The user wants to update sklearn on their workspace and is seeking advice on how to install any python module on Azure Machine Learning Studio to use predict functions of any model developed locally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46523924",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":25.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":20.4471097222,
        "Challenge_title":"Adding python modules to AzureML workspace",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2578.0,
        "Challenge_word_count":249,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458750704640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":186.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.6,
        "Solution_reading_time":17.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"install python module on Azure ML Studio"
    },
    {
        "Answerer_created_time":1376999872723,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Malaysia",
        "Answerer_reputation_count":998.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":0.0276297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create an forecasting experiment using R engine. My data source is pivoted, hence I need to pass row by row.\nThe output works great with single row prediction. But when I try to populate multiple lines, it still gives single row output - for the first record only.<\/p>\n\n<p>I'm trying to loop my result as follows :<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(forecast)\nlibrary(reshape)\nlibrary(dplyr)\nlibrary(zoo)\n#exclude non required columns\nmy.ds &lt;- dataset1[, -c(4,5,6)]\n# set the CIs we want to use here, so we can reuse this vector\ncis &lt;- c(80, 95)\n\nfor (i in 1:nrow(my.ds)) {\nmy.start &lt;- my.ds[i,c(3)]\nmy.product &lt;- my.ds[i, \"Product\"]\nmy.location &lt;- my.ds[i, \"Location\"]\nmy.result &lt;- melt(my.ds[i,], id = c(\"Product\",\"Location\"))\nmy.ts &lt;- ts(my.result$value, frequency=52, start=c(my.start,1))\n# generate the forecast using those ci levels\nf &lt;- forecast(na.interp(my.ts), h=52, level=cis)\n# make a data frame containing the forecast information, including the index\nz &lt;- as.data.frame(cbind(seq(1:52),\n                       f$mean,\n                       Reduce(cbind, lapply(seq_along(cis), function(i) cbind(f$lower[,i], f$upper[,i])))))\n# give the columns better names\nnames(z) &lt;- c(\"index\", \"mean\", paste(rep(c(\"lower\", \"upper\"), times = length(cis)), rep(cis, each = 2), sep = \".\"))\n# manipulate the results as you describe\nzw &lt;- z %&gt;%\n# keep only the variable you want and its index\nmutate(sssf = upper.95 - mean) %&gt;%\nselect(index, mean, sssf) %&gt;%\n# add product and location info\nmutate(product = my.product,\n       location = my.location) %&gt;%\n# rearrange columns so it's easier to read\nselect(product, location, index, mean, sssf)\nzw &lt;- melt(zw, id.vars = c(\"product\", \"location\", \"index\"), measure.vars = c(\"mean\",\"sssf\"))\ndata.set &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n}\n<\/code><\/pre>\n\n<p>This is design of my experiment :\n<a href=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" alt=\"experiment\"><\/a><\/p>\n\n<p>And this is how sample <a href=\"https:\/\/www.dropbox.com\/s\/xgfc7pnyy29frid\/dhf-00009E850%20-%20Copy.csv?dl=0\" rel=\"nofollow noreferrer\" title=\"input file\">input<\/a> looks like :<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" alt=\"Sample input\"><\/a><\/p>\n\n<p>I'm testing using the Excel test workbook downloaded from experiment site.<\/p>",
        "Challenge_closed_time":1480086125487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1480051198437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Batch Run - Single Output. The output works fine with single row prediction, but when the user tries to populate multiple lines, it still gives a single row output for the first record only. The user is trying to loop the result, but it is not working as expected. The user has shared the code and experiment design for reference.",
        "Challenge_last_edit_time":1480086026020,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40798184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":33.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":9.7019583334,
        "Challenge_title":"Azure ML Batch Run - Single Output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":302,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376999872723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Malaysia",
        "Poster_reputation_count":998.0,
        "Poster_view_count":136.0,
        "Solution_body":"<p>I figured out the problem :<\/p>\n\n<pre><code>{\n...\nds &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\ndata.set &lt;- rbind(data.set, ds)\n}\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>I should be merging the rows and then output outside of the loop.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"single output issue"
    },
    {
        "Answerer_created_time":1509012479112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belo Horizonte, MG, Brasil",
        "Answerer_reputation_count":97.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":11.9663136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1568727043932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568683965203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up hyperparameters for a classification training job using AWS SageMaker Training Jobs console with H2o.AutoMl. Specifically, the user is unsure how to set up the 'training' field and is seeking assistance. The user's data is stored on S3 as a CSV file with 250000 columns, 4 categorical columns, and one target column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.9663136111,
        "Challenge_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.3,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"set up hyperparameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":10,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug in the logger behavior of PyTorchLightning after a recent update. The logger starts using `COMET_EXPERIMENT_KEY` but does not respect it if it is already set. The logger overwrites the user's value, deletes the variable, and ignores the set variable in the version function. The user plans to create a pull request to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Discussion_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"logger behavior bug"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":166.3613888889,
        "Challenge_answer_count":0,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770135000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1666171234000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running benchmark.py with WandB due to the config being too large, resulting in a 400 error. The error message suggests that the train_selection array may be too big. The user suggests that the data selections may not need to be uploaded to WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":65.0,
        "Challenge_repo_issue_count":548.0,
        "Challenge_repo_star_count":63.0,
        "Challenge_repo_watch_count":5.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":166.3613888889,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":858,
        "Discussion_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"config too large"
    },
    {
        "Answerer_created_time":1373018880576,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":17.1053752778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Challenge_closed_time":1543906305888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543844726537,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is following AWS Sagemaker documentation to train and deploy an ML model using the high-level Python library provided by Amazon SageMaker. However, the deployment fails with an error message indicating that the account-level service limit for endpoint usage has been exceeded. The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.1053752778,
        "Challenge_title":"AWS Sagemaker Deploy fails",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5932.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373018880576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":305.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.0,
        "Solution_reading_time":2.34,
        "Solution_score_count":7.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"exceeded endpoint usage limit"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1587712154000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an internal server error while deploying a container to AKS using Azure ML CLI. The error occurs sporadically and there is no clear pattern to it. The error message suggests creating a retry loop, but this would not address the underlying issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Discussion_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"sporadic internal server error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1232.805,
        "Challenge_answer_count":0,
        "Challenge_body":"### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions.\n\n\n### Question\n\nI am unable to train alway the same error:\r\n\r\npython train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt\r\ntrain: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5\r\nYOLOv5  2022-11-26 Python-3.9.13 torch-1.13.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Using 'C:\\\\Users\\\\telem\\\\Desktop\\\\Yolo\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nCOMET WARNING: Native output logging mode is not available, falling back to basic output logging\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 633, in <module>\r\n    main(opt)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 95, in train\r\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\__init__.py\", line 132, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(COMET_PREFIX):\r\nKeyError: 'path'\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO: ----------------------------------\r\nCOMET WARNING: Experiment Name is generated at upload time for Offline Experiments unless set explicitly with Experiment.set_name\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Starting saving the offline archive\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload C:\\Users\\telem\\Desktop\\Yolo\\.cometml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip\r\n\r\nI have tested many dataset and alway the same error any advice ?\r\n\n\n### Additional\n\n_No response_",
        "Challenge_closed_time":1673914957000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1669476859000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a logging issue when activating the Comet contrib in Ludwig. Most of the Ludwig log messages disappear, and the expected behavior is to display the log messages when the Comet contrib is activated. The issue is that Ludwig is using the root-level logger configured through `logging.basicConfig`, and the first call to `logging.info` will configure the root logger with no configuration, which will create a StreamHandler pointing to `\/dev\/stderr`. The user recommends moving from using the root logger and configuring the logger through `basicConfig` to using a `ludwig` logger and configuring it manually.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ultralytics\/yolov5\/issues\/10301",
        "Challenge_link_count":4,
        "Challenge_participation_count":15,
        "Challenge_readability":12.5,
        "Challenge_reading_time":59.08,
        "Challenge_repo_contributor_count":305.0,
        "Challenge_repo_fork_count":13969.0,
        "Challenge_repo_issue_count":10524.0,
        "Challenge_repo_star_count":39099.0,
        "Challenge_repo_watch_count":350.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":1232.805,
        "Challenge_title":"Comet Bug: Unable to train on Window 11",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":458,
        "Discussion_body":"\ud83d\udc4b Hello @telemac73, thank you for your interest in YOLOv5 \ud83d\ude80! Please visit our \u2b50\ufe0f [Tutorials](https:\/\/docs.ultralytics.com\/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https:\/\/docs.ultralytics.com\/yolov5\/tutorials\/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https:\/\/docs.ultralytics.com\/yolov5\/tutorials\/hyperparameter_evolution).\n\nIf this is a \ud83d\udc1b Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training \u2753 Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https:\/\/docs.ultralytics.com\/yolov5\/tutorials\/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https:\/\/ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https:\/\/www.python.org\/) with all [requirements.txt](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/requirements.txt) installed including [**PyTorch>=1.7**](https:\/\/pytorch.org\/get-started\/locally\/). To get started:\n```bash\ngit clone https:\/\/github.com\/ultralytics\/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https:\/\/developer.nvidia.com\/cuda)\/[CUDNN](https:\/\/developer.nvidia.com\/cudnn), [Python](https:\/\/www.python.org\/) and [PyTorch](https:\/\/pytorch.org\/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https:\/\/bit.ly\/yolov5-paperspace-notebook\"><img src=\"https:\/\/assets.paperspace.io\/img\/gradient-badge.svg\" alt=\"Run on Gradient\"><\/a> <a href=\"https:\/\/colab.research.google.com\/github\/ultralytics\/yolov5\/blob\/master\/tutorial.ipynb\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"><\/a> <a href=\"https:\/\/www.kaggle.com\/ultralytics\/yolov5\"><img src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\" alt=\"Open In Kaggle\"><\/a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https:\/\/docs.ultralytics.com\/yolov5\/environments\/google_cloud_quickstart_tutorial\/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https:\/\/docs.ultralytics.com\/yolov5\/environments\/aws_quickstart_tutorial\/)\n- **Docker Image**. See [Docker Quickstart Guide](https:\/\/docs.ultralytics.com\/yolov5\/environments\/docker_image_quickstart_tutorial\/) <a href=\"https:\/\/hub.docker.com\/r\/ultralytics\/yolov5\"><img src=\"https:\/\/img.shields.io\/docker\/pulls\/ultralytics\/yolov5?logo=docker\" alt=\"Docker Pulls\"><\/a>\n\n## Status\n\n<a href=\"https:\/\/github.com\/ultralytics\/yolov5\/actions\/workflows\/ci-testing.yml\"><img src=\"https:\/\/github.com\/ultralytics\/yolov5\/actions\/workflows\/ci-testing.yml\/badge.svg\" alt=\"YOLOv5 CI\"><\/a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https:\/\/github.com\/ultralytics\/yolov5\/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/train.py), [validation](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/val.py), [inference](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/detect.py), [export](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/export.py) and [benchmarks](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit. @telemac73 seems to be Comet related. Try pip uninstall comet and rerun your command.\r\n\r\n@DN6 can you take a look at this Comet-related bug above? Thanks!! pip uninstall comet-ml\r\n\r\nhave been fixed my issue Thanks ) same problem with comet_ml,but then how to properly link to comet? @DN6 can you take a look at this error that users are seeing with Comet? Thanks! @HanBinGer can you help us by providing a minimum reproducible example of your error? Thanks! @glenn-jocher \r\nSo, I tried both on my personal PC and in the example of the Collab. The same problem is everywhere. \r\nComet initialization is successful.\r\n\r\nCollab steps:\r\n![image](https:\/\/user-images.githubusercontent.com\/35628064\/205402752-c0761a83-f209-4c31-b87f-ecfe190be1f0.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/35628064\/205402794-edb50abb-ac55-43a6-8a4e-2629e2526347.png)\r\n\r\nThis is my personal step to download custom dataset:\r\n![image](https:\/\/user-images.githubusercontent.com\/35628064\/205402885-8f7cc433-3b0b-441c-a13f-ac937f05d45c.png)\r\n\r\nand when I try to start a train:\r\n![image](https:\/\/user-images.githubusercontent.com\/35628064\/205403011-f82c1995-783e-486c-bc36-248c8fb18186.png)\r\n\r\nThen I get this:\r\n\r\ntrain: weights=\/content\/yolov5\/best.pt, cfg=, data=\/content\/yolov5\/FirstTry-1\/data.yaml, hyp=data\/hyps\/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: up to date with https:\/\/github.com\/ultralytics\/yolov5 \u2705\r\nYOLOv5 \ud83d\ude80 v7.0-21-ga1b6e79 Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 \ud83d\ude80 in ClearML\r\nTensorBoard: Start with 'tensorboard --logdir runs\/train', view at http:\/\/localhost:6006\/\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, tensorboard. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\r\nCOMET INFO: Experiment is live on comet.com https:\/\/www.comet.com\/...\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 633, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"train.py\", line 95, in train\r\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n  File \"\/content\/yolov5\/utils\/loggers\/__init__.py\", line 139, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n  File \"\/content\/yolov5\/utils\/loggers\/comet\/__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  File \"\/content\/yolov5\/utils\/loggers\/comet\/__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(COMET_PREFIX):\r\nKeyError: 'path'\r\nCOMET INFO: ---------------------------\r\nCOMET INFO: Comet.ml Experiment Summary\r\nCOMET INFO: ---------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     url                   : https:\/\/www.comet.com\/...\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO:     os packages         : 1\r\nCOMET INFO: ---------------------------\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, tensorboard. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\r\nCOMET INFO: The Python SDK has 3600 seconds to finish before aborting...\r\nCOMET INFO: Uploading 1 metrics, params and output messages Hi @HanBinGer. Would you be able to share the contents of your `data.yml` file here?  Hi @DN6. This file was automatically generated by Roboflow. Do you think that the problem may be in the paths specified in it?\r\n\r\ndata.yaml:\r\n\r\n\r\n`names:`\r\n`- I`\r\n`- cursor`\r\n`- enemymark`\r\n`- mark`\r\n`- scale`\r\n`- squad`\r\n`- trash`\r\n`nc: 7`\r\n`roboflow:`\r\n`  license: Public Domain`\r\n`  project: firsttry-ra06x`\r\n`  url: https:\/\/universe.roboflow.com\/...`\r\n` version: 1`\r\n`  workspace: wtrecognition`\r\n`test: FirstTry-1\/test\/images`\r\n`train: FirstTry-1\/train\/images`\r\n`val: FirstTry-1\/valid\/images`\r\n @DN6 thanks for taking a look! Hi @HanBinGer, the Comet integration expects the `data.yml` file to have a path variable defined that points to the root directory of your dataset. This is following the [dataset formatting guidelines from the YOLOv5 docs](https:\/\/docs.ultralytics.com\/tutorials\/train-custom-datasets\/). \r\n\r\nWould it be possible to structure your `data.yml` file in the following way? \r\n\r\n```\r\n# Train\/val\/test sets as 1) dir: path\/to\/imgs, 2) file: path\/to\/imgs.txt, or 3) list:[path\/to\/imgs1, path\/to\/imgs2, ..]\r\n\r\npath: ..\/datasets\/coco128  # dataset root dir\r\ntrain: images\/train2017  # train images (relative to 'path') 128 images\r\nval: images\/train2017  # val images (relative to 'path') 128 images\r\ntest:  # test images (optional)\r\n\r\n# Classes (80 COCO classes)\r\nnames:\r\n  0: person\r\n  1: bicycle\r\n  2: car\r\n  ...\r\n  77: teddy bear\r\n  78: hair drier\r\n  79: toothbrush\r\n``` Thx @DN6. It helped, now comet is working fine. Maybe this should be clarified in the guide? I'll make a PR next week to update the guide @glenn-jocher @HanBinGer  @DN6 great, yes please do! \ud83d\udc4b Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https:\/\/ultralytics.com\/yolov5) \ud83d\ude80 resources:\n- **Wiki** \u2013 https:\/\/github.com\/ultralytics\/yolov5\/wiki\n- **Tutorials** \u2013 https:\/\/docs.ultralytics.com\/yolov5\n- **Docs** \u2013 https:\/\/docs.ultralytics.com\n\nAccess additional [Ultralytics](https:\/\/ultralytics.com) \u26a1 resources:\n- **Ultralytics HUB** \u2013 https:\/\/ultralytics.com\/hub\n- **Vision API** \u2013 https:\/\/ultralytics.com\/yolov5\n- **About Us** \u2013 https:\/\/ultralytics.com\/about\n- **Join Our Team** \u2013 https:\/\/ultralytics.com\/work\n- **Contact Us** \u2013 https:\/\/ultralytics.com\/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 \ud83d\ude80 and Vision AI \u2b50!",
        "Discussion_score_count":3.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"logging issue"
    },
    {
        "Answerer_created_time":1621392911643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":2073.4166358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Challenge_closed_time":1621508759232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614044459343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to start a SageMaker training job via AWS Lambda + Gateway using the boto3 client since Lambda does not support the SageMaker Python SDK. However, the user is unable to find the argument or config name to specify a source directory and an entry point for the custom training job that requires data generation on the flight. The user has gone through the documentation for SageMaker.Client.create_training_job but couldn't find any field that allows setting a source directory and entry point.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":49.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":2073.4166358333,
        "Challenge_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":374,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483140008952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco Bay Area, CA, United States",
        "Poster_reputation_count":604.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.2,
        "Solution_reading_time":15.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to specify source directory and entry point"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3566.0353452778,
        "Challenge_answer_count":2,
        "Challenge_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas \n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a [request for a service quota increase](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#service-limit-increase-request-procedure).",
        "Challenge_closed_time":1655317929862,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642480202619,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to check their current service quotas for Amazon SageMaker, but there is no link provided for region-specific quotas, which may have changed after a request for a service quota increase.",
        "Challenge_last_edit_time":1668602220822,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sagemaker-service-quotas",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":14.7,
        "Challenge_reading_time":8.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3566.0353452778,
        "Challenge_title":"How do I check my current SageMaker service quotas?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1212.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655365526940,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"check SageMaker quotas"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1297805556,
        "Challenge_answer_count":1,
        "Challenge_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n```\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n```\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Challenge_closed_time":1669736547972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669725280762,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in passing environment variables in Sagemaker tuner job as there is no equivalent option available for tuner jobs. The SagemakerTuner in the python SDK is also ignoring the environment variables set in the passed estimator. The user is seeking a solution to pass environment variables to the training jobs started by a tuner job programmatically.",
        "Challenge_last_edit_time":1670071482024,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":9.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.1297805556,
        "Challenge_title":"How to pass environment variables in sagemaker tuner job",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":553.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for raising this. Yes, as you point out the `Environment` collection is not supported in the underlying `CreateHyperparameterTuningJob` API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the [SM Py SDK GitHub issue here](https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2934), you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore [setting the variables from your Python script](https:\/\/stackoverflow.com\/a\/5971326) (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the [ENV command](https:\/\/docs.docker.com\/engine\/reference\/builder\/#env)? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\n- Use `sagemaker.image_uris.retrieve(...)` to find the base image URI for your given framework, version, region, etc. You'll need to [authenticate Docker to this registry](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/getting-started-cli.html#cli-authenticate-registry) as well as your own Amazon ECR account.\n- Create a Dockerfile that takes this base image URI as an arg and builds `FROM` it, something like [this example](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/b95b62cd2abd304ee347cacdd3eaf2a76e8b5953\/notebooks\/custom-containers\/train-inf\/Dockerfile)\n- Add the required `ENV` commands to bake in the (static) environment variables you need\n- `docker build` your custom container (passing in the base image URI as a `--build-arg`), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669736547974,
        "Solution_link_count":5.0,
        "Solution_readability":13.1,
        "Solution_reading_time":22.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":211.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to pass environment variables"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.0207547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Because of network problem. The local <code>debug-internal.log<\/code> files of some runs are too large (more than 500MB). To save the disk space, is there any way to avoid the generation of these log files?<\/p>",
        "Challenge_closed_time":1672190667020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671769392303,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the local debug-internal.log files of some runs are too large (more than 500MB) due to a network problem. They are looking for a way to avoid the generation of these log files to save disk space.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/the-debug-internal-log-file-is-too-large-500mb\/3589",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":117.0207547222,
        "Challenge_title":"The debug-internal.log file is too large (>500MB)",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":40,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , thank you for writing in and happy to look into this for you.  <code>debug-internal.log<\/code> files are automatically generated and cannot be disabled by the user.  Please see this github issue thread that was raised about this issue were a user provided <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4223#issuecomment-1236304565\" rel=\"noopener nofollow ugc\">workaround<\/a> solution to address this . Do let me know if this reference helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":6.39,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"large debug log files"
    },
    {
        "Answerer_created_time":1576948018896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Portugal",
        "Answerer_reputation_count":580.0,
        "Answerer_view_count":135.0,
        "Challenge_adjusted_solved_time":0.1342122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey guys so recently i started working with sagemaker and I was testing autopilot and it got a fairly good accuracy and I wanted to test it on some more data so I chose the one with best ACC and created an endpoint. The problem now is that I don't know how to use the endpoit properly. I tried using AWS CLI but I keep getting the following errors:<\/p>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'SW0gaGFwcHk=' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from container-1 with message &quot;'application\/json' is an unsupported content type.&quot;. See https:\/\/eu-west-2.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/autopilottest in account 288240193481 for more information.\n<\/code><\/pre>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'Im happy!' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Invalid base64: &quot;Im happy!&quot;\n<\/code><\/pre>\n<p>Endpoit configuration:\n<a href=\"https:\/\/i.stack.imgur.com\/11qAt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/11qAt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1633716252927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633715769763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering errors while trying to use an endpoint created in Sagemaker. The errors include an unsupported content type and an invalid base64 message. The user has provided the endpoint configuration and the commands used to invoke the endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69499960",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1342122222,
        "Challenge_title":"Sagemaker Endpoint returning strange error",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":521.0,
        "Challenge_word_count":151,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576948018896,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":580.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Ended up fixing the issue by adding <code>--content-type text\/csv<\/code> and using base64 and it worked like a charm.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint errors"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is able to log and fetch metrics to AzureML using Run.log, but is unable to log run parameters like Learning Rate or Momentum. The user has tried to find a solution in the AzureML Python SDK documentation but has been unsuccessful. However, the user has found a way to log parameters using MLflow's mlflow.log_param, which shows up on the AzureML Studio Dashboard. The user is looking for a way to log parameters directly using azureml.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":688.1130463889,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"log parameters with AzureML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":82.4554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.<\/p>\n<p><strong>training code (whentrain.ipynb) :<\/strong><\/p>\n<pre><code>#import libs and packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom azureml.core import Workspace, Dataset\n\n# get existing workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n\n# load the dataset which is placed in the data folder\ndataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\ndataset = dataset.to_pandas_dataframe()\n\n# Create the outputs directories to save the model and images\nos.makedirs('outputs\/model', exist_ok=True)\nos.makedirs('outputs\/output', exist_ok=True)\ndataset['Date'] = pd.to_datetime(dataset['Date'])\ndataset = dataset.set_index('Date')\n###\nscaler = MinMaxScaler()\n\n#inputs\nX = dataset.iloc[:, 1:]\n#output\ny = dataset.iloc[:, :1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n###\n\nmodel1 = RandomForestRegressor(n_estimators = 6,\n                                   max_depth = 10,\n                                   min_samples_leaf= 1,\n                                   oob_score = 'True',\n                                   random_state=42)\nmodel1.fit(X_train, y_train.values.ravel())\n\ny_pred2 = model1.predict(X_test)\n<\/code><\/pre>\n<p><strong>And here is the code on the estimator part (estimator.ipynb):<\/strong><\/p>\n<pre><code>from azureml.core import Experiment\nfrom azureml.core import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import TensorFlow\nfrom azureml.widgets import RunDetails\n\nimport os\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\nexp = Experiment(workspace=workspace, name='azure-exp')\ncluster_name = &quot;gpucluster&quot;\n\ntry:\n    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                           max_nodes=1)\n\n    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\nfrom azureml.core import ScriptRunConfig\nsource_directory = os.getcwd()\n\nfrom azureml.core import Environment\n\nmyenv = Environment(&quot;user-managed-env&quot;)\nmyenv.python.user_managed_dependencies =True\nfrom azureml.core import Dataset\ntest_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n\nsrc = ScriptRunConfig(source_directory=source_directory,\n                      script='whentrain.ipynb',\n\n                      arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                      compute_target=compute_target,\n                      environment=myenv)\nrun = exp.submit(src)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>The error that happens in <strong>run.wait_for_completion<\/strong> states :<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 107, in &lt;module&gt;\n[stderr]    &quot;notebookHasBeenCompleted&quot;: true\n[stderr]NameError: name 'true' is not defined\n[stderr]\n<\/code><\/pre>\n<p>As you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?<\/p>\n<p>I'm running the Notebook on Python 3.<\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>Okay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 115, in &lt;module&gt;\n[stderr]    &quot;source_hidden&quot;: false,\n[stderr]NameError: name 'false' is not defined\n[stderr]\n<\/code><\/pre>",
        "Challenge_closed_time":1640628230640,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640331391010,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a NameError when trying to run a ScriptRunConfig in Azure Machine Learning. The error occurs in the whentrain.ipynb file and states that the name 'true' or 'false' is not defined. The user is unsure of the source of the error and is seeking assistance in resolving it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":61.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":82.4554527778,
        "Challenge_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":413,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b72317d8-d212-487c-8510-7d965e8d135f\">@Ash  <\/a> Ok, I think the issue is here.     <\/p>\n<pre><code>src = ScriptRunConfig(source_directory=source_directory,  \n                       script='whentrain.ipynb',  \n                            \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],  \n                       compute_target=compute_target,  \n                       environment=myenv)  \n<\/code><\/pre>\n<p>The script parameter is set to the notebook &quot;whentrain.ipynb&quot;, This should be a python script *.py which can train your model. Since you are using the notebook filename the entire source of jupyter notebook is loaded and it fails with these errors. You can lookup samples on azure ml notebook github repo for <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train.py\">reference<\/a>. I think if you can convert your whentrain.ipynb file to a python script whentrain.py and save it the current folder structure you should be able to use it in this step.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":12.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"NameError in ScriptRunConfig"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":63.8910877778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Related: <a href=\"https:\/\/community.wandb.ai\/t\/unable-to-manage-columns-in-project-run-table\/3551\/4\" class=\"inline-onebox\">Unable to manage columns in project run table - #4 by artsiom<\/a><\/p>\n<p>I was unable to make step metric columns visible in the Table view. I tried logging metrics both via <code>run.log<\/code> and <code>wandb.log<\/code>, as well as refreshing the page in my browser. When attempting to drag and drop a column name from \u201cHidden Columns\u201d to \u201cVisible Columns\u201d (see the screenshot), a gap is created, but on mouse release the column name returns to \u201cHidden Columns\u201d. Clicking on column names to move them to \u201cVisible\u201d does not work either. The logged values appear in the web interface elsewhere. Manipulation with non-metric columns (e.g. config values, name, state etc) worked flawlessly as expected.<\/p>\n<p>The problem remained <em>for a fraction of a minute<\/em> after I logged a summary metric using <code>wandb.summary[...] = ...<\/code>. In particular, I tried moving all columns by pressing \u201cShow all\u201d, but without any visible result, and I closed the pop-up (on the screenshot). Suddenly, after 10 or so seconds, all columns became visible.<\/p>\n<p>The problem is similar to the one in the linked post. Unlike there, in my case, refreshing the web-page did not seem to help. I\u2019ll take a wild guess and suggest possible reasons for the bug:<\/p>\n<ol>\n<li>Something was going on in your back-end, and I had to wait till all necessary data validation or calculations are completed that would enable adding metric columns. This is unacceptably long time (several minutes), within which I was able to read relevant reference, search issues, and do a couple of empty test runs to see what\u2019s going on.<\/li>\n<li>There is a bug which prevents conversion step metrics to summary metrics unless at least one summary metric is explicitly added via <code>wandb.summary<\/code>.<\/li>\n<\/ol>\n<p>I hope you will be able to get to the bottom of it and fix it.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" data-download-href=\"\/uploads\/short-url\/6GUslld1E38x1uAv9m6acBIMSwH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" alt=\"image\" data-base62-sha1=\"6GUslld1E38x1uAv9m6acBIMSwH\" width=\"518\" height=\"500\" data-dominant-color=\"F7F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">667\u00d7643 8.92 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676062281055,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675832273139,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue where they were unable to add step metric columns to the Table view in the web interface. They tried logging metrics using both `run.log` and `wandb.log`, but were unable to move the column names from \"Hidden Columns\" to \"Visible Columns\". The problem persisted for a short time even after logging a summary metric using `wandb.summary[...] = ...`. The user suggests two possible reasons for the bug: either something was going on in the back-end that required several minutes to complete, or there is a bug preventing the conversion of step metrics to summary metrics unless at least one summary metric is explicitly added via `wandb.summary`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/could-not-add-summary-columns-for-display-in-table\/3841",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":38.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":63.8910877778,
        "Challenge_title":"Could not add summary columns for display in Table",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":348,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a> , I\u2019ve been able to to consistently  reproduce this behavior on my end and flagged it as a bug. I will update you on a timeline for a fix once I have additional info. Thanks again for the insight!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.07,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to add step metric columns"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":360.0336111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943713000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1634647592000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running benchmark.py with WandB due to the config being too large, resulting in a 400 error. The error message suggests that the train_selection array may be too big. The user suggests that the data selections may not need to be uploaded to WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":74.0,
        "Challenge_repo_issue_count":1270.0,
        "Challenge_repo_star_count":755.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":360.0336111111,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":97,
        "Discussion_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"config too large"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":40.8553036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Challenge_closed_time":1550668517447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550539380057,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive and wants to register the model while adding a description to it. However, the register_model method does not seem to have a description flag, and the user is unsure how to add a description to the model so that it appears in the Azure Portal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":11.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":35.8714972222,
        "Challenge_title":"Add model description when registering model after hyperdrive successful run",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408574571227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1550686459150,
        "Solution_link_count":3.0,
        "Solution_readability":21.2,
        "Solution_reading_time":18.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"add model description"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0222222222,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi everyone. Is there a way to save display preferences in the job runs search UI? My main interest is in saving (1) which columns are displayed, and (2) the order in which they are displayed. Thank you.",
        "Challenge_closed_time":1650636762000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650636682000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to save display preferences in the job runs search UI, specifically which columns are displayed and the order in which they are displayed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1499",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":4.2,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0222222222,
        "Challenge_title":"How to persist custom job runs table",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There\u2019s a save button next to the query search, it persist everything configured in the dashboard: https:\/\/polyaxon.com\/docs\/management\/organizations\/searches\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":2.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Polyaxon",
        "Challenge_type":"inquiry",
        "Challenge_summary":"save display preferences"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":201.2384813889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with wandb.sweep(). As I know the params are defined in sweep_id and insert into wandb.sweep() like this:<\/p>\n<pre><code class=\"lang-auto\">    sweep_configuration = {\n    'method': 'bayes',\n    'name': 'I dont believe that I can not just give you a name!',\n    'metric': {'goal': 'minimize', 'name': 'Valid\/final_ber'},\n    'parameters':\n    {\n        'batch_size': {'distribution': 'int_uniform','min': 10,'max': 12},\n        'lr': {'distribution': 'int_uniform','max': -3,'min': -4}\n    }\n    }\n    sweep_id = wandb.sweep(sweep=sweep_configuration, project=args.project, entity=args.entity)\n<\/code><\/pre>\n<p>Now I what I want to do is to extract the params <strong>batch_size<\/strong> and <strong>lr<\/strong> from each sweep into the name of <code>wand.init()<\/code>, because I need these information in name of each run to identify them.<br>\nBut in wandb frame, I cannot get access to the params in <code>wandb.config<\/code> before <code>wandb.init()<\/code>. As a result I cannot define argument <strong>name<\/strong>  in <code>wandb.init()<\/code> with params which are given during each sweep.<\/p>\n<pre><code class=\"lang-auto\">......\nwandb.init(name=f'{wandb.config.lr}_{wandb.config.batch_size}')\n......\n\nRun wnb56ush errored: Error('You must call wandb.init() before wandb.config.lr')\nwandb: ERROR Run wnb56ush errored: Error('You must call wandb.init() before wandb.config.lr')\n<\/code><\/pre>\n<p>Is there a way to get the params given by <code>wandb.sweep()<\/code> before wandb.init()?<br>\nThanks at advance<\/p>",
        "Challenge_closed_time":1680815891260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680091432727,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to extract the hyperparameters 'batch_size' and 'lr' from each sweep and add them to the name of wandb.init() to identify each run. However, they are unable to access the params in wandb.config before wandb.init() and are encountering an error. The user is seeking a solution to get the params given by wandb.sweep() before wandb.init().",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-can-i-extract-params-from-sweep-and-add-them-into-name-of-wandb-init\/4146",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":201.2384813889,
        "Challenge_title":"How can I extract params from sweep and add them into name of wandb.init()",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":183,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/1060111768\">@1060111768<\/a> , it\u2019s not possible to get the sweep parameters before calling <code>wandb.init()<\/code>.<\/p>\n<p>When you run <code>wandb.sweep()<\/code> to define a hyperparameter sweep, it generates a unique sweep ID that is used to link the sweep to the subsequent runs that are generated by the sweep. This sweep ID is used to retrieve the sweep parameters when you initialize WandB by calling <code>wandb.init()<\/code>. The <code>wandb.init()<\/code> function retrieves the sweep parameters from the WandB servers using the sweep ID, and uses them to configure the run. Once you have called <code>wandb.init()<\/code>, you can access the sweep parameters using the <code>config<\/code> object.<\/p>\n<p>Instead of specifying a name in wandb init, rename the run immediately after initializing the run.<br>\nExample:<\/p>\n<pre><code class=\"lang-auto\">run =  wandb.init(config=config)\nrun.name=f\"{wandb.config.lr}_{wandb.config.batch_size}\"\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":12.78,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to access params"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":53.8060766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using aws sagemaker to invoke the endpoint : <\/p>\n\n<pre><code>payload = pd.read_csv('payload.csv', header=None)\n\n&gt;&gt; payload\n\n\n    0   1   2   3   4\n0   setosa  5.1     3.5     1.4     0.2\n1   setosa  5.1     3.5     1.4     0.2\n<\/code><\/pre>\n\n<p>with this code :<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>But I got this problem : <\/p>\n\n<pre><code>ParamValidationError                      Traceback (most recent call last)\n&lt;ipython-input-304-f79f5cf7e0e0&gt; in &lt;module&gt;()\n      1 response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n      2                                    ContentType='text\/csv',\n----&gt; 3                                    Body=payload)\n      4 \n      5 result = json.loads(response['Body'].read().decode())\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    584         }\n    585         request_dict = self._convert_to_request_dict(\n--&gt; 586             api_params, operation_model, context=request_context)\n    587 \n    588         handler, event_response = self.meta.events.emit_until_response(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _convert_to_request_dict(self, api_params, operation_model, context)\n    619             api_params, operation_model, context)\n    620         request_dict = self._serializer.serialize_to_request(\n--&gt; 621             api_params, operation_model)\n    622         prepare_request_dict(request_dict, endpoint_url=self._endpoint.host,\n    623                              user_agent=self._client_config.user_agent,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/validate.py in serialize_to_request(self, parameters, operation_model)\n    289                                                     operation_model.input_shape)\n    290             if report.has_errors():\n--&gt; 291                 raise ParamValidationError(report=report.generate_report())\n    292         return self._serializer.serialize_to_request(parameters,\n    293                                                      operation_model)\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value:         0    1    2    3    4\n0  setosa  5.1  3.5  1.4  0.2\n1  setosa  5.1  3.5  1.4  0.2, type: &lt;class 'pandas.core.frame.DataFrame'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I am just using the same code\/step like in the aws tutorial .  <\/p>\n\n<p>Can you help me to resolve this problem please?<\/p>\n\n<p>thank you<\/p>",
        "Challenge_closed_time":1536693194576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536499492700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to invoke an endpoint with Sagemaker. They are using AWS Sagemaker to invoke the endpoint with a specific code, but they are getting a \"ParamValidationError\" error. The error message indicates that the \"Body\" parameter is invalid, and the user is passing a Pandas DataFrame instead of a valid type like bytes or a file-like object. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52244963",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":34.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":53.8060766667,
        "Challenge_title":"Impossible to invoke endpoint with sagemaker",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4668.0,
        "Challenge_word_count":229,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>The payload variable is a Pandas' DataFrame, while <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">invoke_endpoint()<\/a> expects  <code>Body=b'bytes'|file<\/code>.<\/p>\n\n<p>Try something like this (coding blind):<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=open('payload.csv'))\n<\/code><\/pre>\n\n<p>More on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-inference.html\" rel=\"nofollow noreferrer\">expected formats here<\/a>. \nMake sure the file doesn't include a header.<\/p>\n\n<p>Alternatively, convert your DataFrame to bytes, <a href=\"https:\/\/stackoverflow.com\/questions\/34666860\/converting-pandas-dataframe-to-bytes\">like in this example<\/a>, and pass those bytes instead of passing a DataFrame.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.25,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid parameter error"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":13.3565036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Challenge_closed_time":1655849768796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655801685383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy AWS using CDK and Sagemaker, but is encountering an error when running pytest. They are stuck on step 3 of the process and are unsure of what to add in the infrastructure code. They are also unsure if they should do the whole process on their local terminal or on Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":13.3565036111,
        "Challenge_title":"How to deploy AWS using CDK, sagemaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":161,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604312740476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in pytest"
    },
    {
        "Answerer_created_time":1523298968403,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1754.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":140.9113833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Challenge_closed_time":1568804066440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1568296785460,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a model using the Sagemaker library in Python, but is encountering an error parsing the hyperparameters. They have identified that the missing parameter in their JSON configuration is the content_type parameter, which they have tried to add in various places with no success. The user is seeking guidance on how to properly specify the content_type in their training job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.0,
        "Challenge_reading_time":24.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":140.9113833333,
        "Challenge_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":644.0,
        "Challenge_word_count":182,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":51.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error parsing hyperparameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":755.2722222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221269000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1612502289000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error with CometLogger when using an API key without a save directory. The error occurs because the train loop tries to read the save directory, which is not set. The issue can be resolved by setting the save directory to None.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":755.2722222222,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Discussion_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error with CometLogger"
    },
    {
        "Answerer_created_time":1606374398152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":54.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":336.329535,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Am working on a project of consumer behaviour analysis on websites and predict the malicious activity of users in real-time.\nClick data is being collected for each click made by users.<\/p>\n<p>Am using multiple AWS services like kinesis stream, Lambda and sagemaker. I have created an autoencoder model and\ndeployed it as sagemaker endpoint which will be invoked using lambda when it receives new click data from the website through\nKinesis stream.<\/p>\n<p>Since sagemaker endpoint contains the only model but click data which lambda function receives is raw data with URLs, texts and\ndate. How can I pass raw data into required preprocessing steps and send processed data to sagemaker endpoint in the required format?<\/p>\n<p>Example of raw data:-<\/p>\n<p>{'URL':'www.amazon.com.au\/ref=nav_logo', 'Text':'Home', 'Information':'Computers'}<\/p>",
        "Challenge_closed_time":1624254151543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623043365217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a project to analyze consumer behavior on websites and predict malicious activity in real-time. They are using AWS services like Kinesis stream, Lambda, and SageMaker. They have created an autoencoder model and deployed it as a SageMaker endpoint, which will be invoked using Lambda when it receives new click data from the website through Kinesis stream. However, the raw data received by the Lambda function contains URLs, texts, and dates, and the user needs to preprocess this data before sending it to the SageMaker endpoint in the required format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67866286",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":11.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":336.329535,
        "Challenge_title":"Real-time Data Pre-processing in Lambda for SageMaker Endpoint",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":475.0,
        "Challenge_word_count":129,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604911047620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Victoria, Australia",
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.<\/p>\n<p>Reference:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a><\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":44.6,
        "Solution_reading_time":14.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"need to preprocess data"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":316.8641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539227000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1660398516000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MLFlow while running hyperparameter tuning as it expects an mlruns folder which they have not created. This can be resolved by sticking to the standard and avoiding the need to run `mlflow ui` with the backend store argument.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":31.0,
        "Challenge_repo_fork_count":106.0,
        "Challenge_repo_issue_count":728.0,
        "Challenge_repo_star_count":337.0,
        "Challenge_repo_watch_count":21.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":316.8641666667,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Discussion_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing mlruns folder"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":14.6592575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We want to tune a SageMaker <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/tuner.html?highlight=HyperparameterTuner\" rel=\"nofollow noreferrer\">HyperparameterTuner<\/a> (or something similar) where <em>several<\/em> components of the pipeline have associated hyperparameters. Both components in our case are realized via SageMaker containers for ML algorithms.<\/p>\n<pre><code>model = PipelineModel(..., models = [ our_model, xgb_model ])\ndeploy = Estimator(image_uri = model, ...)\n...\ntuner = HyperparameterTuner(deply, .... tune_parameters, ....)\ntuner.fit(...)\n<\/code><\/pre>\n<p>Now, there is of course the problem how to distribute the <code>tune_parameters<\/code> to the pipeline steps during the tuning.<\/p>\n<p>In scikit-learn this is achieved by specially naming the tuning parameters <code>&lt;StepName&gt;__&lt;ParameterName&gt;<\/code>.<\/p>\n<p>I don't see a way to achieve something similar with SageMaker, though. Also, search of the two keywords brings up the same question <a href=\"https:\/\/stackoverflow.com\/questions\/56308169\/creating-a-model-for-use-in-a-pipeline-from-a-hyperparameter-tuning-job\">here<\/a> but is not really what we want to do.<\/p>\n<p>Any suggestion how to achieve this?<\/p>",
        "Challenge_closed_time":1646418155780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646365382453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to tune a SageMaker PipelineModel with a HyperparameterTuner, but is facing the challenge of how to distribute the tuning parameters to the pipeline steps during the tuning process. The user is looking for suggestions on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71346372",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":14.6592575,
        "Challenge_title":"SageMaker: PipelineModel and Hyperparameter Tuning",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":129,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389092281248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ishikawa Prefecture",
        "Poster_reputation_count":227.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>If both the models need to be jointly optimized, you could run a SageMaker HPO job in script mode and define both the models in the script. Or you could run two HPO jobs, optimize each model, and then create the Pipeline Model. There is no native support for doing an HPO job on a PipelineModel.<\/p>\n<p>I work at AWS and my opinions are my own.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"distribute tuning parameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.1594822222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello everyone, I was trying to execute the example mentioned in the docs - [https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html]().\nI was able to successfully run this example but as soon as I changed the target_device  to `jetson_tx2`, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\n```\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n```\nAccording to the troubleshoot docs [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html](), this seems to be an issue of **model_fn**() function.\nThe inference script used by this example is mentioned here [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py]() , which itself doesn't contain any model_fn() definition but it still worked for target device `ml_c5`.\nSo could anyone please help me with the following questions:\n1. What changes does SageMaker Neo do to the model depending on `target_device` type? Since it seems the same model is loaded in a different way for different target device.\n2. Is there any way to determine how the model is expected to load for a certain target_device type so that I could define the **model_fn**() function myself in the same inference script mentioned above?\n3. At-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for `jetson_tx2` device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Challenge_closed_time":1668766092280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668430718144,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues with the inference script for Amazon Sagemaker Neo compiled models. They were able to successfully run an example script but encountered errors when changing the target device to `jetson_tx2`. The error message suggests an issue with the `model_fn()` function. The user is seeking help with understanding the changes made by SageMaker Neo to the model based on the target device, determining how the model is expected to load for a certain target device, and help with the inference script for the same model that works for `jetson_tx2` device.",
        "Challenge_last_edit_time":1668778694208,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":93.1594822222,
        "Challenge_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":282,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As you mentioned, you changed the Neo compiling target from `ml_c5` to `jetson_tx2`, the compiled model will require runtime from `jetson_tx2`. If you kept other code unchanged, the model will be deployed to a `ml.c5.9xlarge` EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment. \n\nIf you compile the model with `jeston_tx2` as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668766092280,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":6.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"errors with `jetson_tx2` device"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":100.1738869445,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have an OS stable diffusion fine tuner and use Tensorboard locally and am trying to integrate wandb with existing code that is largely just calling writer.log_scalar(\u2026).  I setup my SummaryWriter then call wandb.init, but I\u2019m having all sorts of odd behavior where most of the time only system monitors (gpu temp, memory etc) are logged to wandb and my calls to writer.log_scalar simply never get recorded to wandb.<\/p>\n<p>Everything seems to be failing silently and I don\u2019t know why nothing gets recorded.  The other day testing on two machines it works from one but not the other, and it is also now working from Colab notebook instances or docker container runs.<\/p>\n<p>The runs on <a href=\"http:\/\/wandb.com\" rel=\"noopener nofollow ugc\">wandb.com<\/a> are there and created, console output shows it fires up and links me to the run and the run URL works, etc.  But, only system monitors are showing up, none of my items logged with summarywriter, at least a vast majority of instances.<\/p>\n<p>At one point it was working fine, then started to stop working.  I had thought it was an issue with trying to pass in a dict of dicts to config={main: args, opt_cfg: optimizer_cfg} but even passing in dummy objects or simply config=args it fails.  At one point wanb.init was done before writer instantiation, and that was fixed, so I\u2019m not sure at what point things went sideways as I mostly run locally but many users use Colab\/Vast, etc and wandb is a significantly better solution for those cases.<\/p>\n<p>Is there any log file or debugging I can use to troubleshoot this?  Unfortunately it is just not working and doing so silently without any feedback.<\/p>",
        "Challenge_closed_time":1679780487814,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679419861821,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while integrating wandb with their existing code that uses SummaryWriter to log scalar values. They are experiencing odd behavior where only system monitors are logged to wandb and their calls to writer.log_scalar are not recorded. The issue is occurring on different machines and environments, and the user is unable to troubleshoot it as it fails silently without any feedback. The user is seeking help to debug the issue and find a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/with-tb-summarywriter-only-getting-sys-logs-no-log-scalar-shows-up\/4089",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":21.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":100.1738869445,
        "Challenge_title":"With TB SummaryWriter only getting sys logs, no log_scalar shows up",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":287,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ah I think I found the magic combination to work for my training script.  For posterity in case anyone else has the issues and stumbles on this post.  This is a raw torch trainer.<\/p>\n<p>(ex log_folder = \u201clogs\/projectname20230325_124523\u201d and contains the events.out.tfevents\u2026 file)<\/p>\n<pre><code class=\"lang-auto\">        wandb.tensorboard.patch(root_logdir=log_folder, pytorch=False, tensorboard_x=False, save=False)\n        wandb_run = wandb.init(\n            project=args.project_name,\n            config={\"main_cfg\": vars(args), \"optimizer_cfg\": optimizer_config},\n            name=args.run_name\n            )\n        log_writer = SummaryWriter(log_dir=log_folder...)\n\n        log_writer.add_scalar(...)\n<\/code><\/pre>\n<p>tensorboard 2.12.0<br>\nwandb 0.14.0<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.09,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"writer.log_scalar not recorded"
    },
    {
        "Answerer_created_time":1424791933163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2483.0,
        "Answerer_view_count":192.0,
        "Challenge_adjusted_solved_time":10.4979555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Challenge_closed_time":1631093676416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630949196040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has used Optuna to optimize their network in Python by making multiple activation trials for each layer. However, when they return the study.bestparams object, it only displays one parameter and does not show where the parameter was used or the other activation functions used. The user is looking for a way to precisely display the best settings used by their model and at which layers.",
        "Challenge_last_edit_time":1631055883776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":25.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":40.1334377778,
        "Challenge_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":211,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597774835507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.8,
        "Solution_reading_time":12.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Optuna",
        "Challenge_type":"inquiry",
        "Challenge_summary":"display best settings"
    },
    {
        "Answerer_created_time":1397589101936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dominican Republic",
        "Answerer_reputation_count":563.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":10.2489016667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im creating a model using optuna lightgbm integration, My training set has some categorical features and i pass those features to the model using the <code>lgb.Dataset<\/code> class, here is the code im using ( NOTE: X_train, X_val, y_train, y_val are all pandas dataframes ).<\/p>\n<pre><code>\nimport lightgbm as lgb \n\n        grid = {\n            \n       \n            'boosting': 'gbdt',\n            'metric': ['huber', 'rmse' , 'mape'],\n            'verbose':1\n\n        }\n        \n        X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n        cat_features = [ col for col in X_train if col.startswith('cat') ]\n\n        dval = Dataset(X_val, label=y_val, categorical_feature=cat_features)\n        dtrain = Dataset(X_train, label=y_train,  categorical_feature=cat_features)\n        \n        model = lgb.train(      \n                                    grid,\n                                    dtrain,\n                                    valid_sets=[dval],\n                                    early_stopping_rounds=100)\n                                    \n\n<\/code><\/pre>\n<p>Every time the <code>lgb.train<\/code> function is called, i get the following user warning<\/p>\n<pre><code>\n UserWarning: categorical_column in param dict is overridden.\n\n<\/code><\/pre>\n<p>I believe that lighgbm is not treating my categorical features the way it should, someone knows how to fix this issue? Am i using the parameter correctly?<\/p>",
        "Challenge_closed_time":1613830364163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613793468117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating a model using Optuna lightgbm integration. The training set has categorical features, and the user is passing those features to the model using the lgb.Dataset class. However, every time the lgb.train function is called, the user gets a UserWarning that the categorical_column in the param dict is overridden. The user is seeking help to fix this issue and wants to know if they are using the parameter correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66287854",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":10.2489016667,
        "Challenge_title":"Optuna lightgbm integration giving categorical features error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":140,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586625057632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>In case of picking the name (not indexes) of those columns, add as well the <code>feature_name<\/code> parameters as the <a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Dataset.html#lightgbm.Dataset.__init__\" rel=\"nofollow noreferrer\">documentation states<\/a><\/p>\n<p>That said, your <code>dval<\/code> and <code>dtrain<\/code> will be initialized as follow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>dval = Dataset(X_val, label=y_val, feature_name=cat_features, categorical_feature=cat_features)\ndtrain = Dataset(X_train, label=y_train, feature_name=cat_features, categorical_feature=cat_features)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.8,
        "Solution_reading_time":8.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"overridden categorical_column"
    },
    {
        "Answerer_created_time":1565129860212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.0860186111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Challenge_closed_time":1569516992870,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569502183603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Service Pipeline and is invoking it externally using its rest endpoint. However, they need to monitor its run status periodically and are looking for a method inside the pipeline's rest endpoint to check its status. The user has tried the steps mentioned in a GitHub link but is still facing the issue.",
        "Challenge_last_edit_time":1569502283203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58117200",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1136852778,
        "Challenge_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":898.0,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508663110972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":54.5,
        "Solution_reading_time":14.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"monitor pipeline status"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":21.6879675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Challenge_closed_time":1539831993640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539753916957,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to set hyperparameters for image classification training in Amazon SageMaker. The error message stated that additional hyperparameters are not allowed and specifically mentioned \"precission_dtype\" as unexpected. The user is seeking help to overcome this issue and has reported it to AWS support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":15.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":21.6879675,
        "Challenge_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403185902747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation_count":169.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":71.6,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unexpected hyperparameter"
    },
    {
        "Answerer_created_time":1324351066392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Christchurch, New Zealand",
        "Answerer_reputation_count":1306.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":65.1553044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Challenge_closed_time":1618176409272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617943623520,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the Azure AutoML Python SDK to download and save a model, which warns about the version mismatch between the local and remote versions. The user is seeking guidance on how to match the local Azure AutoML Python SDK version to the remote version.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":64.6627088889,
        "Challenge_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324351066392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, New Zealand",
        "Poster_reputation_count":1306.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618178182616,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"version mismatch"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.6325675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?<\/p>",
        "Challenge_closed_time":1607117314596,
        "Challenge_comment_count":1,
        "Challenge_created_time":1607093437353,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether AutoML can optimize convolutional neural networks based on the number of layers and pool layer parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.4,
        "Challenge_reading_time":2.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.6325675,
        "Challenge_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":31,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"optimize CNN with AutoML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4928227778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to understand the difference between those two function calls:<\/p>\n<p>I am referring to the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-the-summary\">documentation of define_metric<\/a>:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>Is it the \u201cbest\u201d accuracy ever measured (during training) versus the accuracy of the \u201cbest\u201d (validation) model? I understand that wandb does not care what metric I log, but what is the intended use?<\/p>\n<p>Thank you for clarification.<\/p>",
        "Challenge_closed_time":1659448810923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659447036761,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between two function calls in the define_metric documentation of WandB. They want to understand if the \"best\" accuracy refers to the accuracy of the best validation model or the best accuracy ever measured during training. The user is also curious about the intended use of the define_metric function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/understanding-define-metric-parameters\/2836",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4928227778,
        "Challenge_title":"Understanding define_metric parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>,<\/p>\n<p>For each metric logged, there is a summary metric that\u2019ll summarize the logged values as <em>one<\/em> value for each run. By default, W&amp;B uses the <em>latest<\/em> value, but you can update it with <code>wandb.summary['acc'] = best_acc<\/code> or using the two <code>define_metric<\/code> calls you show.<\/p>\n<p>This is then used to decide which value is displayed in plots that only use one value for each run (e.g. Scatter plots).<\/p>\n<pre><code class=\"lang-auto\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>These two calls are both functionally the same, one will show <code>acc.best<\/code> and one will show as <code>acc.max<\/code> in the summary metrics of your run. Both will be the maximum value that you log for <code>acc<\/code> like <code>wandb.log('acc':acc)<\/code> during a run.<\/p>\n<p>You can see the summary metrics of each run by clicking the <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> icon in the top left nav bar in a run.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.87,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":148.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"clarify define_metric function"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":5.6782083333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Challenge_closed_time":1561838537720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561818096170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, they encountered an error stating that the SKLearn NearestNeighbors learner does not have a predict method. The user is now trying to find a way to call this endpoint within Sagemaker or determine if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":5.6782083333,
        "Challenge_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":694.0,
        "Challenge_word_count":192,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511812140112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":169.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing predict method"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.1534905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Challenge_closed_time":1636391832512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636160879946,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to integrate HParams with Wandb to perform hyperparameter tuning for a custom model using TensorFlow. However, when initializing Wandb with wandb.init(), it seems to initialize it for the whole process and doesn't change when it is a new parameter set. The user is calling wandb.init() and logging after each parameter run, but it doesn't create a unique job. The user is seeking a solution to record each call to train_model in a new run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":64.1534905556,
        "Challenge_title":"Using Wandb with HParams on TF",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":579.0,
        "Challenge_word_count":210,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just had a chat with the support and figured out how to fix the problem with over-writing.<\/p>\n<p>Issue was with the init function and there is a flag for reinitializing (<code>reinit=True<\/code>)<\/p>\n<p><code>wandb.init(project=\"newjob1\", entity=\"demou\",reinit=True)<\/code>  this fixed this issue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to create unique job"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":133.3984755556,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer already standardized Artifactory as the centralized image registration. They disabled ECR service at Org level. Now we want to understand the potential impact on customer's day-2-day use of SageMaker Studio as a platform to support their full ML lifecycle. \n\n(If customer only use built-in SageMaker algorithm or framework and use prebuilt SageMaker container images)\n\nEspecially when customer trying to deploy the model to endpoint, does that need ECR service to be enabled in customer account?",
        "Challenge_closed_time":1650300385539,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649820151027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the potential impact on their use of SageMaker Studio if ECR service is disabled at the organization level. They are specifically concerned about deploying models to endpoints and whether ECR service needs to be enabled for this. The user mentions that they use built-in SageMaker algorithms and prebuilt container images.",
        "Challenge_last_edit_time":1668444527276,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUXZB0Oki3QamlQ5ijtVSuzQ\/without-ecr-being-enabled-in-aws-account-at-organization-level-what-s-the-impact-to-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":7.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":133.3984755556,
        "Challenge_title":"Without ECR being enabled in AWS account at Organization Level, what's the impact to SageMaker Studio?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650300385539,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"impact of disabling ECR service"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1705477778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I want to find the best hyperparameters using wandb, however, some combinations of them raises cuda memory error, how could I tell wandb that still runs with new hyperparameter combinations if there are these errors? So I do not need to check that all possible combinations do not raise a memory cuda error. I am afraid that  the whole sweep will stop after a specific number of runs has failed.<\/p>\n<p>Could I use some try except or  tell wandb to always execute new runs (like \u201callow unlimited failed runs\u201d) ?<\/p>\n<p>Thanks in advance<\/p>",
        "Challenge_closed_time":1685374082966,
        "Challenge_comment_count":0,
        "Challenge_created_time":1685369868994,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to find the best hyperparameters using wandb, but some combinations of them raise cuda memory errors. The user is looking for a way to tell wandb to continue running with new hyperparameter combinations even if there are errors, so they do not have to manually check all possible combinations. The user is asking if they can use a try-except or tell wandb to allow unlimited failed runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/allow-unlimited-failed-runs\/4482",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1705477778,
        "Challenge_title":"Allow unlimited failed runs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":11.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is WANDB_AGENT_MAX_INITIAL_FAILURES=1000<\/p>\n<p>Solved <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.7,
        "Solution_reading_time":3.41,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"handle cuda memory errors"
    },
    {
        "Answerer_created_time":1527682322812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":31.9027575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Challenge_closed_time":1587720289803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587603987047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to log a SageMaker DeepAR model with MLFlow for version tracking, but the mlflow.sagemaker module does not have a \"log_model\" function. The user tried to use the mlflow.pyfunc module, but it did not work. The user is seeking help to log the SageMaker model and generate cloudpickle and yaml files with MLFlow.",
        "Challenge_last_edit_time":1587605439876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61377643",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":32.3063211111,
        "Challenge_title":"Tracking SageMaker Estimator with MLFlow",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548023586667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing log_model function"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":5.0310547222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When performing a single-objective optimization with <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Optuna<\/a>, the best parameters of the study are accessible using:<\/p>\n<pre><code>import optuna\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    return (x - 2) ** 2\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2.002108042}\n<\/code><\/pre>\n<p>If I want to perform a multi-objective optimization, this would be become for example :<\/p>\n<pre><code>import optuna\ndef multi_objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    f1 = (x - 2) ** 2\n    f2 = -f1\n    return f1, f2\n\nstudy = optuna.create_study(directions=['minimize', 'maximize'])\nstudy.optimize(multi_objective, n_trials=100)\n<\/code><\/pre>\n<p>This works, but the command <code>study.best_params<\/code> fails with <code>RuntimeError: The best trial of a 'study' is only supported for single-objective optimization.<\/code><\/p>\n<p>How can I get the best parameters for a multi-objective optimization ?<\/p>",
        "Challenge_closed_time":1611273369707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611255257910,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in obtaining the best parameters for a multi-objective optimization using Optuna. While the best parameters can be easily accessed for a single-objective optimization, the command fails for multi-objective optimization, resulting in a runtime error. The user is seeking a solution to obtain the best parameters for multi-objective optimization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65833998",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.0310547222,
        "Challenge_title":"Best parameters of an Optuna multi-objective optimization",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2571.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588846413276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":108.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>In multi-objective optimization, you often end up with more than one best trial, but rather a set of trials. This set if often referred to as the Pareto front. You can get this Pareto front, or the list of trials, via <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.Study.html#optuna.study.Study.best_trials\" rel=\"noreferrer\"><code>study.best_trials<\/code><\/a>, then look at the parameters from each individual trial i.e. <code>study.best_trials[some_index].params<\/code>.<\/p>\n<p>For instance, given your directions of minimizing <code>f1<\/code> and maximizing <code>f2<\/code>, you might end up with a trial that has a small value for <code>f1<\/code> (good) but at the same time small value for <code>f2<\/code> (bad) while another trial might have a large value for both <code>f1<\/code> (bad) and <code>f2<\/code> (good). Both of these trials could be returned from <code>study.best_trials<\/code>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":12.09,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":115.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"runtime error"
    },
    {
        "Answerer_created_time":1416075035300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL",
        "Answerer_reputation_count":3492.0,
        "Answerer_view_count":133.0,
        "Challenge_adjusted_solved_time":6.5771875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to hyper-parameter optimize multiple time series forecasting models on the same data. I'm using the Optuna Sweeper plugin for Hydra. The different models have different hyper-parameters and therefore different search spaces. At the moment my config file looks like this:<\/p>\n<pre><code>defaults:\n  - datasets: data\n  - models: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n#  launcher:\n#    n_jobs: 10\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: \n\n# Ets\n    models.damped_trend:\n      type: categorical\n      choices:\n      - 'True'\n      - 'False'\n\n  # Theta\n    # models.method:\n    #   type: categorical\n    #   choices:\n    #   - 'additive'\n    #   - 'multiplicative' \n<\/code><\/pre>\n<p>Now, when I run the main_val.py file with --multirun, I get the optimal hyper-parameters for Ets. Great. But when I want to run the optimization for another model, in this example Theta, I have to manually comment out the search space for Ets and uncomment the search space for Theta. In reality, each model has much more parameters to optimize and I'm working with 10 different models. This makes my config file quite long and confusing and this commenting\/uncommenting stuff is both annoying and error-prone.<\/p>\n<p>I would like to import the search space for each model from another yaml file. Is that possible?<\/p>\n<p>I tried the following:<\/p>\n<pre><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n#  launcher:\n#    n_jobs: 10\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: search_spaces\n<\/code><\/pre>\n<p>with the file search_spaces\/Ets.yaml looking like this:<\/p>\n<pre><code>models.damped_trend:\n  type: categorical\n  choices:\n  - 'True'\n  - 'False'\n<\/code><\/pre>\n<p>But I got the error:<\/p>\n<pre><code>Validation error while composing config:\n    Cannot assign str to Dict[str, Any]\n        full_key: hydra.sweeper.search_space\n        object_type=OptunaSweeperConf\n<\/code><\/pre>",
        "Challenge_closed_time":1646682876732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646659198857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using the Optuna Sweeper plugin for Hydra to optimize hyper-parameters for multiple time series forecasting models. Each model has different hyper-parameters and search spaces. The user wants to import the search space for each model from another YAML file to avoid manually commenting and uncommenting the search space for each model in the config file. However, the user encountered an error while trying to import the search space from another YAML file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71381726",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":6.5771875,
        "Challenge_title":"When using the optuna plugin for hydra, can I import the search space from another config file?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":404.0,
        "Challenge_word_count":284,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646657863040,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Here are two options:<\/p>\n<ol>\n<li>Use a <a href=\"https:\/\/hydra.cc\/docs\/advanced\/overriding_packages\/#default-list-package-keywords\" rel=\"nofollow noreferrer\"><code>@package<\/code><\/a> directive<\/li>\n<li>Use a <a href=\"https:\/\/omegaconf.readthedocs.io\/en\/2.1_branch\/usage.html#variable-interpolation\" rel=\"nofollow noreferrer\">variable interpolation<\/a><\/li>\n<\/ol>\n<p>In detail:<\/p>\n<h2>Using an <code>@package<\/code> directive<\/h2>\n<p>An <code>@package<\/code> directive can be used to place <code>Ets.yaml<\/code> in the <code>hydra.sweeper.search_space<\/code> package:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces@hydra.sweeper.search_space: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n<\/code><\/pre>\n<h2>Using a variable interpolation:<\/h2>\n<p>A string interpolation can be used to create a reference from <code>hydra.sweeper.search_spaces<\/code> to the top-level <code>search_spaces<\/code> config.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - datasets: data\n  - models: Ets\n  - search_spaces: Ets\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n run:\n  dir: data\/outputs\/${now:%Y-%m-%d}\/${user.user}\/${now:%H-%M-%S}\n sweeper:\n   sampler:\n     seed: 123\n   direction: minimize\n   study_name: main_val\n   storage: null\n   n_trials: 2\n   n_jobs: 4\n\n   search_space: ${search_spaces}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":21.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":130.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error importing YAML file"
    },
    {
        "Answerer_created_time":1634305425288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":43.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":13.8463988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using optuna to tune xgboost model's hyperparameters. I find it stuck at trial 2 (trial_id=3) for a long time(244 minutes). But When I look at the SQLite database which records the trial data, I find <strong>all the trial 2 (trial_id=3) hyperparameters has been calculated<\/strong> except the mean squared error value of trial 2. And the optuna trial 2 (trial_id=3) seems stuck at that step. I want to know why this happened? And how to fix the issue?<\/p>\n<p><strong>Here is the code<\/strong><\/p>\n<pre><code>def xgb_hyperparameter_tuning(): \n    def objective(trial):\n        params = {\n            &quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 1000, 10000, step=100),\n            &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]), \n            &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 1, 20, step=1),\n            &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.0001, 0.2, step=0.001),\n            &quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1.0, 20.0, step=1.0),\n            &quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.1, 1.0, step=0.1),\n            &quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;,0.1, 1.0, step=0.1),\n            &quot;reg_alpha&quot;: trial.suggest_float(&quot;reg_alpha&quot;, 0.0, 11.0, step=0.1),        \n            &quot;reg_lambda&quot;: trial.suggest_float(&quot;reg_lambda&quot;, 0.0, 11.0, step=0.1),\n            &quot;num_parallel_tree&quot;: 10,\n            &quot;random_state&quot;: 16,\n            &quot;n_jobs&quot;: 10,\n            &quot;early_stopping_rounds&quot;: 1000,\n        }\n\n        model = XGBRegressor(**params)\n        mse = make_scorer(mean_squared_error)\n        cv = cross_val_score(estimator=model, X=X_train, y=log_y_train, cv=20, scoring=mse, n_jobs=-1)\n        return cv.mean()\n\n    study = optuna.create_study(study_name=&quot;HousePriceCompetitionXGB&quot;, direction=&quot;minimize&quot;, storage=&quot;sqlite:\/\/\/house_price_competition_xgb.db&quot;, load_if_exists=True)\n    study.optimize(objective, n_trials=100,)\n    return None\n\nxgb_hyperparameter_tuning()\n<\/code><\/pre>\n<p><strong>Here is the output<\/strong><\/p>\n<pre><code>[I 2021-11-16 10:06:27,522] A new study created in RDB with name: HousePriceCompetitionXGB\n[I 2021-11-16 10:08:40,050] Trial 0 finished with value: 0.03599314763859092 and parameters: {'n_estimators': 5800, 'booster': 'gblinear', 'max_depth': 4, 'learning_rate': 0.1641, 'min_child_weight': 17.0, 'colsample_bytree': 0.4, 'subsample': 0.30000000000000004, 'reg_alpha': 10.8, 'reg_lambda': 7.6000000000000005}. Best is trial 0 with value: 0.03599314763859092.\n[I 2021-11-16 10:11:55,830] Trial 1 finished with value: 0.028514652199592445 and parameters: {'n_estimators': 6600, 'booster': 'gblinear', 'max_depth': 17, 'learning_rate': 0.0821, 'min_child_weight': 20.0, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.2, 'reg_alpha': 1.2000000000000002, 'reg_lambda': 7.2}. Best is trial 1 with value: 0.028514652199592445.\n<\/code><\/pre>\n<p><strong>Here is the sqlite database <code>trial_values<\/code> table's data<\/strong><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">trial_value_id<\/th>\n<th style=\"text-align: center;\">trial_id<\/th>\n<th style=\"text-align: center;\">objective<\/th>\n<th style=\"text-align: center;\">value<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">0<\/td>\n<td style=\"text-align: center;\">0.0359931476385909<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">0<\/td>\n<td style=\"text-align: center;\">0.0285146521995924<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p><strong>Here is the sqlite database <code>trial_params<\/code> table's data<\/strong> And you can see all the trial 2 (trial_id=3) hyperparameters has been calculated<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">param_id<\/th>\n<th style=\"text-align: center;\">trial_id<\/th>\n<th style=\"text-align: center;\">param_name<\/th>\n<th style=\"text-align: center;\">param_value<\/th>\n<th style=\"text-align: center;\">distribution_json<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">5800.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">1.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">4.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">4<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.1641<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">5<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">17.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">6<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.4<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">7<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.3<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">8<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">10.8<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">9<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">7.6<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">10<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">6600.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">11<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">1.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">12<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">17.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">13<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.0821<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">14<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">20.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">15<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.7<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">16<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">17<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">1.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">18<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">7.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">19<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">7700.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">20<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">2.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">21<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">4.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">22<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.1221<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">23<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">3.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">24<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.5<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">25<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.1<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">26<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">10.8<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">27<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">1.1<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>",
        "Challenge_closed_time":1637093389323,
        "Challenge_comment_count":6,
        "Challenge_created_time":1637043226340,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Optuna where it is stuck at trial 2 (trial_id=3) for a long time despite the fact that all the trial 2 (trial_id=3) hyperparameters have already been calculated.",
        "Challenge_last_edit_time":1637043542287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69984504",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.1,
        "Challenge_reading_time":195.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":120,
        "Challenge_solved_time":13.9341619445,
        "Challenge_title":"Why optuna stuck at trial 2(trial_id=3) after it has calculated all hyperparameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":454.0,
        "Challenge_word_count":1041,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1634305425288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Although I am not 100% sure, I think I know what happened.<\/p>\n<p>This issue happens because some parameters are not suitable for certain <code>booster type<\/code> and the trial will return <code>nan<\/code> as result and be stuck at the step - calculating the <code>MSE<\/code> score.<\/p>\n<p>To solve the problem, you just need to delete the <code>&quot;booster&quot;: &quot;dart&quot;<\/code>.<\/p>\n<p>In other words, using <code>&quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;]), <\/code> rather than <code>&quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]), <\/code> can solve the problem.<\/p>\n<p>I got the idea when I tuned my LightGBMRegressor Model. I found many trials fail because these trials returned <code>nan<\/code> and they all used the same <code>&quot;boosting_type&quot;=&quot;rf&quot;<\/code>. So I deleted the <code>rf<\/code> and all 100 trials were completed without any error. Then I looked for the <code>XGBRegressor<\/code> issue which I posted above. I found all the trials which were stuck had the same <code>&quot;booster&quot;:&quot;dart&quot;<\/code> either. So I deleted the <code>dart<\/code>, and the <code>XGBRegressor<\/code> run normally.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":17.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":151.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"stuck at trial 2"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":388.0879419444,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Thanks for your good product.<\/p>\n<p>It would be good to add an archive feature for runs.<\/p>\n<p>In a project, we may try many ideas. But most of them result in no outcomes. It would be good to archive those runs to keep the workspace clean.<\/p>\n<p>It is not a good option to delete them, because we may check them in future for some cases, such as ablation study.<\/p>",
        "Challenge_closed_time":1676670702464,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675273585873,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user suggests adding an archive feature for runs in a project to keep the workspace clean. They explain that deleting runs is not a good option as they may need to refer to them in the future for certain cases.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/archive-runs\/3793",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":3.1,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":388.0879419444,
        "Challenge_title":"Archive runs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I believe currently wandb does support multiple selection. But not in the workspace view. In the table view I can select and tag multiple runs at once.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"add archive feature"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"When pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard. This happens only with [full] installation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1187.0,
        "Challenge_repo_issue_count":5827.0,
        "Challenge_repo_star_count":5419.0,
        "Challenge_repo_watch_count":122.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Discussion_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"nested runs in dashboard"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":314.0,
        "Challenge_answer_count":40,
        "Challenge_body":"Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error:\u00a0\n\nQuota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
        "Challenge_closed_time":1684921140000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683790740000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a quota error while trying to use the bison chat model in Vertex AI. The error message indicates that the quota for online prediction requests per base model has been exceeded and the user needs to submit a quota increase request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-bison-chat-model-in\/m-p\/552421#M1857",
        "Challenge_link_count":0,
        "Challenge_participation_count":40,
        "Challenge_readability":9.2,
        "Challenge_reading_time":4.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":314.0,
        "Challenge_title":"Receiving quota error when trying to use bison chat model in Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":0.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"UPDATE: We have raised the default quotas for everyone.\u00a0 This roll out may take a day to reach everyone so.\u00a0 Thank you everyone for your patience and flagging this to us!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"quota exceeded"
    },
    {
        "Answerer_created_time":1624867238152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":94.6778030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For big ML models with many parameters, it is helpful if one can interrupt and resume the hyperparameter optimization search.\nOptuna allows doing that with the RDB backend, which stores the study in a SQlite database (<a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py<\/a>).<\/p>\n<p>However, when interrupting and resuming a study, the results are not the same as that of an uninterrupted study.<\/p>\n<p><strong>Expect<\/strong>: For a fixed seed, the results from an optimization run with <code>n_trials = x<\/code> are identical to a study with <code>n_trials = x\/5<\/code>, that is resumed 5 times and a study, that is interrupted with <code>KeyboardInterrupt<\/code> 5 times and resumed 5 times until <code>n_trials = x<\/code>.<\/p>\n<p><strong>Actual<\/strong>: The results are equal up to the point of the first interruption. From then on, they differ.<\/p>\n<p>The <a href=\"https:\/\/i.stack.imgur.com\/ijIzl.png\" rel=\"nofollow noreferrer\">figures<\/a> show the optimization history of all trials in a study. The left-most figure (A) shows the uninterrupted run, the center one shows a run interrupted by keyboard (B), the right-most figure shows the run interrupted by <code>n_iter<\/code> (C). In B and C, the red dotted line shows the point where the first study was first interrupted. Left of the line, the results are equal to the uninterrupted study, to the right they differ.<\/p>\n<p>Is it possible to interrupt and resume a study, so that another study with the same seed that has not been interrupted generates exactly the same result?\n(Obviously assuming that the objective function behaves in a non-deterministic way.)<\/p>\n<p>Minimal working example to reproduce:<\/p>\n<pre><code>import optuna\nimport logging\nimport sys\nimport numpy as np\n\ndef objective(trial):\n    x = trial.suggest_float(&quot;x&quot;, -10, 10)\n    return (x - 4) ** 2\n\ndef set_study(db_name, \n                study_name, \n                seed, \n                direction=&quot;minimize&quot;):\n    '''\n    Creates a new study in a sqlite database located in results\/ .\n    The study can be resumed after keyboard interrupt by simple creating it\n    using the same command used for the initial creation.\n    '''\n\n    # Add stream handler of stdout to show the messages\n    optuna.logging.get_logger(&quot;optuna&quot;).addHandler(logging.StreamHandler(sys.stdout))\n    sampler = optuna.samplers.TPESampler(seed = seed, n_startup_trials = 0)\n    storage_name = f&quot;sqlite:\/\/\/{db_name}.db&quot;\n    storage = optuna.storages.RDBStorage(storage_name, heartbeat_interval=1)\n\n    study = optuna.create_study(storage=storage, \n                                study_name=study_name, \n                                sampler=sampler, \n                                direction=direction, \n                                load_if_exists=True)\n    return study\n\n\nstudy = set_study('optuna_test', 'optuna_test_study', 1)\n\ntry:\n    # Press CTRL+C to stop the optimization.\n    study.optimize(objective, n_trials=100)  \nexcept KeyboardInterrupt:\n    pass\n\n\ndf = study.trials_dataframe(attrs=(&quot;number&quot;, &quot;value&quot;, &quot;params&quot;, &quot;state&quot;))\n\nprint(df)\n\nprint(&quot;Best params: &quot;, study.best_params)\nprint(&quot;Best value: &quot;, study.best_value)\nprint(&quot;Best Trial: &quot;, study.best_trial)\n# print(&quot;Trials: &quot;, study.trials)\n\n\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n<\/code><\/pre>",
        "Challenge_closed_time":1661932276243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660225615907,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Optuna hyperparameter search not being reproducible with interrupted\/resumed studies. Although Optuna allows interrupting and resuming the hyperparameter optimization search with the RDB backend, the results are not the same as that of an uninterrupted study. The results are equal up to the point of the first interruption, but from then on, they differ. The user is looking for a solution to interrupt and resume a study so that another study with the same seed that has not been interrupted generates exactly the same result.",
        "Challenge_last_edit_time":1661591436152,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73321808",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":44.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":474.0723155556,
        "Challenge_title":"Optuna hyperparameter search not reproducible with interrupted \/ resumed studies",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":376,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1624867238152,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Found whats causing the problem: The random number generator in the sampler is initialized using the seed, but of course it returns a different number if the study is interrupted and resumed (it is then reinitialised)\nThis is especially bad using random search with fixed seed, as the search then basically starts from new.<\/p>\n<p>If one really needs reproducible runs, one can simply extract the rng into a binary file after a run or keyboard interrupt, and resume by overwriting the newly generated rng of the sampler with the saved one.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.9,
        "Solution_reading_time":6.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":91.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"non-reproducible hyperparameter search"
    },
    {
        "Answerer_created_time":1592311727163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":138.6078019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Challenge_closed_time":1603707766790,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603208778703,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an UnexpectedStatusException while attempting to create a multi-model endpoint in AWS Sagemaker using Scikit-learn and a custom training script. The training script succeeds but Sagemaker throws the exception, and the user is unable to find any useful information in the Cloudwatch logs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":32.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":138.6078019445,
        "Challenge_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":221,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":4.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"UnexpectedStatusException"
    },
    {
        "Answerer_created_time":1274693802127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, India",
        "Answerer_reputation_count":1561.0,
        "Answerer_view_count":243.0,
        "Challenge_adjusted_solved_time":93.8230269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When running the Azure ML Online endpoint commands, it works locally. But when I try to deploy it to Azure I get this error.\nCommand - <code>az ml online-deployment create --name blue --endpoint &quot;unique-name&quot; -f endpoints\/online\/managed\/sample\/blue-deployment.yml --all-traffic<\/code><\/p>\n<pre><code>{\n    &quot;status&quot;: &quot;Failed&quot;,\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n        &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.&quot;,\n        &quot;details&quot;: [\n            {\n                &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n                &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.\\nThe build log is available in the workspace blob store \\&quot;coloraiamlsa\\&quot; under the path \\&quot;\/azureml\/ImageLogs\/1673692e-e30b-4306-ab81-2eed9dfd4020\/build.log\\&quot;&quot;,\n                &quot;details&quot;: [],\n                &quot;additionalInfo&quot;: []\n            }\n        ],\n        \n<\/code><\/pre>\n<p>This is the deployment YAML taken straight from <a href=\"https:\/\/github.com\/Azure\/azureml-examples\" rel=\"nofollow noreferrer\">azureml-examples<\/a> repo<\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  local_path: ..\/..\/model-1\/model\/sklearn_regression_model.pkl\ncode_configuration:\n  code: \n    local_path: ..\/..\/model-1\/onlinescoring\/\n  scoring_script: score.py\nenvironment: \n  conda_file: ..\/..\/model-1\/environment\/conda.yml\n  image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\ninstance_type: Standard_F2s_v2\ninstance_count: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1642392913590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642055150693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"DriverFileNotFound\" error while deploying Azure ML Online endpoint commands to Azure. The error message indicates that the driver file named \"score.py\" is not found in the provided dependencies. The user has shared the deployment YAML taken from the Azure ML examples repository.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70692270",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":23.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":93.8230269445,
        "Challenge_title":"Azure ML Online Endpoint deployment DriverFileNotFound Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274693802127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, India",
        "Poster_reputation_count":1561.0,
        "Poster_view_count":243.0,
        "Solution_body":"<p>Finally after lot of head banging, I have been able to consistently repro this bug in another Azure ML Workspace.<\/p>\n<p>I tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly.<\/p>\n<p>At this point I remembered that I had upgraded the Storage Account of my previous AML Workspace to DataLake Gen2.<\/p>\n<p>So I did the same upgrade in this new workspace\u2019s storage account. After the upgrade, when I try to deploy the same endpoint, I get the same <code>DriverFileNotFoundError<\/code>!<\/p>\n<p>It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types<\/a>.<\/p>\n<p>At this point my only option is to recreate a new workspace and deploy my code there. Hope Azure team fixes this soon.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":13.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing driver file"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.9786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Challenge_closed_time":1638449992000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638280869000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where wandb is not compatible with PL 1.6.1 while using Hyperparameter Search, resulting in a FileNotFoundError.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":74.0,
        "Challenge_repo_issue_count":1270.0,
        "Challenge_repo_star_count":755.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":46.9786111111,
        "Challenge_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"compatibility issue"
    },
    {
        "Answerer_created_time":1553609947192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":221.1468555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Challenge_closed_time":1616165232248,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615369010763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered a brokenpipeerror when attempting to use a deepar model to predict from a test.json dataset.",
        "Challenge_last_edit_time":1615369103568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.1,
        "Challenge_reading_time":216.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":142,
        "Challenge_solved_time":221.1726347222,
        "Challenge_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":1241,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1607069622448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"broken pipe error"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":5.6297925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Microsoft Azure Machine Learning and was wondering if anyone had done some experiments on date time features. Doe sit automatically derive additional features like \"day of week\", \"day of month\", \"hour of day\" from them, or do I have to provide these?<\/p>\n\n<p>I could not find any info in the official documentation (and a lack of a Microsoft support forum =)<\/p>",
        "Challenge_closed_time":1434736380420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1434716113167,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Microsoft Azure Machine Learning and is seeking information on whether the platform automatically derives additional features like \"day of week\", \"day of month\", \"hour of day\" from date time features or if they need to provide these features themselves. The user was unable to find any information in the official documentation or a Microsoft support forum.",
        "Challenge_last_edit_time":1445833326870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30937903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6297925,
        "Challenge_title":"How are date features utilized in Microsoft Azure Machine Studio",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":72,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1221999894423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delaware",
        "Poster_reputation_count":2603.0,
        "Poster_view_count":225.0,
        "Solution_body":"<p>Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples below<\/p>\n\n<p>x&lt;-as.Date(\"12\/3\/2009\", \"%m\/%d\/%Y\")<\/p>\n\n<blockquote>\n  <p>months.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"December\"<\/p>\n\n<blockquote>\n  <p>weekdays.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Thursday\"<\/p>\n\n<blockquote>\n  <p>quarters(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Q4\"<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"derive date time features"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1609843354000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the LED model in SageMaker SMP training. They have tried several fixes, including matching the python, transformers, and pytorch versions, but are still facing issues. The error is in the \"modeling_led\" within the transformers module, which is expecting a different input_ids shape. The user tried to unsqueeze input tensors to the \"modeling_led\" to solve the above error, which helped move forward in the process, but they got another error further down in the code. The error message is \"Tensors must have the same number of dimensions: got 4 and 3.\" The user is seeking feedback and assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":25.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":493.0,
        "Challenge_repo_star_count":546.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Discussion_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"input shape error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0147222222,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nIs there some way we could save the output logs to have them be accessible?",
        "Challenge_closed_time":1649327870000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649327817000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of saving and accessing output logs from a Polyaxon run through the cloud UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1470",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.0,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.0147222222,
        "Challenge_title":"Is there a way to have the logs from a polyaxon run viewable via the cloud UI?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"That's not possible I am afraid. Logs, as well as other artifacts, are only viewable via the gateway deployed with the agent.\n\nIn order to provide such option, Polyaxon will have to have access to the artifacts store, but I do not think that we want to provide such functionality at the moment since logs also can include sensitive information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Polyaxon",
        "Challenge_type":"inquiry",
        "Challenge_summary":"save\/access Polyaxon logs"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.2453072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Challenge_closed_time":1587756432003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587755548897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the Azure ML Logger to track metrics in a PythonScriptStep of their Azure ML pipeline. They are trying to import 'get_azureml_logger' but are encountering a 'ModuleNotFoundError'. They tried to add a dependency via pip but it failed with an Auth error. The user is unsure how to proceed and needs to log metrics in the step.",
        "Challenge_last_edit_time":1587809992912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":0.2453072222,
        "Challenge_title":"Log metrics in PythonScriptStep",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":10.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to import logger"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630081759000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a CUDA error 46 while trying to run onnxruntime-gpu on an Azure Machine Learning instance with the openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04 base image. The error message suggests that all CUDA-capable devices are busy or unavailable. The user had previously run similar environments with earlier versions of CUDA and onnxruntime, which makes them think that the issue is related to the compatibility between CUDA versions and onnxruntime. The user is unsure if the issue is related to the difference between cudnn 8.4 used in the docker image and the compatibility list for onnxruntime, which suggests cudnn 8.2.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":274.0,
        "Challenge_repo_star_count":13.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CUDA error 46"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.213155,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Within my optuna study, I want that each trial is separately logged by wandb. Currently, the study is run and the end result is tracked in my wandb dashboard. Instead of showing each trial run separately, the end result over all epochs is shown. So, wandb makes one run out of multiple runs.<\/p>\n<p>I found the following <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/_modules\/optuna\/integration\/wandb.html\" rel=\"noopener nofollow ugc\">docs<\/a> in optuna:<\/p>\n<pre><code>Weights &amp; Biases logging in multirun mode.\n\n    .. code::\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">            import optuna\n            from optuna.integration.wandb import WeightsAndBiasesCallback\n\n            wandb_kwargs = {\"project\": \"my-project\"}\n            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n\n            @wandbc.track_in_wandb()\n            def objective(trial):\n                x = trial.suggest_float(\"x\", -10, 10)\n                return (x - 2) ** 2\n\n\n            study = optuna.create_study()\n            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n\n<\/code><\/pre>\n<p>I implemented this line of code yet it produces the following error:<\/p>\n<p><code>ConfigError: Attempted to change value of key \"learning_rate\" from 5e-05 to     0.0005657929921495451 If you really want to do this, pass allow_val_change=True to config.update()    wandb: Waiting for W&amp;B process to finish... (failed 1).<\/code><\/p>\n<p>Did anyone succeed in implementing logging per trial in a multi-trial study?<\/p>",
        "Challenge_closed_time":1679650682812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679646315454,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to enable logging of each trial separately in their optuna study using wandb. However, currently, wandb is tracking the end result over all epochs instead of showing each trial run separately. The user tried implementing a line of code from the optuna documentation but encountered an error. They are seeking help to successfully implement logging per trial in a multi-trial study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-enable-logging-of-each-trial-separately\/4115",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.213155,
        "Challenge_title":"How to enable logging of each trial separately?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I actually solved it now:<br>\nIt seems that the optimizer that i used caused errors in the generation of a value for the learning rate when starting a new trial. Once I took the optimizer back out, the follwing implementation worked and generated separate logs in my wandb dashboard:<\/p>\n<pre><code class=\"lang-auto\">wandb_kwargs = {\"project\": \"my-project\"}\nwandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n@wandbc.track_in_wandb()\ndef objective(trial):\n    \n    training_args = Seq2SeqTrainingArguments( \n        \"tuning\", \n        num_train_epochs=1,            \n        # num_train_epochs = trial.suggest_categorical('num_epochs', [3, 5, 8]),\n        per_device_eval_batch_size=3, \n        per_device_train_batch_size=3, \n        learning_rate=  trial.suggest_float('learning_rate', low=0.00004, high=0.0001, step=0.0005, log=False),             \n        # per_device_train_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),       \n        # per_device_eval_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),  \n        disable_tqdm=True, \n        predict_with_generate=True,\n        gradient_accumulation_steps=4,\n        # gradient_checkpointing=True,\n        # weight_decay= False\n        seed = 12, \n        warmup_steps=5,\n        # evaluation and logging\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        save_total_limit=1,\n        logging_strategy=\"epoch\",\n        logging_steps = 1, \n        load_best_model_at_end=True,\n        metric_for_best_model = \"eval_loss\",\n        # use_cache=False,\n        push_to_hub=False,\n        fp16=False,\n        remove_unused_columns=True\n    )\n    # optimizer = Adafactor(\n    #     t5dmodel.parameters(),\n    #     lr=trial.suggest_float('learning_rate', low=4e-5, high=0.0001),  #   ('learning_rate', 1e-6, 1e-3),\n    #     # weight_decay=trial.suggest_float('weight_decay', WD_MIN, WD_CEIL),   \n    #     # lr=1e-3,\n    #     eps=(1e-30, 1e-3),\n    #     clip_threshold=1.0,\n    #     decay_rate=-0.8,\n    #     beta1=None,\n    #     # weight_decay= False\n    #     weight_decay=0.1,\n    #     relative_step=False,\n    #     scale_parameter=False,\n    #     warmup_init=False,\n    # )\n    \n    # lr_scheduler = AdafactorSchedule(optimizer)\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=t5dmodel)\n    trainer = Seq2SeqTrainer(model=t5dmodel,\n                            args=training_args,\n                            train_dataset=tokenized_train_dataset['train'],\n                            eval_dataset=tokenized_val_dataset['validation'],\n                            data_collator=data_collator,\n                            tokenizer=tokenizer,\n                           #  optimizers=(optimizer, lr_scheduler)\n                            )       \n    \n    trainer.train()\n    scores = trainer.evaluate() \n    return scores['eval_loss']\n\nif __name__ == '__main__':\n    t5dmodel = AutoModelForSeq2SeqLM.from_pretrained(\"yhavinga\/t5-base-dutch\",  use_cache=False) \n    tokenizer = AutoTokenizer.from_pretrained(\"yhavinga\/t5-base-dutch\", additional_special_tokens=None)\n    \n    features = {\n    'WordRatioFeature': {'target_ratio': 0.8},\n    'CharRatioFeature': {'target_ratio': 0.8},\n    'LevenshteinRatioFeature': {'target_ratio': 0.8},\n    'WordRankRatioFeature': {'target_ratio': 0.8},\n    'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n    }\n    \n    trainset_processed = get_train_data(WIKILARGE_PROCESSED, 0, 10)  \n    print(trainset_processed)\n    valset_processed = get_validation_data(WIKILARGE_PROCESSED, 0,7)\n    print(valset_processed)\n    tokenized_train_dataset = trainset_processed.map((tokenize_train), batched=True, batch_size=1)\n    tokenized_val_dataset =  valset_processed.map((tokenize_train), batched=True, batch_size=1)   \n    print('Triggering Optuna study')\n    study = optuna.create_study( direction='minimize', pruner=optuna.pruners.MedianPruner()) \n    study.optimize(objective, n_trials=4,callbacks=[wandbc],  gc_after_trial=True)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.3,
        "Solution_reading_time":45.02,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":211.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect logging"
    },
    {
        "Answerer_created_time":1437078651387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":901.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":1658.4581091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/train\" rel=\"nofollow noreferrer\">and their Hello World example<\/a>.<\/p>\n<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics<\/code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()<\/code><\/p>\n<p>How can I also see the same metrics on training sets (or even training error for each epoch)?<\/p>\n<p>Training code is basically the same as the link with extra parts of the docs added:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFace\n\n# optionally parse logs for key metrics\n# from the docs: https:\/\/huggingface.co\/docs\/sagemaker\/train#sagemaker-metrics\nmetric_definitions = [\n    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\\-)[0-9]+),?&quot;}\n]\n\n# hyperparameters, which are passed into the training job\nhyperparameters={\n    'epochs': 5,\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n\n# init the model (but not yet trained)\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n# starting the train job with our uploaded datasets as input\nhuggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n\n# does not return metrics on training - only on eval!\nhuggingface_estimator.training_job_analytics.dataframe()\n<\/code><\/pre>",
        "Challenge_closed_time":1639502011652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639148719370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model using Hugging Face's integration with Amazon Sagemaker and can view the metrics generated on the evaluation test set. However, the user is unable to view the same metrics on training sets or training error for each epoch. The user has shared the training code used for the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70306493",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":34.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":98.136745,
        "Challenge_title":"View train error metrics for Hugging Face Sagemaker model",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":218,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437078651387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":901.0,
        "Poster_view_count":145.0,
        "Solution_body":"<p>This can be solved by increasing the number of epochs in training to a more realistic value.<\/p>\n<p>Currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function.<\/p>\n<p>Changes to make:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>hyperparameters={\n    'epochs': 100, # increase the number of epochs to realistic value!\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645119168563,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to view training metrics"
    },
    {
        "Answerer_created_time":1428499037932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leipzig, Deutschland",
        "Answerer_reputation_count":2124.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":1.1770155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1658393400156,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658389162900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Weights & Biases sweeps for hyperparameter search for their NER model. They have done a grid search with about 100 runs and want to create a graph that shows the best 10 runs in terms of f-score, but they are unable to figure out how to do it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1770155556,
        "Challenge_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643710211767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.93,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"create graph of top runs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2063763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <br \/>\nI created custom Azure AI model what I would like to use in PowerBI.    <br \/>\nWhen I open a dataset in PowerBI and after select the &quot;Azure Machine learning&quot; after the pop-up window is empty but I suppose it should contain my custom model(s).    <br \/>\nI followed the below articles:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\">https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate<\/a>    <\/p>\n<p>Kind regards    <br \/>\nTom    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162671-powerbi-azure-ai.png?platform=QnA\" alt=\"162671-powerbi-azure-ai.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162606-azure-ai-model.png?platform=QnA\" alt=\"162606-azure-ai-model.png\" \/>    <\/p>",
        "Challenge_closed_time":1641419590032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641415247077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a custom Azure AI model and wants to use it in PowerBI. However, when they select \"Azure Machine learning\" in PowerBI, the pop-up window is empty and does not show their custom model. The user has followed the provided articles but is still facing this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/684845\/why-does-powerbi-not-see-my-custom-azure-ai-model",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":19.6,
        "Challenge_reading_time":14.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2063763889,
        "Challenge_title":"Why does PowerBI not see my custom Azure AI model?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The product group for Power Bi actively monitors questions over at    <br \/>\n<a href=\"https:\/\/community.powerbi.com\/\">https:\/\/community.powerbi.com\/<\/a>       <\/p>\n<p>--please don't forget to <code>upvote<\/code> and <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/145510-image.png?platform=QnA\" alt=\"145510-image.png\" \/> if the reply is helpful--    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty pop-up window"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":7.9018069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Challenge_closed_time":1560972233448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560943786943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a sklearn model in sagemaker. The error message states that the model data cannot be found at the specified S3 location. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":9.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":7.9018069444,
        "Challenge_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1033.0,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560085651596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":155.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"model data not found"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2.4045722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Challenge_closed_time":1652649592320,
        "Challenge_comment_count":3,
        "Challenge_created_time":1652637573657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble making a PowerShell Get request with a body to an API endpoint. They have tried to append the body as a query string and encode the query parameter, but both attempts have failed. They have successfully made the same request in Python and Postman.",
        "Challenge_last_edit_time":1652640935860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":16.7,
        "Challenge_reading_time":43.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3385175,
        "Challenge_title":"PowerShell Get request with body",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":307,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.6,
        "Solution_reading_time":13.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed PowerShell request"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":20.1381897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Challenge_closed_time":1526179930343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526107432860,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Sagemaker where they are unable to create region-specific security credentials for their endpoint. They have created an S3 bucket and trained and published the Sagemaker endpoint in Ireland server, but when they try to test the endpoint in Postman, they receive a region-specific error. As the user is using a root user account, they are unable to generate region-specific security credentials for Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":20.1381897222,
        "Challenge_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":750.0,
        "Challenge_word_count":161,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":14.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to create region-specific security credentials"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":6.3089888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an R script in Azure Machine Learning that takes two inputs. I have since been working on a project that will take advantage of the webservice I created within Azure. When I use whole numbers as the values, everything works fine. In my C# code, these values are still doubles, and I use ToString to format them for the HTTP request.  I can send the data, and get 100% accurate results back. However, when I send values that actually contain digits after the decimal, I get a bad request response. I think the issue is with how the R script reads in from Azure Machine Learning inputs. So far I have this:<\/p>\n\n<pre><code>#R Script in Azure ML:\n1:    objCoFrame &lt;- maml.mapInputPort(2) # class: data.frame\n2:    objCoVector &lt;- as.vector(objCoFrame[1,])\n<\/code><\/pre>\n\n<p>which was doing the trick with integers. I have also tried <\/p>\n\n<pre><code>2:    objCoVector &lt;- as.vector(as.numeric(objCoFrame[1,]))\n<\/code><\/pre>\n\n<p>but got the same result.<\/p>\n\n<p>The Bad Request Response Content reads:<\/p>\n\n<pre><code>{\n    \"error\":\n    {\n        \"code\":\"BadArgument\",\n        \"message\":\"Invalid argument provided.\",\n        \"details\":\n        [{\n            \"code\":\"InputParseError\",\n            \"target\":\"rhsValues\",\n            \"message\":\"Parsing of input vector failed.  Verify the input vector has the correct number of columns and data types.  Additional details: Input string was not in a correct format..\"\n        }]\n    }\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1458012604667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1457989892307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Azure Machine Learning where they receive a bad request response when sending values with digits after the decimal point. The R script in Azure ML reads inputs in a certain way that works with integers but not with decimal values. The error message suggests that the input vector has the incorrect number of columns and data types.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35998155",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":17.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.3089888889,
        "Challenge_title":"Bad Request Response from Azure Machine Learning",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":202,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457988600436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Omaha, NE, USA",
        "Poster_reputation_count":380.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Can you force the type using Meta-Editor before passing to execute-R<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":0.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"bad request with decimal values"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":3.7756897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Challenge_closed_time":1653626541543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653612949060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to query a list of frequently used compute instance sizes under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. However, the list of resources that can be queried does not include compute under microsoft.machinelearningservices\/ (not classic studio) and Microsoft.Databricks\/workspaces. The user has tried a Kusto query to get VM instance size but it does not show what they have under Azure Machine Learning\/Azure Databricks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.7756897222,
        "Challenge_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":183.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to query compute instances"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5792555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Challenge_closed_time":1610631020420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610628627223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to visualize real-time losses and metrics for a TensorFlow model on an AWS Sagemaker instance using Tensorboard in a Jupyter notebook, but is encountering issues and is seeking guidance on how to get it working.",
        "Challenge_last_edit_time":1610628935100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":4.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6647769444,
        "Challenge_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":715.0,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512023194592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":547.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"issues with Tensorboard visualization"
    },
    {
        "Answerer_created_time":1340284487940,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":151.2120097222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Challenge_closed_time":1544569877892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544027022437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a CapacityError while running two TrainingJob instances on AWS SageMaker, with one instance running fine and the other exiting with the error message \"CapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\" The user is unsure about the meaning of this error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":150.7931819444,
        "Challenge_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3875.0,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1544571385672,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.51,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CapacityError on SageMaker"
    },
    {
        "Answerer_created_time":1531218624572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":16.6869011111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to test different set of parameters in a ML algorithm using Optuna.<\/p>\n\n<p>The automatic sampling of Optuna is very useful, but is there any way to force one specific set of parameters into the proposed batch defined by Optuna?<\/p>\n\n<p>For example if I have a x,y parameters:<\/p>\n\n<pre><code>def objective(trial)\n   x = trial.suggest_uniform('x', -10, 10)\n   y = trial.suggest_uniform('x', -5, 5)\n   return (x+y-2)**2\nstudy = optuna.create_study(study_name='study_name')\nstudy.optimize(objective, n_trials=10)\n<\/code><\/pre>\n\n<p>I would also like to define one set of x=0.1, y=0.2 into the automatic generated one. Is this possible? <\/p>\n\n<p>It could be interesting to compare the \"intuitive\" values of some ML algorithm with other values.<\/p>",
        "Challenge_closed_time":1590642371747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590582298903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to test different sets of parameters in a machine learning algorithm using Optuna. They are looking for a way to force one specific set of parameters into the proposed batch defined by Optuna, in order to compare the \"intuitive\" values of some ML algorithm with other values.",
        "Challenge_last_edit_time":1590673455660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62043096",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":16.6869011111,
        "Challenge_title":"Force one specific set of parameters into the sampled batch",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes. One way to do this would be to use a <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/trial.html#optuna.trial.FixedTrial\" rel=\"nofollow noreferrer\">FixedTrial<\/a>, which would show you the result of your intuitive guess.<\/p>\n\n<p><code>print(objective(optuna.trial.FixedTrial({'x': 0.1, 'y': 0.2})))<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":4.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Optuna",
        "Challenge_type":"inquiry",
        "Challenge_summary":"force specific parameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":116.0143763889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to start a sweep using this yaml file.<\/p>\n<p>sweep.yaml<\/p>\n<pre><code class=\"lang-auto\">method: bayes\nmetric:\n  goal: maximize\n  name: val_f1_score\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    value: 42\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 30\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    value: adam\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  paths:\n    - \n      data:\n        value: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  -\n    use:\n      value: True\n    project:\n      value: Whats-this-rock\n\ndataset:\n  -\n    id:\n      value: [1, 2, 3, 4]\n    dir:\n      value: data\/3_consume\/\n    image:\n      size:\n        value: 124\n      channels:\n        value: 3\n    classes:\n      value: 10\n    sampling:\n      value: None\n\nmodel:\n  -\n    backbone:\n      value: efficientnetv2m\n    use_pretrained_weights:\n      value: True\n    trainable:\n      value: True\n    preprocess:\n      value: True\n    dropout_rate:\n      value: 0.3\n\ncallback:\n  -\n    monitor:\n      value: \"val_f1_score\"\n    earlystopping:\n      patience:\n        value: 10\n    reduce_lr:\n      factor:\n        values: [.9, .7, .5]\n      min_lr: 0.00001\n      patience:\n        values: [1, 2, 3, 4]\n    save_model:\n      status:\n        value: True\n      best_only:\n        value: True\n\nprogram: src\/models\/train.py\n\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">Error: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>Here\u2019s the full traceback of the error:-<\/p>\n<pre><code class=\"lang-auto\">During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 942, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2178, in upsert_sweep\n    raise e\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2175, in upsert_sweep\n    check_retry_fn=no_retry_4xx,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2153, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>I am using hydra and trying to replicate a config.yaml for wandb sweeps<\/p>\n<p>config.yaml<\/p>\n<pre><code class=\"lang-auto\">notes: \"\"\nseed: 42\nlr: 0.001\nepochs: 30\naugmentation: True\nclass_weights: True\noptimizer: adam\nloss: categorical_crossentropy\nmetrics: [\"accuracy\"]\nbatch_size: 64\nnum_classes: 7\n\npaths:\n  data: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  use: True\n  project: Whats-this-rock\n\ndataset:\n  id: [1, 2, 3, 4]\n  dir: data\/3_consume\/\n  image:\n    size: 124\n    channels: 3\n  classes: 10\n  sampling: None\n\nmodel:\n  backbone: efficientnetv2m\n  use_pretrained_weights: True\n  trainable: True\n  preprocess: True\n  dropout_rate: 0.3\n\ncallback:\n  monitor: \"val_f1_score\"\n  earlystopping:\n    patience: 10\n  reduce_lr:\n    factor: 0.4\n    min_lr: 0.00001\n    patience: 2\n  save_model:\n    status: True\n    best_only: True\n\n<\/code><\/pre>",
        "Challenge_closed_time":1663515880848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663098229093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to start a sweep using a YAML file with multi-level nesting. The error message indicates that the \"paths\" hyperparameter configuration is invalid. The user is using Hydra and trying to replicate a config.yaml for wandb sweeps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multi-level-nesting-in-yaml-for-sweeps\/3108",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":116.0143763889,
        "Challenge_title":"Multi-level nesting in yaml for sweeps",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1087.0,
        "Challenge_word_count":350,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is to use dot notation instead of nested parameters as wandb (v0.13.3) sweeps doesn\u2019t support nested parameters.<\/p>\n<pre><code class=\"lang-auto\">sweep.yaml\n\nmethod: bayes\nmetric:\n  goal: maximize\n  name: val_accuracy\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    values: [1, 42, 100]\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 100\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    values: [adam, adamax]\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.70\n      - 0.75\n      - 0.80\n  data_path:\n    value: data\/4_tfds_dataset\/\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv3\n  dataset_id:\n    values:\n      - [1]\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None, oversampling, undersampling]\n  backbone:\n    values:\n      [\n        efficientnetv2m,\n        efficientnetv2,\n        resnet,\n        mobilenetv2,\n        inceptionresnetv2,\n        xception,\n      ]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [True, False]\n  preprocess:\n    value: True\n  dropout_rate:\n    values: [0.3]\n  monitor:\n    value: \"val_accuracy\"\n  earlystopping.use:\n    value: True\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [True]\n  reduce_lr.factor:\n    values: [.9, .7, .5, .3]\n  reduce_lr.patience:\n    values: [1, 3, 5, 7, 13]\n  reduce_lr.min_lr:\n    value: 1e-5\n  save_model:\n    value: False\n\nprogram: src\/models\/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.77,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":159.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid hyperparameter configuration"
    },
    {
        "Answerer_created_time":1225669466307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, United Kingdom",
        "Answerer_reputation_count":236107.0,
        "Answerer_view_count":18730.0,
        "Challenge_adjusted_solved_time":0.1705583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1410733358503,
        "Challenge_comment_count":3,
        "Challenge_created_time":1410732744493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to translate a C# code sample that invokes a web service in Azure ML to F#. The user has everything except the async and await done.",
        "Challenge_last_edit_time":1446025307110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":17.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1705583334,
        "Challenge_title":"C# async\/await to F# using Azure ML example",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":607.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349689794400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation_count":4174.0,
        "Poster_view_count":396.0,
        "Solution_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":18.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":156.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"translate C# to F#"
    },
    {
        "Answerer_created_time":1501350003092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":1294.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":32.1489905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Challenge_closed_time":1595539712143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595423975777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while calling a SageMaker deploy_endpoint function with an a1.small instance. The error message indicates that the account-level service limit for ml.m5.2xlarge instance usage is 0, and the user is unsure if the endpoint needs to be on an ml.m5.2xlarge instance or if the code is acting up.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":32.1489905556,
        "Challenge_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":160,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495758291316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Poster_reputation_count":138.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":5.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"service limit reached"
    },
    {
        "Answerer_created_time":1662621266503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.6069663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Challenge_closed_time":1662652096572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662649911493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is familiar with SageMaker and has performed distributed training experiments. They are now seeking to know if it is possible to test and run SageMaker distributed training in local mode using SageMaker Notebook Instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6069663889,
        "Challenge_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662649653072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":3.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"test SageMaker in local mode"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":14.8662555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Challenge_closed_time":1641974198663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641920680143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about how parallel trials work in GCP Vertex AI when running hyperparameter tuning jobs. They are unsure how multiple trials can run in parallel with only one worker and how each task can occupy 100% of the CPU's resources. The user is questioning if GCP provisions more than one machine to handle parallel trials.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.8662555556,
        "Challenge_title":"How do parallel trials in GCP Vertex AI work?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579801831103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":13.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":113.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"parallel trials in GCP Vertex AI"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":0.4063402778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Challenge_closed_time":1464685093652,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464683630827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to test multiple algorithms at once in Azure Machine Learning Studio to quickly determine which one has the lowest error rate.",
        "Challenge_last_edit_time":1465977920520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37540703",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.2,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4063402778,
        "Challenge_title":"Test multiple algorithms in one experiment",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"test multiple algorithms"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.3609763889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Am using custom data generator as part of my image augmentation. Are you able to use sweeps to try different parameters for such augmentation?<\/p>\n<p>for example:<\/p>\n<pre><code class=\"lang-auto\">idg = CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=config.v_kernel_size,\n                              h_kernel_size=config.h_kernel_size,  \n                             height_shift_range = config.height_shift_range, \n                             width_shift_range = config.width_shift_range, \n                             rotation_range = config.rotation_range, \n                             shear_range = config.shear_range,\n                             zoom_range = config.zoom_range,)\n    return idg\n<\/code><\/pre>",
        "Challenge_closed_time":1677704879784,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677494780269,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using a custom data generator for image augmentation and is wondering if it is possible to use sweeps to try different parameters for the augmentation. They have provided an example of their current code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-sweeps-for-custom-data-generator-in-keras\/3957",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":58.3609763889,
        "Challenge_title":"Using sweeps for custom data generator in keras",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zookcx\">@zookcx<\/a> thanks for writing in! Is in your example the <code>CustomDataGenerator<\/code> a function that you could call from the main training function? Would something like the following work for you?<\/p>\n<pre><code class=\"lang-auto\">import wandb\ndef main():\n    wandb.init(project='custom-data-sweep')\n    data = CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=wandb.config.v_kernel_size,\n                              h_kernel_size=wandb.config.h_kernel_size)\n    wandb.log({'data': data})\n    wandb.finish()\n\n\ndef CustomDataGenerator(rescale = 1 \/ 255.,\n                             horizontal_flip = False, \n                             vertical_flip = False,\n                             v_kernel_size=0,\n                             h_kernel_size=0,\n                          ):\n  \n    # add your own custom data generator logic\n    idg = v_kernel_size + h_kernel_size\n\n    return idg\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">\nsweep_config = {\n    'method': 'grid',\n    'project': 'sweep-configs',\n    'parameters': {\n        'v_kernel_size': {\n            'values': [32, 64, 96, 128, 256]\n        },\n        'h_kernel_size': {\n            'values': [32, 64, 96, 128, 256]\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config)\nwandb.agent(sweep_id, function=main)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":14.43,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":102.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use sweeps for augmentation"
    },
    {
        "Answerer_created_time":1476780098123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":1672.0,
        "Answerer_view_count":159.0,
        "Challenge_adjusted_solved_time":13.8667347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Challenge_closed_time":1624416005592,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624366085347,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed a custom model on AWS Sagemaker inference endpoint but has observed that the CPU utilization metric is maxing out at 100% instead of #vCPU*100%. The inference endpoint is not using all cores, resulting in a limit of one API call per second. The user is seeking settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency or using the multiprocessing python package inside the inference.py file to distribute calculations\/prediction across all available cores.",
        "Challenge_last_edit_time":1628443120180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.8667347222,
        "Challenge_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":661.0,
        "Challenge_word_count":163,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473938483227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Poster_reputation_count":909.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1624470584452,
        "Solution_link_count":23.0,
        "Solution_readability":20.5,
        "Solution_reading_time":99.55,
        "Solution_score_count":5.0,
        "Solution_sentence_count":55.0,
        "Solution_word_count":599.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CPU utilization maxed out"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.8905811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi community,   <br \/>\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants.  I haven't built any models yet, just wondering how feasible the below is.  A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.  <\/p>\n<p>A client uploads a set of standard documents (usually scanned PDF's)  using a file upload in our .net application.    <br \/>\nCan we:  <\/p>\n<ol>\n<li> Use form recogniser to extract key value pairs, after training a custom model.  <\/li>\n<li> Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.  <\/li>\n<li> Report back to the user the missing information so they can correct the document and reupload them?  <br \/>\nPreferably in real time?  So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?  <\/li>\n<\/ol>",
        "Challenge_closed_time":1647513011952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647481005860,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether Azure Form Recognizer or another tool can be used to screen the correctness of uploaded applications, such as funding grant applications. They want to know if it is possible to use form recognizer to extract key value pairs, find missing information, and report back to the user in real-time so they can correct the document and re-upload it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.8905811111,
        "Challenge_title":"Form recognizer to report on missing information in (near) real-time",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d4ec46af-03e5-46ab-ace4-d0dd3b8d93ba\">@Andrew Robertson  <\/a> Yes, you can use Azure form recognizer to analyze a document that is passed to the API and use the result of the analyze operation to report any missing fields in the form back to the user. This is the most widely used use case by most of the customers.     <\/p>\n<p>Form recognizer comes with a set of prebuilt APIs where it can extract common information from invoices, business cards, receipts etc. If you have a form that does not conform to the prebuilt API standards you need to create a custom model to extract the text in the form of a tags and their key:value pairs. The custom models require some basic training with some test forms and if all the forms that need extraction follow the same layout or guidelines the extraction results will be good.     <\/p>\n<p>In the case of custom forms the results are provided in almost real time where the form is submitted or <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/AnalyzeDocument\">POST<\/a> request is sent to the API and an operation id is returned to retrieve the results using <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/GetAnalyzeDocumentResult\">GET<\/a>.  Depending on your pricing tier of your resource if you intend to perform these actions synchronously you might have to limit the rate of requests sent to the API to avoid any TPS errors. If you are using async operations with a slight delay to fetch the results then you can design an application that can take large number of documents and provide results to the users within a short span of time.     <\/p>\n<p>I hope the above information is helpful.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.4,
        "Solution_reading_time":27.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":292.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"screening tool for applications"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.0155555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Challenge_closed_time":1650034426000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649933570000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the full episode data logging in a random agent script using wandb, where some steps are being skipped. The problem is due to wandb counting the episode reward logging steps made before the full data logging. A potential solution suggested is to add another metric to log the timestep and day proportionally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":102.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.0155555556,
        "Challenge_title":"In random agent script wandb full episode data logging skips a few steps",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Discussion_body":"This has been implemented and will be shipped with v0.4.4 \ud83d\ude80",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"skipped logging steps"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":267.6955555556,
        "Challenge_answer_count":0,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342987000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1612379283000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where the helm fetch command for ai-engine, sdk-helper, and mlflow includes the 22.09 release instead of 22.11. The user has provided the command used and version details but has not provided any relevant log output or environment printout.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":621.0,
        "Challenge_repo_star_count":38.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":267.6955555556,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Discussion_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect release fetched"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":560.8433333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Challenge_closed_time":1669124369000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1667105333000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with pycaret and mlflow integration where they are unable to create probabilities in addition to predicted values for binary response models using the scikit learn function \"predict_model\". The issue occurs when they call the calibrated algorithm in a separate notebook for scoring new data. The expected behavior is to see the probabilities model.predict_prob(X), but the code errors out.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":37.27,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":560.8433333333,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":296,
        "Discussion_body":"In addition, runs of `compare_models `and `create_model` get nested recursively as well in pycaret > 2.3.6.\r\nSee screenshot below where the red line shows the behaviour in pycaret==2.3.6 (which is the wanted and expected behaviour) and in orange the nested unwanted behaviour in pycaret 2.3.8 , 2.3.9 and 2.3.10\r\n![image](https:\/\/user-images.githubusercontent.com\/50994394\/200543740-0883c8ba-9f4a-4d1f-8abc-560ae7dbd54e.png)\r\n @nagamatz @tdekelver-bd This issue was recently fixed on last rc release. Can you try installing pycaret with `pip install --pre pycaret` and let me know if you still face the issue. It's not yet fixed with 3.0.0rc4. This is the results with the code in the first post. Now, mlflow is 2.0.1\r\n![\u7121\u984c](https:\/\/user-images.githubusercontent.com\/1991802\/206426156-4b19a4cc-b865-4af3-8281-1b89fc099f28.png)\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to create probabilities"
    },
    {
        "Answerer_created_time":1530826195963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":202.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":4541.5040575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Challenge_closed_time":1660920166467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644569855830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information about the mechanism behind hyperparameter tuning job in AWS Sagemaker. They are specifically trying to bring their own container and minimize cross entropy loss. They are questioning whether the hyperparameters defined in the HyperParameterTuner class get copied into \/opt\/ml\/input\/config\/hyperparameters.json and whether the training image should be adjusted to use the hyperparameters from that file. The user is also confused about how hyperparameters are passed into the training code, as some sample HPO notebooks use argparser to pass in the hyperparameters.",
        "Challenge_last_edit_time":1644570751860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4541.7529547222,
        "Challenge_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446577693503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"AWS Sagemaker HPO mechanism"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":7.9073555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Challenge_closed_time":1613773903383,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613745436903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user trained multiple models with different configurations for a custom hyperparameter search using pytorch_lightning and TensorboardLogger for logging. When running the training script, ClearML auto-creates a Task and connects the logger output to the server. However, when there are multiple configurations, the logged values have different names, making them incomparable. The user is seeking advice on how to properly use a loop in a script using completely separated tasks.",
        "Challenge_last_edit_time":1614004159640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.9073555556,
        "Challenge_title":"ClearML multiple tasks in single script changes logged value names",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":172,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":11.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":135.0,
        "Tool":"ClearML",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incomparable logged values"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.1527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Challenge_closed_time":1661948109000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661861159000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running train_model from examples after installing graphnet from scratch and signing up to WandB. The error occurred due to the absence of a directory called \"wandb\" and can be fixed by creating the folder manually. The user suggests automatically creating the folder if it is not present.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.9,
        "Challenge_reading_time":59.04,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":65.0,
        "Challenge_repo_issue_count":548.0,
        "Challenge_repo_star_count":63.0,
        "Challenge_repo_watch_count":5.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":24.1527777778,
        "Challenge_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":330,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing directory"
    },
    {
        "Answerer_created_time":1558310526776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":70.5894952778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Challenge_closed_time":1651509693110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651238636737,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a pre-trained sklearn model on AWS Sagemaker using a serverless deployment method. They have saved the model using joblib.dump and uploaded it to S3, along with an inference script. They have also created a tar file with the model data and inference code and uploaded it to S3. However, the endpoint creation is stuck on \"creating\" and the user is receiving an error message in Cloudwatch. The user has tried different folder structures and accounts, but the issue persists. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":1651255570927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.9,
        "Challenge_reading_time":128.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":60,
        "Challenge_solved_time":75.2934369445,
        "Challenge_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":397.0,
        "Challenge_word_count":816,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558310526776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":21.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint creation stuck"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":25.8737591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a neural-network where I'm using Optuna to find some optimal hyper-parameters e.g batch-size etc.<\/p>\n<p>I want to save the nets-parameters when Optuna finds a new best parameter-combination.<\/p>\n<p>I have tried the following two approahces:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>SCORE = 0\ndef objective(trial):\n    BATCH_SIZE = trial.suggest_int(&quot;BATCH_SIZE&quot;,20,100)\n    LEARNING_RATE = trial.suggest_float(&quot;LEARNING_RATE&quot;,0.05,1)\n    DROPOUT = trial.suggest_float(&quot;DROPOUT&quot;,0.1,0.9)\n\n    Y_SCORE,Y_VAL = train_NN(X,y,word_model,BATCH_SIZE,250,LEARNING_RATE,DROPOUT)\n    y_val_pred = Y_SCORE.argmax(axis=1)\n    labels = encode.inverse_transform(np.arange(6))\n    a = classification_report(Y_VAL, y_val_pred,zero_division=0,target_names=labels,output_dict=True)\n    score = a.get(&quot;macro avg&quot;).get(&quot;f1-score&quot;)\n    if score&gt;SCORE: #New best weights found - save the net-parameters\n        SCORE = score\n        torch.save(net,&quot;..\/model_weights.pt&quot;)\n    return score\n<\/code><\/pre>\n<p>which fails with <code>UnboundLocalError: local variable 'SCORE' referenced before assignment<\/code> but if I move <code>SCORE=0<\/code> inside the function at the top, it resets at each trial.<\/p>\n<p>The reason I want to save the weights right away, and not just run another training with <code>study.best_params<\/code> at the end, is that sometimes the random initialization of the weights has an impact and gives a higher score (although if the training is robust, it should not make a difference) - but that is not the point\/the issue.<\/p>",
        "Challenge_closed_time":1628000129956,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627906984423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save the neural network's parameters when Optuna finds a new best parameter combination. They have tried two approaches, but both have failed. The first approach resulted in an \"UnboundLocalError\" while the second approach resets the score at each trial. The user wants to save the weights right away instead of running another training with \"study.best_params\" at the end.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68621547",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":25.8737591667,
        "Challenge_title":"\"do something\" when Optuna finds new best parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":483.0,
        "Challenge_word_count":165,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461069934027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3961.0,
        "Poster_view_count":616.0,
        "Solution_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/62144904\/python-how-to-retrive-the-best-model-from-optuna-lightgbm-study\/62164601#62164601\">This answer<\/a> is helpful to save the neural network's weights; We can use the callback function to save the model's checkpoint.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":3.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed to save parameters"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":127.0472208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Challenge_closed_time":1517706762272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517249392277,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in taking advantage of the speedup from distributing the job to many instances by increasing the instance count argument of the training algorithm when using a custom algorithm in AWS Sagemaker. Increasing the instance count duplicates the training on every instance, leading to no speedup. The user suspects that there is something special they need to do to control how the training is handled differently for a particular instance inside their custom train() function. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":14.0,
        "Challenge_reading_time":29.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":127.0472208333,
        "Challenge_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":295,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368093601223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":249.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no speedup with increased instance count"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":377.9588433334,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Challenge_closed_time":1663850808295,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662490156459,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking advice on the appropriate workflow to run multiple experiments with different initial conditions and collect the results as a group. They are considering using a combination of sweeps and run groups, but are unsure of the best approach and how to collect the results and assets for a group. They are also interested in using the WandB platform to visualize the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.0,
        "Challenge_reading_time":24.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":377.9588433334,
        "Challenge_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":304,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jlperla\">@jlperla<\/a> thank you for the detailed information, and great to hear that the grouping issue has been now resolved. Regarding your question using the API to filter runs, you could do that indeed with the following command:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"sweep\": \"sweep_id\"})<\/code><br>\nAlternatively you can use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/tags#how-to-add-tags\">API to tag all your runs<\/a> based on <code>my_sweep_name<\/code> identifier and then query runs as follows:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"tags\": \"my_sweep_name\"})<\/code><br>\nIs my_sweep_name defined in your config? In that case you could do <code>filters={\"config.sweep_name\": \"my_sweep_name\"}<\/code>.<\/p>\n<p>Would any of these work for you? Please let me know if you have any further questions or issues with this!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":11.58,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"run multiple experiments"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":18.2316369445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Challenge_closed_time":1651629955790,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651564321897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use cross validation in Sagemaker hyperparameter tuning jobs to avoid relying too much on the train validation split, but is unsure if it is possible and has not found any information through extensive googling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.2316369445,
        "Challenge_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use cross validation"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":14.2479583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1664021303563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663970010913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error in implementing evaluation metrics in a regression model using Azure ML Designer. The error occurred when calling the dataset for the operation, and the user is confused as to why the same approach worked for linear regression but not for logistic regression. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":14.2479583334,
        "Challenge_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652172570283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_readability":14.1,
        "Solution_reading_time":30.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in evaluation metrics"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":4.4438166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Challenge_closed_time":1660907847087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660891849347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble understanding the meaning of \"config=wandb.config\" in the tutorials of wandb while setting up a sweep for their logistic regression model. They have provided the code they have used for the sweep and training, and are seeking an explanation of the steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":33.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.4438166667,
        "Challenge_title":"What is the meaning of 'config = wandb.config'?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":209,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557474244067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":626.0,
        "Poster_view_count":140.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":28.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":33.0,
        "Solution_word_count":255.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"wandb config explanation"
    },
    {
        "Answerer_created_time":1538275960603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":9.2060972223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Challenge_closed_time":1570669528590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570636386640,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to find the default quota for Azure Machine Learning Compute in the web portal and is seeking information on how to find out the current quota values using SDK, CLI, or REST API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.2060972223,
        "Challenge_title":"Azure Machine Learning Compute quota?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":354.0,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.3,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"find current quota values"
    },
    {
        "Answerer_created_time":1442334437952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":2272.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":0.1509027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use the azure ML designer (preview).<\/p>\n\n<p>referencing this - <a href=\"https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>using my own input sheet which has four columns and some decimal values. nothing fancy and identical to the sample datasets provided. <\/p>\n\n<p>I do this step (from the linked document above)<\/p>\n\n<p><em>Select the Train Model module.\nIn the module details pane to the right of the canvas, select Edit column selector.\nIn the Label column dialog box, expand the drop-down menu and select Column names.\nIn the text box, enter price to specify the value that your model is going to predict.<\/em><\/p>\n\n<p>and I get this (but there are no errors in the actual designer window.<\/p>\n\n<p>\"Failed to parse column picker rules\"<\/p>",
        "Challenge_closed_time":1582126496627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582125953377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while using the Azure ML designer preview. They are trying to use their own input sheet with four columns and decimal values, similar to the sample datasets provided in the documentation. However, when they try to select the Train Model module and edit the column selector, they receive an error message stating \"Failed to parse column picker rules.\"",
        "Challenge_last_edit_time":1582126556240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60303714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1509027778,
        "Challenge_title":"Failed to parse column picker rules - azure ML designer",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>Okay, I found an answer myself. Hope that is okay.<\/p>\n\n<p>In my input sheet, the title was something like this \"Interest Rate %\". Looks like azure was trying to say that it does not like special characters it the column names.<\/p>\n\n<p>I edited my original csv file in excel, and removed the % in all the titles. <\/p>\n\n<p>Then, created a new data store. problem solved. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":4.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed to parse column picker rules"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.8393819445,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Challenge_closed_time":1647593923663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647450501888,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with wandb.watch while trying to log a pytorch model. They have tried the suggested solutions from the documentation and community threads but nothing seems to work. Even after running the code from a colab notebook, nothing is being logged except for a JSON file when log_graph is set to True. The user is using wandb version 0.12.10.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":39.8393819445,
        "Challenge_title":"Wandb.watch with pytorch not logging anything",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1548.0,
        "Challenge_word_count":128,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.5,
        "Solution_reading_time":14.19,
        "Solution_score_count":null,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"wandb.watch not logging"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8547805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I manually stop a run in a sweep so that the sweep agent will just continue with the next run?<\/p>",
        "Challenge_closed_time":1654829989272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654794512062,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to manually cancel a run in a sweep and proceed with the next run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/can-i-manually-cancel-a-run-in-a-sweep\/2583",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":1.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.8547805556,
        "Challenge_title":"Can I manually cancel a run in a sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":29,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fryderykkogl\">@fryderykkogl<\/a> ,<\/p>\n<p>Yes, you can manually stop runs directly from the webUI. In the sweeps run table, select the options menu for the run you want to stop, and select  <code>stop run<\/code>. The sweep will continue to the next run configuration. See this image for reference. Please let us know if you have any followup questions.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc.png\" data-download-href=\"\/uploads\/short-url\/bC6fK8EPrbIPohDGGY4pPBG24Ec.png?dl=1\" title=\"Screen Shot 2022-06-09 at 7.57.28 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png\" alt=\"Screen Shot 2022-06-09 at 7.57.28 PM\" data-base62-sha1=\"bC6fK8EPrbIPohDGGY4pPBG24Ec\" width=\"394\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_591x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_788x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-06-09 at 7.57.28 PM<\/span><span class=\"informations\">952\u00d71206 94.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":19.5,
        "Solution_reading_time":26.34,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":115.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"cancel sweep run"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.5084697222,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>To back up the code, I want to upload my training code to Wandb every time I run it. Is this possible?<\/p>",
        "Challenge_closed_time":1644912819264,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892988773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to upload their training code to WandB every time they run it to back up the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-upload-code-to-wandb-every-time-i-run-it\/1922",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.5084697222,
        "Challenge_title":"How do I upload code to WandB every time I run it",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":211.0,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No, wandb does not have an option to store code. Why do you want to save the code?<\/p>\n<ol>\n<li>\n<p>Are you changing the hparams in your code in every run?  - Then you could try using wandb.sweep() instead, as it visualizes your model\u2019s performance for different hparams<\/p>\n<\/li>\n<li>\n<p>Are you using different architectures while training? -   Wandb artifacts logs datasets and model\/training data. There are functions that track all your parameters (wandb. watch() iirc). This leads to an ONNX format of your model being saved.  This ONNX model can be visualized, and you could use that to see what model was trained. Or you could even save a string in your config file with details of the model you\u2019re training<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":8.92,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"upload code to WandB"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":49.2027833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As far as I can tell there is no way to use the Excel add in for Azure ML using the new Azure ML service, it only works for the Classic. Is there any plan to provide a replacement add in that brings this functionality to the new Azure ML before Classic stops being supported in 2024?<\/p>",
        "Challenge_closed_time":1647857647663,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647680517643,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in using the Excel add-in for Azure ML with the new Azure ML service as it only works for the Classic version. They are inquiring about any plans to provide a replacement add-in before the Classic version stops being supported in 2024.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/778717\/replacement-for-azure-ml-classic-excel-add-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":4.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":49.2027833334,
        "Challenge_title":"Replacement for Azure ML Classic Excel Add In",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=325bba53-0a8f-4bf1-ab22-527f5cbac10d\">@Tim Cahill  <\/a>  Thanks for the question. Currently it's on roadmap to support in the near  future.  Excel add in feature similar to studio classic, it will be built on top on v2 online endpoints.    <br \/>\nCurrently, managed endpoints are not integrated with Designer, we need to first provide capability to do a no code designer deployment on v2 online endpoints and integrating excel add in for v2 endpoints.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":5.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"replacement for Excel add-in"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":579.0058813889,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I\u2019ve been trying to find some documentation, I don\u2019t want to save all the hyperparameters each epoch, just the learning rate.<br>\nWould be so great if you can help me out.<\/p>\n<p>Cheers,<\/p>\n<p>Oli<\/p>",
        "Challenge_closed_time":1679602846363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677518425190,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help on how to log the learning rate with PyTorch Lightning when using a scheduler, without saving all the hyperparameters each epoch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-the-learning-rate-with-pytorch-lightning-when-using-a-scheduler\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":579.0058813889,
        "Challenge_title":"How to log the learning rate with pytorch lightning when using a scheduler?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019m also wondering how this is done! Whether within a sweep configuration or not - when using a lr scheduler, I am trying to track the lr at epoch during training, as it is now dynamic. Even within a sweep, you will have some initial lr  determined during the sweep, but it will not stay constant for the duration of training.<\/p>\n<p>edit:<\/p>\n<p>The example on the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/1.2.10\/api\/pytorch_lightning.callbacks.lr_monitor.html#learning-rate-monitor\" rel=\"noopener nofollow ugc\">lightning site here<\/a> worked for me:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateMonitor\n&gt;&gt;&gt; lr_monitor = LearningRateMonitor(logging_interval='step')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_monitor])\n<\/code><\/pre>\n<p>Passing the <code>WandBLogger<\/code> to the trainer I see my lr is logged on the <code>wandb<\/code> dashboard.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":12.09,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"log learning rate"
    },
    {
        "Answerer_created_time":1539831335196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":137.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":870.7459847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Challenge_closed_time":1583860547412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580725861867,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to use Keras CSVLogger callbacks in Sagemaker script mode. The log file fails to write on S3, resulting in a \"No such file or directory\" error. The user has tried creating a file beforehand using TensorFlow backend, but it doesn't work. The user is looking for a solution to write the CSV file to the S3 bucket location before the fit method tries to write the callback. The user is open to other suggestions to get callbacks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":31.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":870.7459847222,
        "Challenge_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":292,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":19.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":221.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CSVLogger fails to write"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.5540683334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:  <\/p>\n<p>best_run, fitted_model = remote_run.get_output()  <br \/>\nfitted_model  <\/p>\n<p>When runnning the code, I get the following error:   <\/p>\n<p>AttributeError: 'DataTransformer' object has no attribute 'enable_dnn'  <\/p>\n<p>Any help would be much appreciated.   <\/p>",
        "Challenge_closed_time":1613128985603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613083790957,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a remote run model built using the AutoML function. The code is producing an error message stating that the 'DataTransformer' object has no attribute 'enable_dnn'. The user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":12.5540683334,
        "Challenge_title":"Remote run model unable to be saved",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=f0b99777-9086-42ac-b2e2-2b069641e943\">@Bernardo Jaccoud  <\/a> Did your run configure enable_dnn i.e bert <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#bert-integration-in-automated-ml\">settings<\/a> of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"attribute error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.1185105556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Challenge_closed_time":1659047787071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658741360433,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's script crashed during training and did not save the latest epoch information, resulting in a large offset in epoch numbers. The user is seeking a way to edit the epoch number and add a certain value to each entry, and is wondering if there is a cleaner way to perform such an update.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.1185105556,
        "Challenge_title":"Update offline run before syncing",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"script crash and offset in epoch numbers"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":459.2678666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an experiment to create recommendations (using the Movie Ratings sample database), but without using the ratings. I simply consider that if a user has rated certain movies, then he would be interested by other movies that have been rated by users that have also rated his movies.<\/p>\n\n<p>I can consider, for instance, that ratings are 1 (exists in the database) or 0 (does not exist), but in that case, how do I transform the initial data to reflect this?<\/p>\n\n<p>I couldn't find any kind of examples or tutorials about this kind of scenario, and I don't really know how to proceed. Should I transform the data before injecting it into an algorithm? And\/or is there any kind of specific algorithm that I should use?<\/p>",
        "Challenge_closed_time":1471354453808,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469698571557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create recommendations using the Movie Ratings sample database in Azure ML without using ratings. They want to consider if a user has rated certain movies, then they would be interested in other movies that have been rated by users that have also rated their movies. The user is unsure how to transform the initial data to reflect this and is seeking advice on whether to transform the data before injecting it into an algorithm and if there is a specific algorithm to use.",
        "Challenge_last_edit_time":1469701089488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38632533",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":459.9672919444,
        "Challenge_title":"Recommendations without ratings (Azure ML)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":910.0,
        "Challenge_word_count":132,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461857379436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lille, France",
        "Poster_reputation_count":133.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are<\/em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)<\/p>\n\n<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:<\/p>\n\n<pre><code>library(dplyr)\ndf &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs\nall_movies &lt;- unique(df[['movie']])\nall_users &lt;- unique(df[['user']])\nn &lt;- 30  # number of random pairs to start with\n\nnegative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),\n                                    user = sample(all_users, n, replace=TRUE),\n                                    rating = rep(0, n))          \nacceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))\ndf &lt;- rbind(df, acceptable_negative_observations)\nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>\n\n<p>Alternatively, you could try a method like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning\" rel=\"nofollow\">association rule learning<\/a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Frequently-bought-together-market-basket-analyses-using-ARULES-1\" rel=\"nofollow\">nice example<\/a> of how to do this in R\/AML in the Cortana Intelligence Gallery.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"transform data for recommendations"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is able to log and fetch metrics to AzureML using Run.log, but is unable to log run parameters like Learning Rate or Momentum. The user has tried to find a solution in the AzureML Python SDK documentation but has been unsuccessful. However, the user has found a way to log parameters using MLflow's mlflow.log_param, which shows up on the AzureML Studio Dashboard. The user is looking for a way to log parameters directly using azureml.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":688.1130463889,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"log parameters with AzureML"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11.8299888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Challenge_closed_time":1579719419647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579686060327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in throwing an exception from within an MLflow project. They are executing a function using mlflow.run but are getting an ExecutionException. The user is seeking a way to get the exception being raised where they are executing mlflow.run or to send an mlflow.exceptions.ExecutionException with a custom message set from within the project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.2664777778,
        "Challenge_title":"How can I throw an exception from within an MLflow project?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472932425400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1579728648287,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ExecutionException raised"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1663557360000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to see any models in the `mlflow ui` during training, despite several models having already converged and logged to the file system. The user has tried both the nightly and release branch and has confirmed the issue on the latest version and develop branch of pycaret. The user has provided a reproducible example and expects to see the models that have already converged in the `mlflow ui` dashboard.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":145,
        "Discussion_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"models not visible"
    }
]