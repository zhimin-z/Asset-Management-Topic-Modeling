[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Challenge_closed_time":1642173,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642148915000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":2.13,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb api key for github ci",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":14,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/settings\/secrets\r\n\r\n```\r\njobs:\r\n  weekday_job:\r\n    runs-on: ubuntu-latest\r\n    env:\r\n      DAY_OF_WEEK: Mon\r\n    steps:\r\n      - name: \"Hello world when it's Monday\"\r\n        if: ${{ env.DAY_OF_WEEK == 'Mon' }}\r\n        run: echo \"Hello $FIRST_NAME $middle_name $Last_Name, today is Monday!\"\r\n        env:\r\n          WANDB_API_KEY: $github.SECRETS.WANDB_API\r\n          WANDB_NAME: github_ci_tests\r\n          WANDB_NAME: Octocat\r\n```\r\n\r\n\r\nhttps:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.8,
        "Solution_reading_time":5.33,
        "Solution_score_count":null,
        "Solution_sentence_count":4,
        "Solution_word_count":35,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568567773507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":18.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Incremental learning with a built-in sagemaker algorithm",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445974115332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington, United States",
        "Poster_reputation_count":5939.0,
        "Poster_view_count":324.0,
        "Solution_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":19.0,
        "Solution_reading_time":10.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":65,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to understand the difference between those two function calls:<\/p>\n<p>I am referring to the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-the-summary\">documentation of define_metric<\/a>:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>Is it the \u201cbest\u201d accuracy ever measured (during training) versus the accuracy of the \u201cbest\u201d (validation) model? I understand that wandb does not care what metric I log, but what is the intended use?<\/p>\n<p>Thank you for clarification.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659447036761,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/understanding-define-metric-parameters\/2836",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Understanding define_metric parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>,<\/p>\n<p>For each metric logged, there is a summary metric that\u2019ll summarize the logged values as <em>one<\/em> value for each run. By default, W&amp;B uses the <em>latest<\/em> value, but you can update it with <code>wandb.summary['acc'] = best_acc<\/code> or using the two <code>define_metric<\/code> calls you show.<\/p>\n<p>This is then used to decide which value is displayed in plots that only use one value for each run (e.g. Scatter plots).<\/p>\n<pre><code class=\"lang-auto\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>These two calls are both functionally the same, one will show <code>acc.best<\/code> and one will show as <code>acc.max<\/code> in the summary metrics of your run. Both will be the maximum value that you log for <code>acc<\/code> like <code>wandb.log('acc':acc)<\/code> during a run.<\/p>\n<p>You can see the summary metrics of each run by clicking the <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> icon in the top left nav bar in a run.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.1,
        "Solution_reading_time":15.87,
        "Solution_score_count":null,
        "Solution_sentence_count":13,
        "Solution_word_count":148,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1659198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658586252000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":261.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548817091420,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1224733422316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":7707.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":19.6,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":43,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621374020000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_participation_count":7,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":325,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score_count":null,
        "Solution_sentence_count":28,
        "Solution_word_count":260,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634647592000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":97,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":18.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":20,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1599815370823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Western Australia, Australia",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611052310507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65789715",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608633925527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dubai - United Arab Emirates",
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":8.0,
        "Solution_reading_time":10.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9,
        "Solution_word_count":109,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1523298968403,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1754.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1568296785460,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.0,
        "Challenge_reading_time":24.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":644.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":51.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":19,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We want to be able to quickly test changes to <code>entry_script.py<\/code>. We can test minor changes with unit tests but we want to run a model in the context of our other backend pieces, locally. <\/p>\n\n<p>So we run <code>az ml model deploy<\/code> with a deployment config with a <code>computeType<\/code> of <code>LOCAL<\/code>. Non-local deployment is slow, but we were hoping that local deployment would be faster. Unfortunately it isn't. In some cases it can take up to 20 minutes to deploy a model to a local endpoint.<\/p>\n\n<p>Is there a way to speed this up for faster edit-debug loops or a better way of handling this scenario?<\/p>\n\n<p>Few things I was thinking of:<\/p>\n\n<ul>\n<li>I was thinking <code>az ml service update<\/code> could be an option but even that takes a long time.<\/li>\n<li>Editing the file directly in the container is an option, but this is still annoying to manually synchronize with changes in your local filesystem.<\/li>\n<li>I was thinking of a folder mount in the container, but it seems there is some magic AzureML does, for example copying the <code>entry_script.py<\/code> to <code>\/var\/azureml-app\/main.py<\/code>. We could maybe emulate this by creating a <code>dist<\/code> folder locally that matches the layout and mounting that to the container, but I'm not sure if this folder layout would change or there's other things that AzureML does.<\/li>\n<\/ul>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576266473653,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1576476751263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59328925",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":17.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to speed up local AzureML model deployment?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":339.0,
        "Challenge_word_count":230,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254279877887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":153.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>Please follow the below notebook, If you want to test deploying a model rapidly you should check out \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/deployment\/deploy-to-local\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/deployment\/deploy-to-local<\/a><\/p>\n\n<p>the SDK enables building and running the docker locally and updating in place as you iterate on your script to save time.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":18.5,
        "Solution_reading_time":6.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":44,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can I train models in parallel? Is is possible to train model in parallel on like hyperdrive?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653904605807,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/869619\/parallel-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Parallel training",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3d6a9d61-6cf9-45d8-870d-2fbbf147f56d\">@Chungsun  <\/a>  Thanks for the question. The max number of parallel tasks is limited by number of cores in the cluster (excluding master node).    <br \/>\nThe demand for parallelism comes from two sources: 1. The cross validation which address multiple combination of train-val datasets &amp; parameters 2. The training algorithm itself which can be parallelized.    <\/p>\n<p>\u2022\tYou can run multiple runs in a distributed fashion across AML clusters, meaning that each cluster node can be running a run in parallel to other nodes running other runs. For instance, that\u2019s what we also do with Pipeline steps, HyperParameter Tunning child runs and for Azure AutoML child runs.    <\/p>\n<p> <a href=\"https:\/\/github.com\/microsoft\/solution-accelerator-many-models\"> https:\/\/aka.ms\/many-models<\/a> is a solution accelerator that will help you walk through to run many models.     <br \/>\nIn the HyperDriveConfig there is AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run. So there would be 1 execution per node.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":12.1,
        "Solution_reading_time":18.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12,
        "Solution_word_count":163,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581064035000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615398273417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to setup AWS sagemaker - Resource limit Error",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530468231707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":357.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":51,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi!<\/p>\n<p>I have a Pay-As-You-Go account (with monthly amounts of free services) and want to create\/deploy a machine learning model in Azure ML Studio. Is it included in my subscription or do I have to pay more? I'm asking because in order to start working in ML Studio, I need to create a 'compute instance' and there is no free option.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673619039086,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160625\/is-machine-learning-studio-included-in-my-subscrip",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Is Machine Learning Studio included in my subscription?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@Marco <\/a>, Thanks for using Microsoft Q&amp;A Platform.  <\/p>\n<p>There are no additional costs associated with using Machine Learning service under a <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/purchase-options\/pay-as-you-go\/\">Pay-as-you-go<\/a> subscription.  But, for creating a compute instance it charges since it run on Azure infrastructure. If you have any doubts, always use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/\">Azure pricing calculator<\/a> to get an estimate of the costs before you create anything.<\/p>\n<p>Please visit here to know more information on <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning\/\">Machine Learning pricing<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-plan-manage-cost\">cost management<\/a>.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,\nVasavi<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":13.5,
        "Solution_reading_time":12.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9,
        "Solution_word_count":99,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453718439993,
        "Challenge_favorite_count":5.0,
        "Challenge_last_edit_time":1453911336527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Request Response latency",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1128.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446116840792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":311.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Solution_comment_count":11.0,
        "Solution_last_edit_time":1453911048927,
        "Solution_link_count":0,
        "Solution_readability":8.0,
        "Solution_reading_time":16.44,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12,
        "Solution_word_count":218,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Bonjour    <br \/>\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning    <br \/>\nA l'\u00e9tape Create compute resources    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute<\/a>    <\/p>\n<p>On me demande Search for and select Standard_DS11_v2    <\/p>\n<p>Hors, l'interface me dit que je n'ai pas les quotas disponibles.    <br \/>\nJ'utilise l'offre d'essai \u00e0 200 USD.    <br \/>\nComment faire pour que cela fonctionne ?    <br \/>\nCordialement    <br \/>\nThibaut<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638368598000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/647767\/comment-s-lectionner-standard-ds11-v2",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.5,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=e0b4def2-2525-4e3c-954a-129251c1bdb4\">@Thibaut Jacquin  <\/a> For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.4,
        "Solution_reading_time":10.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":86,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655132901000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":null,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":298,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score_count":null,
        "Solution_sentence_count":29,
        "Solution_word_count":312,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi community,   <br \/>\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants.  I haven't built any models yet, just wondering how feasible the below is.  A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.  <\/p>\n<p>A client uploads a set of standard documents (usually scanned PDF's)  using a file upload in our .net application.    <br \/>\nCan we:  <\/p>\n<ol>\n<li> Use form recogniser to extract key value pairs, after training a custom model.  <\/li>\n<li> Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.  <\/li>\n<li> Report back to the user the missing information so they can correct the document and reupload them?  <br \/>\nPreferably in real time?  So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?  <\/li>\n<\/ol>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647481005860,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Form recognizer to report on missing information in (near) real-time",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d4ec46af-03e5-46ab-ace4-d0dd3b8d93ba\">@Andrew Robertson  <\/a> Yes, you can use Azure form recognizer to analyze a document that is passed to the API and use the result of the analyze operation to report any missing fields in the form back to the user. This is the most widely used use case by most of the customers.     <\/p>\n<p>Form recognizer comes with a set of prebuilt APIs where it can extract common information from invoices, business cards, receipts etc. If you have a form that does not conform to the prebuilt API standards you need to create a custom model to extract the text in the form of a tags and their key:value pairs. The custom models require some basic training with some test forms and if all the forms that need extraction follow the same layout or guidelines the extraction results will be good.     <\/p>\n<p>In the case of custom forms the results are provided in almost real time where the form is submitted or <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/AnalyzeDocument\">POST<\/a> request is sent to the API and an operation id is returned to retrieve the results using <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/GetAnalyzeDocumentResult\">GET<\/a>.  Depending on your pricing tier of your resource if you intend to perform these actions synchronously you might have to limit the rate of requests sent to the API to avoid any TPS errors. If you are using async operations with a slight delay to fetch the results then you can design an application that can take large number of documents and provide results to the users within a short span of time.     <\/p>\n<p>I hope the above information is helpful.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":14.4,
        "Solution_reading_time":27.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13,
        "Solution_word_count":292,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi team, I am trying to reconfigure queues for one of our agents, and one queue (used by several projects) is full and has several runs queued. Is there a feature to drain a queue, like a button or an API to stop all runs for a specific queue in all projects? (edited)",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650719473000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1500",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.1,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to stop all runs attached to a specific queue",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Under all runs, you can just filter the table by that queue and then stop the runs using the selection and multi-run action:\n\nIf you use the same name for your queues in all agents you can additionally select the specific agent using the filter:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":18.5,
        "Solution_reading_time":2.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":45,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>if I tune a model with the LightGBMTunerCV I always get this massive result of the cv_agg's binary_logloss. If I do this with a bigger dataset, this (unnecessary) io slows down the performance of the optimization process.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nimport optuna\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nbreast_cancer = load_breast_cancer()\n\nX_train, X_test, Y_train, Y_test = train_test_split(breast_cancer.data, breast_cancer.target)\n\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=breast_cancer.feature_names.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=breast_cancer.feature_names.tolist())\ncallbacks = [lgb.log_evaluation(period=0)]\ntuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True)\n\n\ntuner.run()\n<\/code><\/pre>\n<p>And the output:<\/p>\n<pre><code>feature_fraction, val_score: 0.327411:  43%|###################2      | 3\/7 [00:00&lt;00:00, 13.84it\/s]\n[1] cv_agg's binary_logloss: 0.609496 + 0.009315\n[2] cv_agg's binary_logloss: 0.554522 + 0.00607596\n[3] cv_agg's binary_logloss: 0.512217 + 0.0132959\n[4] cv_agg's binary_logloss: 0.479142 + 0.0168108\n[5] cv_agg's binary_logloss: 0.440044 + 0.0166129\n[6] cv_agg's binary_logloss: 0.40653 + 0.0200005\n[7] cv_agg's binary_logloss: 0.382273 + 0.0242429\n[8] cv_agg's binary_logloss: 0.363559 + 0.03312\n<\/code><\/pre>\n<p>Is there any way to get rid of this output?<\/p>\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637743478823,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70093026",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"Supressing optunas cv_agg's binary_logloss output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":200.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398509643447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dortmund, Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can pass verbose_eval parameter with value None in LightGBMTunerCV().<\/p>\n<p>Example:<\/p>\n<pre><code>tuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True, verbose_eval=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":18.1,
        "Solution_reading_time":4.25,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":23,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to start a sweep using this yaml file.<\/p>\n<p>sweep.yaml<\/p>\n<pre><code class=\"lang-auto\">method: bayes\nmetric:\n  goal: maximize\n  name: val_f1_score\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    value: 42\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 30\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    value: adam\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  paths:\n    - \n      data:\n        value: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  -\n    use:\n      value: True\n    project:\n      value: Whats-this-rock\n\ndataset:\n  -\n    id:\n      value: [1, 2, 3, 4]\n    dir:\n      value: data\/3_consume\/\n    image:\n      size:\n        value: 124\n      channels:\n        value: 3\n    classes:\n      value: 10\n    sampling:\n      value: None\n\nmodel:\n  -\n    backbone:\n      value: efficientnetv2m\n    use_pretrained_weights:\n      value: True\n    trainable:\n      value: True\n    preprocess:\n      value: True\n    dropout_rate:\n      value: 0.3\n\ncallback:\n  -\n    monitor:\n      value: \"val_f1_score\"\n    earlystopping:\n      patience:\n        value: 10\n    reduce_lr:\n      factor:\n        values: [.9, .7, .5]\n      min_lr: 0.00001\n      patience:\n        values: [1, 2, 3, 4]\n    save_model:\n      status:\n        value: True\n      best_only:\n        value: True\n\nprogram: src\/models\/train.py\n\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">Error: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>Here\u2019s the full traceback of the error:-<\/p>\n<pre><code class=\"lang-auto\">During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 942, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2178, in upsert_sweep\n    raise e\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2175, in upsert_sweep\n    check_retry_fn=no_retry_4xx,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2153, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>I am using hydra and trying to replicate a config.yaml for wandb sweeps<\/p>\n<p>config.yaml<\/p>\n<pre><code class=\"lang-auto\">notes: \"\"\nseed: 42\nlr: 0.001\nepochs: 30\naugmentation: True\nclass_weights: True\noptimizer: adam\nloss: categorical_crossentropy\nmetrics: [\"accuracy\"]\nbatch_size: 64\nnum_classes: 7\n\npaths:\n  data: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  use: True\n  project: Whats-this-rock\n\ndataset:\n  id: [1, 2, 3, 4]\n  dir: data\/3_consume\/\n  image:\n    size: 124\n    channels: 3\n  classes: 10\n  sampling: None\n\nmodel:\n  backbone: efficientnetv2m\n  use_pretrained_weights: True\n  trainable: True\n  preprocess: True\n  dropout_rate: 0.3\n\ncallback:\n  monitor: \"val_f1_score\"\n  earlystopping:\n    patience: 10\n  reduce_lr:\n    factor: 0.4\n    min_lr: 0.00001\n    patience: 2\n  save_model:\n    status: True\n    best_only: True\n\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663098229093,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multi-level-nesting-in-yaml-for-sweeps\/3108",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"Multi-level nesting in yaml for sweeps",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1087.0,
        "Challenge_word_count":350,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is to use dot notation instead of nested parameters as wandb (v0.13.3) sweeps doesn\u2019t support nested parameters.<\/p>\n<pre><code class=\"lang-auto\">sweep.yaml\n\nmethod: bayes\nmetric:\n  goal: maximize\n  name: val_accuracy\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    values: [1, 42, 100]\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 100\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    values: [adam, adamax]\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.70\n      - 0.75\n      - 0.80\n  data_path:\n    value: data\/4_tfds_dataset\/\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv3\n  dataset_id:\n    values:\n      - [1]\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None, oversampling, undersampling]\n  backbone:\n    values:\n      [\n        efficientnetv2m,\n        efficientnetv2,\n        resnet,\n        mobilenetv2,\n        inceptionresnetv2,\n        xception,\n      ]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [True, False]\n  preprocess:\n    value: True\n  dropout_rate:\n    values: [0.3]\n  monitor:\n    value: \"val_accuracy\"\n  earlystopping.use:\n    value: True\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [True]\n  reduce_lr.factor:\n    values: [.9, .7, .5, .3]\n  reduce_lr.patience:\n    values: [1, 3, 5, 7, 13]\n  reduce_lr.min_lr:\n    value: 1e-5\n  save_model:\n    value: False\n\nprogram: src\/models\/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.77,
        "Solution_score_count":null,
        "Solution_sentence_count":15,
        "Solution_word_count":159,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598478428030,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609427499190,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":14.7,
        "Solution_reading_time":18.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11,
        "Solution_word_count":154,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1336700249823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":885.0,
        "Answerer_view_count":127.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1542890499740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Sacred - pass all parameters as one",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":735.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443016881603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":9385.0,
        "Poster_view_count":1033.0,
        "Solution_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":16.5,
        "Solution_reading_time":5.35,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5,
        "Solution_word_count":33,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>If I call <code>wandb.run.log_code(\".\")<\/code>, all python source code files in the current directory are saved in W&amp;B cloud. That\u2019s what I want.<\/p>\n<p>However, only changes in main training file where I call <code>wandb.init()<\/code> can be shown in <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/code#code-comparer\">Code Comparer<\/a>. The change in other file (like <code>helper_funcs.py<\/code>) will not appear in code panel. Do you have any suggestions about it?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661772961165,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/code-comparer-can-only-show-the-difference-of-main-training-file\/3020",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Code Comparer can only show the difference of main training file",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Yao, thanks for writing in! As you have said, you can\u2019t compare in the panel other files than the one where you call <code>wandb.init()<\/code>, but you can compare them in the artefacts tab. I send you a video on how to do this. Please let me know if this would work for you.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.8,
        "Solution_reading_time":3.41,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":54,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1547395160296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":30365.0,
        "Answerer_view_count":5514.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594553641273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594581031423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1210.0,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.84,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7,
        "Solution_word_count":97,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636160879946,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Using Wandb with HParams on TF",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":579.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just had a chat with the support and figured out how to fix the problem with over-writing.<\/p>\n<p>Issue was with the init function and there is a flag for reinitializing (<code>reinit=True<\/code>)<\/p>\n<p><code>wandb.init(project=\"newjob1\", entity=\"demou\",reinit=True)<\/code>  this fixed this issue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":37,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":998.4904036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training an image segmentation model on azure ML pipeline. During the testing step, I'm saving the output of the model to the associated blob storage. Then I want to find the IOU (Intersection over Union) between the calculated output and the ground truth. Both of these set of images lie on the blob storage. However, IOU calculation is extremely slow, and I think it's disk bound. In my IOU calculation code, I'm just loading the two images (commented out other code), still, it's taking close to 6 seconds per iteration, while training and testing were fast enough. <\/p>\n\n<p>Is this behavior normal? How do I debug this step?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568713801943,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57971689",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Disk I\/O extremely slow on P100-NC6s-V2",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>A few notes on the drives that an AzureML remote run has available:<\/p>\n\n<p>Here is what I see when I run <code>df<\/code> on a remote run (in this one, I am using a blob <code>Datastore<\/code> via <code>as_mount()<\/code>):<\/p>\n\n<pre><code>Filesystem                             1K-blocks     Used  Available Use% Mounted on\noverlay                                103080160 11530364   86290588  12% \/\ntmpfs                                      65536        0      65536   0% \/dev\ntmpfs                                    3568556        0    3568556   0% \/sys\/fs\/cgroup\n\/dev\/sdb1                              103080160 11530364   86290588  12% \/etc\/hosts\nshm                                      2097152        0    2097152   0% \/dev\/shm\n\/\/danielscstorageezoh...-620830f140ab 5368709120  3702848 5365006272   1% \/mnt\/batch\/tasks\/...\/workspacefilestore\nblobfuse                               103080160 11530364   86290588  12% \/mnt\/batch\/tasks\/...\/workspaceblobstore\n<\/code><\/pre>\n\n<p>The interesting items are <code>overlay<\/code>, <code>\/dev\/sdb1<\/code>, <code>\/\/danielscstorageezoh...-620830f140ab<\/code> and <code>blobfuse<\/code>:<\/p>\n\n<ol>\n<li><code>overlay<\/code> and <code>\/dev\/sdb1<\/code> are both the mount of the <strong>local SSD<\/strong> on the machine (I am using a STANDARD_D2_V2 which has a 100GB SSD).<\/li>\n<li><code>\/\/danielscstorageezoh...-620830f140ab<\/code> is the mount of the <strong>Azure File Share<\/strong> that contains the project files (your script, etc.). It is also the <em>current working directory<\/em> for your run.<\/li>\n<li><strong><code>blobfuse<\/code><\/strong> is the blob store that I had requested to mount in the <code>Estimator<\/code> as I executed the run.<\/li>\n<\/ol>\n\n<p>I was curious about the performance differences between these 3 types of drives. My mini benchmark was to download and extract this file: <a href=\"http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz\" rel=\"nofollow noreferrer\">http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz<\/a> (it is a 220 MB tar file that contains about 3600 jpeg images of flowers).<\/p>\n\n<p>Here the results:<\/p>\n\n<pre><code>Filesystem\/Drive         Download_and_save       Extract\nLocal_SSD                               2s            2s  \nAzure File Share                        9s          386s\nPremium File Share                     10s          120s\nBlobfuse                               10s          133s\nBlobfuse w\/ Premium Blob                8s          121s\n<\/code><\/pre>\n\n<p>In summary, writing small files is much, much slower on the network drives, so it is highly recommended to use \/tmp or Python <code>tempfile<\/code> if you are writing smaller files. <\/p>\n\n<p>For reference, here the script I ran to measure: <a href=\"https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535<\/a><\/p>\n\n<p>And this is how I ran it: <a href=\"https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572308367396,
        "Solution_link_count":6,
        "Solution_readability":11.2,
        "Solution_reading_time":34.57,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17,
        "Solution_word_count":293,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1423640080283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":457.0,
        "Answerer_view_count":125.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to import mlflow after having launched a log with opencensus Azure.\nThe MLFlow import runs forever.<\/p>\n<p>My environment is the following:<\/p>\n<ul>\n<li>Python 3.7<\/li>\n<li>opencensus-ext-azure 1.0.7<\/li>\n<li>opencensus-ext-logging 0.1.0<\/li>\n<li>mlflow 1.15.0<\/li>\n<\/ul>\n<p>Here is the code to repoduce the bug:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619011270043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67196775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a workaround to make opencensus work with MLFlow?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>I found a workaround, not the cleanest one though.<\/p>\n<p>I import mlflow at the beginning even if it's not useful this way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":16.6,
        "Solution_reading_time":6.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":40,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1495636394672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610053277393,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65619881",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495636394672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":23,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1310482059760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":10753.0,
        "Answerer_view_count":895.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1587620263500,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker usage of EC2 instances",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":615.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":10.9,
        "Solution_reading_time":6.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6,
        "Solution_word_count":44,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464683630827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1465977920520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37540703",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.2,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Test multiple algorithms in one experiment",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":138,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548952048437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1549513217232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2360.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361821819380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Puebla",
        "Poster_reputation_count":147.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1548962267732,
        "Solution_link_count":2,
        "Solution_readability":9.7,
        "Solution_reading_time":13.54,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10,
        "Solution_word_count":135,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1384322523967,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":574.0,
        "Answerer_view_count":66.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655778299153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":122.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544467691223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Zacatecas, Mexico",
        "Poster_reputation_count":131.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":17,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, simple Q. I want to launch a tensorboard with the tensorboard profiler pip installed from the GUI. At the moment I am using this:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"uuid: XX\"\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nHowever I want to runPatch it so that it installs:\n\npip install -U tensorboard-plugin-profile\n\nIs there a way to easily do this?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651747273000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1502",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to patch a multi-run downstream operation, for example tensorboard:multi-run",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can add the following runPatch:\n\n...\npatchStrategy: replace\nrunPatch:\n  container:\n    command: [\"bash\", \"-c\"]\n    args:\n      - \"pip install -U tensorboard-plugin-profile && tensorboard --logdir={{globals.artifacts_path}} --port={{globals.ports[0]}} --path_prefix={{globals.base_url}} --host=0.0.0.0\"\n\nFor the specific case of tensorboard, we will add a new input plugins of type List[str] to the tensorboard component versions , so instead of patching the component, users can pass a parameter:\n\nparams:\n  plugins: { value: [tensorboard-plugin-profile, tensorboard-plugin-custom, ...] }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.0,
        "Solution_reading_time":7.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":60,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1565697423932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Uzbekistan",
        "Answerer_reputation_count":602.0,
        "Answerer_view_count":117.0,
        "Challenge_adjusted_solved_time":8037.6370116666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630917852487,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1630917952870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to optimize for multiple metrics in Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1887.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565697423932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Uzbekistan",
        "Poster_reputation_count":602.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659853446112,
        "Solution_link_count":1,
        "Solution_readability":18.8,
        "Solution_reading_time":18.47,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12,
        "Solution_word_count":113,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1340784274407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":150.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I can't seem to get the logger to work in the online azure notesbooks workspace. I'm using python 3.6 environment.\nRunning this import:<\/p>\n\n<pre><code>from azureml.logging import get_azureml_logger\n<\/code><\/pre>\n\n<p>gives me the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'azureml.logging\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521752364630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438358",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"'azureml.logging' module not found",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1597.0,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1326743808803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":635.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>The solution is <\/p>\n\n<p>pip install \"<a href=\"https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\" rel=\"nofollow noreferrer\">https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D<\/a>\"<\/p>\n\n<p>I found the blob url here. It's inside docker container dependencies.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":49.8,
        "Solution_reading_time":11.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":25,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow the steps given here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data<\/a>    <\/p>\n<p>I've tried regions east us2 and east us for creating the instance but it fails after taking more than half an hour. I tried virtual machine sizes - Standard_DS11_v2 &amp; Standard_DS3_v2.    <\/p>\n<p>Any help would be appreciated.     <\/p>\n<p>Edit - I don't have any other instances running in my subscription, so it should not be a quota issue. The error message says &quot;An internal server error occurred.&quot;.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616247083753,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/323742\/unable-to-creata-a-compute-instance",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":9.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to creata a compute instance",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Good day <a href=\"\/users\/na\/?userid=79ab735d-44c2-44c3-954b-5a6233041e68\">@Aatish Suman  <\/a>      <\/p>\n<p>Did you read the comment in the compute page?    <\/p>\n<p>Please confirm that you are using an account which fit the limitations    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/79891-image.png?platform=QnA\" alt=\"79891-image.png\" \/>    <\/p>\n<p>For more information please check this post:    <\/p>\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/\">https:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/<\/a>    <\/p>\n<p>Note: I followed the tutorial which you provided the link to and it is working well for me. Therefore, I assume the issue is related to the above comment.     <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.9,
        "Solution_reading_time":10.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":75,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I tried to deploy example from Amazon SageMaker<\/p>\n\n<pre><code>xgb_predictor = xgb.deploy(initial_instance_count=1,\n                           instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/p>\n\n<p>Any idea how to fix this? <\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1556178339113,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1556182433123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55844248",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Resource Limit Exceeded- xgb.deploy",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1093.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556178046967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>AWS is using soft limits to prevent customers from making a mistake that might cause them more money than they expected. When you are starting to use a new service, such as Amazon SageMaker, you will hit these soft limits and you need to ask specifically to raise them using the \"Support\" link on the top right side of your AWS management console. <\/p>\n\n<p>Here is a link to guide on how to do that: <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/<\/a><\/p>\n\n<p>You will usually get the limit increased within a couple of days. In the meanwhile, you can choose a smaller instance (such as t2) that are often available.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.4,
        "Solution_reading_time":9.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":107,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"When calculating the cost of SageMaker Studio Notebooks in [the AWS Pricing Calculator](https:\/\/calculator.aws\/#\/addService\/SageMaker), it asks you for the \"Number of Studio Notebook instances per data scientist per month.\"\n\nHow do you reason about this? What would be the use case for having multiple instances for one data scientist? Would that happen if an individual is working on multiple projects, which have different kernels and library dependencies?\n\nI imagine most of the time it will be 1 Studio Notebook instance per data scientist per month, instead of 2 or more instances per data scientist?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670602460560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1670949831716,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5kNZTb_yRR6VUo1Reg6lxg\/how-do-you-choose-the-number-of-studio-notebook-instances-per-data-scientist",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How do you choose the number of Studio Notebook Instances per Data Scientist?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @yann_stoneman, you're right. Up to 4 apps can run on the same instances, so different kernels could still be run on the same instance. For example, a data scientist could be working on a tabular use case, and an image processing use case - so they might have a CPU and GPU instance running. Or they might use a larger instance for data processing or data wrangler feature. \n\nDepending on your data scientists' projects and use cases, I'd account for at most 2 instances per data scientist running concurrently. If your users already use SageMaker Notebook Instances, you can use the commonly used resource type as the Studio instance resource type for estimates - that way you can get a closer estimate to the actual costs. \n\nIf you're allowing for shared spaces (real time collaboration), include additional instances in your estimate - the users will now be able to use a private space through their user profile (unique to one user) and a shared space (this instance can be accessed across profiles). \n\nI'd also recommend using a plugin to shut down idle instances as a best practice when your teams are onboarded to Studio, so these instances are shut down if there are no notebooks actively running (ref: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/)",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1670809472327,
        "Solution_link_count":1,
        "Solution_readability":13.4,
        "Solution_reading_time":16.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":207,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1615961068168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up a locust based framework for ML load test and need to create custom metrics and logs for which the example that I am following is using 'MetricsServiceV2Client' in 'google.cloud.logging_v2' lib.\nIn the Vertex Workbench on GCP inspite being on v3.0 of the google-cloud-logging lib I am getting an issue of import<\/p>\n<p>from google.cloud import logging_v2\nfrom google.cloud.logging_v2 import MetricsServiceV2Client<\/p>\n<p>error: cannot import name 'MetricsServiceV2Client' from 'google.cloud.logging_v2' (\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/logging_v2\/<strong>init<\/strong>.py)<\/p>\n<p>Interestingly when I test the import in  google's cloud console I am able to import without any issue. What could be the issue ?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645102105297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71158453",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"module 'google.cloud.logging_v2' has no attribute 'MetricsServiceV2Client' Vertex WorkBench",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1615961068168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>from google.cloud.logging_v2.services.metrics_service_v2 import MetricsServiceV2Client this works !!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":21.4,
        "Solution_reading_time":1.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":7,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655994282873,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656234648443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":20.6,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.9,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":48,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Within my optuna study, I want that each trial is separately logged by wandb. Currently, the study is run and the end result is tracked in my wandb dashboard. Instead of showing each trial run separately, the end result over all epochs is shown. So, wandb makes one run out of multiple runs.<\/p>\n<p>I found the following <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/_modules\/optuna\/integration\/wandb.html\" rel=\"noopener nofollow ugc\">docs<\/a> in optuna:<\/p>\n<pre><code>Weights &amp; Biases logging in multirun mode.\n\n    .. code::\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">            import optuna\n            from optuna.integration.wandb import WeightsAndBiasesCallback\n\n            wandb_kwargs = {\"project\": \"my-project\"}\n            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n\n            @wandbc.track_in_wandb()\n            def objective(trial):\n                x = trial.suggest_float(\"x\", -10, 10)\n                return (x - 2) ** 2\n\n\n            study = optuna.create_study()\n            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n\n<\/code><\/pre>\n<p>I implemented this line of code yet it produces the following error:<\/p>\n<p><code>ConfigError: Attempted to change value of key \"learning_rate\" from 5e-05 to     0.0005657929921495451 If you really want to do this, pass allow_val_change=True to config.update()    wandb: Waiting for W&amp;B process to finish... (failed 1).<\/code><\/p>\n<p>Did anyone succeed in implementing logging per trial in a multi-trial study?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679646315454,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-enable-logging-of-each-trial-separately\/4115",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to enable logging of each trial separately?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":167,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I actually solved it now:<br>\nIt seems that the optimizer that i used caused errors in the generation of a value for the learning rate when starting a new trial. Once I took the optimizer back out, the follwing implementation worked and generated separate logs in my wandb dashboard:<\/p>\n<pre><code class=\"lang-auto\">wandb_kwargs = {\"project\": \"my-project\"}\nwandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n@wandbc.track_in_wandb()\ndef objective(trial):\n    \n    training_args = Seq2SeqTrainingArguments( \n        \"tuning\", \n        num_train_epochs=1,            \n        # num_train_epochs = trial.suggest_categorical('num_epochs', [3, 5, 8]),\n        per_device_eval_batch_size=3, \n        per_device_train_batch_size=3, \n        learning_rate=  trial.suggest_float('learning_rate', low=0.00004, high=0.0001, step=0.0005, log=False),             \n        # per_device_train_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),       \n        # per_device_eval_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),  \n        disable_tqdm=True, \n        predict_with_generate=True,\n        gradient_accumulation_steps=4,\n        # gradient_checkpointing=True,\n        # weight_decay= False\n        seed = 12, \n        warmup_steps=5,\n        # evaluation and logging\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        save_total_limit=1,\n        logging_strategy=\"epoch\",\n        logging_steps = 1, \n        load_best_model_at_end=True,\n        metric_for_best_model = \"eval_loss\",\n        # use_cache=False,\n        push_to_hub=False,\n        fp16=False,\n        remove_unused_columns=True\n    )\n    # optimizer = Adafactor(\n    #     t5dmodel.parameters(),\n    #     lr=trial.suggest_float('learning_rate', low=4e-5, high=0.0001),  #   ('learning_rate', 1e-6, 1e-3),\n    #     # weight_decay=trial.suggest_float('weight_decay', WD_MIN, WD_CEIL),   \n    #     # lr=1e-3,\n    #     eps=(1e-30, 1e-3),\n    #     clip_threshold=1.0,\n    #     decay_rate=-0.8,\n    #     beta1=None,\n    #     # weight_decay= False\n    #     weight_decay=0.1,\n    #     relative_step=False,\n    #     scale_parameter=False,\n    #     warmup_init=False,\n    # )\n    \n    # lr_scheduler = AdafactorSchedule(optimizer)\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=t5dmodel)\n    trainer = Seq2SeqTrainer(model=t5dmodel,\n                            args=training_args,\n                            train_dataset=tokenized_train_dataset['train'],\n                            eval_dataset=tokenized_val_dataset['validation'],\n                            data_collator=data_collator,\n                            tokenizer=tokenizer,\n                           #  optimizers=(optimizer, lr_scheduler)\n                            )       \n    \n    trainer.train()\n    scores = trainer.evaluate() \n    return scores['eval_loss']\n\nif __name__ == '__main__':\n    t5dmodel = AutoModelForSeq2SeqLM.from_pretrained(\"yhavinga\/t5-base-dutch\",  use_cache=False) \n    tokenizer = AutoTokenizer.from_pretrained(\"yhavinga\/t5-base-dutch\", additional_special_tokens=None)\n    \n    features = {\n    'WordRatioFeature': {'target_ratio': 0.8},\n    'CharRatioFeature': {'target_ratio': 0.8},\n    'LevenshteinRatioFeature': {'target_ratio': 0.8},\n    'WordRankRatioFeature': {'target_ratio': 0.8},\n    'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n    }\n    \n    trainset_processed = get_train_data(WIKILARGE_PROCESSED, 0, 10)  \n    print(trainset_processed)\n    valset_processed = get_validation_data(WIKILARGE_PROCESSED, 0,7)\n    print(valset_processed)\n    tokenized_train_dataset = trainset_processed.map((tokenize_train), batched=True, batch_size=1)\n    tokenized_val_dataset =  valset_processed.map((tokenize_train), batched=True, batch_size=1)   \n    print('Triggering Optuna study')\n    study = optuna.create_study( direction='minimize', pruner=optuna.pruners.MedianPruner()) \n    study.optimize(objective, n_trials=4,callbacks=[wandbc],  gc_after_trial=True)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":24.3,
        "Solution_reading_time":45.02,
        "Solution_score_count":null,
        "Solution_sentence_count":25,
        "Solution_word_count":211,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1526732158023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130stanbul, T\u00fcrkiye",
        "Answerer_reputation_count":888.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting an error on my modeling of lightgbm searching for optimal auc. Any help would be appreciated.<\/p>\n<pre><code>import optuna  \nfrom sklearn.model_selection import StratifiedKFold\nfrom optuna.integration import LightGBMPruningCallback\ndef objective(trial, X, y):\n    param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;auc&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),\n        &quot;lambda_l2&quot;: trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n    }\n\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, &quot;auc&quot;)\n        \n        model = lgb.LGBMClassifier(**param)\n        \n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            early_stopping_rounds=100,\n            callbacks=[pruning_callback])\n        \n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n        auc_scores[idx] = roc_auc_score(y_test, preds)\n        \n    return np.mean(cv_scores), np.mean(auc_scores)\n    \n\n\nstudy = optuna.create_study(direction=&quot;minimize&quot;, study_name=&quot;LGBM Classifier&quot;)\nfunc = lambda trial: objective(trial, sample_df[cols_to_keep], sample_df[target])\n\nstudy.optimize(func, n_trials=1)\n<\/code><\/pre>\n<blockquote>\n<p>Trial 0 failed because of the following error: ValueError('The\nintermediate values are inconsistent with the objective values in\nterms of study directions. Please specify a metric to be minimized for\nLightGBMPruningCallback.',)*<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648760527507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71699098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":30.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Optuna LightGBM LightGBMPruningCallback",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380052343127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":419.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>Your objective function returns two values but you specify only one direction when creating the study. Try this:<\/p>\n<pre><code>study = optuna.create_study(directions=[&quot;minimize&quot;, &quot;maximize&quot;], study_name=&quot;LGBM Classifier&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1655773889523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":390.0,
        "Answerer_view_count":240.0,
        "Challenge_adjusted_solved_time":7.7833411111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657835239540,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657863259568,
        "Solution_link_count":1,
        "Solution_readability":11.8,
        "Solution_reading_time":6.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":56,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Challenge_closed_time":1656947,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646758611000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":241.0,
        "Challenge_repo_fork_count":1404.0,
        "Challenge_repo_issue_count":4035.0,
        "Challenge_repo_star_count":12007.0,
        "Challenge_repo_watch_count":327.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Can we close this @paxcema and @ea-rus  I think we need to merge the above PR after checking there are no conflicts (because it's a bit outdated by now), but once merged we can close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.31,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":37,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":4,
        "Solution_word_count":37,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601310463000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":22.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"0332ede5",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-3.5,
        "Solution_reading_time":0.12,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":1,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1404227763680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1807.0,
        "Answerer_view_count":207.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run the Deepracer log analysis tool from <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a> on my local laptop. However I get below error while trying to run step [5] \"Create an IAM role\". <\/p>\n\n<pre><code>try:\n    sagemaker_role = sagemaker.get_execution_role()\nexcept:\n    sagemaker_role = get_execution_role('sagemaker')\n\nprint(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nCouldn't call 'get_role' to get Role ARN from role name arn:aws:iam::26********:root to get Role path.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      1 try:\n----&gt; 2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in get_execution_role(sagemaker_session)\n   3302     )\n-&gt; 3303     raise ValueError(message.format(arn))\n   3304 \n\nValueError: The current AWS identity is not a role: arn:aws:iam::26********:root, therefore it cannot be used as a SageMaker execution role\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n----&gt; 4     sagemaker_role = get_execution_role('sagemaker')\n      5 \n      6 print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nNameError: name 'get_execution_role' is not defined\n<\/code><\/pre>\n\n<p>Does anybody know what needs to be done to execute above code without errors? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585337019477,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60892850",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.3,
        "Challenge_reading_time":24.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Deepracer log_analysis tool - sagemaker role errors",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1404227763680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":1807.0,
        "Poster_view_count":207.0,
        "Solution_body":"<p>AWS support recommended below solution:<\/p>\n\n<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws\/sagemaker-python-sdk#300 (comment)<\/p>\n\n<p>The steps in the work-around given in the above link are:<\/p>\n\n<ol>\n<li><p>Login to the AWS console -> IAM -> Roles -> Create Role<\/p><\/li>\n<li><p>Create an IAM role and select the \"SageMaker\" service<\/p><\/li>\n<li><p>Give the role \"AmazonSageMakerFullAccess\" permission<\/p><\/li>\n<li><p>Review and create the role<\/p><\/li>\n<li><p>Next, also attach the \"AWSRoboMakerFullAccess\" permission policy to the above created role (as required in the Github notebook [1]).<\/p><\/li>\n<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:<\/p><\/li>\n<\/ol>\n\n<pre><code>try:\n   sagemaker_role = sagemaker.get_execution_role()\n except ValueError:\n   iam = boto3.client('iam')\n   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']\n<\/code><\/pre>\n\n<p>In the above snippet, replace the \"\" text with the IAM role name created in Step 4.<\/p>\n\n<p>References:<\/p>\n\n<p>[1] <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a><\/p>\n\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html<\/a><\/p>\n\n<p>[3] aws\/sagemaker-python-sdk#300<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":16.3,
        "Solution_reading_time":24.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12,
        "Solution_word_count":181,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1579709239420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1579710725728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":47.7,
        "Challenge_reading_time":60.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":null,
        "Challenge_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1716.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":11.6,
        "Solution_reading_time":27.59,
        "Solution_score_count":4.0,
        "Solution_sentence_count":19,
        "Solution_word_count":289,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1386098048127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":619.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592493163510,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592579151163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to find memory leak in Python MXNet?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":21.7,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649410139000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get model references logged by a specific run?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.6,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":47,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>For runs I do:<\/p>\n<pre><code class=\"lang-auto\">wandb.run.get_url()\n<\/code><\/pre>\n<p>how do I do the same but for sweeps given the <code>sweep_id<\/code>?<\/p>\n<hr>\n<p>fulls sample run:<\/p>\n<pre><code class=\"lang-auto\">\"\"\"\nMain Idea:\n- create sweep with a sweep config &amp; get sweep_id for the agents (note, this creates a sweep in wandb's website)\n- create agent to run a setting of hps by giving it the sweep_id (that mataches the sweep in the wandb website)\n- keep running agents with sweep_id until you're done\n\nnote:\n    - Each individual training session with a specific set of hyperparameters in a sweep is considered a wandb run.\n\nref:\n    - read: https:\/\/docs.wandb.ai\/guides\/sweeps\n\"\"\"\n\nimport wandb\nfrom pprint import pprint\nimport math\nimport torch\n\nsweep_config: dict = {\n    \"project\": \"playground\",\n    \"entity\": \"your_wanbd_username\",\n    \"name\": \"my-ultimate-sweep\",\n    \"metric\":\n        {\"name\": \"train_loss\",\n         \"goal\": \"minimize\"}\n    ,\n    \"method\": \"random\",\n    \"parameters\": None,  # not set yet\n}\n\nparameters = {\n    'optimizer': {\n        'values': ['adam', 'adafactor']}\n    ,\n    'scheduler': {\n        'values': ['cosine', 'none']}  # todo, think how to do\n    ,\n    'lr': {\n        \"distribution\": \"log_uniform_values\",\n        \"min\": 1e-6,\n        \"max\": 0.2}\n    ,\n    'batch_size': {\n        # integers between 32 and 256\n        # with evenly-distributed logarithms\n        'distribution': 'q_log_uniform_values',\n        'q': 8,\n        'min': 32,\n        'max': 256,\n    }\n    ,\n    # it's often the case that some hps we don't want to vary in the run e.g. num_its\n    'num_its': {'value': 5}\n}\nsweep_config['parameters'] = parameters\npprint(sweep_config)\n\n# create sweep in wandb's website &amp; get sweep_id to create agents that run a single agent with a set of hps\nsweep_id = wandb.sweep(sweep_config)\nprint(f'{sweep_id=}')\n\n\ndef my_train_func():\n    # read the current value of parameter \"a\" from wandb.config\n    # I don't think we need the group since the sweep name is already the group\n    run = wandb.init(config=sweep_config)\n    print(f'{run=}')\n    pprint(f'{wandb.config=}')\n    lr = wandb.config.lr\n    num_its = wandb.config.num_its\n\n    train_loss: float = 8.0 + torch.rand(1).item()\n    for i in range(num_its):\n        # get a random update step from the range [0.0, 1.0] using torch\n        update_step: float = lr * torch.rand(1).item()\n        wandb.log({\"lr\": lr, \"train_loss\": train_loss - update_step})\n    run.finish()\n\n\n# run the sweep, The cell below will launch an agent that runs train 5 times, usingly the randomly-generated hyperparameter values returned by the Sweep Controller.\nwandb.agent(sweep_id, function=my_train_func, count=5)\n<\/code><\/pre>\n<p>cross: <a href=\"https:\/\/stackoverflow.com\/questions\/75852199\/how-do-i-print-the-wandb-sweep-url-in-python\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">machine learning - How do I print the wandb sweep url in python? - Stack Overflow<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679892176317,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-print-the-wandb-sweep-url-in-python\/4133",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":11.0,
        "Challenge_reading_time":34.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I print the wandb sweep url in python?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":227.0,
        "Challenge_word_count":335,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>like <code>wandb.get_sweep_url()<\/code>? Thanks!<\/p>\n<p><a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run?_gl=1*uk130d*_ga*MTYwMTE3MDYzNS4xNjUyMjI2MTE1*_ga_JH1SJHJQXJ*MTY4MDAxNTk2Ny4yNjguMS4xNjgwMDE2MTMyLjQ3LjAuMA\" class=\"inline-onebox\">Run | Weights &amp; Biases Documentation<\/a>\u2026<span class=\"hashtag\">#get_sweep_url<\/span><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":36.1,
        "Solution_reading_time":4.66,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":11,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1410333105327,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Turin, Metropolitan City of Turin, Italy",
        "Answerer_reputation_count":477.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":0.0448425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636727119830,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636755379243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":18.3,
        "Challenge_reading_time":121.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":124,
        "Challenge_solved_time":null,
        "Challenge_title":"How to change the directory of mlflow logs?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":636,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410333105327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Turin, Metropolitan City of Turin, Italy",
        "Poster_reputation_count":477.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636755540676,
        "Solution_link_count":0,
        "Solution_readability":20.1,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":68,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas \n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a [request for a service quota increase](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#service-limit-increase-request-procedure).",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642480202619,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668602220822,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sagemaker-service-quotas",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":14.7,
        "Challenge_reading_time":8.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I check my current SageMaker service quotas?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1212.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655365526940,
        "Solution_link_count":1,
        "Solution_readability":13.5,
        "Solution_reading_time":4.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":42,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1384343462316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562066976100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":30.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I specify mlflow MLproject with zero parameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":353.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384343462316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":478.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":43,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1498252453503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":298.7422783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529660522477,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1530173488990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.8,
        "Challenge_reading_time":31.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1531248961192,
        "Solution_link_count":1,
        "Solution_readability":25.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4,
        "Solution_word_count":49,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hy, I\u2019m in love with wandb, but I have a problem\u2026<\/p>\n<p>I have a simple question\u2026<\/p>\n<p>How can I analyze hyperparameters\u2026As seen in this picture, without actually creating a sweep.<\/p>\n<p>In my own code\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.1&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ9LbpPclu5VUg_KiYT_9MyY2AbgyxXn6tmqz8qoKH2kUghMnyxeJstBhkIK4wCOgqfFHueuZ6ul6juIl6zvWD3lcsPXIvZAnZatibVLxPjneVvO-xSUoWLyCpM&amp;disp=emb&amp;realattid=ii_l5aqmkag2\" alt=\"68747470733a2f2f692e696d6775722e636f6d2f5455333451465a2e706e67.png\" width=\"339\" height=\"205\"><\/p>\n<p>I\u2019m preforming learning and for every model i\u2019m sending config with hyperparams\u2026<\/p>\n<p>wandb.finish(quiet=True)<br>\nwandb.init(<br>\nentity=var.WANDB_ENTITY,<br>\nproject=f\u2019{var.version} | {var.INPUT_DATASET}',<br>\ndir=str(var.working_dir),<br>\nconfig=utils.keras.hyper_params(hp))<\/p>\n<p>But in dashboard I dont see hyperparameters dashboard\u2026 And this makes me really sad !<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657181342018,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/analyzing-hyperparameters-without-actualy-performing-a-sweep\/2719",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":21.3,
        "Challenge_reading_time":15.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Analyzing hyperparameters without actualy performing a sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":143.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I can\u2019t see the images above, but if you would like to create a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/parallel-coordinates\">parallel coordinates plot<\/a>, you can do so using the UI by clicking \u201cadd panel\u201d in your workspace and choosing Parallel Coordinates.<\/p>\n<p>If you need to do this programmatically, one <em>very<\/em> recent feature would be to create a W&amp;B Report using our Api. You can programatically define what plots show up. It is a very new feature so it\u2019ll become better documented and more stable over time.<\/p>\n<p>Here\u2019s how you would create a Parallel Coordinates plot programmatically and save it in a report using Python.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport wandb.apis.reports as wb\napi = wandb.Api()\nproject = 'pytorch-sweeps-demo'\nwandb.require('report-editing') # this is needed as of version 0.12.21 but will likely not be needed in future.\nreport = wb.Report(\n    project=project,\n    title='Sweep Results',\n    blocks=[\n            wb.PanelGrid(panels=[\n                 wb.ParallelCoordinatesPlot(\n                     columns=[wb.reports.PCColumn('batch_size'), wb.reports.PCColumn('epoch'), wb.reports.PCColumn('loss')])\n            ], runsets=[wb.RunSet(project=project)]),\n    ]\n)\nreport.save()\n<\/code><\/pre>\n<p>This will then show up in the Reports tab on your project.<br>\nAs this is a very fresh API, there may be issues or features that are not supported yet. I do apologise if that happens to you, I\u2019ll be happy to follow up and provide help.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.5,
        "Solution_reading_time":18.39,
        "Solution_score_count":null,
        "Solution_sentence_count":15,
        "Solution_word_count":187,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Challenge_closed_time":1650063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649790124000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":20.9,
        "Challenge_reading_time":56.91,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":286,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Facing the same issue but with W and B. see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12529 and https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12714 This was fixed and included in the 1.6.1 release. Could you try upgrading lightning? \r\n`pip install --upgrade pytorch-lightning` \ud83e\udd1f  @awaelchli not the same bug, but my colab link to reproduce the bug is now throwing another error.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":9.5,
        "Solution_reading_time":5.29,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":49,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1562578633160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609067497070,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.9,
        "Challenge_reading_time":27.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2479.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1374284754808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":279.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.87,
        "Solution_score_count":8.0,
        "Solution_sentence_count":2,
        "Solution_word_count":18,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":18.1665508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660737627797,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620645674183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1660803027380,
        "Solution_link_count":1,
        "Solution_readability":13.1,
        "Solution_reading_time":22.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18,
        "Solution_word_count":182,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1243547542743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6438.0,
        "Answerer_view_count":922.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449073471520,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1454300528203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5063.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373475615300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":2037.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":8.0,
        "Solution_sentence_count":6,
        "Solution_word_count":109,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":20.5317583333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using optuna for parameter optimisation of my custom models. <\/p>\n\n<p>Is there any way to sample parameters until current params set was not tested before? I mean, do try sample another params if there were some trial in the past with the same set of parameters. <\/p>\n\n<p>In some cases it is impossible, for example, when there is categorial distribution and <code>n_trials<\/code> is greater than number os possible unique sampled values. <\/p>\n\n<p>What I want: have some config param like <code>num_attempts<\/code> in order to sample parameters up to <code>num_attempts<\/code> in for-loop until there is a set that was not tested before, else - to run trial on the last sampled set. <\/p>\n\n<p>Why I need this: just because it costs too much to run heavy models several times on the same parameters. <\/p>\n\n<p>What I do now: just make this \"for-loop\" thing but it's messy.<\/p>\n\n<p>If there is another smart way to do it - will be very grateful for information.  <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573568558157,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1573569572550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58820574",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to sample parameters without duplicates in optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2121.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>To the best of my knowledge, there is no direct way to handle your case for now.\nAs a workaround, you can check for parameter duplication and skip the evaluation as follows:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # Sample parameters.\n    x = trial.suggest_int('x', 0, 10)\n    y = trial.suggest_categorical('y', [-10, -5, 0, 5, 10])\n\n    # Check duplication and skip if it's detected.\n    for t in trial.study.trials:\n        if t.state != optuna.structs.TrialState.COMPLETE:\n            continue\n\n        if t.params == trial.params:\n            return t.value  # Return the previous value without re-evaluating it.\n\n            # # Note that if duplicate parameter sets are suggested too frequently,\n            # # you can use the pruning mechanism of Optuna to mitigate the problem.\n            # # By raising `TrialPruned` instead of just returning the previous value,\n            # # the sampler is more likely to avoid sampling the parameters in the succeeding trials.\n            #\n            # raise optuna.structs.TrialPruned('Duplicate parameter set')\n\n    # Evaluate parameters.\n    return x + y\n\n# Start study.\nstudy = optuna.create_study()\n\nunique_trials = 20\nwhile unique_trials &gt; len(set(str(t.params) for t in study.trials)):\n    study.optimize(objective, n_trials=1)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1573643486880,
        "Solution_link_count":0,
        "Solution_readability":11.4,
        "Solution_reading_time":14.88,
        "Solution_score_count":10.0,
        "Solution_sentence_count":17,
        "Solution_word_count":149,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Challenge_closed_time":1662562,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657876344000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":16.2,
        "Challenge_reading_time":101.65,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow fails to log to a tracking server",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":588,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"cc @sgugger  I'm not the one who wrote or supports the ML Flow callback :-) @noise-field wrote the integration two years ago, do you have an idea of why it doesn't seem to work anymore @noise-field? @juliensimon, I had an error message similar (I think). I found that the issue was related to values with empty string values  (https:\/\/github.com\/mlflow\/mlflow\/issues\/6253), and it looks like there is a patch in the upcoming MLFLOW version 1.28 (not yet released)\r\n\r\nIn my case, I had to set `mp_parameters` to `None` instead of leaving it as an empty string (the default value), and I see your error message has `{'key': 'mp_parameters', 'value': ''}`.\r\n\r\nWhile later MLflow version fix will address this issue, I think setting the `mp_parameters` to `None` instead of an empty string is cleaner. However, I'm not sure about the extent of this change.\r\n\r\n OK, I'll give it a try and I'll let you know. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":8.5,
        "Solution_reading_time":15.15,
        "Solution_score_count":null,
        "Solution_sentence_count":12,
        "Solution_word_count":195,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.3760377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573137628147,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to disable logs in Kedro",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479159384132,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Illinois, United States",
        "Poster_reputation_count":513.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573142581883,
        "Solution_link_count":1,
        "Solution_readability":12.4,
        "Solution_reading_time":12.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8,
        "Solution_word_count":90,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1249258805300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":909.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533924861517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.4,
        "Challenge_reading_time":47.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1355.0,
        "Challenge_word_count":285,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378935265347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":22.1,
        "Solution_reading_time":11.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":71,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641920680143,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How do parallel trials in GCP Vertex AI work?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579801831103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":16.6,
        "Solution_reading_time":13.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":113,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660398516000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":14.8,
        "Solution_reading_time":77.33,
        "Solution_score_count":null,
        "Solution_sentence_count":61,
        "Solution_word_count":510,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>I\u2019m implementing W&amp;B into an existing project in which Agent, Model creation and Environment are constructed in classes. The code structure in the Python file (<code>AIAgent.py<\/code>) looks like this:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nconfig = {\n    'layer_sizes': [17, 16, 12, 4],\n    'batch_minsize': 32,\n    'max_memory': 100_000,\n    'episodes': 2,\n    'epsilon': 1.0,\n    'epsilon_decay': 0.998,\n    'epsilon_min': 0.01,\n    'gamma': 0.9,\n    'learning_rate': 0.001,\n    'weight_decay': 0,\n    'optimizer': 'sgd',\n    'activation': 'relu',\n    'loss_function': 'mse'\n}\n\nclass AIAgent:\n    def __init__(self):\n        self.config = config\n        self.pipeline(self.config)\n\n\n    def pipeline(self, config):\n        wandb.init()\n        config = wandb.config\n\n        model, criterion, optimizer = self.make(config)\n        self.train(model, criterion, optimizer, config) \n\n\n    def make(self, config):\n        model = LinearQNet(config).to(device)\n\n        if config['loss_function'] == 'mse':\n            criterion = nn.MSELoss()\n\n        if config['optimizer'] == 'adam':\n            optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=config['weight_decay'], amsgrad=False)\n \n        wandb.watch(model, criterion, log='all', log_freq=1)\n        summary(model)\n\n        return model, criterion, optimizer\n\n\n    def train(self, model, criterion, optimizer, config):\n        for episode in range(1, config['episodes'] + 1):\n            while True:\n                # Where the training is performed\n\n                if done:\n                    if (episode % 1) == 0:\n                        wandb.log({'episode': episode, 'epsilon': epsilon, 'score': score, 'loss': loss_mean, 'reward': reward_mean, 'score_mean': score_mean, 'images': [wandb.Image(img) for img in env_images]}, step=episode})\n                    break\n\n            if episode &lt; config['episodes']:\n                game.game_reset()\n            else:\n                wandb.finish()\n                break\n\n\nclass LinearQNet(nn.Module):\n    def __init__(self, config):\n        super(LinearQNet, self).__init__()\n        self.config = config\n        # Where the NN is configured\n\n\nif __name__ == '__main__':\n    AIAgent.__init__(AIAgent())\n<\/code><\/pre>\n<p>I\u2019m currently initializing the sweep configuration via a .yaml file calling  <code>wandb sweep sweep.yaml<\/code>. The sweep.yaml file looks like this:<\/p>\n<pre><code class=\"lang-auto\">program: AIAgent.py\nproject: evaluation-sweep-1\nmethod: random\nmetric:\n  name: score_mean\n  goal: maximize\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\nparameters:\n  layer_sizes:\n    distribution: constant\n    value: [17, 16, 512, 4]\n  batch_minsize:\n    distribution: int_uniform\n    max: 1024\n    min: 32\n  max_memory:\n    distribution: constant\n    value: 100_000\n  episodes:\n    distribution: constant\n    value: 50\n  epsilon:\n    distribution: constant\n    value: 1.0\n  epsilon_decay:\n    distribution: constant\n    value: 0.995\n  epsilon_min:\n    distribution: constant\n    value: 0.01\n  gamma:\n    distribution: uniform\n    max: 0.99\n    min: 0.8\n  learning_rate:\n    distribution: uniform\n    max: 0.1\n    min: 0.0001  \n  weight_decay:\n    distribution: constant\n    value: 0\n  optimizer:\n    distribution: categorical\n    values: ['sgd', 'adam', 'adamw']\n  activation:\n    distribution: categorical\n    values: ['relu', 'sigmoid', 'tanh', 'leakyrelu']\n  loss_function:\n    distribution: constant\n    value: 'mse'\nearly_terminate:\n  type: hyperband\n  min_iter: 5\n<\/code><\/pre>\n<p>Besides general feedback on the implementation I\u2019m a bit dumbfounded with a current bug. The sweeps run fine and show up in the W&amp;B interface but every sweep is performed twice under the same name of which only the loffing of the first is displayed and the second runs \u2018silently\u2019 in the environment without update of wandb.log. Does anybody have an idea what the reason for this might be?<\/p>\n<p>Thanks,<br>\nTobias<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652719487380,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-integration-using-class-and-sweep-running-twice-under-the-same-name\/2433",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":45.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb integration using class and sweep running twice under the same name",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":320.0,
        "Challenge_word_count":392,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Tobias,<\/p>\n<p>Looks like the source of this bug is this line: <code>AIAgent.__init__(AIAgent())<\/code> which is calling 2 constructors: 1 from <code>AIAgent.__init__()<\/code> and 1 from <code>AIAgent()<\/code>. This, in turn calls <code>pipeline<\/code> twice, which ends up meaning 2 calls to <code>wandb.init()<\/code> and therefore you see 2 runs.<\/p>\n<p>I would suggest changing that line to just <code>AIAgent<\/code> to prevent this error.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.6,
        "Solution_reading_time":6.14,
        "Solution_score_count":null,
        "Solution_sentence_count":7,
        "Solution_word_count":60,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1535382420716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.9537672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641209554070,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535382420716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641216587632,
        "Solution_link_count":1,
        "Solution_readability":15.2,
        "Solution_reading_time":3.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":25,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664420413000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":222,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":17.8,
        "Solution_reading_time":12.75,
        "Solution_score_count":null,
        "Solution_sentence_count":18,
        "Solution_word_count":71,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612502289000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.4,
        "Solution_reading_time":21.92,
        "Solution_score_count":null,
        "Solution_sentence_count":17,
        "Solution_word_count":326,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Dears, \n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : \nError : \n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n-------------------\nCode:\nfrom sagemaker.tuner import (\n    IntegerParameter,\n    CategoricalParameter,\n    ContinuousParameter,\n    HyperparameterTuner,\n)\nfrom sagemaker import image_uris\n\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    use_spot_instances=True,  # use spot instances\n    max_run=1800,  # max training time in seconds\n    max_wait=1800,  # seconds to wait for spot instance\n    output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path),\n    sagemaker_session=sess,\n)\nfreq = \"D\"\ncontext_length = 300\n\ndeepar.set_hyperparameters(\n    time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length)\n)\n\nCan you please help in solving the error? \nI have to do that in af-south-1 region. \n\nThanks \nBasem\n\nhyperparameter_ranges = {\n    \"mini_batch_size\": IntegerParameter(100, 400),\n    \"epochs\": IntegerParameter(200, 400),\n    \"num_cells\": IntegerParameter(30, 100),\n    \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]),\n    \"learning_rate\": ContinuousParameter(0.0001, 0.1),\n}\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner(\n    deepar,\n    objective_metric_name,\n    hyperparameter_ranges,\n    max_jobs=10,\n    strategy=\"Bayesian\",\n    objective_type=\"Minimize\",\n    max_parallel_jobs=10,\n    early_stopping_type=\"Auto\",\n)\n\ns3_input_train = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\"\n)\ns3_input_test = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\"\n)\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False)\ntuner.wait()",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652655830031,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668587554704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/clienterror-an-error-occurred-unknownoperationexception-when-calling-the-createhyperparametertuningjob-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The error message indicates that `CreateHyperParameterTuningJob` operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653062956983,
        "Solution_link_count":0,
        "Solution_readability":13.5,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":28,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363369778320,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":7643.0,
        "Answerer_view_count":515.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented a PyTorch <code>Dataset<\/code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset<\/code> implementation is as follows.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>\n\n<p>I am following this <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/master\/src\/sagemaker\/pytorch\" rel=\"noreferrer\">example<\/a> and this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_cnn_cifar10\/pytorch_local_mode_cifar10.ipynb\" rel=\"noreferrer\">one too<\/a>, and I run the <code>estimator<\/code> as follows.<\/p>\n\n<pre><code>inputs = {\n 'train': 'file:\/\/images',\n 'eval': 'file:\/\/images'\n}\nestimator = PyTorch(entry_point='pytorch-train.py',\n                            role=role,\n                            framework_version='1.0.0',\n                            train_instance_count=1,\n                            train_instance_type=instance_type)\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I get the following error.<\/p>\n\n<blockquote>\n  <p>FileNotFoundError: [Errno 2] No such file or directory: '.\/images'<\/p>\n<\/blockquote>\n\n<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.<\/p>\n\n<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data\/cifar10')\n<\/code><\/pre>\n\n<p>If I take a peek at <code>inputs<\/code>, it is just a string literal <code>s3:\/\/sagemaker-us-east-3-184838577132\/data\/cifar10<\/code>. The code to create a <code>Dataset<\/code> and a <code>DataLoader<\/code> is shown <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_mnist\/mnist.py#L41\" rel=\"noreferrer\">here<\/a>, which does not help unless I track down the source and step through the logic.<\/p>\n\n<p>I think what needs to happen inside my <code>ImageDataset<\/code> is to supply the <code>S3<\/code> path and use the <code>AWS CLI<\/code> or something to query the files and acquire their content. I do not think the <code>AWS CLI<\/code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. <\/p>\n\n<p>There must be a recipe or something to create a custom <code>Dataset<\/code> backed by <code>S3<\/code> files, right?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546416160377,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54003052",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":38.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I implement a PyTorch Dataset for use with AWS SageMaker?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4597.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363369778320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":7643.0,
        "Poster_view_count":515.0,
        "Solution_body":"<p>I was able to create a PyTorch <code>Dataset<\/code> backed by S3 data using <code>boto3<\/code>. Here's the snippet if anyone is interested.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.s3 = boto3.resource('s3')\n        self.bucket = self.s3.Bucket(path)\n        self.files = [obj.key for obj in self.bucket.objects.all()]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        # we need to download the file from S3 to a temporary file locally\n        # we need to create the local file name\n        obj = self.bucket.Object(img_name)\n        tmp = tempfile.NamedTemporaryFile()\n        tmp_name = '{}.jpg'.format(tmp.name)\n\n        # now we can actually download from S3 to a local place\n        with open(tmp_name, 'wb') as f:\n            obj.download_fileobj(f)\n            f.flush()\n            f.close()\n            image = Image.open(tmp_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.8,
        "Solution_reading_time":16.51,
        "Solution_score_count":12.0,
        "Solution_sentence_count":18,
        "Solution_word_count":136,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1590520808976,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604221291310,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64630198",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":686.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":41.9,
        "Solution_reading_time":6.98,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4,
        "Solution_word_count":21,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n to_wandb not sectioning by train\/test and overrides runs by checks\r\n\r\n**To Reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**Expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Challenge_closed_time":1649580,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649320792000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1210",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] to_wandb not sectioning by train\/test and overrides runs by checks",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Initial fix -  from 'name' to 'header', already applied in my local deepchecks environment (screenshots after that fix)\r\n\r\nSee example (note both DataDuplicates and CalibrationScore behavior):\r\n\r\nCode that ran:\r\n`custom_suite = Suite('Custom Evaluation',CalibrationScore(), CalibrationScore(),\r\n                     DataDuplicates(), DataDuplicates(columns=['Total Value']))\r\nsuite_res = custom_suite.run(train_ds, test_ds, rf_clf)\r\nsuite_res.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"my_run\")`\r\n\r\n### Suite Result\r\n#### Calibration Metric\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167715-0db6398d-4822-4e35-a886-741753982ab5.png)\r\n\r\n#### Data Duplicates - \r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167770-5e0450ce-a2ff-493f-8495-8779379d7a86.png)\r\n\r\n### W&B Logging - Suite Result\r\n#### Data Duplicates - appears 3 times (??)\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162168073-f79f6f3b-33fc-4453-8aff-4ae47652e979.png)\r\n\r\n\r\n#### Calibration Metric\r\nOnly one result (for each, after the above fix applied):\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162169354-b810fb42-2422-4ab5-8e5e-1bd377609fa0.png)\r\n\r\n\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":21.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":null,
        "Solution_sentence_count":6,
        "Solution_word_count":73,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Challenge_closed_time":1655127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646156442000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Support writing out dataset profiles as json format with mlflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"How can i retrieve the profile while inside the start_run()? This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":23,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654584250764,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Challenge_link_count":10,
        "Challenge_participation_count":5,
        "Challenge_readability":17.8,
        "Challenge_reading_time":37.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to early stop bad runs in sweeps to save time",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":663.0,
        "Challenge_word_count":193,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.9,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":90,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655665054170,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.6,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't associate sweeps with project",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":9,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I run this line right before the training loop:<\/p>\n<pre><code class=\"lang-auto\">wandb.config = {\n\"train_steps\": train_steps,\n\"batch_size\": batch_size,\n\"unet1_dim\": unet1_dim,\n\"unet2_dim\": unet2_dim,\n\"unet_training\": unet_training,\n}\n<\/code><\/pre>\n<p>But on the dashboard, I can\u2019t seem to find anything related to batch size. All of the data logged with <code>wandb.log<\/code> is present, but nothing about my hyperparameters. It may be relevant that I\u2019m running offline and syncing with the command <code>wandb sync --sync-all<\/code>.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676583103752,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/batch-size-and-other-config-parameters-are-inaccessible-in-the-dashboard\/3897",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Batch size and other config parameters are inaccessible in the dashboard",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":143.0,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Jaden!<\/p>\n<p>Could try logging your  <code>wandb.config<\/code>  using the following code snippet? Just in case, some further documentation on logging your config can be found <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/config#overview\">here<\/a>.<\/p>\n<pre><code class=\"lang-auto\">config = { \"train_steps\": train_steps, \"batch_size\": batch_size, \"unet1_dim\": unet1_dim, \"unet2_dim\": unet2_dim, \"unet_training\": unet_training }\nwandb.init(project = '&lt;your-project&gt;', config = config)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.8,
        "Solution_reading_time":6.76,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":44,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1539356592836,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":8552.0,
        "Answerer_view_count":772.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While optimizing parameters for xgboost I encountered a problem with the roc_auc_score metric. I get significantly different results during cross-validation compared to the results on the training data.<\/p>\n\n<pre><code>class OptunaHyperparamsSearch:\ndef __init__(self, X_train, y_train, **kwargs):\n    ...\n\ndef objective(self, trial):\n\n    ...\n\n    cv_results = xgb.cv(param, self.dtrain, num_boost_round=5, metrics=['auc'], nfold=5, verbose_eval=True)\n\n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].idxmax()\n\n    param['n_estimators'] = boost_rounds\n    trial.set_user_attr('param', param)\n\n    print('boost_rounds: ', boost_rounds)\n    print('train-auc-mean', cv_results['train-auc-mean'][boost_rounds])\n\n    return mean_auc\n\ndef best_model(self, n_trials=100, save_path=None):\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(self.objective, n_trials=n_trials)\n\n    best_params = study.best_trial.user_attrs['param']\n    best_model = xgb.XGBClassifier(**best_params)\n    best_model.fit(self.X_train, self.y_train)\n\n    return best_model\n<\/code><\/pre>\n\n<p>After running code:<\/p>\n\n<pre><code>search = OptunaHyperparamsSearch(X_train, y_train)\nmodel = search.best_model(n_trials=1)\n<\/code><\/pre>\n\n<p>I received:<\/p>\n\n<pre><code>[0] train-auc:0.777869+0.00962852   test-auc:0.771169+0.025347\n[1] train-auc:0.786905+0.00865646   test-auc:0.777492+0.0255523\n[2] train-auc:0.793305+0.00480249   test-auc:0.785307+0.0198732\n[3] train-auc:0.79595+0.00349561    test-auc:0.789897+0.0158569\n[4] train-auc:0.796818+0.00407504   test-auc:0.789997+0.016069\nboost_rounds:  4\ntrain-auc-mean 0.796818\n[I 2020-06-04 10:12:25,093] Finished trial#0 with value: 0.7899968 with parameters: {'booster': 'dart', 'reg_lambda': 0.8001057111479173, 'reg_alpha': 0.0016960618598770582, 'max_depth': 8, 'min_child_weight': 4, 'learning_rate': 0.0602235073221647, 'gamma': 0.0011248451567255984, 'colsample_bytree': 0.911487203002922, 'subsample': 0.9057485217255851, 'grow_policy': 'lossguide', 'scale_pos_weight': 0.5865962792358733, 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0009459988874640169, 'skip_drop': 8.103200442539776e-05}. Best is trial#0 with value: 0.7899968.\n<\/code><\/pre>\n\n<p>So the result is about 0.8 (<strong>train-auc-mean 0.796818<\/strong>). And after that running:<\/p>\n\n<pre><code>y_pred = model.predict(X_train)\nprint(roc_auc_score(y_train, y_pred))\n<\/code><\/pre>\n\n<p>I received:<\/p>\n\n<pre><code>0.598231710442728\n<\/code><\/pre>\n\n<p>So it's impossible. I tried also use customize function:<\/p>\n\n<pre><code>from sklearn.metrics import roc_auc_score\n\ndef PyAUC(predt: np.ndarray, dtrain: xgb.DMatrix):\n    y = dtrain.get_label()\n    return 'PyAUC', roc_auc_score(y, predt)\n<\/code><\/pre>\n\n<p>and pass them by <code>feval<\/code> to <code>xgb.cv<\/code>, setting <code>param['disable_default_eval_metric'] = 1<\/code> and without defining metrics and the result was the same.<\/p>\n\n<p>Then I tried to use RandomizedSearchCV:<\/p>\n\n<pre><code>params = {\n    'min_child_weight': [1, 5, 10],\n    'gamma': [0.5, 1, 1.5, 2, 5],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'max_depth': [3, 4, 5]\n    }\nalg = XGBClassifier(learning_rate=0.01, n_estimators=5, objective='binary:logistic',\n                silent=True, nthread=1)\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(alg, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train, y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n\nprint('\\n All results:')\nprint(random_search.cv_results_)\n\ny_pred = random_search.predict(X_train)\nprint(roc_auc_score(y_train, y_pred))\n<\/code><\/pre>\n\n<p>The output was:<\/p>\n\n<pre><code>All results:\n{'mean_fit_time': array([0.27621794, 0.40631523, 0.36202598, 0.32188687, 0.34574351,\n   0.2747798 , 0.31780529, 0.32190156, 0.34060073, 0.25945067]), 'std_fit_time': array([0.02603387, 0.04572275, 0.09460844, 0.01841953, 0.08391794,\n   0.03654419, 0.01583525, 0.03670047, 0.01035465, 0.03085039]), 'mean_score_time': array([0.01927972, 0.0143033 , 0.01697631, 0.01260743, 0.02442002,\n   0.02089334, 0.0182806 , 0.0132216 , 0.01498265, 0.01320119]), 'std_score_time': array([0.00609847, 0.00671443, 0.00613005, 0.00410744, 0.00384849,\n   0.00516041, 0.00505873, 0.00276774, 0.00023382, 0.00546102]), 'param_subsample': masked_array(data=[1.0, 0.6, 0.8, 1.0, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_min_child_weight': masked_array(data=[5, 1, 5, 5, 1, 10, 1, 1, 1, 1],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_max_depth': masked_array(data=[3, 5, 5, 5, 4, 4, 5, 3, 5, 4],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_gamma': masked_array(data=[5, 1.5, 1, 5, 1, 1.5, 5, 2, 0.5, 1.5],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_colsample_bytree': masked_array(data=[1.0, 0.8, 0.8, 0.6, 1.0, 0.6, 0.6, 0.8, 0.6, 0.6],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'params': [{'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 1.5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'min_child_weight': 10, 'max_depth': 4, 'gamma': 1.5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 3, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 0.5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 1.5, 'colsample_bytree': 0.6}], 'split0_test_score': array([0.75734333, 0.78965043, 0.78929122, 0.77842559, 0.78669592,\n   0.77856369, 0.7803955 , 0.77733652, 0.78884686, 0.77706318]), 'split1_test_score': array([0.7564997 , 0.78553601, 0.78621578, 0.77250155, 0.78589665,\n   0.77237991, 0.77235486, 0.77187115, 0.78573708, 0.77046652]), 'split2_test_score': array([0.75575839, 0.77356843, 0.79002323, 0.77134164, 0.76641651,\n   0.76965581, 0.77133806, 0.76749842, 0.79029943, 0.77043647]), 'split3_test_score': array([0.74596394, 0.77188117, 0.76967513, 0.76816388, 0.76832059,\n   0.76795065, 0.76942182, 0.76217902, 0.76846871, 0.75720452]), 'split4_test_score': array([0.78099172, 0.80616938, 0.80491224, 0.80371433, 0.81990511,\n   0.82052725, 0.80327483, 0.80598102, 0.8171982 , 0.8052647 ]), 'mean_test_score': array([0.75931142, 0.78536108, 0.78802352, 0.7788294 , 0.78544696,\n   0.78181546, 0.77935701, 0.77697323, 0.79011006, 0.77608708]), 'std_test_score': array([0.01159822, 0.0124273 , 0.0112318 , 0.01287854, 0.01920727,\n   0.01968907, 0.01253142, 0.0153379 , 0.01563886, 0.01595216]), 'rank_test_score': array([10,  4,  2,  7,  3,  5,  6,  8,  1,  9], dtype=int32)}\n0.6093407594278569\n<\/code><\/pre>\n\n<p>So still the same problem: during cross-validation score about 0.8 and after that 0.6. I suppose that different metrics are used.<\/p>\n\n<p>The solution I found was to pass in RandomizedSearchCV: <code>scoring=make_scorer(roc_auc_score)<\/code>. This solved the problem giving the same result in cross-validation and after that about 0.6.<\/p>\n\n<p>Can anyone explain what the problem was because I still don't understand it? And I still don't know how to solve it using optuna optimalization.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591267109627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62192616",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":104.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":102,
        "Challenge_solved_time":null,
        "Challenge_title":"strange behavior of roc_auc_score, 'roc_auc', 'auc'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":387.0,
        "Challenge_word_count":713,
        "Platform":"Stack Overflow",
        "Poster_created_time":1591262821660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cracow, Polska",
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>You're using <code>model.predict<\/code>, but the ROC curve and <code>roc_auc_score<\/code> needs the predicted probabilities (or other confidence measures, maybe); use <code>model.predict_proba<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/q\/30623637\/10495893\">Scikit-learn : roc_auc_score<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":15.7,
        "Solution_reading_time":4.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":23,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660154954560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660167642767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542606952710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.8,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":10,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm training a model in Azure ML Studio and the Net# specification I'm using doesn't match the NET# specification in the training output.<\/p>\n\n<p>Here's my experiment - <a href=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and here are my NN params - <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and finally here is the NET# specification in the Hyperparams output -<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ehImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ehImq.png\" alt=\"enter image description here\"><\/a>\nIt's not using two hidden layers and it's also using sigmoid instead of ReLu. Is this expected behavior?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1508267201980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46797468",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":12.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Incorrect neural network schema in training output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274124294967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":655.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>There is an issue with using custom NET# and parameter sweeps together: it switches over to using the default fully connected topology. <\/p>\n\n<p>Unfortunately, the workaround is to train the model for each parameter value separately.  <\/p>\n\n<p>-Roope  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.5,
        "Solution_reading_time":3.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?\nI'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607968009000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925925012,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":51,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612842503288,
        "Solution_link_count":1,
        "Solution_readability":26.1,
        "Solution_reading_time":3.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":20,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing an issue while invoking the Pytorch model Endpoint. Please check the below error for detail.<\/p>\n<p>Error Message:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4.<\/p>\n<\/blockquote>\n<p>Automatically restarted the Endpoint after this error. No specific log in cloud watch.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617966706043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1631708929472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67020040",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Pytorch model - An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4):",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":407.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582178023440,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Palanpur, Gujarat, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>There may be a few issues here we can explore the paths and ways to resolve.<\/p>\n<ol>\n<li>Inference Code Error\nSometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.<\/li>\n<\/ol>\n<pre><code>from sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)\nprint(rf_pred.predict(payload).decode('utf-8'))\n<\/code><\/pre>\n<p>For more information about the different serializers based off the type of data you are feeding in check the following link.\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html<\/a><\/p>\n<ol start=\"2\">\n<li>Throttling Limits Reached\nSometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/<\/a><\/p>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979329227,
        "Solution_link_count":4,
        "Solution_readability":12.9,
        "Solution_reading_time":22.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16,
        "Solution_word_count":201,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n```\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n```\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669725280762,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1670071482024,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":9.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass environment variables in sagemaker tuner job",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":553.0,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for raising this. Yes, as you point out the `Environment` collection is not supported in the underlying `CreateHyperparameterTuningJob` API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the [SM Py SDK GitHub issue here](https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2934), you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore [setting the variables from your Python script](https:\/\/stackoverflow.com\/a\/5971326) (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the [ENV command](https:\/\/docs.docker.com\/engine\/reference\/builder\/#env)? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\n- Use `sagemaker.image_uris.retrieve(...)` to find the base image URI for your given framework, version, region, etc. You'll need to [authenticate Docker to this registry](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/getting-started-cli.html#cli-authenticate-registry) as well as your own Amazon ECR account.\n- Create a Dockerfile that takes this base image URI as an arg and builds `FROM` it, something like [this example](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/b95b62cd2abd304ee347cacdd3eaf2a76e8b5953\/notebooks\/custom-containers\/train-inf\/Dockerfile)\n- Add the required `ENV` commands to bake in the (static) environment variables you need\n- `docker build` your custom container (passing in the base image URI as a `--build-arg`), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669736547974,
        "Solution_link_count":5,
        "Solution_readability":13.1,
        "Solution_reading_time":22.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13,
        "Solution_word_count":211,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1663710134840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662878679543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":17.1,
        "Solution_reading_time":9.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":67,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612379283000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.78,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":27,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to use the Azure Pricing estimate in the Azure Pricing Calculator, the &quot;Estimated monthly costs&quot; seems to include but also far exceeds the compute cost.  Does this Estimated Monthly cost include the other resources that get created?     <br \/>\neg. Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), Key Vault    <\/p>\n<p>Thank you    <br \/>\nPeter    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/52085-image.png?platform=QnA\" alt=\"52085-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1609267518950,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/213635\/does-the-estimated-monthly-costs-for-azure-machine",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.3,
        "Challenge_reading_time":8.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Does the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator include all other non-compute \"additional resources\" created in the workspace",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Peter.    <\/p>\n<p>Thanks for reaching out. I tried your selections but I don't have the same service as you. Have you selected other services in you calculator?     <\/p>\n<p>For your question, the estimated price is only for Azure Machine Learning Service. You need to select all services you need in the calculator like below:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51998-image.png?platform=QnA\" alt=\"51998-image.png\" \/>    <\/p>\n<p>Please note I only use random number for the example.     <\/p>\n<p><strong>From the number I guess you have selected 2 Machine Learning Services and also other services since they added to your basket when you clicked them,<\/strong> you can click the button to see what you have all as below.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51969-image.png?platform=QnA\" alt=\"51969-image.png\" \/>    <\/p>\n<p>Also you are selecting Reservation service, detail: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations\">https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":13.0,
        "Solution_reading_time":15.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":129,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613745436903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1614004159640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"ClearML multiple tasks in single script changes logged value names",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.9,
        "Solution_reading_time":11.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":135,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi :  <\/p>\n<p>I am planing to use k-means to form algorithm to do project. However, I am aware that there are certain shortcomings to find the optimal groups using k-means.  <\/p>\n<p>Could you please tell the limitation and provide me with a detailed example?  <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647856847267,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/780362\/machine-learning-algorithms-questions",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":3.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"machine learning algorithms questions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @hideonbush again,  <\/p>\n<p>Generally to think about k-means, please refer to below cons and pros. If you can provide more details and how you want to develop your project, I can share more:  <\/p>\n<p>Pros:  <\/p>\n<ul>\n<li> K-means is very simple, highly flexible, and efficient.   <\/li>\n<li> Easy to adjust and interpret the clustering results. Easy to explain the results in contrast to Neural Networks.  <\/li>\n<li> The efficiency of k-means implies that the algorithm is good at segmenting a dataset.  <\/li>\n<li> An instance can change cluster (move to another cluster) when the centroids are recomputed  <\/li>\n<\/ul>\n<p>Cons  <\/p>\n<ul>\n<li> It does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them.   <\/li>\n<li> When doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the center of the clusters is recalculated until an adequate &quot;center'' is found for the number of clusters requested.  <\/li>\n<li> The order of the data input has an impact on the final results.  <\/li>\n<\/ul>\n<p>Hope this helps!  <\/p>\n<p>Regards,  <br \/>\nYutong  <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.0,
        "Solution_reading_time":19.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15,
        "Solution_word_count":265,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi everyone I have a step in vertex ai pipelines that looks like this:\n\ntranscribe_task = transcribe_audios(audio_files=download_task.output)\ntranscribe_task.set_cpu_limit(\"2\").set_memory_limit(\n\"8G\"\n).add_node_selector_constraint(\"NVIDIA_TESLA_T4\").set_gpu_limit(\"1\")\n\nyet that task is not executed due to:\n\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/custom_model_training_nvidia_t4_gpus, cause=null; Failed to create custom job for the task.\n\nBut that quota is not listed anywhere in the quota manager, how can I enable GPU in Vertex AI pipelines?",
        "Challenge_closed_time":1675667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675635060000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-not-listed-in-Vertex-AI-pipelines\/m-p\/518466#M1211",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Quota not listed in Vertex AI pipelines?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.4,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":35,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1504001058088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2101.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560855399150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":42.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow Error while deploying the Model to local REST server",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1840.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504001058088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2101.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":10.8,
        "Solution_reading_time":17.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13,
        "Solution_word_count":134,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m soon going to start implementing W&amp;B for my neural network\u2019s hyperparameter tuning. This is in preparation for an academic paper I\u2019m writing on the subject. The software seems very pragmatic and well-polished, so I\u2019m quite excited to get started.<\/p>\n<p>Its visualizations in particular seem to be of a very high quality. Some present sophisticated functionality that other experiment trackers can\u2019t touch. With proper citation, can these be included for publication?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638456342347,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/publishing-graphs-visualizations\/1457",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Publishing Graphs\/Visualizations",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":221.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Logan,<\/p>\n<p>I\u2019m so happy you\u2019re excited to use our product! Our engineers have worked very hard in order to get it to where it is today. We would love for you to use our graphs in your paper. We have a few examples of how to do so here (<a href=\"https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases\" class=\"inline-onebox-loading\">https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases<\/a>).<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":9.9,
        "Solution_reading_time":5.79,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":55,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1457261731840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC",
        "Answerer_reputation_count":584.0,
        "Answerer_view_count":270.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527707311910,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1130.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":21.1,
        "Solution_reading_time":11.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":62,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1522638099293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":15.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio vs. Workbench",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3387.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434736108840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dallas, TX, United States",
        "Poster_reputation_count":2045.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":10.7,
        "Solution_reading_time":10.52,
        "Solution_score_count":6.0,
        "Solution_sentence_count":7,
        "Solution_word_count":134,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1543778671427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want suggest ratio in Optuna.<\/p>\n<p>The ratio is <code>X_1, X_2, ..., X_k<\/code> bounded to <code>\u2211X_i = 1<\/code> and <code>0 &lt;= X_i &lt;= 1<\/code> for all <code>i<\/code>.<\/p>\n<p>Optuna doesn't offer Dirichlet distribution.<\/p>\n<p>I tried this but it doesn't work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    k = 10\n    ratios = np.zeros(k)\n    \n    residual = 1\n    for i in range(k - 1):\n        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, residual)\n        residual -= ratios[i]\n        \n#     ratios[k - 1] = trial.suggest_float(f'ratio_{k - 1}', residual, residual)\n    ratios[k - 1] = residual\n    return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n<\/code><\/pre>\n<p>And I tried this and finished without any errors. However, this is inconsistent because degree of freedom is <code>k - 1<\/code> for the bound but suggest <code>k<\/code> times.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    k = 10\n    ratios = np.zeros(k)\n    \n    for i in range(k):\n        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, 1)\n    \n    ratios \/= ratios.sum()\n    return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n<\/code><\/pre>\n<p>How can I suggest ratio or multiple variables with bound?<\/p>\n<p>This is a simple example so it's differentiable but I need variables in more complex objective.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610595386710,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1610596223483,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65713063",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How to suggest multivariate of ratio (with bound) in Optuna?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1543778671427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":99.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>This worked.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Objective:\n    def __init__(self):\n        self.max = 1\n    def __call__(self, trial):\n        k = 10\n        ratios = np.zeros(k)\n\n        for i in range(k):\n            ratios[i] = trial.suggest_float(f'ratio_{i}', 0, self.max)\n\n        ratios \/= ratios.sum()\n        self.max = (self.max + ratios.max()) \/ 2\n        return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(Objective(), n_trials=100)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.6,
        "Solution_reading_time":5.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":38,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How could I request quota for the NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs  <\/p>\n<p>They both do not appear as an option on the usual form to request quota increase in any of the 4 US regions I looked  <\/p>\n<p>Thanks  <\/p>\n<p>Manuel<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629921673763,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/528034\/access-to-ncast4-v3-series-and-nd-a100-v4-series-v",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Access to NCasT4_v3-series and ND A100 v4-series VMs",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=8de1093a-60f2-479b-b811-ff2bab24e3cd\">@Manuel Reyes Gomez  <\/a> ,    <\/p>\n<p>the VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/126428-image.png?platform=QnA\" alt=\"126428-image.png\" \/>    <\/p>\n<p>Source: <a href=\"https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&amp;regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\">https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&amp;regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3<\/a>    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":26.6,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":58,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642918837000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":null,
        "Solution_sentence_count":23,
        "Solution_word_count":376,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648517982110,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to see all SageMaker service quota limits?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":21.6,
        "Solution_reading_time":5.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":32,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a SageMaker Training Job with a custom algorithm and the input data from s3. The SagaMaker AIM role ARN has a Read\/Put policy on the specified S3 bucket folder, but while creating the job I get a client error:<\/p>\n\n<p><code>ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.<\/code><\/p>\n\n<p>Unfortunately no more error info is provided in the SageMaker dashboard to investigate further. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541002596347,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53087851",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":6.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker: ClientError: Data download failed:NoSuchKey (404): The specified key does not exist",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1219.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>SageMaker team member here.<\/p>\n\n<p>The problem here is that the training job in question was setup with S3DataType=ManifestFile. In this case SageMaker expects to be able to download a single manifest file from the location specified by the S3Uri, if the file does not exist in S3 we get a 404 which is what we're sending back as the error here.<\/p>\n\n<p>See here for documentation on S3DataType\/S3Uri and manifests: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType<\/a><\/p>\n\n<p>We will work to make this error message a bit more user-friendly, thanks for calling this out!<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.9,
        "Solution_reading_time":10.17,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":90,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1231865173143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mountain View, CA",
        "Answerer_reputation_count":29068.0,
        "Answerer_view_count":2064.0,
        "Challenge_adjusted_solved_time":4.0522230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591767515617,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":25.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow and Hydra causing crash when used together",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":718.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583072555672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1591782103620,
        "Solution_link_count":1,
        "Solution_readability":9.3,
        "Solution_reading_time":22.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16,
        "Solution_word_count":200,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1536318503563,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":150.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here's question proposed at the end of the chapter in 70-774 exam reference book. <\/p>\n\n<blockquote>\n  <p>If you connect a neural network with a Tune Model Hyperparameters module configured\n  with Random Sweep and Maximum number of runs on random sweep = 1, how\n  many neural networks are trained during the execution of the experiment? Why? If you\n  connect a validation dataset to the third input of the Tune Model Hyperparameters\n  module, how many neural networks are trained now?<\/p>\n<\/blockquote>\n\n<p>And the answer is :<\/p>\n\n<blockquote>\n  <p>Without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data\n  with the best combination of hyperparameters). With the validation set only 1 neural\n  network is trained, so the best model is not trained using the validation set if you provide\n  it.<\/p>\n<\/blockquote>\n\n<p>Where does 10 come from? As far as I understand the number should be 2 and 1 respectively. Shouldn't it create n-folds where n is equal to the number of runs?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539013176610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52705769",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Tune Model Hyper Parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528790837107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":610.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":10.5,
        "Solution_reading_time":7.59,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5,
        "Solution_word_count":107,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hey all,<\/p>\n<p>I wondered if there was a way to track system power consumption caused by model development? I\u2019ve checked the W&amp;B docs and can\u2019t see anything.<br>\nIdeally I\u2019d love to be able to keep track of runs to see how much power is used by different runs but also the whole project.<\/p>\n<p>Elsewhere I\u2019ve seen packages such as <a href=\"https:\/\/pypi.org\/project\/energyusage\/\" rel=\"noopener nofollow ugc\">energyusage<\/a> but ideally would like to use something more integrated and could be aggregated across runs for whole projects.<br>\nIf something already exists I\u2019d love to hear about it, otherwise either if W&amp;B fancied adding this functionality that would be great or if it came to it if anyone would like to help me with this project.<\/p>\n<p>Thanks,<\/p>\n<p>Jeff.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658216876213,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/track-power-energy-consumption\/2774",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":10.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Track power\/energy consumption?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":84.0,
        "Challenge_word_count":124,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nWe have this example that shows how to do this with CodeCarbon.<br>\n<a href=\"https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases--VmlldzoxMzM1NDg3\">https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases\u2013VmlldzoxMzM1NDg3<\/a><\/p>\n<p>You would use that library and log the info yourself. I do appreciate the feature request to integrate these more tightly.<br>\nThanks, hope this helps<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":24.1,
        "Solution_reading_time":7.58,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":40,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8,
        "Solution_word_count":36,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi everyone. Is there a way to save display preferences in the job runs search UI? My main interest is in saving (1) which columns are displayed, and (2) the order in which they are displayed. Thank you.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650636682000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1499",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":4.2,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to persist custom job runs table",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There\u2019s a save button next to the query search, it persist everything configured in the dashboard: https:\/\/polyaxon.com\/docs\/management\/organizations\/searches\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":18.2,
        "Solution_reading_time":2.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":17,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Challenge_closed_time":1651060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645701717000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":148.0,
        "Challenge_repo_fork_count":956.0,
        "Challenge_repo_issue_count":3383.0,
        "Challenge_repo_star_count":6165.0,
        "Challenge_repo_watch_count":89.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"fixed by https:\/\/github.com\/deepset-ai\/haystack\/pull\/2337",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":21.0,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":3,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>When trying to make a new sweep I get the following error<br>\n<code>AttributeError: 'SettingsStatic' object has no attribute 'git_root'<\/code><\/p>\n<p>Seems to be repeated no matter what I try.<br>\nThe full log from <code>debug-internal.log<\/code><\/p>\n<pre><code class=\"lang-bash\">2022-10-27 21:15:00,236 INFO    StreamThr :3165542 [internal.py:wandb_internal():88] W&amp;B internal server running at pid: 3165542, started at: 2022-10-27 21:15:00.235637\n2022-10-27 21:15:00,237 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: status\n2022-10-27 21:15:00,374 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: status\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: header\n2022-10-27 21:15:00,376 INFO    WriterThread:3165542 [datastore.py:open_for_write():75] open: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: run\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [dir_watcher.py:__init__():216] watching files in: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_start_run_threads():928] run started: xj06c5a6 with start time 1666905300.0\n2022-10-27 21:15:00,760 DEBUG   SenderThread:3165542 [sender.py:send():303] send: summary\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end\n2022-10-27 21:15:00,761 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: check_version\n2022-10-27 21:15:00,761 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: check_version\n2022-10-27 21:15:01,040 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: run_start\n2022-10-27 21:15:01,044 DEBUG   HandlerThread:3165542 [meta.py:__init__():34] meta init\n2022-10-27 21:15:01,381 INFO    WriterThread:3165542 [datastore.py:close():279] close: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:01,761 INFO    Thread-14 :3165542 [dir_watcher.py:_on_file_created():275] file\/dir created: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [sender.py:finish():1331] shutting down sender\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [dir_watcher.py:finish():362] shutting down directory watcher\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():392] scan: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json wandb-summary.json\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml config.yaml\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [file_pusher.py:finish():168] shutting down file pusher\n2022-10-27 21:15:02,764 INFO    SenderThread:3165542 [file_pusher.py:join():173] waiting for file pusher\n2022-10-27 21:15:04,333 INFO    Thread-19 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml\n2022-10-27 21:15:04,349 INFO    Thread-18 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:04,954 ERROR   StreamThr :3165542 [internal.py:wandb_internal():163] Thread HandlerThread:\nTraceback (most recent call last):\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 263, in _process\n    self._hm.handle(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 130, in handle\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 140, in handle_request\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 672, in handle_request_run_start\n    run_meta = meta.Meta(settings=self._settings, interface=self._interface)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/meta.py\", line 40, in __init__\n    root=self._settings.git_root,\nAttributeError: 'SettingsStatic' object has no attribute 'git_root'\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666905593478,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-error-attributeerror-settingsstatic-object-has-no-attribute-git-root\/3335",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.2,
        "Challenge_reading_time":67.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":null,
        "Challenge_title":"Sweep error - AttributeError: 'SettingsStatic' object has no attribute 'git_root'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":170.0,
        "Challenge_word_count":327,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Leslie,<\/p>\n<p>Found the issue! I was even though I had the correct conda env activated running the <code>wandb sweep<\/code> command with my local new version of wandb while the python code and env where running with and old 12.XX version.<\/p>\n<p>I changed it to use the same install for both and now it works.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.9,
        "Solution_reading_time":3.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":55,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know that you can log metrics as your experiment progresses. For example the training loss over epochs for your DL model.<\/p>\n<p>I was wondering if it was possible to do something similar for text. In my particular case I have a text model that generates some example text after each epoch and I wish to see what it's like. For example:<\/p>\n<pre><code>Epoch 1:\ntHi is RubisH\nEpoch 2:\nOk look slight better\nEpoch 3:\nI can speak English better than William Shakespeare\n<\/code><\/pre>\n<p>The workaround I can think of is to log this to a text file and push that as an artifact in mlflow. Was wondering if there was something else more native to mlflow.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616642359947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66792575",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Log text in mlflow",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1295.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372398800110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation_count":9065.0,
        "Poster_view_count":952.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_param\" rel=\"nofollow noreferrer\">log_param\/log_params<\/a> for that. For long texts maybe it's better to use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_text\" rel=\"nofollow noreferrer\">log_text<\/a> instead...<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.7,
        "Solution_reading_time":4.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":22,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi! The following error happens while trying to create an endpoint from a successful trained model:\n\n* In the web console: \n> The customer:primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.\n * CloudWatch logs: \n> exec: \"serve\": executable file not found in $PATH\n\nIm deploying the model using a Lambda step, just as in this [notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb). The Lambda step is successful, and I can see in the AWS web console that the model configuration is created with success. \n\nThe exact same error happens when I  create an endpoint for the registered model in the AWS web console, under Inference -> Models. In the console I can see that an inference container was created for the model, with the following characteristics:\n* Image: 763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39\n* Mode: single model\n* Environment variables (Key Value): \n> SAGEMAKER_CONTAINER_LOG_LEVEL\t20\n\n> SAGEMAKER_PROGRAM\tinference.py\n\n> SAGEMAKER_REGION\teu-west-3\n\n> SAGEMAKER_SUBMIT_DIRECTORY\t\/opt\/ml\/model\/code\n \nI absolutely have no clue what is wrong and I could not find anything relevant online about this problem. Is it necessary to provide an custom docker image for inference or something?\n\nFor more details, please find below the pipeline model steps code. Any help would be much appreciated!\n```\nmodel = Model(\n    image_uri=estimator.training_image_uri(),\n    model_data=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=sagemaker_session,\n    role=sagemaker_role,\n    source_dir='code',\n    entry_point='inference.py'\n)\nstep_model_create = ModelStep(\n        name=\"CreateModelStep\",\n        step_args=model.create(instance_type=\"ml.m5.large\")\n )\n\nregister_args = model.register(\n        content_types=[\"*\"],\n        response_types=[\"application\/json\"],\n        inference_instances=[\"ml.m5.large\"],\n        transform_instances=[\"ml.m5.large\"],\n        model_package_group_name=\"test\",\n        approval_status=\"Approved\"\n)\nstep_model_register = ModelStep(name=\"RegisterModelStep\", step_args=register_args)\n```",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670280332617,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1670628195195,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0JgbfwUoS5m6VH4TvtZmkg\/error-creating-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":29.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Error Creating Endpoint",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":218,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, the problem here is that your inference model's container URI `763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39` is using a **training** image, not an **inference** image for TensorFlow. Because the images are each optimized for their own function, the serving executable is not available in the training container in this case.\n\nUsually, the framework-specific SDK classes will handle this lookup for you (for example `TensorFlowModel(...)` as used in the notebook you linked, or when calling `sagemaker.tensorflow.TensorFlow.deploy(...)` from the Estimator class.\n\nI see here though that you're using the generic `Model`, so guess you don't know (or don't want to commit to) the framework and version at the point the Lambda function runs?\n\nMy suggestions would be:\n\n- Can you use the Pipelines `ModelStep` to create your model before calling the Lambda deployment function? Similarly to how your linked notebook uses `CreateModelStep`. This would build your framework & version into the pipeline definition itself, but should mean that the selection of inference container image gets handled properly & automatically.\n- If you really need to be dynamic, I think you might need to find a way of looking up at least the *framework* from the training job. From my testing, you can use `estimator = sagemaker.tensorflow.TensorFlow.attach(\"training-job-name\")` and then `model = estimator.create_model(...)` to correctly infer the specific inference container *version* from a training job, but it still relies on knowing that TensorFlow is the correct framework. I'm not aware of a framework-agnostic equivalent? So could e.g. try describing the training job, manually inferring which framework it uses from that information, and then using the relevant framework estimator class' [attach()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.attach) method to figure out the specifics and create your model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1670290340638,
        "Solution_link_count":1,
        "Solution_readability":11.4,
        "Solution_reading_time":25.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18,
        "Solution_word_count":266,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":2.15403,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>I am referring to the links below to use Tensorboard in Sagemaker Script Mode method.<\/strong><\/p>\n<p><a href=\"https:\/\/www.tensorflow.org\/tensorboard\/get_started\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tensorboard\/get_started<\/a><\/p>\n<p><a href=\"https:\/\/levelup.gitconnected.com\/how-to-use-tensorboard-in-an-amazon-sagemaker-notebook-instance-a41ce2fd973f\" rel=\"nofollow noreferrer\">https:\/\/levelup.gitconnected.com\/how-to-use-tensorboard-in-an-amazon-sagemaker-notebook-instance-a41ce2fd973f<\/a><\/p>\n<p><a href=\"https:\/\/towardsdatascience.com\/using-tensorboard-in-an-amazon-sagemaker-pytorch-training-job-a-step-by-step-tutorial-19b2b9eb4d1c\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/using-tensorboard-in-an-amazon-sagemaker-pytorch-training-job-a-step-by-step-tutorial-19b2b9eb4d1c<\/a><\/p>\n<p><strong>Below is my tensorboard callback in my training script which is a .py file<\/strong><\/p>\n<pre><code>model = create_model()\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nlog_dir = &quot;logs\/fit\/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nmodel.fit(x=x_train, \n          y=y_train, \n          epochs=5, \n          validation_data=(x_test, y_test), \n          callbacks=[tensorboard_callback])\n<\/code><\/pre>\n<p><strong>In a notebook, I am creating the below Tensorflow Estimator where I am passing the above Script file name as entry_point.<\/strong><\/p>\n<pre><code>estimator = TensorFlow(\n    entry_point='Script_File.py',\n    train_instance_type=train_instance_type,\n    train_instance_count=1,\n    model_dir=model_dir,\n    hyperparameters=hyperparameters,\n    role=sagemaker.get_execution_role(),\n    base_job_name='tf-fashion-mnist',\n    framework_version='1.12.0', \n    py_version='py3',\n    output_path=&lt;S3 Path&gt;,\n    script_mode=True,\n)\n<\/code><\/pre>\n<p><strong>I am using the below code in my notebook to start the training.<\/strong><\/p>\n<pre><code>estimator.fit(inputs)\n<\/code><\/pre>\n<p><strong>Once training is done, I am using the below code in a Terminal(have tried in my Notebook cell as well) to launch tensorboard.<\/strong><\/p>\n<pre><code>tensorboard --logdir logs\/fit\n<\/code><\/pre>\n<p>But in the tensorboard I am not able to view any graphs. It is showing the message &quot;Failed to fetch runs&quot;.\nIs there something that I am missing? Or do I have to do any extra setting in my script to see my logs in Tensorboard?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605782220677,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1607679075168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64909903",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":19.5,
        "Challenge_reading_time":33.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use Tensorboard in AWS Sagemaker",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605774298436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":5.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Your tensorboard <code>logdir<\/code> is not <code>logs\/fit<\/code>.. but there is the current date appended. Try using a <code>logs\/fit<\/code> as <code>log_dir<\/code> and see if it's working.<\/p>\n<p>EDIT<\/p>\n<p>If you want to use tensorboard locally you have to send tensorboard logs to S3 and read from there. In order to do this you have to do what your third linked example does, so include sagemaker debugger:<\/p>\n<blockquote>\n<p>from sagemaker.debugger import TensorBoardOutputConfig<\/p>\n<p>tensorboard_output_config = TensorBoardOutputConfig(\ns3_output_path='s3:\/\/path\/for\/tensorboard\/data\/emission',\ncontainer_local_output_path='\/local\/path\/for\/tensorboard\/data\/emission'\n)<\/p>\n<\/blockquote>\n<p>then your tensorboard command will be something like:<\/p>\n<blockquote>\n<p>AWS_REGION= &lt;your-region&gt; AWS_LOG_LEVEL=3 tensorboard --logdir\ns3:\/\/path\/for\/tensorboard\/data\/emission<\/p>\n<\/blockquote>\n<p>Alternatively if you want to use tensorboard in the notebook you have to do what the second linked example does, so simply install in a cell and run tensorboard with something like:<\/p>\n<p>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1607686829676,
        "Solution_link_count":1,
        "Solution_readability":16.1,
        "Solution_reading_time":15.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":125,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/172214-image.png?platform=QnA\" alt=\"172214-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644315390737,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/726898\/azure-subscription-cost",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":4.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Subscription Cost",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>If you want to review your costs and what resources are being charged, then the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview#understand-cost-management\">Cost Analysis blade<\/a> will allow you to drill down work this out. Please let us know if this helps    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.3,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":36,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>How do I create a separate API so that I can log metrics from test pipelines? It doesn\u2019t make sense to use a personal key for that.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675375672133,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/if-im-logging-metrics-for-mlops-from-a-test-pipeline-how-do-i-create-a-separate-api-key-for-that\/3803",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"If I'm logging metrics for MLOps from a test pipeline, how do I create a separate api key for that?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.8,
        "Solution_reading_time":4.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":29,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1564790214540,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569018204223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AML run.log() and run.log_list() fail without error",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465320834943,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":677.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1,
        "Solution_word_count":8,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662490156459,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.0,
        "Challenge_reading_time":24.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":304,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jlperla\">@jlperla<\/a> thank you for the detailed information, and great to hear that the grouping issue has been now resolved. Regarding your question using the API to filter runs, you could do that indeed with the following command:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"sweep\": \"sweep_id\"})<\/code><br>\nAlternatively you can use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/tags#how-to-add-tags\">API to tag all your runs<\/a> based on <code>my_sweep_name<\/code> identifier and then query runs as follows:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"tags\": \"my_sweep_name\"})<\/code><br>\nIs my_sweep_name defined in your config? In that case you could do <code>filters={\"config.sweep_name\": \"my_sweep_name\"}<\/code>.<\/p>\n<p>Would any of these work for you? Please let me know if you have any further questions or issues with this!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.7,
        "Solution_reading_time":11.58,
        "Solution_score_count":null,
        "Solution_sentence_count":8,
        "Solution_word_count":104,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":34,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587755548897,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1587809992912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Log metrics in PythonScriptStep",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":16.2,
        "Solution_reading_time":10.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":71,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645548595867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1.8311227778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645542424610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":12.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get wandb to pass arguments by position?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":270.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440414980200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":5.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1645549016652,
        "Solution_link_count":1,
        "Solution_readability":11.2,
        "Solution_reading_time":23.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21,
        "Solution_word_count":173,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611668079743,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570029753880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Philippines",
        "Poster_reputation_count":98.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.8,
        "Solution_reading_time":3.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":61,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1308769948883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":35204.0,
        "Answerer_view_count":4971.0,
        "Challenge_adjusted_solved_time":30.0725488889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":7,
        "Challenge_created_time":1658770653850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660741967556,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":10.5,
        "Challenge_reading_time":31.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Hyperparameter data types and scales not being validated",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":326,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1660850228732,
        "Solution_link_count":0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":88,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":13,
        "Challenge_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661403318517,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Challenge_link_count":6,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":39.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"All records are lost in a project without any action",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":297.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.7,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":29,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":14,
        "Challenge_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_closed_time":1582730,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578967122000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Challenge_link_count":0,
        "Challenge_participation_count":14,
        "Challenge_readability":15.3,
        "Challenge_reading_time":1.92,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@alla15747 Hi, thanks for reaching out to us. Could you please share your environment file so that we can know the details of this issue? @alla15747  please make sure that azureml-train-automl-runtime is installed in your environment if you using sdk>=1.0.76 or azureml-train-automl if using older version I'm running the code on the compute target and not my local machine. SDK 1.0.72\r\nHow to install packages in Azure Devops environment like azureml-train-automl? (base) C:\\Users\\aabdel137>pip freeze\r\nabsl-py==0.8.1\r\nadal==1.2.2\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.7\r\nanaconda-project==0.8.3\r\nansiwrap==0.8.4\r\napplicationinsights==0.11.9\r\nasn1crypto==0.24.0\r\nastor==0.8.0\r\nastroid==2.3.1\r\nastropy==3.2.1\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nazure-common==1.1.23\r\nazure-graphrbac==0.61.1\r\nazure-mgmt-authorization==0.60.0\r\nazure-mgmt-containerregistry==2.8.0\r\nazure-mgmt-keyvault==2.0.0\r\nazure-mgmt-resource==5.1.0\r\nazure-mgmt-storage==6.0.0\r\nazureml-contrib-interpret==1.0.72\r\nazureml-contrib-notebook==1.0.72\r\nazureml-core==1.0.72\r\nazureml-dataprep==1.1.29\r\nazureml-dataprep-native==13.1.0\r\nazureml-explain-model==1.0.72\r\nazureml-interpret==1.0.72.1\r\nazureml-pipeline==1.0.72\r\nazureml-pipeline-core==1.0.72\r\nazureml-pipeline-steps==1.0.72\r\nazureml-sdk==1.0.72\r\nazureml-telemetry==1.0.72\r\nazureml-train==1.0.72\r\nazureml-train-core==1.0.72\r\nazureml-train-restclients-hyperdrive==1.0.72\r\nazureml-widgets==1.0.72\r\nBabel==2.7.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.5\r\nbackports.os==0.1.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.7.1\r\nbitarray==0.9.3\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nbokeh==1.2.0\r\nboto==2.49.0\r\nBottleneck==1.2.1\r\ncertifi==2019.6.16\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.2.1\r\nclyent==1.2.2\r\ncolorama==0.4.1\r\ncomtypes==1.1.7\r\nconda==4.7.10\r\nconda-build==3.18.8\r\nconda-package-handling==1.3.11\r\nconda-verify==3.4.2\r\ncontextlib2==0.5.5\r\ncoverage==4.5.4\r\ncryptography==2.7\r\ncycler==0.10.0\r\nCython==0.29.12\r\ncytoolz==0.10.0\r\ndask==2.1.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\ndistributed==2.1.0\r\ndistro==1.4.0\r\ndocker==4.1.0\r\ndocutils==0.14\r\ndotnetcore2==2.1.10\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nflake8-formatter-junit-xml==0.0.6\r\nFlask==1.1.1\r\nfusepy==3.0.1\r\nfuture==0.17.1\r\ngast==0.3.2\r\ngevent==1.4.0\r\nglob2==0.7\r\ngoogle-pasta==0.1.7\r\ngreenlet==0.4.15\r\ngrpcio==1.24.3\r\nh5py==2.9.0\r\nheapdict==1.0.0\r\nhtml5lib==1.0.1\r\nidna==2.8\r\nimageio==2.5.0\r\nimagesize==1.1.0\r\nimportlib-metadata==0.23\r\ninterpret-community==0.1.0.3.3\r\ninterpret-core==0.1.18\r\nipykernel==5.1.1\r\nipython==7.6.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.0\r\nisodate==0.6.0\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.13.3\r\njeepney==0.4.1\r\nJinja2==2.10.1\r\njmespath==0.9.4\r\njoblib==0.13.2\r\njson5==0.8.4\r\njsonpickle==1.2\r\njsonschema==3.0.1\r\njunit-xml==1.8\r\njupyter==1.0.0\r\njupyter-client==5.3.1\r\njupyter-console==6.0.0\r\njupyter-core==4.5.0\r\njupyterlab==1.0.2\r\njupyterlab-server==1.0.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==18.0.0\r\nkiwisolver==1.1.0\r\nkmodes==0.10.1\r\nlazy-object-proxy==1.4.2\r\nlibarchive-c==2.8\r\nllvmlite==0.29.0\r\nlocket==0.2.0\r\nlxml==4.3.4\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.0\r\nmccabe==0.6.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmkl-service==2.0.2\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmsrest==0.6.10\r\nmsrestazure==0.6.2\r\nmultipledispatch==0.6.0\r\nnavigator-updater==0.2.1\r\nnbconvert==5.5.0\r\nnbformat==4.4.0\r\nndg-httpsclient==0.5.1\r\nnetworkx==2.3\r\nnltk==3.4.4\r\nnose==1.3.7\r\nnotebook==6.0.0\r\nnumba==0.44.1\r\nnumexpr==2.6.9\r\nnumpy==1.16.4\r\nnumpydoc==0.9.1\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopenpyxl==2.6.2\r\npackaging==19.2\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\npapermill==1.2.1\r\nparso==0.5.0\r\npartd==1.0.0\r\npath.py==12.0.1\r\npathlib2==2.3.4\r\npathspec==0.6.0\r\npatsy==0.5.1\r\npep8==1.7.1\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.0\r\nply==3.11\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.10.0\r\npsutil==5.6.3\r\npy==1.8.0\r\npy4j==0.10.7\r\npyasn1==0.4.7\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.3\r\npyflakes==2.1.1\r\nPygments==2.4.2\r\nPyJWT==1.7.1\r\npylint==2.4.2\r\npyodbc==4.0.26\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npypiwin32==223\r\npyreadline==2.1\r\npyrsistent==0.14.11\r\nPySocks==1.7.0\r\npyspark==2.4.4\r\npytest==5.2.2\r\npytest-arraydiff==0.3\r\npytest-astropy==0.5.0\r\npytest-cov==2.7.1\r\npytest-doctestplus==0.3.0\r\npytest-openfiles==0.3.2\r\npytest-remotedata==0.3.1\r\npython-dateutil==2.8.0\r\npython-dotenv==0.10.3\r\npytz==2019.1\r\nPyWavelets==1.0.3\r\npywin32==223\r\npywinpty==0.5.5\r\nPyYAML==5.1.1\r\npyzmq==18.0.0\r\nQtAwesome==0.5.7\r\nqtconsole==4.5.1\r\nQtPy==1.8.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.2.0\r\nrope==0.14.0\r\nruamel-yaml==0.15.46\r\nruamel.yaml==0.15.89\r\nscikit-image==0.15.0\r\nscikit-learn==0.21.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nSecretStorage==3.1.1\r\nSend2Trash==1.5.0\r\nshap==0.29.3\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.12.0\r\nsklearn==0.0\r\nsnowballstemmer==1.9.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.8\r\nSphinx==2.1.2\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.1.2\r\nspyder==3.3.6\r\nspyder-kernels==0.5.1\r\nSQLAlchemy==1.3.5\r\nstatsmodels==0.10.0\r\nsympy==1.4\r\ntables==3.5.2\r\ntblib==1.4.0\r\ntenacity==5.1.5\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-gpu==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntextwrap3==0.9.2\r\ntf-estimator-nightly==1.14.0.dev2019031401\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.37.0\r\ntraitlets==4.3.2\r\ntyped-ast==1.4.0\r\nunicodecsv==0.14.1\r\nunittest-xml-reporting==2.5.2\r\nurllib3==1.24.2\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.15.4\r\nwidgetsnbextension==3.5.0\r\nwin-inet-pton==1.1.0\r\nwin-unicode-console==0.5\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nxlrd==1.2.0\r\nXlsxWriter==1.1.8\r\nxlwings==0.15.8\r\nxlwt==1.3.0\r\nzict==1.0.0\r\nzipp==0.6.0\r\n AutoML became a part of default distribution (azureml-sdk) since 1.0.83\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2020-01-06\r\n\r\nif your client, that I believe pins the version of azureml sdk packages for the remote environment is 1.0.83, you will have automl on remote. \r\n\r\nIf you want to stay with 1.0.72 you can either reference automl extras azureml-sdk[automl] or explicitly reference azureml-train-automl (prefered).\r\n\r\nI would recommend to do both, upgrade client to the latest version and explicitly reference packages you need for your particular scenario not relying on metapackages like azureml-sdk\r\n\r\nOur reference doc will help you to get a set of the packages needed for your scenario\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py\r\n\r\nBy design every AzureML Python SDK package will bring necessary internal dependencies (of course except some corner cases :) )\r\n Thanks Vizhur, do you mind showing an example on how to reference azureml-train-automl in Azure Devops or Portal? \r\nThank you for the links! Not sure about your particular scenario, would you mind to share your ADO scenario so I can think of how to update it? MY scenario is implementing MLOPs example with automl step. Let me try to pull some relevant folks into the thread For AutoML, all the remote dependencies will get taken care of and will match whatever local dependencies are installed, e.g. if you have azureml-train-automl==1.0.72 installed, that version will be installed remotely for the training job.\r\nWe provide 2 clients to submitting these remote jobs currently, a thin client for submitting some types of remote jobs which is included as part of azureml-sdk, and a fuller client which enables more experiences such as Pipeline runs as part of azureml-train-automl. Since it looks like you are trying to use Pipelines, you will need to install the full azureml-train-automl client.\r\n\r\nFurthermore, the namespace for AutoMLStep changed recently, if you are using <1.0.76 the namespace would be \"from azureml.train.automl import AutoMLStep\", for >=1.0.76, you'll want to use \"from azureml.train.automl.runtime import AutoMLStep\" instead. I'm have sdk 1.0.72 installed. And I'm using from azureml.train.automl import AutoMLStep. Is there anyway to check the sdk version on the compute target machine? From your pip freeze, it doesn't look like you have the AutoML SDK installed. For the pipelines experience, you will need to have the SDK installed locally, not just on the target compute. Could you run \"pip install azureml-train-automl\"? @alla15747 \r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion. @SKrupa - Are you running your own code or a particular notebook sample from this repo?\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":16.1,
        "Solution_reading_time":117.81,
        "Solution_score_count":null,
        "Solution_sentence_count":38,
        "Solution_word_count":786,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460494806016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459942893017,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Recommendations API's Parameter",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459941581603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.9,
        "Solution_reading_time":22.87,
        "Solution_score_count":3.0,
        "Solution_sentence_count":18,
        "Solution_word_count":227,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1350376620496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colombo, Sri Lanka",
        "Answerer_reputation_count":207537.0,
        "Answerer_view_count":32114.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a free trial account on MS Azure and I'm following this tutorial.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>I'm stuck when I try to \"submit the pipeline\".<\/p>\n\n<p>The reason seems to be that I can't create a compute instance or a training cluster on a free plan.\nI still have 200USDs of free credits. I guess there must be a solution?<\/p>\n\n<hr>\n\n<p>Error messages:<\/p>\n\n<pre><code>Invalid graph: The pipeline compute target is invalid.\n\n400: Compute Test3 in state Failed, which is not able to use\n\nCompute instance: creation failed\nThe specified subscription has a total vCPU quota of 0 and is less than the requested compute training cluster and\/or compute instance's min nodes of 1 which maps to 4 vCPUs\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586681686103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1586690281396,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61168984",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":12.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML free trial: how to submit pipeline?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343682543648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":135.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Please check the announcement from MS Team regarding this:<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/<\/a><\/p>\n\n<p>All the free trials will not work as of now<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":26.4,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":23,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1425965839876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Cruz, CA",
        "Answerer_reputation_count":3256.0,
        "Answerer_view_count":164.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648703157780,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1648705670280,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425965839876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Cruz, CA",
        "Poster_reputation_count":3256.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":19.6,
        "Solution_reading_time":40.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":36,
        "Solution_word_count":200,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1463745452883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":248.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently playing around with tensorboard on Sagemaker studio. According to aws, it is supposedly possible. However, I keep encountering error code 500 after launching tensorboard and changing the file path to .....\/proxy\/{port number}<\/p>\n<p>Great if somebody can assists on this topic :)<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657288188783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72912418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I get tensorboard to work with sagemaker studio?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":22.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=\"https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample\" rel=\"nofollow noreferrer\">https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample<\/a><\/p>\n<p>Did you follow those instructions?<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":15.9,
        "Solution_reading_time":3.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651128218337,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":43.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.2,
        "Solution_reading_time":13.93,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9,
        "Solution_word_count":150,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443482042487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":1995.0,
        "Answerer_view_count":150.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using an Amazon SageMaker Notebook that has 72 cores and 144 GB RAM, and I carried out 2 tests with a sample of the whole data to check if the Dask cluster was working.<\/p>\n\n<p>The sample has 4500 rows and 735 columns from 5 different \"assets\" (I mean 147 columns for each asset). The code is filtering the columns and creating a feature matrix for each filtered Dataframe.<\/p>\n\n<p>First, I initialized the cluster as follows, I received 72 workers, and got 17 minutes of running. (I assume I created 72 workers with one core each.)<\/p>\n\n<pre><code>    from dask.distributed import Client, LocalCluster\n    cluster = LocalCluster(processes=True,n_workers=72,threads_per_worker=72)\n\n    def main():\n      import featuretools as ft\n      list_columns = list(df_concat_02.columns)\n\n      list_df_features=[]\n      from tqdm.notebook import tqdm\n\n      for asset in tqdm(list_columns,total=len(list_columns)):\n        dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n        es = ft.EntitySet()  \n        es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                      index = 'index', \n                                      time_index = 'Date')\n        fm, features = ft.dfs(entityset=es, \n                              target_entity='MARKET',\n                              trans_primitives = ['divide_numeric'],\n                              agg_primitives = [],\n                              max_depth=1,\n                              verbose=True,\n                              dask_kwargs={'cluster': client.scheduler.address}\n\n                              )\n        list_df_features.append(fm)\n      return list_df_features\n\n    if __name__ == \"__main__\":\n        list_df = main()\n<\/code><\/pre>\n\n<p>Second, I initialized the cluster as follows, I received 9 workers, and got 3,5 minutes of running. (I assume I created 9 workers with 8 cores each.)<\/p>\n\n<pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(processes=True)\n\ndef main():\n  import featuretools as ft\n  list_columns = list(df_concat_02.columns)\n\n  list_df_features=[]\n  from tqdm.notebook import tqdm\n\n  for asset in tqdm(list_columns,total=len(list_columns)):\n    dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n    es = ft.EntitySet()  \n    es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                  index = 'index', \n                                  time_index = 'Date')\n    fm, features = ft.dfs(entityset=es, \n                          target_entity='MARKET',\n                          trans_primitives = ['divide_numeric'],\n                          agg_primitives = [],\n                          max_depth=1,\n                          verbose=True,\n                          dask_kwargs={'cluster': client.scheduler.address}\n\n                          )\n    list_df_features.append(fm)\n  return list_df_features\n\nif __name__ == \"__main__\":\n    list_df = main()\n<\/code><\/pre>\n\n<p>For me, it's mind-blowing because I thought that 72 workers could carry the work out faster! Once I'm not a specialist neither in Dask nor in FeatureTools I guess that I'm setting something wrong.<\/p>\n\n<p>I would appreciate any kind of help and advice!<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583540635953,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583589065656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60573260",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":34.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"Why does Featuretools slows down when I increase the number of Dask workers?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":297,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>You are correctly setting <code>dask_kwargs<\/code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.7,
        "Solution_reading_time":4.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":57,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539753916957,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":15.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403185902747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation_count":169.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":71.6,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":17,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/105827-image.png?platform=QnA\" alt=\"105827-image.png\" \/>    <\/p>\n<p>I tried to deploy a VM to Azure Machine Learning, but I get the error message &quot;You do not have enough quota for the following VM sizes. Click here to view and request quota.&quot; And the VM cannot be deployed.    <\/p>\n<p>But I have enough quota (24 CPUs).    <\/p>\n<p>What is causing the problem?    <\/p>\n<p>I'm using Azure's Free trial plan.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437136\/cant-deploy-a-vm-on-the-azure-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=80eeb45c-8aa8-49ab-81df-1bb291fc79a5\">@ShoM  <\/a> ,    <\/p>\n<p>there are different quotas in Azure:    <\/p>\n<ul>\n<li> There are quotas for <code>vCPUs per Azure Region<\/code>    <\/li>\n<li> In addition there are quotas for <code>vCPUs per VM Series<\/code>    <\/li>\n<\/ul>\n<p>Both quotas (for Azure Region and VM Series) must fit the requirements.    <\/p>\n<p>It seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.    <br \/>\nYou can check your quotas by the link you marked with the red line in your screenshot.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.6,
        "Solution_reading_time":9.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":111,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676042396664,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.8,
        "Challenge_reading_time":5.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Rerun a deleted run in wandb sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.3,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":4,
        "Solution_word_count":51,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1621392911643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614044459343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":49.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":374,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483140008952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco Bay Area, CA, United States",
        "Poster_reputation_count":604.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":20.2,
        "Solution_reading_time":15.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":71,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":9,
        "Challenge_created_time":1643658605453,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645284643350,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Challenge_link_count":2,
        "Challenge_participation_count":10,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":514.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622195346030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7,
        "Solution_readability":9.0,
        "Solution_reading_time":29.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":30,
        "Solution_word_count":255,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597814570000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":299,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":null,
        "Solution_sentence_count":13,
        "Solution_word_count":124,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619492179000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Private github repo fail for using log_model and log_metric",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is very hard to debug, you need to check if the git repo has some more information like a hard-coded NO_OP.\n\nAlso I would check if you added the local cache folder .polyaxon to your .gitignore and .dockerignore. If this folder was added to you git repo, then indeed the metrics and artifacts will be saved to a different run.\n\nYou can also validate that the run is running with the correct run-uuid:\n\n...\nprint(tracking.TRACKING_RUN.run_uuid)\n...\n\nIf the run_uuid does not correspond to the currently running run, then the cache is bundled somewhere (git repo or docker image).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.4,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":97,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I've created an azure for students account wiht $100 free credit and started using Azure Notebooks to train some ML models. I've created a GPU instance which costs $1.20\/hr. I've been using it for at least 1.5h now and what's weird is that no usage is being shown on my dashboard, and on the sponsorship page it's showing that it is not active and that I haven't used any of my credit:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239817-image.png?platform=QnA\" alt=\"239817-image.png\" \/>    <\/p>\n<p>On the other hand when I go to my subscriptions it says it's active:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/239809-image.png?platform=QnA\" alt=\"239809-image.png\" \/>    <\/p>\n<p>Is something wrong or does it take a while to see usage statistics\/credit spending?    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662893566663,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1002201\/azure-for-students-showing-no-usage-despite-using",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure for students showing no usage despite using it",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":125,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>Usually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/understand\/download-azure-daily-usage\">download-azure-daily-usage<\/a>    <\/p>\n<p>==    <br \/>\nPlease &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.1,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":57,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1501040260887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588429621780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation_count":21.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10,
        "Solution_word_count":45,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1539831335196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":137.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580725861867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":31.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.6,
        "Solution_reading_time":19.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12,
        "Solution_word_count":221,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653612949060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.1,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13,
        "Solution_word_count":183,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used to use Azure Machine Learning Studio (classic).  <br \/>\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.  <br \/>\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).  <br \/>\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.  <\/p>\n<p>How can I process at the same speed as classic?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635432791567,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\">Optimize Data Processing<\/a>. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/170450\/\">(1)<\/a>, <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html\">(2)<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":10.3,
        "Solution_reading_time":18.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16,
        "Solution_word_count":188,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458983415440,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cluj-Napoca, Romania",
        "Answerer_reputation_count":12439.0,
        "Answerer_view_count":1795.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659671305703,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660033763876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.9,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"HuggingFace Trainer() cannot report to wandb",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587186468848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Taipei, Taiwan R.O.C",
        "Poster_reputation_count":163.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.0,
        "Solution_reading_time":5.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":50,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1354644345812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3969.0,
        "Answerer_view_count":356.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I just wanted to test how good can neural network approximate multiplication function (regression task). \nI am using Azure Machine Learning Studio. I have 6500 samples, 1 hidden layer\n(I have tested 5 \/30 \/100 neurons per hidden layer), no normalization. And default parameters \n<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn906030.aspx\" rel=\"nofollow\">Learning rate - 0.005, Number of learning iterations - 200, The initial learning weigh - 0.1,\n The momentum - 0 [description]<\/a>. I got extremely bad accuracy, close to 0.\n<em>At the same time boosted Decision forest regression shows very good approximation.<\/em><\/p>\n\n<p>What am I doing wrong? This task should be very easy for NN.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1464596393980,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1491916922983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37520849",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't approximate simple multiplication function in neural network with 1 hidden layer",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4342.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354644345812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3969.0,
        "Poster_view_count":356.0,
        "Solution_body":"<p>Big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have zero gradient.\nWe can use two approaches:<\/p>\n\n<p>1) Devide by constant. We are just deviding everything before the learning and multiply after.<\/p>\n\n<p>2) Make log-normalization. It makes multiplication into addition:<\/p>\n\n<pre><code>m = x*y =&gt; ln(m) = ln(x) + ln(y).\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1465820459196,
        "Solution_link_count":0,
        "Solution_readability":11.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":5.0,
        "Solution_sentence_count":5,
        "Solution_word_count":57,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use <code>[OPTUNA][1]<\/code> with <code>sklearn<\/code> <code>[MLPRegressor][1]<\/code> model.<\/p>\n<p>For almost all hyperparameters it is quite straightforward how to set OPTUNA for them.\nFor example, to set the learning rate:\n<code>learning_rate_init = trial.suggest_float('learning_rate_init ',0.0001, 0.1001, step=0.005)<\/code><\/p>\n<p>My problem is how to set it for <code>hidden_layer_sizes<\/code> since it is a tuple. So let's say I would like to have two hidden layers where the first will have 100 neurons and the second will have 50 neurons. Without OPTUNA I would do:<\/p>\n<p><code>MLPRegressor( hidden_layer_sizes =(100,50))<\/code><\/p>\n<p>But what if I want OPTUNA to try different neurons in each layer? e.g., from 100 to 500, how can I set it? the <code>MLPRegressor<\/code> expects a tuple<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636648248167,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1660311769156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931757",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":11.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to set hidden_layer_sizes in sklearn MLPRegressor using optuna trial",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517513644076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":995.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You could set up your objective function as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport warnings\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nwarnings.filterwarnings('ignore')\n\nX, y = make_regression(random_state=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n\ndef objective(trial):\n\n    params = {\n        'learning_rate_init': trial.suggest_float('learning_rate_init ', 0.0001, 0.1, step=0.005),\n        'first_layer_neurons': trial.suggest_int('first_layer_neurons', 10, 100, step=10),\n        'second_layer_neurons': trial.suggest_int('second_layer_neurons', 10, 100, step=10),\n        'activation': trial.suggest_categorical('activation', ['identity', 'tanh', 'relu']),\n    }\n\n    model = MLPRegressor(\n        hidden_layer_sizes=(params['first_layer_neurons'], params['second_layer_neurons']),\n        learning_rate_init=params['learning_rate_init'],\n        activation=params['activation'],\n        random_state=1,\n        max_iter=100\n    )\n\n    model.fit(X_train, y_train)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=3)\n# [I 2021-11-11 18:04:02,216] A new study created in memory with name: no-name-14c92e38-b8cd-4b8d-8a95-77158d996f20\n# [I 2021-11-11 18:04:02,283] Trial 0 finished with value: 161.8347337123744 and parameters: {'learning_rate_init ': 0.0651, 'first_layer_neurons': 20, 'second_layer_neurons': 40, 'activation': 'tanh'}. Best is trial 0 with value: 161.8347337123744.\n# [I 2021-11-11 18:04:02,368] Trial 1 finished with value: 159.55535852658082 and parameters: {'learning_rate_init ': 0.0551, 'first_layer_neurons': 90, 'second_layer_neurons': 70, 'activation': 'relu'}. Best is trial 1 with value: 159.55535852658082.\n# [I 2021-11-11 18:04:02,440] Trial 2 finished with value: 161.73980822730888 and parameters: {'learning_rate_init ': 0.0051, 'first_layer_neurons': 100, 'second_layer_neurons': 30, 'activation': 'identity'}. Best is trial 1 with value: 159.55535852658082.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.4,
        "Solution_reading_time":28.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24,
        "Solution_word_count":174,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1549041651583,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":3106.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":52.7292055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#workspace-level-quota\" rel=\"nofollow noreferrer\">link<\/a>), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597248542537,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597249191852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63380531",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369863777596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation_count":335.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.<\/p>\n<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest<\/code> command in the CLI.<\/p>\n<p>There are a few commands that may interest you:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspacesandcomputes\/usages\/list\" rel=\"nofollow noreferrer\">Usage<\/a> and Quotas for a region:\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/usages?api-version=2019-05-01<\/code>\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/quotas?api-version=2020-04-01<\/code><\/p>\n<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1597439016992,
        "Solution_link_count":3,
        "Solution_readability":18.8,
        "Solution_reading_time":20.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":131,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566218806593,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609778640296,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"trains with grid search",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416942229380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1566242122212,
        "Solution_link_count":2,
        "Solution_readability":14.6,
        "Solution_reading_time":8.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":79,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using this simple script, using the example blog post. However, it fails because of <code>wandb<\/code>. It was of no use to make <code>wandb<\/code> OFFLINE as well.<\/p>\n<pre><code>from datasets import load_dataset, load_metric\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          Trainer, TrainingArguments)\nimport wandb\n\n\nwandb.init()\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\ndataset = load_dataset('glue', 'mrpc')\nmetric = load_metric('glue', 'mrpc')\n\ndef encode(examples):\n    outputs = tokenizer(\n        examples['sentence1'], examples['sentence2'], truncation=True)\n    return outputs\n\nencoded_dataset = dataset.map(encode, batched=True)\n\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased', return_dict=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Evaluate during training and a bit more often\n# than the default to be able to prune bad trials early.\n# Disabling tqdm is a matter of preference.\ntraining_args = TrainingArguments(\n    &quot;test&quot;, eval_steps=500, disable_tqdm=True,\n    evaluation_strategy='steps',)\n\ntrainer = Trainer(\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=encoded_dataset[&quot;train&quot;],\n    eval_dataset=encoded_dataset[&quot;validation&quot;],\n    model_init=model_init,\n    compute_metrics=compute_metrics,\n)\n\ndef my_hp_space(trial):\n    return {\n        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True),\n        &quot;weight_decay&quot;: trial.suggest_float(&quot;weight_decay&quot;, 0.1, 0.3),\n        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 10),\n        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 20, 40),\n        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [32, 64]),\n    }\n\n\ntrainer.hyperparameter_search(\n    direction=&quot;maximize&quot;,\n    backend=&quot;optuna&quot;,\n    n_trials=10,\n    hp_space=my_hp_space\n)\n<\/code><\/pre>\n<p><code>Trail 0<\/code> finishes successfully, but next <code>Trail 1<\/code> crashes with following error:<\/p>\n<pre><code>  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 138, in _objective\n    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1376, in train\n    self.log(metrics)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1688, in log\n    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 371, in on_log\n    return self.call_event(&quot;on_log&quot;, args, state, control, logs=logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 378, in call_event\n    result = getattr(callback, event)(\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 754, in on_log\n    self._wandb.log({**logs, &quot;train\/global_step&quot;: state.global_step})\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/preinit.py&quot;, line 38, in preinit_wrapper\n    raise wandb.Error(&quot;You must call wandb.init() before {}()&quot;.format(name))\nwandb.errors.Error: You must call wandb.init() before wandb.log()\n<\/code><\/pre>\n<p>Any help is highly appreciated.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":8,
        "Challenge_created_time":1627015026463,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68494108",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":21.3,
        "Challenge_reading_time":50.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":null,
        "Challenge_title":"Hyperparam search on huggingface with optuna fails with wandb error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":532.0,
        "Challenge_word_count":252,
        "Platform":"Stack Overflow",
        "Poster_created_time":1335098927163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":860.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>Please check running the code on the latest versions of wandb and transformers. Works fine for me with <code>wandb 0.11.0<\/code> and <code>transformers 4.9.0<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":23,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Challenge_closed_time":1576023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575967432000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Comet PAPI Depreciated",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":44,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Challenge_closed_time":1623275,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613369681000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.88,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting Errors with wandb sweeps ",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I got this error when I tried to run wandb sweeps for a regressionn classifcation. It complained when I included the required num_labels. After removing it, the error is what I get. Are there some additional settings required besides setting regression=True. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":null,
        "Solution_sentence_count":7,
        "Solution_word_count":70,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599546516000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":532,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":null,
        "Solution_sentence_count":16,
        "Solution_word_count":206,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1278673735383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":242.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570719704473,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Deepar Prediction Quantiles Explained",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457123770467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver",
        "Poster_reputation_count":246.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.1,
        "Solution_reading_time":5.51,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7,
        "Solution_word_count":75,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>FLAML looks like it performs better than Azure AutoML for hyperparameter tuning (based on the benchmarking in the Arxiv paper): <a href=\"https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf\">https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf<\/a>  <\/p>\n<p>Is it now being used or is there a plan to integrate it for the hyperparameter tuning in Azure Machine Learning Services? If so, when is that expected to become available?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1623298947587,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/429832\/does-azure-automl-use-(or-plan-to-use)-flaml-for-t",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Does Azure AutoML use (or plan to use) FLAML for the hyperparameter tuning?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=44a7ffc5-e97c-4dec-95a0-445a9835aab3\">@Rainer Hillermann  <\/a> Thanks, We are not using the FLAML for Azure AutoML for the hyperparameter tuning, You can raise a user voice request <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\">here<\/a> so the community can vote and provide their feedback, the product team then checks this feedback and implements the feature in future releases.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":15.6,
        "Solution_reading_time":5.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":51,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1069.7181213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am exploring VertexAI pipelines and understand that it is a managed alternative to, say, AI Platform pipelines (where you have to deploy a GKE cluster to be able to run Kubeflow pipelines). What I am not clear on is whether VertexAI will autoscale the cluster depending on the load. In the answer to a <a href=\"https:\/\/stackoverflow.com\/questions\/68343475\/how-to-scale-out-kubeflow-pipelines-using-vertex-ai-or-it-just-done-automatic\">similar question<\/a>, it is mentioned that for pipeline steps that use GCP resources such as Dataflow etc., autoscaling will be done automatically. In the google docs, it is mentioned that for components, one can <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline#specify-machine-type\" rel=\"nofollow noreferrer\">set resources<\/a>, such as CPU_LIMIT GPU_LIMIT etc. My question is, can these limits be set for any type of component, i.e., Google Cloud pipeline components or Custom components, whether Python function-based or those packaged as a container image? Secondly, do these limits mean that the components resources will autoscale till they hit those limits? And what happens if these options are not even specified, how are the resources allocated then, will they autoscale as VertexAI sees fit?<\/p>\n<p>Links to relevant docs and resources would be really helpful.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628219043833,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68675606",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Autoscaling VertexAI pipeline components",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":524.0,
        "Challenge_word_count":183,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>To answer your questions,<\/p>\n<p><strong>1. Can these limits be set for any type of components?<\/strong><\/p>\n<blockquote>\n<p>Yes. Because, these limits are applicable to all Kubeflow components and are not specific to any particular type of component.\nThese components could be implemented to perform tasks with a set amount of resources.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>2. Do these limits mean that the component resources will autoscale till they hit the limits?<\/strong><\/p>\n<blockquote>\n<p>No, there is no autoscaling performed by Vertex AI. Based on the limits set, Vertex AI chooses one suitable VM to perform the task.\nHaving a pool of workers is supported in Google Cloud Pipeline Components such as \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomContainerTrainingJobRunOp<\/a>\u201d and \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomPythonPackageTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomPythonPackageTrainingJobRunOp<\/a>\u201d as part of Distributed Training in Vertex AI. Otherwise, only 1 machine is used per step.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>3. What happens if these limits are not specified? Does Vertex AI scale the resources as it sees fit?<\/strong><\/p>\n<blockquote>\n<p>If the limits are not specified, an \u201c<a href=\"https:\/\/cloud.google.com\/compute\/docs\/general-purpose-machines#e2_machine_types\" rel=\"nofollow noreferrer\">e2-standard-4<\/a>\u201d VM is used for task execution as the default option.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>EDIT:<\/strong> I have updated the links with the latest version of the documentation.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1632070029070,
        "Solution_link_count":3,
        "Solution_readability":13.1,
        "Solution_reading_time":25.5,
        "Solution_score_count":3.0,
        "Solution_sentence_count":19,
        "Solution_word_count":190,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666171234000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":null,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":858,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":127,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1619518958572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation_count":98.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a new experiment on mlflow but I have this problem:<\/p>\n<pre><code>Exception: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n<\/code><\/pre>\n<p>This is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport mlflow.sklearn\nimport sys\n\nmlflow.set_experiment(&quot;New experiment 2&quot;)\n\nmlflow.set_tracking_uri('http:\/\/mlflow:5000')\nst= mlflow.start_run(run_name='Test2')\nid = st.info.run_id\nmlflow.log_metric(&quot;score&quot;, score)\nmlflow.sklearn.log_model(model, &quot;wineModel&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620805743553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67499339",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Create mlflow experiment: Run with UUID is already active",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":675.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620748388616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You have to run mlflow.end_run() to finish the first experiment. Then you can create another<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":15,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>To back up the code, I want to upload my training code to Wandb every time I run it. Is this possible?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892988773,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-upload-code-to-wandb-every-time-i-run-it\/1922",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I upload code to WandB every time I run it",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":211.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No, wandb does not have an option to store code. Why do you want to save the code?<\/p>\n<ol>\n<li>\n<p>Are you changing the hparams in your code in every run?  - Then you could try using wandb.sweep() instead, as it visualizes your model\u2019s performance for different hparams<\/p>\n<\/li>\n<li>\n<p>Are you using different architectures while training? -   Wandb artifacts logs datasets and model\/training data. There are functions that track all your parameters (wandb. watch() iirc). This leads to an ONNX format of your model being saved.  This ONNX model can be visualized, and you could use that to see what model was trained. Or you could even save a string in your config file with details of the model you\u2019re training<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.7,
        "Solution_reading_time":8.92,
        "Solution_score_count":null,
        "Solution_sentence_count":10,
        "Solution_word_count":122,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683306015132,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1683644479959,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.9,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I tell if my Notebook instance is frozen?",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683362949783,
        "Solution_link_count":2,
        "Solution_readability":10.8,
        "Solution_reading_time":37.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22,
        "Solution_word_count":488,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Challenge_closed_time":1624218,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618393073000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Local wandb logging is borked",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I want to find the best hyperparameters using wandb, however, some combinations of them raises cuda memory error, how could I tell wandb that still runs with new hyperparameter combinations if there are these errors? So I do not need to check that all possible combinations do not raise a memory cuda error. I am afraid that  the whole sweep will stop after a specific number of runs has failed.<\/p>\n<p>Could I use some try except or  tell wandb to always execute new runs (like \u201callow unlimited failed runs\u201d) ?<\/p>\n<p>Thanks in advance<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1685369868994,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/allow-unlimited-failed-runs\/4482",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Allow unlimited failed runs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":11.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The solution is WANDB_AGENT_MAX_INITIAL_FAILURES=1000<\/p>\n<p>Solved <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":24.7,
        "Solution_reading_time":3.41,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":13,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":55,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1592311727163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603208778703,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":32.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.1,
        "Solution_reading_time":4.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":55,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Challenge_closed_time":1635553,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631568762000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":26.53,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":226,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"`log_metrics` is part of the implementation of `LightningLoggerBase` yet using the experiment property returns the MlFlowClient which can be used to access methods specific to mlflow. So simply removing the experiment property from your calls should solve your problem.\r\n\r\nThe log_metric option of the mlflow client requires different args, see [here](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/mlflow.py#L226) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.5,
        "Solution_reading_time":8.64,
        "Solution_score_count":null,
        "Solution_sentence_count":6,
        "Solution_word_count":87,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652672302869,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.5,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Run best model off sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.1,
        "Solution_reading_time":8.17,
        "Solution_score_count":null,
        "Solution_sentence_count":7,
        "Solution_word_count":50,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1472651915230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Challenge_topic":"Performance Optimization",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":102.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.8,
        "Solution_reading_time":18.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11,
        "Solution_word_count":211,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a model quality monitor job, using the class ModelQualityMonitor from Sagemaker model_monitor, and i think i have all the import statements defined yet i get the message cannot import name error<\/p>\n<pre><code>from sagemaker import get_execution_role, session, Session\nfrom sagemaker.model_monitor import ModelQualityMonitor\n                \nrole = get_execution_role()\nsession = Session()\n\nmodel_quality_monitor = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=session\n)\n<\/code><\/pre>\n<p>Any pointers are appreciated<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607727203003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65259702",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS sagemaker model monitor- ImportError: cannot import name 'ModelQualityMonitor'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":544.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546959992036,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3<\/code> Amazon SageMaker notebook, I don't get any errors at all.<\/p>\n<p>Example screenshot output showing no errors:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined<\/code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker<\/code> and then see if this resolves your error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.4,
        "Solution_reading_time":9.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":88,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Limit Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":176,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I run this code but gives me the mentioned error:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-plaintext\">def train_model(ver=None):\n\n    run = wandb.init(\n    project=\"sdg_test_case1\",\n    name=f\"advanced_analytic_EDA_model_v{ver}\",\n    # config=param,\n    )\n\n\n    param = {\n    \"objective\" : \"binary:logistic\",\n    \"eval_metric\": 'auc',\n    \"predictor\":\"gpu_predictor\",\n    \"random_state\": seed,\n    \"scale_pos_weight\": 2.66,\n    \"max_depth \": wandb.config.max_depth,\n    \"learning_rate\": wandb.config.learning_rate,\n    \"gamma\": wandb.config.gamma,\n    \"min_child_weight\": wandb.config.min_child_weight,\n    \"subsample\": wandb.config.subsample\n\n    }\n\n    bst = xgb.train(\n        param,\n        xg_train,\n        evals=watchlist,\n        verbose_eval=5,\n        callbacks=[WandbCallback()],\n        num_boost_round=50,\n        early_stopping_rounds=3\n    )\n\n    pred = bst.predict(xg_val)\n    pred_class = pred.round()\n\n\n    auc = roc_auc_score(y_test, pred)\n    acc = accuracy_score(y_test, pred_class)\n\n    conf = confusion_matrix(y_test, pred_class)\n    tn, fp, fn, tp=conf.ravel()\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    specificity = tn \/ (tn + fp)\n\n    # print(f\"AUC of ROC: {(auc)}\")\n    # print(f\"accuracy: {(acc)}\")\n    # print(f\"precision: {(precision)}\")\n    # print(f\"recall: {(recall)}\")\n    # print(f\"specificity: {(specificity)}\")\n\n    wandb.log({\n        \"accuracy\":acc,\n        \"precision\":precision,\n        \"recall\":recall,\n        \"specificity\":specificity,\n    })\n    wandb.summary[\"AUC\"] = auc\n\nsweep_config = {\n    \"method\": \"random\",\n    \"metric\": {\"name\": \"AUC\", \"goal\": \"maximize\"},\n    \"parameters\":{\n        \"max_depth\": {\"values\":[2,3,4,5,6,7,8]},\n        \"subsample\": {'distribution': 'uniform','min': 0.7,'max': 1},\n        \"gamma\": {'distribution': 'uniform','min': 0,'max': 0.1},\n        \"learning_rate\": {'distribution': 'uniform','min': 0,'max': 0.1},\n        \"min_child_weight\" : {'distribution': 'uniform','min': 0.9,'max': 1.3}\n\n    }\n}\n\nver = 1\nsweep_id = wandb.sweep(sweep_config,project=\"sdg_test_case\")\nwandb.agent(sweep_id=sweep_id, function=train_model(ver), count=10)\nwandb.finish()\n\n<\/code><\/pre>\n<p>I ran the example <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/quickstart\">here<\/a> and it ran flawlessly, I don\u2019t know where is the issue is, the two codes are similar.<br>\nThank you for your time!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676039090033,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/getting-error-class-wandb-sdk-wandb-config-config-object-has-no-attribute-max-depth\/3859",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":15.5,
        "Challenge_reading_time":28.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting error <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'max_depth'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>solved it by not giving any arguments to the train_model function.<br>\n<code>wandb.agent(sweep_id=sweep_id, function=train_model, count=10) <\/code><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.1,
        "Solution_reading_time":2.06,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":15,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1634305425288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":43.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using optuna to tune xgboost model's hyperparameters. I find it stuck at trial 2 (trial_id=3) for a long time(244 minutes). But When I look at the SQLite database which records the trial data, I find <strong>all the trial 2 (trial_id=3) hyperparameters has been calculated<\/strong> except the mean squared error value of trial 2. And the optuna trial 2 (trial_id=3) seems stuck at that step. I want to know why this happened? And how to fix the issue?<\/p>\n<p><strong>Here is the code<\/strong><\/p>\n<pre><code>def xgb_hyperparameter_tuning(): \n    def objective(trial):\n        params = {\n            &quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 1000, 10000, step=100),\n            &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]), \n            &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 1, 20, step=1),\n            &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.0001, 0.2, step=0.001),\n            &quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1.0, 20.0, step=1.0),\n            &quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.1, 1.0, step=0.1),\n            &quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;,0.1, 1.0, step=0.1),\n            &quot;reg_alpha&quot;: trial.suggest_float(&quot;reg_alpha&quot;, 0.0, 11.0, step=0.1),        \n            &quot;reg_lambda&quot;: trial.suggest_float(&quot;reg_lambda&quot;, 0.0, 11.0, step=0.1),\n            &quot;num_parallel_tree&quot;: 10,\n            &quot;random_state&quot;: 16,\n            &quot;n_jobs&quot;: 10,\n            &quot;early_stopping_rounds&quot;: 1000,\n        }\n\n        model = XGBRegressor(**params)\n        mse = make_scorer(mean_squared_error)\n        cv = cross_val_score(estimator=model, X=X_train, y=log_y_train, cv=20, scoring=mse, n_jobs=-1)\n        return cv.mean()\n\n    study = optuna.create_study(study_name=&quot;HousePriceCompetitionXGB&quot;, direction=&quot;minimize&quot;, storage=&quot;sqlite:\/\/\/house_price_competition_xgb.db&quot;, load_if_exists=True)\n    study.optimize(objective, n_trials=100,)\n    return None\n\nxgb_hyperparameter_tuning()\n<\/code><\/pre>\n<p><strong>Here is the output<\/strong><\/p>\n<pre><code>[I 2021-11-16 10:06:27,522] A new study created in RDB with name: HousePriceCompetitionXGB\n[I 2021-11-16 10:08:40,050] Trial 0 finished with value: 0.03599314763859092 and parameters: {'n_estimators': 5800, 'booster': 'gblinear', 'max_depth': 4, 'learning_rate': 0.1641, 'min_child_weight': 17.0, 'colsample_bytree': 0.4, 'subsample': 0.30000000000000004, 'reg_alpha': 10.8, 'reg_lambda': 7.6000000000000005}. Best is trial 0 with value: 0.03599314763859092.\n[I 2021-11-16 10:11:55,830] Trial 1 finished with value: 0.028514652199592445 and parameters: {'n_estimators': 6600, 'booster': 'gblinear', 'max_depth': 17, 'learning_rate': 0.0821, 'min_child_weight': 20.0, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.2, 'reg_alpha': 1.2000000000000002, 'reg_lambda': 7.2}. Best is trial 1 with value: 0.028514652199592445.\n<\/code><\/pre>\n<p><strong>Here is the sqlite database <code>trial_values<\/code> table's data<\/strong><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">trial_value_id<\/th>\n<th style=\"text-align: center;\">trial_id<\/th>\n<th style=\"text-align: center;\">objective<\/th>\n<th style=\"text-align: center;\">value<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">0<\/td>\n<td style=\"text-align: center;\">0.0359931476385909<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">0<\/td>\n<td style=\"text-align: center;\">0.0285146521995924<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p><strong>Here is the sqlite database <code>trial_params<\/code> table's data<\/strong> And you can see all the trial 2 (trial_id=3) hyperparameters has been calculated<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">param_id<\/th>\n<th style=\"text-align: center;\">trial_id<\/th>\n<th style=\"text-align: center;\">param_name<\/th>\n<th style=\"text-align: center;\">param_value<\/th>\n<th style=\"text-align: center;\">distribution_json<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">5800.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">1.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">4.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">4<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.1641<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">5<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">17.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">6<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.4<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">7<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.3<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">8<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">10.8<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">9<\/td>\n<td style=\"text-align: center;\">1<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">7.6<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">10<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">6600.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">11<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">1.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">12<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">17.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">13<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.0821<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">14<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">20.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">15<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.7<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">16<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">17<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">1.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">18<\/td>\n<td style=\"text-align: center;\">2<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">7.2<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">19<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">n_estimators<\/td>\n<td style=\"text-align: center;\">7700.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1000, &quot;high&quot;: 10000, &quot;step&quot;: 100}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">20<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">booster<\/td>\n<td style=\"text-align: center;\">2.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;CategoricalDistribution&quot;, &quot;attributes&quot;: {&quot;choices&quot;: [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">21<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">max_depth<\/td>\n<td style=\"text-align: center;\">4.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;IntUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1, &quot;high&quot;: 20, &quot;step&quot;: 1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">22<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">learning_rate<\/td>\n<td style=\"text-align: center;\">0.1221<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0001, &quot;high&quot;: 0.1991, &quot;q&quot;: 0.001}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">23<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">min_child_weight<\/td>\n<td style=\"text-align: center;\">3.0<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 1.0, &quot;high&quot;: 20.0, &quot;q&quot;: 1.0}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">24<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">colsample_bytree<\/td>\n<td style=\"text-align: center;\">0.5<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">25<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">subsample<\/td>\n<td style=\"text-align: center;\">0.1<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.1, &quot;high&quot;: 1.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">26<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">reg_alpha<\/td>\n<td style=\"text-align: center;\">10.8<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<tr>\n<td style=\"text-align: center;\">27<\/td>\n<td style=\"text-align: center;\">3<\/td>\n<td style=\"text-align: center;\">reg_lambda<\/td>\n<td style=\"text-align: center;\">1.1<\/td>\n<td style=\"text-align: center;\">{&quot;name&quot;: &quot;DiscreteUniformDistribution&quot;, &quot;attributes&quot;: {&quot;low&quot;: 0.0, &quot;high&quot;: 11.0, &quot;q&quot;: 0.1}}<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1637043226340,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1637043542287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69984504",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.1,
        "Challenge_reading_time":195.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":120,
        "Challenge_solved_time":null,
        "Challenge_title":"Why optuna stuck at trial 2(trial_id=3) after it has calculated all hyperparameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":454.0,
        "Challenge_word_count":1041,
        "Platform":"Stack Overflow",
        "Poster_created_time":1634305425288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Although I am not 100% sure, I think I know what happened.<\/p>\n<p>This issue happens because some parameters are not suitable for certain <code>booster type<\/code> and the trial will return <code>nan<\/code> as result and be stuck at the step - calculating the <code>MSE<\/code> score.<\/p>\n<p>To solve the problem, you just need to delete the <code>&quot;booster&quot;: &quot;dart&quot;<\/code>.<\/p>\n<p>In other words, using <code>&quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;]), <\/code> rather than <code>&quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;]), <\/code> can solve the problem.<\/p>\n<p>I got the idea when I tuned my LightGBMRegressor Model. I found many trials fail because these trials returned <code>nan<\/code> and they all used the same <code>&quot;boosting_type&quot;=&quot;rf&quot;<\/code>. So I deleted the <code>rf<\/code> and all 100 trials were completed without any error. Then I looked for the <code>XGBRegressor<\/code> issue which I posted above. I found all the trials which were stuck had the same <code>&quot;booster&quot;:&quot;dart&quot;<\/code> either. So I deleted the <code>dart<\/code>, and the <code>XGBRegressor<\/code> run normally.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":10.6,
        "Solution_reading_time":17.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12,
        "Solution_word_count":151,
        "Tool":"Optuna"
    }
]