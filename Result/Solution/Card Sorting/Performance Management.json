[
    {
        "Answerer_created_time":1424791933163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2483.0,
        "Answerer_view_count":192.0,
        "Challenge_adjusted_solved_time":10.4979555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Challenge_closed_time":1631093676416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630949196040,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1631055883776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":25.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":40.1334377778,
        "Challenge_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":211,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597774835507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.8,
        "Solution_reading_time":12.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1423224680703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":1034.0,
        "Answerer_view_count":207.0,
        "Challenge_adjusted_solved_time":0.2080508334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running some optimization with optuna, and I'd like to produce plots with the same scale on both axes, but so far I was unable to find out how.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(study_name=study_name,\n                            storage=f&quot;sqlite:\/\/\/{results_folder}\/{study_name}.db&quot;,\n                            directions=[&quot;maximize&quot;, &quot;maximize&quot;],\n                            load_if_exists=True)\n# I tried either\nfig_pareto = optuna.visualization.plot_pareto_front(study, target_names=['precision', 'recall'])\nfig_pareto.show()\n\n# or\nfig, ax = plt.subplots()\noptuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall']) \nax.axis(&quot;equal&quot;)\nax.set_xlim(0.7, 1)\nax.set_ylim(0.7, 1)\ntarget_names=['precision', 'recall'])\nplt.savefig(&quot;some_name.png&quot;)\n\n<\/code><\/pre>\n<p>but without success.\nThis is what the saved plot looks like:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ejZ2f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ejZ2f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>With the first method, the pictures open in an interactive view in the browser and I can resize them, but there's no option to precisely make them square.<\/p>\n<p>When using the second way, it looks like calling:<\/p>\n<pre><code>optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall']) \n<\/code><\/pre>\n<p>is not linking the produced plot to the ax object?\nIf I do:<\/p>\n<pre><code>pareto_plot = optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall'])  \n<\/code><\/pre>\n<p>the pareto_plot is an AxesSubplot object, can I manually load it in the axes?<\/p>",
        "Challenge_closed_time":1660906320883,
        "Challenge_comment_count":2,
        "Challenge_created_time":1660901645950,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660905571900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73414713",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.3,
        "Challenge_reading_time":22.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.2985925,
        "Challenge_title":"Optuna, change axes ratio in plots",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423224680703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Italy",
        "Poster_reputation_count":1034.0,
        "Poster_view_count":207.0,
        "Solution_body":"<p>Sorry, I had the answer right there.\nI had to do:<\/p>\n<pre><code>pareto = optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall'])\npareto.axis('equal')\nplt.savefig(&quot;pareto_plot.png&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.8,
        "Solution_reading_time":3.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":18.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9119444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456108000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583449225000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9119444444,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2472222222,
        "Challenge_answer_count":1,
        "Challenge_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Challenge_closed_time":1603455348000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603454458000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668589944840,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2472222222,
        "Challenge_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":944.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\n**Note:** This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"EnforceInstanceType\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"sagemaker:CreateTrainingJob\",\n                    \"sagemaker:CreateHyperParameterTuningJob\"\n                ],\n                \"Resource\": \"*\",\n                \"Condition\": {\n                    \"ForAllValues:StringLike\": {\n                        \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                    }\n                }\n            }\n    \n         ]\n    }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925584228,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":305.2888888889,
        "Challenge_answer_count":9,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017877000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642918837000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":305.2888888889,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":376.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":197.6242483334,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hi everyone,<\/p>\n<p>regarding the different chart types, I am somehow missing the option \u2018<strong>Scatter plot<\/strong>\u2019 next to \u2018Line plot\u2019, \u2018Area plot\u2019, and \u2018Percent area plot\u2019. Would be great if you could add this feature in the future (or in case it can easily be done somehow else, please let me know how it works \u2013 I already tried custom scatter plots, but it seems as if they are meant for comparing different runs, not matrices from a single run).<\/p>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Challenge_closed_time":1657831541559,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657120094265,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/scatter-plot-instead-of-line-plot\/2706",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":197.6242483334,
        "Challenge_title":"Scatter plot instead of Line plot",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a> ,<\/p>\n<p>You can create a line chart with a dotted non connected line through the legend category within chart edit.  I believe that is what you are looking for, see image below. In terms of modifying how a metric is being logged, can you expand on your meaning behind this? You can currently update metrics for a run, after it has finished., see <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Finance-Prediction?workspace=user-mohammadbakir\">here<\/a>.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea.png\" data-download-href=\"\/uploads\/short-url\/wDbjAXgfUtaaLAXhq2VTVU9YU6C.png?dl=1\" title=\"DotChart\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png\" alt=\"DotChart\" data-base62-sha1=\"wDbjAXgfUtaaLAXhq2VTVU9YU6C\" width=\"690\" height=\"198\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1035x297.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1380x396.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">DotChart<\/span><span class=\"informations\">1819\u00d7524 30 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":24.0,
        "Solution_reading_time":26.56,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":108.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI'm trying to run a training job and make it resume automatically whenever it is preempted or it encounters an issue.\nI'm using for this the \"termination\" and \"maxRetries\" field to restart the job.\nAfter a problem happens, the job is restarted automatically starting from where the problem has happened if I look at the logs. However, nothing is being saved to the artifacts and any call to tracking.log_metric doesn't seem to have an effect. If I look at the logs, the job then continues until it reaches the end. However instead of just ending, it just keeps restarting (from the point where the problem occurred) until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page.\nAny idea what could cause such a problem and if there is anything I could do to avoid it?",
        "Challenge_closed_time":1649329911000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649329501000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1474",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1138888889,
        "Challenge_title":"Auto-resume for deep learning training is not working",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Polyaxon provides several strategies to restart, restrat with copy mode, and resumes jobs. The auto-resume behavior is enabled by default\n\nNote that resuming a job can only work if your code supports loading the last checkpoint.\n\nHere's a quick debugging logic to check that the resuming process works as expected:\n\nmain.py\ndef main():\n    tracking.init()\n    checkpoint_path = tracking.get_outputs_path(\"checkpoint.json\")\n    checkpoint_path_exists = os.path.exists(checkpoint_path)\n    print(\"[CHECKPOINT] path found: {}\".format(checkpoint_path_exists))\n    if checkpoint_path_exists:\n        with open(checkpoint_path, \"r\") as checkpoint_file:\n            checkpoint = json.loads(checkpoint_file.read())\n            print(\"[CHECKPOINT] last content: {}\".format(checkpoint))\n    else:\n      print(\"[CHECKPOINT] init ...\")\n      checkpoint = {\n        \"last_time\": time.time(),\n        \"last_index\": 0,\n        \"array\": [],\n      }\n    for i in range(checkpoint[\"last_index\"] + 1, 300):\n      print(\"[CHECKPOINT] step {}\".format(i))\n      tracking.log_progress((i + 1)\/300)\n      tracking.log_metric(name=\"index\", value=i, step=i)\n      checkpoint[\"array\"].append(i)\n      checkpoint[\"last_index\"] = i\n      checkpoint[\"last_time\"] = time.time()\n      if i in [10, 50]:\n        print(\"[CHECKPOINT] Saving last content ...\")\n        with open(checkpoint_path, \"w\") as checkpoint_file:\n          checkpoint_file.write(json.dumps(checkpoint))\n        raise ValueError(\"Error was raised at {}\".format(i))\n      time.sleep(1)\npolyaxonfile.yaml\nversion: 1.1\nkind: component\ntermination:\n  maxRetries: 3\nrun:\n  kind: job\n  container:\n    image: polyaxon\/polyaxon-examples:artifacts\n    workingDir: \"{{ globals.run_artifacts_path }}\/uploads\"\n    command: [\"\/bin\/bash\", -c]\n    args: [\"pip install -U polyaxon --no-cache && python3 main.py\"]\nLogged a dummy metric that resumes from last checkpoint and (apart from the warning regression that I mentioned) the job succeeds after after two failures (you can see the first chart where the x-axis is the time that there's gap time)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":24.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":194.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.3151194444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi    <br \/>\nI am unable to create a VM in Compute. Status is at Creating for an hour and then it fails.     <br \/>\nI tried several times without luck.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78763-image.png?platform=QnA\" alt=\"78763-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78781-image.png?platform=QnA\" alt=\"78781-image.png\" \/>    <\/p>\n<p>Does anyone know how to solve this?<\/p>",
        "Challenge_closed_time":1616052673920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615983139490,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/318685\/cant-create-a-vm-in-compute-creation-failed",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":19.3151194444,
        "Challenge_title":"Can't create a VM in compute - Creation failed",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I was able to delete all the failed VMs and create one today.  <br \/>\nThe solution in this case was to wait it out.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Challenge_closed_time":1548817091420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548817091420,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1224733422316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":7707.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1564790214540,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":52.6982252778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p><strong>SDK version<\/strong>: <code>1.0.43<\/code><\/p>\n\n<p>To minimize clicking and compare accuracy between <code>PipelineRun<\/code>s, I'd like to log a metric from inside a <code>PythonScriptStep<\/code> to the parent <code>PipelineRun<\/code>. I thought I could do this like:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\nfoo = 0.80\nrun.parent.log(\"accuracy\",foo)\n<\/code><\/pre>\n\n<p>however I get this error.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Traceback (most recent call last):\n  File \"get_metrics.py\", line 62, in &lt;module&gt;\n    run.parent.log(\"geo_mean\", top3_runs)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 459, in parent\n    return None if parent_run_id is None else get_run(self.experiment, parent_run_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 1713, in get_run\n    return next(runs)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 297, in _rehydrate_runs\n    yield factory(experiment, run_dto)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py\", line 325, in _from_dto\n    return PipelineRun(experiment=experiment, run_id=run_dto.run_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py\", line 74, in __init__\n    service_endpoint=_service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_graph_context.py\", line 46, in __init__\n    service_endpoint=service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 118, in create_provider\n    service_endpoint=service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 133, in create_service_caller\n    service_endpoint = _AevaWorkflowProvider.get_endpoint_url(workspace, experiment_name)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 153, in get_endpoint_url\n    workspace_name=workspace.name, workspace_id=workspace._workspace_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py\", line 749, in _workspace_id\n    self.get_details()\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py\", line 594, in get_details\n    self._subscription_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py\", line 507, in show_workspace\n    AzureMachineLearningWorkspaces, subscription_id).workspaces,\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py\", line 112, in _get_service_client\n    all_subscription_list, tenant_id = self._get_all_subscription_ids()\nTypeError: 'NoneType' object is not iterable\n<\/code><\/pre>\n\n<h3>Update<\/h3>\n\n<p>On further investigation, I tried just printing the <code>parent<\/code> attribute of the run with the line below and got the same <a href=\"https:\/\/gist.github.com\/swanderz\/13a33babf8b09ff89791bdfa66e3148a\" rel=\"nofollow noreferrer\">Traceback<\/a><\/p>\n\n<p><code>print(\"print run parent attribute\", run.parent)<\/code><\/p>\n\n<p>The <code>get_properties()<\/code> method the below. I'm guessing that azureml just uses the <code>azureml.pipelinerunid<\/code> property for pipeline tree hierarchy, and that the <code>parent<\/code> attribute has been left for any user-defined hierarchies.<\/p>\n\n<pre><code>{\n    \"azureml.runsource\": \"azureml.StepRun\",\n    \"ContentSnapshotId\": \"45bdecd3-1c43-48da-af5c-c95823c407e0\",\n    \"StepType\": \"PythonScriptStep\",\n    \"ComputeTargetType\": \"AmlCompute\",\n    \"azureml.pipelinerunid\": \"e523d575-c373-46d2-a4bc-1717f5e34ec2\",\n    \"_azureml.ComputeTargetType\": \"batchai\",\n    \"AzureML.DerivedImageName\": \"azureml\/azureml_dfd7f4f952ace529f986fe919909c3ec\"\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1568685167387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568330937317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568495453776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57915603",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":59.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":98.3972416667,
        "Challenge_title":"log metric from inside PythonScriptStep to parent PipelineRun",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":363.0,
        "Challenge_word_count":265,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>Please upgrade your SDK to the latest version. Seems like this issue was fixed sometime after 1.0.43.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":1.35,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":71.3592933333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I can only draw image with [wandb.Image(Numpy.array()),] like this<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b48aa9bfc94603e638f56ff8452ed88b900f00db.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"11:54AM - 20 November 2020\">AIcrowd Forum \u2013 20 Nov 20<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:600\/325;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e05a631c976b165047261523c356b3fa7e5eab41.gif\" class=\"thumbnail animated\" width=\"600\" height=\"325\"><\/div>\n\n<h3><a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\">MaskRCNN integrated with WandB and DIRECT SUBMIT FROM COLAB!<\/a><\/h3>\n\n  <p>Hi everyone!    @rohitmidha23 and me have been following this challenge for quite a while. We have written a starter notebook using MaskRCNN. We further integrate MaskRCNN with WandB which really helps to keep track of the various experiments that...<\/p>\n\n  <p>\n    <span class=\"label1\">Reading time: 1 mins \ud83d\udd51<\/span>\n      <span class=\"label2\">Likes: 17 \u2764<\/span>\n  <\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p>\nBut how can I draw many images with a bar like this<br>\n<a href=\"https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp<\/a><\/p>",
        "Challenge_closed_time":1652432634120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652175740664,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-draw-many-images-with-a-bar\/2391",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":17.9,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":71.3592933333,
        "Challenge_title":"How to draw many images with a bar",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":144,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Oh\uff0cyes! I got it ,the step slider.<br>\nIt\u2019s on the left top of my panel. Thanks!<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7477d027660f227b355c1b7090095a0ca0e72264.png\" alt=\"FireShot Capture 043 - warm-sea-50 - deepfillv2_512x512_dv5_0pv8_1 \u2013 Weights &amp; Biases_ - 192.168.23.40\" data-base62-sha1=\"gCk5gzKgcCCqUAxrDVgTMn9CtF2\" width=\"274\" height=\"249\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.42,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1618.0830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Challenge_closed_time":1624218172000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618393073000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0830555556,
        "Challenge_title":"Local wandb logging is borked",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":336.0223277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Challenge_closed_time":1661377323147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660154954560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660167642767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":339.5468297222,
        "Challenge_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542606952710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1303910479480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4196.0,
        "Answerer_view_count":67.0,
        "Challenge_adjusted_solved_time":3788.8637213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Challenge_closed_time":1631884865500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618244956103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3788.8637213889,
        "Challenge_title":"Continue stopped run in MLflow",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.8990052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I just started a Pay-As-You-Go account for the Azure Machine Learning training, when I tried to create a budget, I get the error that says &quot;Cost Management budgets is not supported for this account. Please change to another scope.&quot; <\/p>\n<p>I do not know what means or how to go about it.<\/p>\n<p>Please assist me<\/p>\n<p>Jonathan<\/p>",
        "Challenge_closed_time":1674963239872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674956403453,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165089\/cost-management-budgets-is-not-supported-for-this",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.8990052778,
        "Challenge_title":"Cost Management budgets is not supported for this account. Please change to another scope.",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Jonathan,<\/p>\n<p>Are you able to open Cost Management and set the scope to your Pay-As-You-Go subscription?   For example, open Cost Management Budgets using link below, then click on Scope and set the scope to be the correct subscription:<\/p>\n<p><a href=\"https:\/\/portal.azure.com\/#view\/Microsoft_Azure_CostManagement\/Menu\/%7E\/budgets\/openedBy\/AzurePortal\">https:\/\/portal.azure.com\/#view\/Microsoft_Azure_CostManagement\/Menu\/~\/budgets\/openedBy\/AzurePortal<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/af6ac92e-9bab-46c3-ab9e-1c03e404897e?platform=QnA\" alt=\"azure budget scope\" \/><\/p>\n<p>If the above is useful please click Accept Answer.<\/p>\n<p>Thanks.<\/p>\n<p>-TP<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":9.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":58.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.5462247223,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Challenge_closed_time":1652695869278,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652672302869,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.5,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.5462247223,
        "Challenge_title":"Run best model off sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":580.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":8.17,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1645048145367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":72.2380869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a custom job with<\/p>\n<pre><code>gcloud ai custom-jobs create --region=us-west1 --display-name=test-job --config=trainjob.yaml\n<\/code><\/pre>\n<p>where <code>trainjob.yaml<\/code> is<\/p>\n<pre><code>workerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-4\n  replicaCount: 1\n  containerSpec:\n    imageUri: eu.gcr.io\/myproject\/myimage\n<\/code><\/pre>\n<p>I can see the list of the job via<\/p>\n<pre><code>gcloud ai custom-jobs list --region=us-west1\n<\/code><\/pre>\n<p>. Can this list seen in the UI? For AI Platform product there is jobs but I don't see anything like this in Vertex AI<\/p>",
        "Challenge_closed_time":1658154433088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657900255003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72996624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":70.6050236111,
        "Challenge_title":"Can the list of custom jobs in vertex AI custom seen in the UI?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1485529624880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1991.0,
        "Poster_view_count":107.0,
        "Solution_body":"<p>I don't know if it is exactly what you are looking for, but you can see the custom training jobs details using the UI at <code>Console<\/code> &gt; <code>Vertex AI<\/code> &gt; <code>Training<\/code> &gt; <code>Custom Jobs<\/code> or following the next <a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/training\/custom-jobs\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1658160312116,
        "Solution_link_count":1.0,
        "Solution_readability":15.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":42.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":74.1955488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run tensorboard from my Jupyter notebook in Sagemaker. The below is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport datetime, os\n\n%load_ext tensorboard\nlogs_base_dir = &quot;.\/logs&quot;\nos.makedirs(logs_base_dir)\n\n!tensorboard --logdir=data\/ --host localhost --port=8080\n<\/code><\/pre>\n<p>The output I get looks fine:\n<code>TensorBoard 1.14.0 at http:\/\/localhost:8080\/ (Press CTRL+C to quit)<\/code>\nbut when I click on the link, I'm taken to a page with ERR_CONNECTION_REFUSED.<\/p>\n<p>Does anyone have suggestions about what to try next? Thanks so much!<\/p>\n<p>Tensorflow: 1.14\nPython: 2<\/p>",
        "Challenge_closed_time":1624263423263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1623953190457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623996319287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68024476",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":86.1757794445,
        "Challenge_title":"Running Tensorboard in Jupyter in Sagemaker: this site can't be reached",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340392287000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1843.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>Based on the comments it seems to me that you are trying to open the wrong URL. If I understand you question, you are not running in a local environment, so you can not open <code>localhost<\/code>. The right URL for <code>sagemaker<\/code> from the docs is <code>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":4.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":124.4336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Challenge_closed_time":1600665871000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600217910000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":124.4336111111,
        "Challenge_title":"Wandb Run stalling",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So this appears to be a problem on the Weights and Biases end of things. https:\/\/github.com\/wandb\/client\/issues\/1243 This is fixed see original issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":1.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1598380609848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oxford, UK",
        "Answerer_reputation_count":38531.0,
        "Answerer_view_count":4137.0,
        "Challenge_adjusted_solved_time":0.0715297222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1607094854147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607094596640,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1607191847983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":24.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.0715297222,
        "Challenge_title":"Saving an Matlabplot as an MLFlow artifact",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5219.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316705139196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":6711.0,
        "Poster_view_count":819.0,
        "Solution_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":7.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7593.3588888889,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
        "Challenge_closed_time":1666742778000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639406686000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/11043",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":36.09,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":7593.3588888889,
        "Challenge_title":"RichProgressBar doesn't display progress bar when using Comet logger.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":251,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Is there any update for this bug? Any update on this issue?\r\nI'm experiencing the same problem when using `comet` logger and `RichProgressBar` @ItamarKanter @JackLin-Authme I just tried this and can see the rich progress bar working fine. Is it possible that I am using a newer version of either rich or comet that now fixed the problem? Do you still have documentation of what version(s) you were using?\r\n\r\nI'm closing the issue now, but if you find any more issues related to this we can continue the investigation.  I still experience the issue. Adding more information on this, the progress bar DO show, however only after it has been completed. Moreover, any `rich.print` calls show no color, including the progress bar itself. The only solution I found is to stop using the Comet logger.\r\n\r\npackage versions:\r\npytorch-lightning     1.9.0\r\ncomet-ml                 3.32.0\r\nrich                          13.3.1\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":15.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":215.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1615961068168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":6.8178152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up a locust based framework for ML load test and need to create custom metrics and logs for which the example that I am following is using 'MetricsServiceV2Client' in 'google.cloud.logging_v2' lib.\nIn the Vertex Workbench on GCP inspite being on v3.0 of the google-cloud-logging lib I am getting an issue of import<\/p>\n<p>from google.cloud import logging_v2\nfrom google.cloud.logging_v2 import MetricsServiceV2Client<\/p>\n<p>error: cannot import name 'MetricsServiceV2Client' from 'google.cloud.logging_v2' (\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/logging_v2\/<strong>init<\/strong>.py)<\/p>\n<p>Interestingly when I test the import in  google's cloud console I am able to import without any issue. What could be the issue ?<\/p>",
        "Challenge_closed_time":1645126649432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645102105297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71158453",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.8178152778,
        "Challenge_title":"module 'google.cloud.logging_v2' has no attribute 'MetricsServiceV2Client' Vertex WorkBench",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1615961068168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>from google.cloud.logging_v2.services.metrics_service_v2 import MetricsServiceV2Client this works !!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.4,
        "Solution_reading_time":1.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663557360000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":145,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.2109844445,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>When trying to make a new sweep I get the following error<br>\n<code>AttributeError: 'SettingsStatic' object has no attribute 'git_root'<\/code><\/p>\n<p>Seems to be repeated no matter what I try.<br>\nThe full log from <code>debug-internal.log<\/code><\/p>\n<pre><code class=\"lang-bash\">2022-10-27 21:15:00,236 INFO    StreamThr :3165542 [internal.py:wandb_internal():88] W&amp;B internal server running at pid: 3165542, started at: 2022-10-27 21:15:00.235637\n2022-10-27 21:15:00,237 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: status\n2022-10-27 21:15:00,374 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: status\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: header\n2022-10-27 21:15:00,376 INFO    WriterThread:3165542 [datastore.py:open_for_write():75] open: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: run\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [dir_watcher.py:__init__():216] watching files in: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_start_run_threads():928] run started: xj06c5a6 with start time 1666905300.0\n2022-10-27 21:15:00,760 DEBUG   SenderThread:3165542 [sender.py:send():303] send: summary\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end\n2022-10-27 21:15:00,761 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: check_version\n2022-10-27 21:15:00,761 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: check_version\n2022-10-27 21:15:01,040 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: run_start\n2022-10-27 21:15:01,044 DEBUG   HandlerThread:3165542 [meta.py:__init__():34] meta init\n2022-10-27 21:15:01,381 INFO    WriterThread:3165542 [datastore.py:close():279] close: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:01,761 INFO    Thread-14 :3165542 [dir_watcher.py:_on_file_created():275] file\/dir created: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [sender.py:finish():1331] shutting down sender\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [dir_watcher.py:finish():362] shutting down directory watcher\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():392] scan: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json wandb-summary.json\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml config.yaml\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [file_pusher.py:finish():168] shutting down file pusher\n2022-10-27 21:15:02,764 INFO    SenderThread:3165542 [file_pusher.py:join():173] waiting for file pusher\n2022-10-27 21:15:04,333 INFO    Thread-19 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml\n2022-10-27 21:15:04,349 INFO    Thread-18 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:04,954 ERROR   StreamThr :3165542 [internal.py:wandb_internal():163] Thread HandlerThread:\nTraceback (most recent call last):\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 263, in _process\n    self._hm.handle(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 130, in handle\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 140, in handle_request\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 672, in handle_request_run_start\n    run_meta = meta.Meta(settings=self._settings, interface=self._interface)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/meta.py\", line 40, in __init__\n    root=self._settings.git_root,\nAttributeError: 'SettingsStatic' object has no attribute 'git_root'\n<\/code><\/pre>",
        "Challenge_closed_time":1666942353022,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666905593478,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-error-attributeerror-settingsstatic-object-has-no-attribute-git-root\/3335",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.2,
        "Challenge_reading_time":67.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":10.2109844445,
        "Challenge_title":"Sweep error - AttributeError: 'SettingsStatic' object has no attribute 'git_root'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":327,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Leslie,<\/p>\n<p>Found the issue! I was even though I had the correct conda env activated running the <code>wandb sweep<\/code> command with my local new version of wandb while the python code and env where running with and old 12.XX version.<\/p>\n<p>I changed it to use the same install for both and now it works.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":3.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":267.6955555556,
        "Challenge_answer_count":2,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342987000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612379283000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":267.6955555556,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1661172608640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":22.9405238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Challenge_closed_time":1661173080163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661090494277,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":22.9405238889,
        "Challenge_title":"Is there any way to log 'git hash' in hydra?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1603378831587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2554944445,
        "Challenge_answer_count":6,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/105827-image.png?platform=QnA\" alt=\"105827-image.png\" \/>    <\/p>\n<p>I tried to deploy a VM to Azure Machine Learning, but I get the error message &quot;You do not have enough quota for the following VM sizes. Click here to view and request quota.&quot; And the VM cannot be deployed.    <\/p>\n<p>But I have enough quota (24 CPUs).    <\/p>\n<p>What is causing the problem?    <\/p>\n<p>I'm using Azure's Free trial plan.<\/p>",
        "Challenge_closed_time":1623772934643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437136\/cant-deploy-a-vm-on-the-azure-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2554944445,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=80eeb45c-8aa8-49ab-81df-1bb291fc79a5\">@ShoM  <\/a> ,    <\/p>\n<p>there are different quotas in Azure:    <\/p>\n<ul>\n<li> There are quotas for <code>vCPUs per Azure Region<\/code>    <\/li>\n<li> In addition there are quotas for <code>vCPUs per VM Series<\/code>    <\/li>\n<\/ul>\n<p>Both quotas (for Azure Region and VM Series) must fit the requirements.    <\/p>\n<p>It seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.    <br \/>\nYou can check your quotas by the link you marked with the red line in your screenshot.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":9.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":11.9435663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1651171215176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651128218337,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":43.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":11.9435663889,
        "Challenge_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.93,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":78.9828208333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hey Guys, i just started using wandb and so far everything is working pretty well, however I noticed that there seems to be a problem with strings as parameters in parallel coordinates chart<\/p>\n<p>Attached is a screenshot to illustrate the problem<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7.jpeg\" data-download-href=\"\/uploads\/short-url\/yfjVQxyNGXrfluwDks1707BnsO3.jpeg?dl=1\" title=\"wandb_string_problem\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg\" alt=\"wandb_string_problem\" data-base62-sha1=\"yfjVQxyNGXrfluwDks1707BnsO3\" width=\"690\" height=\"374\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1035x561.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1380x748.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb_string_problem<\/span><span class=\"informations\">1502\u00d7816 332 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>So now I wonder if I made a mistake or if I have to wait for a fix from you.<\/p>\n<p>Kind regards<br>\nChris<\/p>",
        "Challenge_closed_time":1641515228648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641230890493,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/string-bug-in-the-parallel-coordinates-chart\/1674",
        "Challenge_link_count":6,
        "Challenge_participation_count":5,
        "Challenge_readability":22.4,
        "Challenge_reading_time":25.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":78.9828208333,
        "Challenge_title":"String bug in the parallel coordinates chart",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":109,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chrismartin\">@chrismartin<\/a>,<\/p>\n<p>This issue has been fixed. Parallel Coordinate charts  with strings should behave as expected now.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.61,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.8544463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Challenge_closed_time":1593515579540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593508903533,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609467668432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.8544463889,
        "Challenge_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1593557456892,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":21.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":219.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.8749861111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p><strong>Framework: Pytorch<\/strong><br>\n<strong>wandb version : 0.13.3<\/strong><br>\n<strong>workspace: Google colab<\/strong><\/p>\n<pre><code class=\"lang-python\">config = dict(\n    dropout = 0.4,\n    train_batch = 3,\n    val_batch = 1,\n    test_batch = 1,\n    learning_rate = 0.001,\n    epochs = 5,\n    architecture = \"CNN\",\n    model_name = \"efficientnet-b0\",\n    infra = \"Colab\",\n    dataset=\"dysphagia_dataset2\"\n    )\n\n<\/code><\/pre>\n<p>My test function<\/p>\n<pre><code class=\"lang-auto\">def test_model():\n    running_correct = 0.0\n    running_total = 0.0\n    true_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data in dataloaders[TEST]:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.append(labels.item())\n            outputs = model_ft(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            pred_labels.append(preds.item())\n            running_total += labels.size(0)\n            running_correct += (preds == labels).sum().item()\n        acc = running_correct\/running_total\n    return (true_labels, pred_labels, running_correct, running_total, acc)\n\n\ntrue_labels, pred_labels, running_correct, running_total, acc = test_model()\n\n<\/code><\/pre>\n<p><strong>Error<\/strong><\/p>\n<pre><code class=\"lang-bash\">AttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-26-b7dbeaddcbbb&gt; in &lt;module&gt;\n----&gt; 1 true_labels, pred_labels, running_correct, running_total, acc = test_model()\n      2 \n\n4 frames\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    254             bins = torch.Tensor(bins_np)\n    255 \n--&gt; 256         wandb.run._log(\n    257             {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},\n    258             commit=False,\n\nAttributeError: 'NoneType' object has no attribute '_log'\n<\/code><\/pre>\n<p>This is how i initialize training:<\/p>\n<pre><code class=\"lang-python\">model_ft = train_model(model_ft, \n                       criterion, \n                       optimizer_ft,\n                       config\n                       )\n<\/code><\/pre>\n<p>my wandb init:<\/p>\n<pre><code class=\"lang-python\">wandb.init(config=config,\n           name='efficientnet0+albumentions',\n           group='pytorch-efficientnet-baseline', \n           project='dysphagia_image_classification',\n           job_type='train')\nconfig = wandb.config\n\n<\/code><\/pre>",
        "Challenge_closed_time":1662752690228,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662745940278,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/getting-attributeerror-nonetype-object-has-no-attribute-log-when-trying-to-run-test-set\/3090",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.4,
        "Challenge_reading_time":28.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1.8749861111,
        "Challenge_title":"Getting `AttributeError : 'NoneType' object has no attribute '_log' `when trying to run test set",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1176.0,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Finally caught my mistake <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<pre><code class=\"lang-auto\">model_ft._fc = nn.Sequential(\n    nn.BatchNorm1d(num_features=num_ftrs),    \n    nn.Linear(num_ftrs, 512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Linear(512, 128),\n    nn.ReLU(),\n    nn.BatchNorm1d(num_features=128),\n    nn.Dropout(p=config.dropout), # Error due to this\n    nn.Linear(128, 2),\n    )\n\nmodel_ft = model_ft.to(device)\n\n<\/code><\/pre>\n<p>I was calling my test function outside of  wandb(only used wandb for training)and wandb must have call <code>.finish<\/code> so, it must have set the my config dict:-&gt; None  as I was passing it to wandb.config.<\/p>\n<p>Now , my model class use one of the config (dropout) but I passed my config file into wandb config so, it set it to None after my model finish training. So, when my def test function use my model, the dropout hyparameter value is None now!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":13.41,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":115.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.9786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Challenge_closed_time":1638449992000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638280869000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":46.9786111111,
        "Challenge_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1301.8425,
        "Challenge_answer_count":6,
        "Challenge_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Challenge_closed_time":1662562977000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657876344000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":16.2,
        "Challenge_reading_time":101.65,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":1301.8425,
        "Challenge_title":"MLflow fails to log to a tracking server",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":588,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"cc @sgugger  I'm not the one who wrote or supports the ML Flow callback :-) @noise-field wrote the integration two years ago, do you have an idea of why it doesn't seem to work anymore @noise-field? @juliensimon, I had an error message similar (I think). I found that the issue was related to values with empty string values  (https:\/\/github.com\/mlflow\/mlflow\/issues\/6253), and it looks like there is a patch in the upcoming MLFLOW version 1.28 (not yet released)\r\n\r\nIn my case, I had to set `mp_parameters` to `None` instead of leaving it as an empty string (the default value), and I see your error message has `{'key': 'mp_parameters', 'value': ''}`.\r\n\r\nWhile later MLflow version fix will address this issue, I think setting the `mp_parameters` to `None` instead of an empty string is cleaner. However, I'm not sure about the extent of this change.\r\n\r\n OK, I'll give it a try and I'll let you know. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":15.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1396913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi team,     <br \/>\nI am learning about the quota for machine learning service and I have a general doubt.    <\/p>\n<p>I can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.    <br \/>\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.     <\/p>\n<p>so that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.    <\/p>\n<p>I am able to find out  the-  get details query but this one doesnt give me details specific to each resource-group or the workspace.    <\/p>\n<p>HTTP query -&gt; <a href=\"https:\/\/management.azure.com\/subscriptions\/%7Bsubs_id%7D\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01\">https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01<\/a><\/p>",
        "Challenge_closed_time":1667069947336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667069444447,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":13.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1396913889,
        "Challenge_title":"how to set Quota at resource group level?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7bafa6a3-9285-4b1c-a003-4490a74d05b5\">@JA  <\/a> ,    <\/p>\n<p>quotas can be set on Azure Subscription level only.    <br \/>\nThere is no option to apply quotas for different Azure Resource Groups.    <br \/>\nThere are 2 options I can see for your requirement:    <br \/>\nUse 2 Azure Subscriptions for each Resource Group    <br \/>\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8375316667,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). \nFor this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used. \n\nAre there any costs associated with having an endpoint with 0 instances? \n\nThanks!",
        "Challenge_closed_time":1666371029704,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666360814590,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668547705502,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sagemaker-endpoint-to-zero",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.8375316667,
        "Challenge_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":468.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\n1. Try using [SageMaker Serverless Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\n2. You can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\n3. There is also an option of [SageMaker asynchronous inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html) but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1666796875366,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":12.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":144.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.4716361111,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hy, I\u2019m in love with wandb, but I have a problem\u2026<\/p>\n<p>I have a simple question\u2026<\/p>\n<p>How can I analyze hyperparameters\u2026As seen in this picture, without actually creating a sweep.<\/p>\n<p>In my own code\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.1&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ9LbpPclu5VUg_KiYT_9MyY2AbgyxXn6tmqz8qoKH2kUghMnyxeJstBhkIK4wCOgqfFHueuZ6ul6juIl6zvWD3lcsPXIvZAnZatibVLxPjneVvO-xSUoWLyCpM&amp;disp=emb&amp;realattid=ii_l5aqmkag2\" alt=\"68747470733a2f2f692e696d6775722e636f6d2f5455333451465a2e706e67.png\" width=\"339\" height=\"205\"><\/p>\n<p>I\u2019m preforming learning and for every model i\u2019m sending config with hyperparams\u2026<\/p>\n<p>wandb.finish(quiet=True)<br>\nwandb.init(<br>\nentity=var.WANDB_ENTITY,<br>\nproject=f\u2019{var.version} | {var.INPUT_DATASET}',<br>\ndir=str(var.working_dir),<br>\nconfig=utils.keras.hyper_params(hp))<\/p>\n<p>But in dashboard I dont see hyperparameters dashboard\u2026 And this makes me really sad !<\/p>",
        "Challenge_closed_time":1657219039908,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657181342018,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/analyzing-hyperparameters-without-actualy-performing-a-sweep\/2719",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":21.3,
        "Challenge_reading_time":15.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.4716361111,
        "Challenge_title":"Analyzing hyperparameters without actualy performing a sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I can\u2019t see the images above, but if you would like to create a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/parallel-coordinates\">parallel coordinates plot<\/a>, you can do so using the UI by clicking \u201cadd panel\u201d in your workspace and choosing Parallel Coordinates.<\/p>\n<p>If you need to do this programmatically, one <em>very<\/em> recent feature would be to create a W&amp;B Report using our Api. You can programatically define what plots show up. It is a very new feature so it\u2019ll become better documented and more stable over time.<\/p>\n<p>Here\u2019s how you would create a Parallel Coordinates plot programmatically and save it in a report using Python.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport wandb.apis.reports as wb\napi = wandb.Api()\nproject = 'pytorch-sweeps-demo'\nwandb.require('report-editing') # this is needed as of version 0.12.21 but will likely not be needed in future.\nreport = wb.Report(\n    project=project,\n    title='Sweep Results',\n    blocks=[\n            wb.PanelGrid(panels=[\n                 wb.ParallelCoordinatesPlot(\n                     columns=[wb.reports.PCColumn('batch_size'), wb.reports.PCColumn('epoch'), wb.reports.PCColumn('loss')])\n            ], runsets=[wb.RunSet(project=project)]),\n    ]\n)\nreport.save()\n<\/code><\/pre>\n<p>This will then show up in the Reports tab on your project.<br>\nAs this is a very fresh API, there may be issues or features that are not supported yet. I do apologise if that happens to you, I\u2019ll be happy to follow up and provide help.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":18.39,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":187.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":2,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635882064000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":208.0,
        "Challenge_repo_issue_count":1067.0,
        "Challenge_repo_star_count":1173.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1620426047396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":386.5795511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Challenge_closed_time":1620426900467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619035214083,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":386.5795511111,
        "Challenge_title":"How to get multiple lines exported to wandb",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":840.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1577734207070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":422.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":6.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.5032686111,
        "Challenge_answer_count":2,
        "Challenge_body":"When calculating the cost of SageMaker Studio Notebooks in [the AWS Pricing Calculator](https:\/\/calculator.aws\/#\/addService\/SageMaker), it asks you for the \"Number of Studio Notebook instances per data scientist per month.\"\n\nHow do you reason about this? What would be the use case for having multiple instances for one data scientist? Would that happen if an individual is working on multiple projects, which have different kernels and library dependencies?\n\nI imagine most of the time it will be 1 Studio Notebook instance per data scientist per month, instead of 2 or more instances per data scientist?",
        "Challenge_closed_time":1670809472327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670602460560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1670949831716,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5kNZTb_yRR6VUo1Reg6lxg\/how-do-you-choose-the-number-of-studio-notebook-instances-per-data-scientist",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":57.5032686111,
        "Challenge_title":"How do you choose the number of Studio Notebook Instances per Data Scientist?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":214.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @yann_stoneman, you're right. Up to 4 apps can run on the same instances, so different kernels could still be run on the same instance. For example, a data scientist could be working on a tabular use case, and an image processing use case - so they might have a CPU and GPU instance running. Or they might use a larger instance for data processing or data wrangler feature. \n\nDepending on your data scientists' projects and use cases, I'd account for at most 2 instances per data scientist running concurrently. If your users already use SageMaker Notebook Instances, you can use the commonly used resource type as the Studio instance resource type for estimates - that way you can get a closer estimate to the actual costs. \n\nIf you're allowing for shared spaces (real time collaboration), include additional instances in your estimate - the users will now be able to use a private space through their user profile (unique to one user) and a shared space (this instance can be accessed across profiles). \n\nI'd also recommend using a plugin to shut down idle instances as a best practice when your teams are onboarded to Studio, so these instances are shut down if there are no notebooks actively running (ref: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/)",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1670809472327,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":16.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.66,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Challenge_closed_time":1619508955000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619492179000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.66,
        "Challenge_title":"Private github repo fail for using log_model and log_metric",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is very hard to debug, you need to check if the git repo has some more information like a hard-coded NO_OP.\n\nAlso I would check if you added the local cache folder .polyaxon to your .gitignore and .dockerignore. If this folder was added to you git repo, then indeed the metrics and artifacts will be saved to a different run.\n\nYou can also validate that the run is running with the correct run-uuid:\n\n...\nprint(tracking.TRACKING_RUN.run_uuid)\n...\n\nIf the run_uuid does not correspond to the currently running run, then the cache is bundled somewhere (git repo or docker image).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":97.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1621.4025,
        "Challenge_answer_count":6,
        "Challenge_body":"When training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and wandb enabled:\r\n```\r\nFile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\n",
        "Challenge_closed_time":1591178971000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585341922000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/287",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.04,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1621.4025,
        "Challenge_title":"wandb RuntimeError",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":104,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There does seem to be an issue with XLNet and ALBERT when using wandb. I haven't found the exact cause yet. I'll look into it again when I can. Thank you. Maybe it should be reported to wandb because it is from its `wandb.watch`?\r\nI temporarily fixed this with `wandb.watch(model, log=None)`. Does it still log the metrics when `log` is set to `None`? Yes, but without gradients. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n Was able to fix it for me with `pip install --upgrade wandb`",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":7.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":108.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":100.7332322222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Challenge_closed_time":1558879357103,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558813444520,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.3090508334,
        "Challenge_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558812981692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1559176084156,
        "Solution_link_count":1.0,
        "Solution_readability":18.7,
        "Solution_reading_time":18.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":2.2880941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Challenge_closed_time":1548960285576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548952048437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1549513217232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.2880941667,
        "Challenge_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2360.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361821819380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Puebla",
        "Poster_reputation_count":147.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1548962267732,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":13.54,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":135.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":185.8020663889,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Dear W&amp;B Community,<\/p>\n<p>I have system metrics logged like the \u201c<em>time per step<\/em>\u201d or \u201c<em>time per backward pass<\/em>\u201d for a model.<br>\nWhen doing this on different hardware, I would like to compare the effect this has on these metrics.<br>\nIn the following examples, I profile the basic Torch CIFAR10 model on a 1,2,4,8,16 and 32 CPU VM.<\/p>\n<p>When looking at a <code>Linechart<\/code>, the full history of these metrics is visible, however, it is very hard to compare them due to the overlapping and oscillation:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351.png\" data-download-href=\"\/uploads\/short-url\/zZROm2jlGDN4WUrxx2lQXAA8jYZ.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_22_16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_22_16 PM\" data-base62-sha1=\"zZROm2jlGDN4WUrxx2lQXAA8jYZ\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_22_16 PM<\/span><span class=\"informations\">3539\u00d71859 509 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>When using a <code>Barchart<\/code>, only the last value is visualized:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/48a4597177e867b3eb511112ad23b561f18f1137.png\" data-download-href=\"\/uploads\/short-url\/amCuG3pzRgnimYoyoJeru5muDMH.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_20_31 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_20_31 PM\" data-base62-sha1=\"amCuG3pzRgnimYoyoJeru5muDMH\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_20_31 PM<\/span><span class=\"informations\">3539\u00d71859 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The functionality that would be nice is to group values based on their count or occurrence, as grouping by runs already works perfectly. Here\u2019s the same data but run through <code>seaborn.barplot<\/code>:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479.png\" data-download-href=\"\/uploads\/short-url\/7VTQur5SLq8cPHTQtTqGrDuwPRn.png?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png\" alt=\"download\" data-base62-sha1=\"7VTQur5SLq8cPHTQtTqGrDuwPRn\" width=\"690\" height=\"427\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1035x640.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1380x854.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">download<\/span><span class=\"informations\">3777\u00d72341 159 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Would this be possible to implement? Or does anybody know a way to get that functionality?<\/p>\n<p>My current workaround is to download the data manually and run it through seaborn. Unfortunately, I did not understand the errors I\u2019ve gotten with the <code>Custom Chart<\/code> functionality when trying to port Vega examples to use wandb as a data basis.<\/p>\n<p>I\u2019d be very glad if anybody can point me to a tutorial on how to migrate existing Vega examples to be used with wandb (and the common problems, like differences between v3\/v4\/v5, as these seemed to be an issue for me).<\/p>",
        "Challenge_closed_time":1664274024356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663605136917,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/barchart-grouping-by-time-step-count\/3157",
        "Challenge_link_count":18,
        "Challenge_participation_count":7,
        "Challenge_readability":21.3,
        "Challenge_reading_time":80.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":185.8020663889,
        "Challenge_title":"Barchart Grouping by Time\/Step\/Count",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":796.0,
        "Challenge_word_count":367,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Alexander,<\/p>\n<p>Thanks for sending this detailed explanation! I have been exploring it and I think that the issue here is that, in lines 22, 29 and 43 you have \u201cdata\u201d: \u201ctable\u201d but as the name has been changed to \u201cwandb\u201d, then you should have \u201cdata\u201d: \u201cwandb\u201d. To solve the error between lines 4 and 6, you can use <span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span> and it is solved, but it seems that it is not affecting to the chart.<\/p>\n<pre><code>\"data\": [{ \"name\": \"wandb\" }]\n<\/code><\/pre>\n<p>Please let me know if this would be useful for you!<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.17,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":96.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":5.0310547222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When performing a single-objective optimization with <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Optuna<\/a>, the best parameters of the study are accessible using:<\/p>\n<pre><code>import optuna\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    return (x - 2) ** 2\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2.002108042}\n<\/code><\/pre>\n<p>If I want to perform a multi-objective optimization, this would be become for example :<\/p>\n<pre><code>import optuna\ndef multi_objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    f1 = (x - 2) ** 2\n    f2 = -f1\n    return f1, f2\n\nstudy = optuna.create_study(directions=['minimize', 'maximize'])\nstudy.optimize(multi_objective, n_trials=100)\n<\/code><\/pre>\n<p>This works, but the command <code>study.best_params<\/code> fails with <code>RuntimeError: The best trial of a 'study' is only supported for single-objective optimization.<\/code><\/p>\n<p>How can I get the best parameters for a multi-objective optimization ?<\/p>",
        "Challenge_closed_time":1611273369707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611255257910,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65833998",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.0310547222,
        "Challenge_title":"Best parameters of an Optuna multi-objective optimization",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2571.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588846413276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":108.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>In multi-objective optimization, you often end up with more than one best trial, but rather a set of trials. This set if often referred to as the Pareto front. You can get this Pareto front, or the list of trials, via <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.Study.html#optuna.study.Study.best_trials\" rel=\"noreferrer\"><code>study.best_trials<\/code><\/a>, then look at the parameters from each individual trial i.e. <code>study.best_trials[some_index].params<\/code>.<\/p>\n<p>For instance, given your directions of minimizing <code>f1<\/code> and maximizing <code>f2<\/code>, you might end up with a trial that has a small value for <code>f1<\/code> (good) but at the same time small value for <code>f2<\/code> (bad) while another trial might have a large value for both <code>f1<\/code> (bad) and <code>f2<\/code> (good). Both of these trials could be returned from <code>study.best_trials<\/code>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":12.09,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":115.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.0502777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, \r\n\r\nI'm using `wandb` logger (and `csv` as well), I found recently `hydra` config no longer save to `wandb` 's`config.yaml` file.\r\nBefore:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.EarlyStopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.ModelCheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichModelSummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichProgressBar\r\nckpt_path:\r\n  desc: null\r\n  value: None\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.MNISTDataModule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.MNISTLitModule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.SimpleDenseNet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.Adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.ReduceLROnPlateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.Trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nNow:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nThis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#L172\r\nAny idea how to restore to previous state?",
        "Challenge_closed_time":1670781696000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670626715000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/478",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":16.9,
        "Challenge_reading_time":71.23,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":43.0502777778,
        "Challenge_title":"Question: How to save hydra config to wandb config.yaml",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":551,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.4975,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Challenge_closed_time":1663248037000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663015846000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":16.2,
        "Challenge_reading_time":63.87,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":64.4975,
        "Challenge_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":522,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":49.3241666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Challenge_closed_time":1603665805000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603488238000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":11.05,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":49.3241666667,
        "Challenge_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.2159175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to tune hyperparameters similar to the following guide: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters<\/a>    <\/p>\n<p>My PyTorch Dataset in train.py contains:    <\/p>\n<pre><code>ws = Workspace.from_config()  \nds = Dataset.get_by_name(ws, 'train')  \ndf = ds.to_pandas_dataframe()  \n<\/code><\/pre>\n<p>This code works fine when run from the command-line, but when I submit a hyperparam tuning job to each node of a training cluster, I get the following error:    <\/p>\n<blockquote>\n<p>We could not find config.json in: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/adamml\/azureml\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/azureml\/HD_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.    <\/p>\n<\/blockquote>\n<p>If I manually pass my subscription id, resource group, and workspace id, I don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. Is there a way to do a non-interactive login?<\/p>",
        "Challenge_closed_time":1595534742240,
        "Challenge_comment_count":4,
        "Challenge_created_time":1595346764937,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/50386\/non-interactive-login-to-registered-dataset",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.3,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":52.2159175,
        "Challenge_title":"Non-interactive login to registered dataset",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":137,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>If I read the post correctly, you were trying to get an registered dataset within a submitted run. There, Workspace.from_config() won't work since there is no config.json file as the error suggested.    <\/p>\n<p>And when you created an auth object which is InteractiveLoginAuth, it is expected to perform interactive login.    <\/p>\n<p>Within a run the recommended way to connect to current workspace it via:    <\/p>\n<pre><code>   from azureml.core import Run  \n   run = Run.get_context().experiment.workspace  \n<\/code><\/pre>\n<p>Meanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. If that fit your scenario better, please refer to the example in this document <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets<\/a>    <\/p>\n<pre><code>   from azureml.core import Dataset, Run  \n     \n   run = Run.get_context()  \n   # get the input dataset by name  \n   dataset = run.input_datasets['titanic']  \n     \n   # load the TabularDataset to pandas DataFrame  \n   df = dataset.to_pandas_dataframe()  \n<\/code><\/pre>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":15.54,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.9051222222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing run data keys in several of my runs. Specifically, I have logged a metric in my code, the metric is tracked in the online wandb UI, but when I try accessing the data using the following code<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nrun = api.run(\"xxxxxx\")\nrun.history()[['_step', 'metric_name']]\n<\/code><\/pre>\n<p>It throws a <code>KeyError: \"['metric_name'] not in index\"<\/code>.<\/p>\n<p>When I print out <code>run.history()<\/code> in table format, it does show \u2018metric_name\u2019 as one of the columns; \u2018metric_name\u2019 also appears as a key in <code>run.summary<\/code>. I wonder what is the issue here?<\/p>\n<p>Would appreciate any help. Thank you!<\/p>",
        "Challenge_closed_time":1670943419675,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670785361235,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-access-run-data-via-run-history\/3530",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.9051222222,
        "Challenge_title":"Cannot access run data via run.history()",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":126.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/toruowo\">@toruowo<\/a> thank you for writing in! Can you please change your last line to:<\/p>\n<pre><code class=\"lang-auto\">run.history(keys=['_step', 'metric_name'])\n<\/code><\/pre>\n<p>Would this work for you?  Please let me know if you have any further issues or questions. You may also find some more <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#public-api-examples\">API examples here<\/a> if that helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.0455230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1660860269032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":39.0455230556,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":143.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":6.0792997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Challenge_closed_time":1659991938292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659970052813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":54.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":6.0792997222,
        "Challenge_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":339,
        "Platform":"Stack Overflow",
        "Poster_created_time":1659969003172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":88.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.5714147222,
        "Challenge_answer_count":10,
        "Challenge_body":"<p>It would be great to plot hparams without doing sweeps, most of the time I\u2019m doing experiments and I would love the plot to be across runs and not as a sweep. It might be complex to make this feature automated, but I\u2019m fine if it\u2019s within one run, would be great to have something like <code>wandb.plots.ParallelCoordinates<\/code><\/p>",
        "Challenge_closed_time":1671425374288,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671394517195,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-create-parallel-coordinates-plot-without-sweeps\/3566",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.5714147222,
        "Challenge_title":"How to create Parallel Coordinates plot without sweeps",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":719.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Challenge_closed_time":1591023634000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588434434000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.8,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":719.2222222222,
        "Challenge_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":171,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ceyzaguirre4 pls ^^",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":0.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":18.1665508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Challenge_closed_time":1660802705056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660737627797,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":18.0770163889,
        "Challenge_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620645674183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1660803027380,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":22.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":182.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1340284487940,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":151.2120097222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Challenge_closed_time":1544569877892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544027022437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":150.7931819444,
        "Challenge_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3875.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1544571385672,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.51,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.6752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Challenge_closed_time":1576023863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575967432000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.6752777778,
        "Challenge_title":"Comet PAPI Depreciated",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":44,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1424791933163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2483.0,
        "Answerer_view_count":192.0,
        "Challenge_adjusted_solved_time":1.1469897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to optimize a custom model (no fancy ML whatsoever) that has 13 parameters, 12 of which I know to be normally distributed. I've gotten decent results using the <code>hyperopt<\/code> library:<\/p>\n<pre><code>space = {\n    'B1': hp.normal('B1', B1['mean'], B1['std']),\n    'B2': hp.normal('B2', B2['mean'], B2['std']),\n    'C1': hp.normal('C1', C1['mean'], C1['std']),\n    'C2': hp.normal('C2', C2['mean'], C2['std']),\n    'D1': hp.normal('D1', D1['mean'], D1['std']),\n    'D2': hp.normal('D2', D2['mean'], D2['std']),\n    'E1': hp.normal('E1', E1['mean'], E1['std']),\n    'E2': hp.normal('E2', E2['mean'], E2['std']),\n    'F1': hp.normal('F1', F1['mean'], F1['std']),\n    'F2': hp.normal('F2', F2['mean'], F2['std'])\n}\n<\/code><\/pre>\n<p>where I can specify the shape of the search space per parameter to be normally distributed.<\/p>\n<p>I've got 32 cores and the default <code>Trials()<\/code> object only uses one of them.  <code>Hyperopt<\/code> suggests two ways to parallelize the search process, both of which I could not get to work on my windows machine for the life of me, so I've given up and want to try a different framework.<\/p>\n<p>Even though Bayesian Hyper Parameter Optimization as far as I know is based on the idea that values are distributed according to a distribution, and the normal distribution is so prevalent that it is literally called normal. I cannot find a way to specify to <code>Optuna<\/code> that my parameters have a <code>mean<\/code> and a <code>standard deviation<\/code>.<\/p>\n<p>How do i tell <code>Optuna<\/code> the <code>mean<\/code> and <code>standard deviation<\/code> of my parameters?<\/p>\n<p>The only distributions I can find in the documentation are: <code>suggest_uniform()<\/code>, <code>suggest_loguniform()<\/code> and <code>suggest_discrete_uniform()<\/code>.<\/p>\n<p>Please tell me if I am somehow misunderstanding loguniform distrubution (It looks somewhat similar, but I can't specify a standard deviation?) or the pruning process.<\/p>\n<p>As you might be able to tell from my text, I've spent a frustrating amount of time trying to figure this out and gotten exactly nowhere, any help will be highly appreciated!<\/p>\n<p>Special thanks to dankal444 for this elegant solution (i will replace the mean and std with my own values):<\/p>\n<pre><code>from scipy.special import erfinv\nspace = {\n    'B1': (erfinv(trial.suggest_float('B1', -1, 1))-mean)*std,\n    'B2': ...\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1636670797220,
        "Challenge_comment_count":2,
        "Challenge_created_time":1636666668057,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636679968800,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69935219",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":31.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":1.1469897222,
        "Challenge_title":"How to search a set of normally distributed parameters using optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":322,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349369044408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":380.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>You can cheat <code>optuna<\/code> by using uniform distribution and transforming it into normal distribution. To do that one of the method is <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.erfinv.html\" rel=\"nofollow noreferrer\">inversed error function<\/a> implemented in <code>scipy<\/code>.<\/p>\n<p>Function takes uniform distribution from in range &lt;-1, 1&gt; and converts it to standard normal distribution<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import special\n\n\nx = np.linspace(-1, 1)\nplt.plot(x, special.erfinv(x))\nplt.xlabel('$x$')\nplt.ylabel('$erf(x)$')\n\nmean = 2\nstd = 3\nrandom_uniform_data = np.random.uniform(-1 + 0.00001, 1-0.00001, 1000)\nrandom_gaussianized_data = (special.erfinv(random_uniform_data) - mean) * std\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\naxes[0].hist(random_uniform_data, 30)\naxes[1].hist(random_gaussianized_data, 30)\naxes[0].set_title('uniform distribution samples')\naxes[1].set_title('erfinv(uniform distribution samples)')\nplt.show()\n<\/code><\/pre>\n<p>This is how the function looks like:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/E363H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E363H.png\" alt=\"inverse error function\" \/><\/a><\/p>\n<p>And below example of transforming of uniform distribution into normal with custom mean and standard deviation (see code above)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rspUX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rspUX.png\" alt=\"transforming uniform to normal distribution\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.7,
        "Solution_reading_time":20.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":143.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.9730555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that [in EC2 it's possible][1], however I don't see it mentioned in Amazon SageMaker documentation.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/elastic-inference\/latest\/developerguide\/basics.html",
        "Challenge_closed_time":1604506639000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604477936000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926687134,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sagemaker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":4.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.9730555556,
        "Challenge_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, they cant be; multi-attach is only supported with EC2.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612946445792,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660651109000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":161,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":396.3547583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Challenge_closed_time":1652802212843,
        "Challenge_comment_count":5,
        "Challenge_created_time":1651375335713,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":23.4,
        "Challenge_reading_time":91.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":396.3547583334,
        "Challenge_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":467,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.5,
        "Solution_reading_time":27.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":180.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1428499037932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leipzig, Deutschland",
        "Answerer_reputation_count":2124.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":1.1770155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1658393400156,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658389162900,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1770155556,
        "Challenge_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643710211767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.93,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":459.2678666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an experiment to create recommendations (using the Movie Ratings sample database), but without using the ratings. I simply consider that if a user has rated certain movies, then he would be interested by other movies that have been rated by users that have also rated his movies.<\/p>\n\n<p>I can consider, for instance, that ratings are 1 (exists in the database) or 0 (does not exist), but in that case, how do I transform the initial data to reflect this?<\/p>\n\n<p>I couldn't find any kind of examples or tutorials about this kind of scenario, and I don't really know how to proceed. Should I transform the data before injecting it into an algorithm? And\/or is there any kind of specific algorithm that I should use?<\/p>",
        "Challenge_closed_time":1471354453808,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469698571557,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1469701089488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38632533",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":459.9672919444,
        "Challenge_title":"Recommendations without ratings (Azure ML)",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":910.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461857379436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lille, France",
        "Poster_reputation_count":133.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are<\/em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)<\/p>\n\n<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:<\/p>\n\n<pre><code>library(dplyr)\ndf &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs\nall_movies &lt;- unique(df[['movie']])\nall_users &lt;- unique(df[['user']])\nn &lt;- 30  # number of random pairs to start with\n\nnegative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),\n                                    user = sample(all_users, n, replace=TRUE),\n                                    rating = rep(0, n))          \nacceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))\ndf &lt;- rbind(df, acceptable_negative_observations)\nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>\n\n<p>Alternatively, you could try a method like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning\" rel=\"nofollow\">association rule learning<\/a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Frequently-bought-together-market-basket-analyses-using-ARULES-1\" rel=\"nofollow\">nice example<\/a> of how to do this in R\/AML in the Cortana Intelligence Gallery.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":167.8033111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been testing some small examples with MLflow tracking but for my usecase I would like to have the weights saved after each epoch. \nSometimes I kill the runs before they are completely finished (I cannot use earlystopping), but what I experience now is that the weights do not get saved to the tracking ui server.\nIs there a way to do this after each epoch?<\/p>",
        "Challenge_closed_time":1572525493020,
        "Challenge_comment_count":2,
        "Challenge_created_time":1571921401100,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58541794",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":167.8033111111,
        "Challenge_title":"MLflow saving weights after each epoch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534367348472,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":344.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints\/weights are saved to disk, you can log them with <code>mlflow_log_artifact()<\/code> or <code>mlflow_log_artifacts()<\/code>.  From the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#logging-functions\" rel=\"nofollow noreferrer\">docs<\/a>,<\/p>\n\n<blockquote>\n  <p><strong>mlflow.log_artifact()<\/strong> logs a local file or directory as an artifact,\n  optionally taking an artifact_path to place it in within the run\u2019s\n  artifact URI. Run artifacts can be organized into directories, so you\n  can place the artifact in a directory this way.<\/p>\n  \n  <p><strong>mlflow.log_artifacts()<\/strong> logs all the files in a given directory as\n  artifacts, again taking an optional artifact_path.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":10.15,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":330.3667077778,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I make the following sweep (yaml) file:<\/p>\n<pre><code class=\"lang-auto\">program: train_mnist.py\nmethod: grid\nparameters:\n  lr_schedule:\n    values: [ step, cyclic ]\n  epoch_total:\n    values: [ 2, 4 ]\nmetric:\n  goal: maximize\n  name: test-result\/accuracy\nproject: my-mnist-test-project\nname: MNIST-Sweep-Test\ndescription: test sweep demo\n<\/code><\/pre>\n<p>and I use <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/advanced-sweeps\/local-controller#running-the-local-controller-from-the-command-line\">local controller<\/a> to perform sweep locally. However, it seems block here:<\/p>\n<pre><code class=\"lang-auto\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Created sweep with ID: o2mzl569\nwandb: View sweep at: https:\/\/wandb.ai\/geyao\/my-mnist-test-project\/sweeps\/o2mzl569\nwandb: Run sweep agent with: wandb agent geyao\/my-mnist-test-project\/o2mzl569\nwandb: Starting wandb controller...\nSweep: o2mzl569 (grid) | Runs: 0\n\n# ------blocked here!------\n<\/code><\/pre>\n<p>When I turn off the network, it will be:<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Network error (ConnectionError), entering retry loop.\n<\/code><\/pre>\n<p>Why local controller tries to connect the network? How can I perform local sweep with\/without network in the right way?<\/p>",
        "Challenge_closed_time":1661992753334,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660803433186,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/local-controller-seems-block\/2955",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.1,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":330.3667077778,
        "Challenge_title":"Local controller seems block",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":340.0,
        "Challenge_word_count":148,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , the local controller doesn\u2019t have the full functionality of W&amp;B cloud, and is not intended for actual hyperparameter optimization workloads. It\u2019s intended for development and debugging of new algorithms for the Sweeps tool. You don\u2019t need to connect to W&amp;B cloud service to use the controller.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":4.54,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":86.4928975,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>How do I create a separate API so that I can log metrics from test pipelines? It doesn\u2019t make sense to use a personal key for that.<\/p>",
        "Challenge_closed_time":1675687046564,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675375672133,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/if-im-logging-metrics-for-mlops-from-a-test-pipeline-how-do-i-create-a-separate-api-key-for-that\/3803",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":86.4928975,
        "Challenge_title":"If I'm logging metrics for MLOps from a test pipeline, how do I create a separate api key for that?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":4.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":179.8598372222,
        "Challenge_answer_count":3,
        "Challenge_body":"<pre><code class=\"lang-auto\">wandb.log(train_stats, step=train_step)\nwandb.log(test_stats, step=test_step)\n<\/code><\/pre>\n<p>In my code, training and testing happen in parallel. I get training stats at every step, but testing stats are available only once in a while (I don\u2019t pause training to wait for testing results). For instance, I may get testing stats at steps [10, 12, 20, 33].<br>\nThe problem with this is that above commands doesn\u2019t work if <code>test_step<\/code> and <code>train_step<\/code> are not the same. Only the first <code>log<\/code> succeeds, and the second is not logged at all (no error, the program keeps running). If I pass the same step, e.g., <code>wandb.log(test_stats, step=train_step)<\/code> then it works.<\/p>\n<p>Is it possible to log stats with different steps?<\/p>",
        "Challenge_closed_time":1684432077048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683784581634,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/log-stats-with-different-global-steps\/4375",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":10.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":179.8598372222,
        "Challenge_title":"Log stats with different global steps",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/parisi\">@parisi<\/a> ,<\/p>\n<p>You\u2019re welcome! I\u2019m glad to hear that <code>define_metric<\/code> is working well for your use case. Unfortunately, there isn\u2019t a direct method to hide the <code>steps charts<\/code> in the SDK. However, you can drag the steps chart to the hidden panel section of your main\/run workspace. By doing so, any subsequent runs logged will automatically have the steps chart hidden from view. Marking initial inquiry resolved on our end, but please do reach out again anytime we could be of help.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":6.92,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593379921000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1359957233372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2.74399,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Challenge_closed_time":1548282026927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548272148563,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.74399,
        "Challenge_title":"How can I quickly debug a SageMaker training script?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1902.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.7,
        "Solution_reading_time":13.03,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.1185691666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Challenge_closed_time":1637159193836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637158421443,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637158766987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2145536111,
        "Challenge_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586517832390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":34.3028483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1643782095707,
        "Challenge_comment_count":9,
        "Challenge_created_time":1643658605453,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645284643350,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Challenge_link_count":2,
        "Challenge_participation_count":10,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":34.3028483334,
        "Challenge_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":514.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622195346030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":9.0,
        "Solution_reading_time":29.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":255.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.3083333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Challenge_closed_time":1626409391000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626357881000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":11.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.3083333333,
        "Challenge_title":"mlflow run_name unexpected argument error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"That's because you are looking at the docs under \"latest\" which is the development version. On the master branch, there is a run_name argument but 1.3.x does not have that. You are probably using Lightning 1.3.x. \r\nIf you want, you can install Lightning rc0 to test the features before the 1.4 release.   Here are the docs for the stable version (1.3.x) https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html Okay got it. Thanks for clarifying ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7810055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1599436122230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599411710610,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1599438192360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":46.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.7810055556,
        "Challenge_title":"Run.get_context() gives the same run id",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2523.0,
        "Challenge_word_count":324,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":57.41,
        "Solution_score_count":7.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":417.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8547805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I manually stop a run in a sweep so that the sweep agent will just continue with the next run?<\/p>",
        "Challenge_closed_time":1654829989272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654794512062,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/can-i-manually-cancel-a-run-in-a-sweep\/2583",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":1.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.8547805556,
        "Challenge_title":"Can I manually cancel a run in a sweep?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fryderykkogl\">@fryderykkogl<\/a> ,<\/p>\n<p>Yes, you can manually stop runs directly from the webUI. In the sweeps run table, select the options menu for the run you want to stop, and select  <code>stop run<\/code>. The sweep will continue to the next run configuration. See this image for reference. Please let us know if you have any followup questions.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc.png\" data-download-href=\"\/uploads\/short-url\/bC6fK8EPrbIPohDGGY4pPBG24Ec.png?dl=1\" title=\"Screen Shot 2022-06-09 at 7.57.28 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png\" alt=\"Screen Shot 2022-06-09 at 7.57.28 PM\" data-base62-sha1=\"bC6fK8EPrbIPohDGGY4pPBG24Ec\" width=\"394\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_394x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_591x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_788x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/51667cd84dc068e0d2d43d2997688afbb76431fc_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-06-09 at 7.57.28 PM<\/span><span class=\"informations\">952\u00d71206 94.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":19.5,
        "Solution_reading_time":26.34,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":115.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":91.8788925,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Challenge_closed_time":1564380155503,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564049391490,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1564533527096,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57199472",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":91.8788925,
        "Challenge_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":7689.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393062808772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":133.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":7.92,
        "Solution_score_count":9.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.1534905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Challenge_closed_time":1636391832512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636160879946,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":64.1534905556,
        "Challenge_title":"Using Wandb with HParams on TF",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just had a chat with the support and figured out how to fix the problem with over-writing.<\/p>\n<p>Issue was with the init function and there is a flag for reinitializing (<code>reinit=True<\/code>)<\/p>\n<p><code>wandb.init(project=\"newjob1\", entity=\"demou\",reinit=True)<\/code>  this fixed this issue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1336700249823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":885.0,
        "Answerer_view_count":127.0,
        "Challenge_adjusted_solved_time":2447.0429822222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Challenge_closed_time":1551699854476,
        "Challenge_comment_count":2,
        "Challenge_created_time":1542890499740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2447.0429822222,
        "Challenge_title":"Sacred - pass all parameters as one",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":735.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443016881603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":9385.0,
        "Poster_view_count":1033.0,
        "Solution_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":5.35,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":33.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1457261731840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC",
        "Answerer_reputation_count":584.0,
        "Answerer_view_count":270.0,
        "Challenge_adjusted_solved_time":11057.4897425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Challenge_closed_time":1567514274983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527707311910,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":11057.4897425,
        "Challenge_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1130.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.1,
        "Solution_reading_time":11.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":1,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600305917000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.9419444444,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Challenge_closed_time":1602141183000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602116192000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":18.0,
        "Challenge_reading_time":59.98,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":6.9419444444,
        "Challenge_title":"mlflow logger complains about missing run_id",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Here, mlflow logger is actually an MlflowClient object. so you ll need to use the function calls specified in this doc - https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/tracking\/client.html . These functions needs run_id as first argument which can be accessed as self.logger.run_id @nazim1021 thx for clarification! @qianyu-berkeley feel free to reopen if needed... Thanks! Same problem here, working with @nazim1021 suggestion. What about adding it to the doc? > What about adding it to the doc?\r\n\r\ngood idea, mind send a PR? :]",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.0,
        "Solution_reading_time":7.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":1.5477430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Challenge_closed_time":1506100257932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506094686057,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1514341154200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":9.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.5477430556,
        "Challenge_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506066897167,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1513514205487,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.41,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.2286933333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>Say I have a run that had a good result, and I want to re-run it. What would be the recommended way to do this? How can I download a previous run\u2019s config values to create a new run with the same config?<\/p>\n<p>Thanks, Carlos<\/p>",
        "Challenge_closed_time":1645540745456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645381522160,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/loading-config-values-of-a-previous-run-to-reproduce-it\/1955",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":44.2286933333,
        "Challenge_title":"Loading config values of a previous run to reproduce it",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Carlos, you can get the config by using our Public API. Here is a code snippet you can use:<\/p>\n<p>import wandb<br>\napi = wandb.Api()<\/p>\n<p>run = api.run(\"\/\/&lt;run_id&gt;\")<br>\nrun.file(\u201cconfig.yaml\u201d).download()<\/p>\n<p>You can also use the W&amp;B Launch feature which is in beta at the moment. Here is a <a href=\"https:\/\/docs.wandb.ai\/guides\/launch\">link<\/a> to the documentation.<\/p>\n<p>Best,<br>\nArman<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.2,
        "Solution_reading_time":5.35,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1367358004727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":6421.0,
        "Answerer_view_count":642.0,
        "Challenge_adjusted_solved_time":11.8625777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Challenge_closed_time":1582835892307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582793187027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":11.8625777778,
        "Challenge_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":869.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":215.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":602.3895386111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Challenge_closed_time":1524806701632,
        "Challenge_comment_count":1,
        "Challenge_created_time":1522638099293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":15.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":602.3895386111,
        "Challenge_title":"Azure Machine Learning Studio vs. Workbench",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":3387.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434736108840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dallas, TX, United States",
        "Poster_reputation_count":2045.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":10.52,
        "Solution_score_count":6.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3142.3483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1670522892000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659210438000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":47.39,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":3142.3483333333,
        "Challenge_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":420,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@Yard1 Can you give me a hand here? I ended up spending a lot of time in figuring out where is it coming from. The names inside `containers\/regression.py` seems to be fine but even then the run name is wrong.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/204138706-db0d0cc3-9a08-46d3-8037-fbaee414876b.png)\r\n\r\nAny ideas?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.6558561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi there! I was wondering, how do I deal with having multiple variables to log, but one of those variables I only want to log every 100 timesteps? The wandb docs seem to suggest that I need to collect all my metrics into one log function call, but in my scenario above where I want to track one variable every step and another variable every 100 steps, I would need multiple log calls. I saw the docs for the define metrics function, but I\u2019m not quite sure if that\u2019s the way to handle this. How do I approach this in PyTorch? Thanks!<\/p>\n<p>As an example, I currently have this Tensorboard logging that I\u2019m trying to convert to wandb:<\/p>\n<pre><code class=\"lang-auto\">print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\nwriter.add_scalar(\"charts\/episodic_return\", info[\"episode\"][\"r\"], global_step)\n\nif global_step % 100 == 0:\n    writer.add_scalar(\n        \"losses\/qf1_values\", qf1_a_values.mean().item(), global_step\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1673569740548,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673362179466,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-two-variables-at-different-increments-of-timesteps\/3674",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":57.6558561111,
        "Challenge_title":"How to log two variables at different increments of timesteps?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":138.0,
        "Challenge_word_count":142,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chulabhaya\">@chulabhaya<\/a> , happy to help. The approach you are considering is correct. You can set a check in place and log a dictionary with the values you want and set the step value.<\/p>\n<pre><code class=\"lang-auto\">for i in range (300):\n    if i%100==0:\n        wandb.log({\"value\": i, \"value\": 100}, step =i)\n    else:\n        wandb.log({\"value\": 100})\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">defined metric<\/a> function allows you to have more control over the representation of your x axis and also how that axes is incremented. There are a few examples listed in the linked doc on how it functions. Please let me know if you have any questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":9.48,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":473.2511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Challenge_closed_time":1634716886000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633013182000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":473.2511111111,
        "Challenge_title":"raise Error if pre-initialized DVC option is being changed",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586647457000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":88.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603479422000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":688.1130463889,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1536872853240,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Copenhagen, Denmark",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":4.0756455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to fit XGBClassifier to my dataset after hyperparameter tuning using optuna and I keep getting this warning:<\/p>\n<blockquote>\n<p>the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'<\/p>\n<\/blockquote>\n<p>Below is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#XGBC MODEL\nmodel = XGBClassifier(random_state = 69)\n\ncross_rfc_score = -1 * cross_val_score(model, train_x1, train_y,\n                           cv = 5, n_jobs = -1, scoring = 'neg_mean_squared_error')\nbase_rfc_score = cross_rfc_score.mean()\n<\/code><\/pre>\n<p>But if I use Optuna and then fit the obtained parameters it gives me the warning. Below is the code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01)\n    n_estimators = trial.suggest_int('n_estimators', 10, 500)\n    sub_sample = trial.suggest_float('sub_sample', 0.0, 1.0)\n    max_depth = trial.suggest_int('max_depth', 1, 20)\n\n    params = {'max_depth' : max_depth,\n           'n_estimators' : n_estimators,\n           'sub_sample' : sub_sample,\n           'learning_rate' : learning_rate}\n\n    model.set_params(**params)\n\n    return np.mean(-1 * cross_val_score(model, train_x1, train_y,\n                            cv = 5, n_jobs = -1, scoring = 'neg_mean_squared_error'))\n\nxgbc_study = optuna.create_study(direction = 'minimize')\nxgbc_study.optimize(objective, n_trials = 10)\n\nxgbc_study.best_params\noptuna_rfc_mse = xgbc_study.best_value\n\nmodel.set_params(**xgbc_study.best_params)\nmodel.fit(train_x1, train_y)\nxgbc_optuna_pred = model.predict(test_x1)\nxgbc_optuna_mse1 = mean_squared_error(test_y, xgbc_optuna_pred)\n<\/code><\/pre>\n<p>The full warning is:<\/p>\n<blockquote>\n<p>Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.<\/p>\n<\/blockquote>\n<p>I want <code>MSE<\/code> as my metric of choice.<\/p>",
        "Challenge_closed_time":1628083548827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628068876503,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1628327539816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68648689",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":26.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":4.0756455556,
        "Challenge_title":"The default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5691.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1614760458072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":396.0,
        "Poster_view_count":70.0,
        "Solution_body":"<p>Just as described <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\" rel=\"noreferrer\">here<\/a>, try to add <code>eval_metric<\/code> to your <code>.fit<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model.fit(train_x1, train_y, eval_metric='rmse')\n<\/code><\/pre>\n<p>as optimizing <code>rmse<\/code> and <code>mse<\/code> is leading towards the same results.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.0,
        "Solution_reading_time":5.27,
        "Solution_score_count":6.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1466.8491666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'mlflow.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nThose two linting functions caused the template create WFs (and sometimes even local) to fail\r\n\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThey should pass. We should discuss why they fail and how to fix!\r\nSo currently they are outcommented!\r\n",
        "Challenge_closed_time":1613430703000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608150046000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/171",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1466.8491666667,
        "Challenge_title":"subprocess.call and mlflow.log_artifact checks inconsistent in linter",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@Emiller88 the linter should for all templates just check that these methods are called in the templates. Ideally you just need to add those two lines to the linter checks.\r\n\r\nI won't explain the original issue here since I just expect it to work :) If it still doesn't I will reassign @Imipenem and me.\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.0741666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Challenge_closed_time":1654420353000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654326486000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.4,
        "Challenge_reading_time":61.14,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":210,
        "Challenge_solved_time":26.0741666667,
        "Challenge_title":"wandb logger not working",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":584,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"`wandb-callbacks` haven't been maintained for a while and it might not work correctly with recent lightning and hydra releases. \r\n\r\nHave you trained using the `main` branch?\r\n\r\nI'm preparing new release and will fix the callbacks when it's ready https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/308\r\n So i managed to get it working using a fresh conda environment: \r\ntorch==1.10.0 with CUDA10.2\r\npytorch-lightning==1.6.4\r\nwandb == 0.12.17\r\n\r\nI doesnt check if all the callbacks work properly but my initial problem is solved. Thank you for your help! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":77.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.045,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, simple Q. I want to launch a tensorboard with the tensorboard profiler pip installed from the GUI. At the moment I am using this:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"uuid: XX\"\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nHowever I want to runPatch it so that it installs:\n\npip install -U tensorboard-plugin-profile\n\nIs there a way to easily do this?",
        "Challenge_closed_time":1651747435000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651747273000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1502",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.045,
        "Challenge_title":"How to patch a multi-run downstream operation, for example tensorboard:multi-run",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can add the following runPatch:\n\n...\npatchStrategy: replace\nrunPatch:\n  container:\n    command: [\"bash\", \"-c\"]\n    args:\n      - \"pip install -U tensorboard-plugin-profile && tensorboard --logdir={{globals.artifacts_path}} --port={{globals.ports[0]}} --path_prefix={{globals.base_url}} --host=0.0.0.0\"\n\nFor the specific case of tensorboard, we will add a new input plugins of type List[str] to the tensorboard component versions , so instead of patching the component, users can pass a parameter:\n\nparams:\n  plugins: { value: [tensorboard-plugin-profile, tensorboard-plugin-custom, ...] }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":7.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":131.5469241667,
        "Challenge_answer_count":10,
        "Challenge_body":"<p>Hi,<br>\nI\u2019m currently working on a self-supervised representation learning project, and to evaluate the quality of my models I train a linear classifier on the outputs of my (frozen) trained encoder and look at the downstream classification accuracy.<\/p>\n<p>This evaluation procedure is done separately from the training of the encoder, however is there still a way to add the metrics computed during this evaluation phase to the standard metrics I log during the training phase, in the same run panel?<\/p>\n<p>More generally, can I add metrics to a run that is already finished?<\/p>\n<p>Thanks a lot!<\/p>",
        "Challenge_closed_time":1674087539911,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673613970984,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/log-custom-metrics-for-a-run-outside-of-the-training-loop\/3696",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":131.5469241667,
        "Challenge_title":"Log custom metrics for a run outside of the training loop",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":234.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a> , appreciate your your additional feedback.<\/p>\n<p>This approach of first logging , <code>loss<\/code>, to a run, then revisiting\/resuming a run to log different metric, <code>accuracy<\/code>, starting from <strong>step zero<\/strong> again is not supported. The wandb logging step must be monotonically increasing in each call, otherwise the <code>step<\/code> value is ignored during your call to <code>log()<\/code>. Now if you are not interested in logging accuracy at step 0, you <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#resuming-guidance\">could resume<\/a> the previously finished run using its un id and log additional metrics to the run. this however is problematic as the new metric is logged starting at the last known\/registered step for the run.<\/p>\n<p>One approach to get around the issue you are running into  is to assign each of the runs to a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\">specific group<\/a>. Example set <code>group = version_0<\/code> for any runs that logs metrics for this specific version of the model. You could then set grouping in the workspace to help with tracking  the different metrics for each experiment, <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Group-Viz-Test\/groups\/L2\/workspace?workspace=user-mohammadbakir\">see this example workspace<\/a>.<\/p>\n<p>Hope this helps and please let us know if you have additional questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":18.63,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":183.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.4997222222,
        "Challenge_answer_count":0,
        "Challenge_body":"I have setup a community self-hosted polyaxon, and the config abourt ui is\n\nui:\n  enabled: true\n  offline: false\n  adminEnabled: true\n\n\naccording to the documentation, if I set adminEnabled to true, there should be an admin dashboard. But I did not find it, there is no difference to turn it on\/off",
        "Challenge_closed_time":1648196727000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648187728000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1460",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.4997222222,
        "Challenge_title":"Does Community UI has Admin dashboard?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The admin page is under http:\/\/localhost:8000\/_admin\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":0.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":2068.9864736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Challenge_closed_time":1506066553020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506066265147,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1506066568087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.0799647222,
        "Challenge_title":"How to configure comet (comet.ml) to track Keras?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1208.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1513514919392,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":61.4036480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Challenge_closed_time":1658304934100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657892681357,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658083880967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":30.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":114.5146508333,
        "Challenge_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.8672141667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Challenge_closed_time":1640309961152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640245639181,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":13.8,
        "Challenge_reading_time":10.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8672141667,
        "Challenge_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":254.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.08,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.8330322222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,   <br \/>\nour team is using AzureML for company's Machine Learning project.  <\/p>\n<p><strong>My question is this:<\/strong>  <\/p>\n<blockquote>\n<p>What's the <strong>price difference<\/strong> between <strong>AzureML<\/strong> and <strong>AzureVM<\/strong> when executing python script?  <\/p>\n<\/blockquote>\n<p>if we use AzureVM, we may use Azure Registry together, i think.    <\/p>\n<p>Because our team is newbie in Azure, we are unfamiliar with this price policy  <\/p>\n<p>thanks.  <\/p>",
        "Challenge_closed_time":1625239530936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625211332020,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/460862\/price-difference-between-azure-ml-vs-azure-vm",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.8330322222,
        "Challenge_title":"Price Difference between Azure ML vs Azure VM",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2088fd6d-27cc-47a6-9fa7-93bb2a8db8a2\">@\ubc15\uc601\ubbfc(Park Young Min)\/\ube44\uc804)DX\ud300  <\/a> Thank you for your query!!!    <\/p>\n<p>Depending upon your requirement there are different possibilities.    <\/p>\n<p>Now as per your statement you want to understand cost associated with Azure ML and Azure VM for running Python Script.    <\/p>\n<p>Now as such for Machine Learning on Azure the Machine Learning surcharges are free and you are only charged for series of VM you use.    <\/p>\n<p>It has been best described in example <a href=\"https:\/\/azure.microsoft.com\/en-in\/pricing\/details\/machine-learning\/#purchase-options\">here<\/a>:    <\/p>\n<p>You will be billed daily. For billing purposes, a day commences at midnight UTC. Bills are generated monthly.    <\/p>\n<p>Training:    <br \/>\nAs a specific example, let\u2019s say you train a model for 100 hours using 10 DS14 v2 VMs on an Basic workspace in US West 2. For a billing month of 30 days, your bill will be as follows:    <\/p>\n<p>Azure VM Charge: (10 machines * $1.196 per machine) * 100 hours = $1,196    <\/p>\n<p>Azure Machine Learning Charge: (10 machines * 16 cores * $0 per core) * 100 hours = $0    <\/p>\n<p>Total: $1,196 + $0 = $1,196    <\/p>\n<p>Inferencing:    <br \/>\nAs a specific example, let\u2019s say you deploy a model for inferencing all day for a 30-day billing month using 10 DS14 v2 VMs in Basic in US West 2. For a billing month of 30 days, your bill will be as follows:    <\/p>\n<p>Azure VM Charge: (10 machines * $1.196 per machine) * (24 hours * 30 days) = $8,611.20    <\/p>\n<p>Azure Machine Learning Charge: (10 machines * 16 cores * $0 per core) * (24 hours * 30 days) = $0    <\/p>\n<p>Total: $8,611.20 + $0 = $8,611.20    <\/p>\n<p>This already includes the use of VM.    <\/p>\n<p>Now the other scenario you are talking about is around deployment of Python App where you might need to make use of containers and the cost associated with them which is a seperate topic.    <\/p>\n<p>Now let us come to your basic question where I do assume that you might want to explore the options available to you for running Python Script in Azure.    <\/p>\n<p>Both Azure Automation and Azure Functions support running Python scripts and do not require the creation of any VM's for same.    <\/p>\n<p>For Azure Function you can refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-functions\/functions-reference-python?tabs=application-level\">this<\/a>. and for cost you can refer to <a href=\"https:\/\/azure.microsoft.com\/en-in\/pricing\/details\/functions\/\">this<\/a>.    <\/p>\n<p>For Azure Automation you can refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/automation\/learn\/automation-tutorial-runbook-textual-python2\">this<\/a>. For cost incurred for automation you can refer to <a href=\"https:\/\/azure.microsoft.com\/en-in\/pricing\/details\/automation\/\">this<\/a> which mostly depends upon the Job you create.    <\/p>\n<p>Now if you check both the option they are not billed particularly for Python script you are running but more around resources being used for what time which you can have a rough estimation from Pricing calculator links mentioned in above links.    <\/p>\n<p>Further now if you want to deploy or run your Python App in Azure there are mainly 4 ways as mentioned <a href=\"https:\/\/azure.microsoft.com\/en-in\/get-started\/python\/\">here<\/a>:    <\/p>\n<ul>\n<li> <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/azure-functions\/create-first-function-vs-code-python\">Create a simple Python web app on Azure<\/a>    <\/li>\n<li> <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/azure-functions\/tutorial-vs-code-serverless-python\">Build and deploy a serverless Python app<\/a>    <\/li>\n<li> <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/notebooks\/use-machine-learning-services-jupyter-notebooks?toc=%2Fpython%2Fazure%2FTOC.json\">Try Azure Machine Learning scenarios in a preconfigured environment<\/a>    <\/li>\n<li> <a href=\"https:\/\/learn.microsoft.com\/en-in\/python\/azure\/?view=azure-python\">See more ways to use Python on Azure<\/a>    <\/li>\n<\/ul>\n<p>You don't need to worry about the Python script incurring you charges but you can check the price of associated resource you are using basically any Azure resource here on <a href=\"https:\/\/azure.microsoft.com\/en-in\/pricing\/calculator\/\">Pricing Calculator<\/a> as well.    <\/p>\n<p>Hope it helps :) !!!    <\/p>\n<p>Please <strong>&quot;Accept as Answer&quot;<\/strong> if it helped so it can help others in community looking for help on similar topics.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":9.8,
        "Solution_reading_time":55.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":34.0,
        "Solution_word_count":570.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5953230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello, my endpoint seems be throttled so I want to know what is the default limitation. I did some research but found nothing in the official docs, please help me point the obvious data. <\/p>",
        "Challenge_closed_time":1672328170423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1672326027260,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1145811\/is-there-a-limit-for-endpoint-bandwidth-quota",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5953230556,
        "Challenge_title":"Is there a limit for endpoint bandwidth quota",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=80951d13-b803-4a3c-a513-64b99f671835\">@jim jones  <\/a>     <\/p>\n<p>Thanks for reaching out to us for this issue. Mirroring traffic uses your endpoint bandwidth quota <strong>(default 5 MBPS)<\/strong>. Your endpoint bandwidth will be throttled if you exceed the allocated quota. For information on monitoring bandwidth throttling, see Monitor managed online endpoints.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274868-image.png?platform=QnA\" alt=\"274868-image.png\" \/>    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":12.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1642181068947,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":22.2882166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a HyperparameterTuner on an Estimator for an LDA model in a SageMaker notebook using mxnet but am running into errors related to the feature_dim hyperparameter in my code. I believe this is related to the differing dimensions of the train and test datasets but I'm not 100% certain if this is the case or how to fix it.<\/p>\n<h2>Estimator Code<\/h2>\n<p>[note that I'm setting the feature_dim to the training dataset's dimensions]<\/p>\n<pre><code>vocabulary_size = doc_term_matrix_train.shape[1]\n\nlda = sagemaker.estimator.Estimator(\n        container,\n        role,\n        output_path=&quot;s3:\/\/{}\/{}\/output&quot;.format(bucket, prefix),\n        train_instance_count=1,\n        train_instance_type=&quot;ml.c4.2xlarge&quot;,\n        sagemaker_session=session\n        )\n\nlda.set_hyperparameters(\n    mini_batch_size=40\n    feature_dim=vocabulary_size,\n    )\n\n<\/code><\/pre>\n<h2>Hyperparameter Tuning Job<\/h2>\n<pre><code>#s3_input_train and s3_input_test hold doc_term matrices of the test\/train corpus \n\ns3_input_train = 's3:\/\/{}\/{}\/train'.format(bucket, prefix)\ns3_input_test ='s3:\/\/{}\/{}\/test\/'.format(bucket, prefix)\ndata_channels = {'train': s3_input_train, 'test': s3_input_test}\n\nhyperparameter_ranges = {\n    &quot;alpha0&quot;: ContinuousParameter(0.1, 1.5, scaling_type=&quot;Logarithmic&quot;),\n    &quot;num_topics&quot;:IntegerParameter(3, 10)}\n\n# Configure HyperparameterTuner\nmy_tuner = HyperparameterTuner(estimator=lda,  \n                               objective_metric_name='test:pwll',\n                               hyperparameter_ranges=hyperparameter_ranges,\n                               max_jobs=5,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(data_channels, job_name='run-3', include_cls_metadata=False)\n<\/code><\/pre>\n<h2>Cloudwatch Logs<\/h2>\n<p>When I run the above, the tunings fail and when I look into Cloudwatch to see the logs, the error is typically:<\/p>\n<p><em>[01\/19\/2022 19:42:22 ERROR 140234465695552] Algorithm Error: index 11873 is out of bounds for axis 1 with size 11873 (caused by IndexError)<\/em><\/p>\n<p>I replicated the above because 11873 is the number of features in my test dataset, so I think there's a connection but I'm not sure exactly what's going on. When I try &quot;11873&quot; as the value for feature_dim, the error complains that the data has 32465 features (corresponding to the training set). Summing the two values also gives the following error:<\/p>\n<p><em>[01\/20\/2022 13:44:01 ERROR 140125082621760] Customer Error: The supplied feature_dim parameter does not have the same dimensionality of the data. (feature_dim) 44338 != 32465 (data).<\/em><\/p>\n<p>Lastly, one of the last logs in Cloudwatch reports the following, suggesting that &quot;all data&quot; is being fit into a matrix with the dimensions of the test data:<\/p>\n<p><em>[01\/20\/2022 14:49:52 INFO 140411440904000] Loaded all data into matrix with shape: (11, 11873)<\/em><\/p>\n<p>How do I define feature_dim given the test and training data sets?<\/p>",
        "Challenge_closed_time":1642773499220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642643048200,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1642693261640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70779880",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":37.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":36.2363944444,
        "Challenge_title":"SageMaker Hyperparameter Tuning for LDA, clarifying feature_dim",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":325,
        "Platform":"Stack Overflow",
        "Poster_created_time":1642181068947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>I have resolved this issue. My problem was that I was splitting the data into test and train BEFORE converting the data into doc-term matrices, which resulted in test and train datasets of different dimensionality, which threw off SageMaker's algorithm. Once I convereted all of the input data into a doc-term matrix, and THEN split it into test and train, the hyperparameter optimization operation completed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1558539179612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4461.4114333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Challenge_closed_time":1607117580710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591056499550,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4461.4114333334,
        "Challenge_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392607100776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation_count":133.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.0608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1613510526000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612927107000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":162.0608333333,
        "Challenge_title":"MlflowLogger fail when logging long parameters",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Dear @ducthienbui97,\n\nThanks for opening a PR.\n\nBest,\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1373018880576,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":17.1053752778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Challenge_closed_time":1543906305888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543844726537,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.1053752778,
        "Challenge_title":"AWS Sagemaker Deploy fails",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5932.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373018880576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":305.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.0,
        "Solution_reading_time":2.34,
        "Solution_score_count":7.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":118.9246497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've run an automl job at AWS Autopilot using the F1 metric. I'd like to see the value of precision and recall too. How?<\/p>",
        "Challenge_closed_time":1578755923276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578327794537,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59615549",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":118.9246497222,
        "Challenge_title":"Precision and recall at AWS Autopilot",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499272954707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Depending on the algo used, they might be visible in the training log of the top candidate.<\/p>\n\n<p>What you could also do is keep a test set on the side, and use it to compute precision, recall and other metrics on the trained model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":2.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.213155,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Within my optuna study, I want that each trial is separately logged by wandb. Currently, the study is run and the end result is tracked in my wandb dashboard. Instead of showing each trial run separately, the end result over all epochs is shown. So, wandb makes one run out of multiple runs.<\/p>\n<p>I found the following <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/_modules\/optuna\/integration\/wandb.html\" rel=\"noopener nofollow ugc\">docs<\/a> in optuna:<\/p>\n<pre><code>Weights &amp; Biases logging in multirun mode.\n\n    .. code::\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">            import optuna\n            from optuna.integration.wandb import WeightsAndBiasesCallback\n\n            wandb_kwargs = {\"project\": \"my-project\"}\n            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n\n            @wandbc.track_in_wandb()\n            def objective(trial):\n                x = trial.suggest_float(\"x\", -10, 10)\n                return (x - 2) ** 2\n\n\n            study = optuna.create_study()\n            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n\n<\/code><\/pre>\n<p>I implemented this line of code yet it produces the following error:<\/p>\n<p><code>ConfigError: Attempted to change value of key \"learning_rate\" from 5e-05 to     0.0005657929921495451 If you really want to do this, pass allow_val_change=True to config.update()    wandb: Waiting for W&amp;B process to finish... (failed 1).<\/code><\/p>\n<p>Did anyone succeed in implementing logging per trial in a multi-trial study?<\/p>",
        "Challenge_closed_time":1679650682812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679646315454,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-enable-logging-of-each-trial-separately\/4115",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.213155,
        "Challenge_title":"How to enable logging of each trial separately?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":101.0,
        "Challenge_word_count":167,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I actually solved it now:<br>\nIt seems that the optimizer that i used caused errors in the generation of a value for the learning rate when starting a new trial. Once I took the optimizer back out, the follwing implementation worked and generated separate logs in my wandb dashboard:<\/p>\n<pre><code class=\"lang-auto\">wandb_kwargs = {\"project\": \"my-project\"}\nwandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n@wandbc.track_in_wandb()\ndef objective(trial):\n    \n    training_args = Seq2SeqTrainingArguments( \n        \"tuning\", \n        num_train_epochs=1,            \n        # num_train_epochs = trial.suggest_categorical('num_epochs', [3, 5, 8]),\n        per_device_eval_batch_size=3, \n        per_device_train_batch_size=3, \n        learning_rate=  trial.suggest_float('learning_rate', low=0.00004, high=0.0001, step=0.0005, log=False),             \n        # per_device_train_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),       \n        # per_device_eval_batch_size= trial.suggest_categorical('batch_size', [6, 8, 12, 18]),  \n        disable_tqdm=True, \n        predict_with_generate=True,\n        gradient_accumulation_steps=4,\n        # gradient_checkpointing=True,\n        # weight_decay= False\n        seed = 12, \n        warmup_steps=5,\n        # evaluation and logging\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        save_total_limit=1,\n        logging_strategy=\"epoch\",\n        logging_steps = 1, \n        load_best_model_at_end=True,\n        metric_for_best_model = \"eval_loss\",\n        # use_cache=False,\n        push_to_hub=False,\n        fp16=False,\n        remove_unused_columns=True\n    )\n    # optimizer = Adafactor(\n    #     t5dmodel.parameters(),\n    #     lr=trial.suggest_float('learning_rate', low=4e-5, high=0.0001),  #   ('learning_rate', 1e-6, 1e-3),\n    #     # weight_decay=trial.suggest_float('weight_decay', WD_MIN, WD_CEIL),   \n    #     # lr=1e-3,\n    #     eps=(1e-30, 1e-3),\n    #     clip_threshold=1.0,\n    #     decay_rate=-0.8,\n    #     beta1=None,\n    #     # weight_decay= False\n    #     weight_decay=0.1,\n    #     relative_step=False,\n    #     scale_parameter=False,\n    #     warmup_init=False,\n    # )\n    \n    # lr_scheduler = AdafactorSchedule(optimizer)\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=t5dmodel)\n    trainer = Seq2SeqTrainer(model=t5dmodel,\n                            args=training_args,\n                            train_dataset=tokenized_train_dataset['train'],\n                            eval_dataset=tokenized_val_dataset['validation'],\n                            data_collator=data_collator,\n                            tokenizer=tokenizer,\n                           #  optimizers=(optimizer, lr_scheduler)\n                            )       \n    \n    trainer.train()\n    scores = trainer.evaluate() \n    return scores['eval_loss']\n\nif __name__ == '__main__':\n    t5dmodel = AutoModelForSeq2SeqLM.from_pretrained(\"yhavinga\/t5-base-dutch\",  use_cache=False) \n    tokenizer = AutoTokenizer.from_pretrained(\"yhavinga\/t5-base-dutch\", additional_special_tokens=None)\n    \n    features = {\n    'WordRatioFeature': {'target_ratio': 0.8},\n    'CharRatioFeature': {'target_ratio': 0.8},\n    'LevenshteinRatioFeature': {'target_ratio': 0.8},\n    'WordRankRatioFeature': {'target_ratio': 0.8},\n    'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n    }\n    \n    trainset_processed = get_train_data(WIKILARGE_PROCESSED, 0, 10)  \n    print(trainset_processed)\n    valset_processed = get_validation_data(WIKILARGE_PROCESSED, 0,7)\n    print(valset_processed)\n    tokenized_train_dataset = trainset_processed.map((tokenize_train), batched=True, batch_size=1)\n    tokenized_val_dataset =  valset_processed.map((tokenize_train), batched=True, batch_size=1)   \n    print('Triggering Optuna study')\n    study = optuna.create_study( direction='minimize', pruner=optuna.pruners.MedianPruner()) \n    study.optimize(objective, n_trials=4,callbacks=[wandbc],  gc_after_trial=True)\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.3,
        "Solution_reading_time":45.02,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":211.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1498252453503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":298.7422783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Challenge_closed_time":1530057890672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529660522477,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1530173488990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.8,
        "Challenge_reading_time":31.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":110.3800541667,
        "Challenge_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1531248961192,
        "Solution_link_count":1.0,
        "Solution_readability":25.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":755.505,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Challenge_closed_time":1591023635000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588303817000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.81,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":755.505,
        "Challenge_title":"Comet logger cannot be pickled after creating an experiment",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ceyzaguirre4 pls ^^ I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.\r\n\r\nWandB uses a lambda function at some point.\r\n\r\nDoes the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"..\/train.py\", line 140, in <module>\r\n    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)\r\n  File \"..\/train.py\", line 117, in main\r\n    trainer.fit(model)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n```\r\n\r\nalso related: \r\n#1704 I had the same error as @jeremyjordan  `can't pickle _thread.lock objects`. This happened when I added the  `logger` and additional `callbacks` in `from_argparse_args`, as explained here https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/hyperparameters.html\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])\r\n```\r\nI could make the problem go away by directly overwriting the members of `Trainer`\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams)\r\ntrainer.logger = logger\r\ntrainer.callbacks.append(PrinterCallback())\r\n``` Same issue as @F-Barto using a wandb logger across 2 nodes with `ddp`. same issue when using wandb logger with ddp same here.. @joseluisvaz your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :\/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks. Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine  With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue. I still have this error with 1.5.10 on macOS\r\n\r\n```\r\nError executing job with overrides: ['train.pl_trainer.fast_dev_run=False', 'train.pl_trainer.gpus=0', 'train.pl_trainer.precision=32', 'logging.wandb_arg.mode=offline']\r\nTraceback (most recent call last):\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 78, in main\r\n    train(conf)\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 70, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/plugins\/training_type\/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/epoch\/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 198, in __iter__\r\n    self._apply_patch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 133, in _apply_patch\r\n    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 181, in loader_iters\r\n    loader_iters = self.dataloader_iter.loader_iters\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 537, in loader_iters\r\n    self._loader_iters = self.create_loader_iters(self.loaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 577, in create_loader_iters\r\n    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 104, in apply_to_collection\r\n    v = apply_to_collection(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 96, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 177, in __iter__\r\n    self._loader_iter = iter(self.loader)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n``` I still see this bug as well with WandB logger. Currently having this issue with wandbLogger.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.8,
        "Solution_reading_time":127.3,
        "Solution_score_count":23.0,
        "Solution_sentence_count":105.0,
        "Solution_word_count":638.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":50.6452397222,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>If I call <code>wandb.run.log_code(\".\")<\/code>, all python source code files in the current directory are saved in W&amp;B cloud. That\u2019s what I want.<\/p>\n<p>However, only changes in main training file where I call <code>wandb.init()<\/code> can be shown in <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/code#code-comparer\">Code Comparer<\/a>. The change in other file (like <code>helper_funcs.py<\/code>) will not appear in code panel. Do you have any suggestions about it?<\/p>",
        "Challenge_closed_time":1661955284028,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661772961165,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/code-comparer-can-only-show-the-difference-of-main-training-file\/3020",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":50.6452397222,
        "Challenge_title":"Code Comparer can only show the difference of main training file",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Yao, thanks for writing in! As you have said, you can\u2019t compare in the panel other files than the one where you call <code>wandb.init()<\/code>, but you can compare them in the artefacts tab. I send you a video on how to do this. Please let me know if this would work for you.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":3.41,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.8393819445,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Challenge_closed_time":1647593923663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647450501888,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":39.8393819445,
        "Challenge_title":"Wandb.watch with pytorch not logging anything",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523.0,
        "Challenge_word_count":128,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.5,
        "Solution_reading_time":14.19,
        "Solution_score_count":null,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3566.0353452778,
        "Challenge_answer_count":2,
        "Challenge_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas \n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a [request for a service quota increase](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#service-limit-increase-request-procedure).",
        "Challenge_closed_time":1655317929862,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642480202619,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668602220822,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sagemaker-service-quotas",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":14.7,
        "Challenge_reading_time":8.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3566.0353452778,
        "Challenge_title":"How do I check my current SageMaker service quotas?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1178.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655365526940,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":3,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600043448000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7267.8997222222,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580225794000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_participation_count":10,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":703.9719444444,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":312.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.1083627778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to use the Azure Pricing estimate in the Azure Pricing Calculator, the &quot;Estimated monthly costs&quot; seems to include but also far exceeds the compute cost.  Does this Estimated Monthly cost include the other resources that get created?     <br \/>\neg. Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), Key Vault    <\/p>\n<p>Thank you    <br \/>\nPeter    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/52085-image.png?platform=QnA\" alt=\"52085-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1609285909056,
        "Challenge_comment_count":1,
        "Challenge_created_time":1609267518950,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/213635\/does-the-estimated-monthly-costs-for-azure-machine",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.3,
        "Challenge_reading_time":8.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.1083627778,
        "Challenge_title":"Does the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator include all other non-compute \"additional resources\" created in the workspace",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Peter.    <\/p>\n<p>Thanks for reaching out. I tried your selections but I don't have the same service as you. Have you selected other services in you calculator?     <\/p>\n<p>For your question, the estimated price is only for Azure Machine Learning Service. You need to select all services you need in the calculator like below:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51998-image.png?platform=QnA\" alt=\"51998-image.png\" \/>    <\/p>\n<p>Please note I only use random number for the example.     <\/p>\n<p><strong>From the number I guess you have selected 2 Machine Learning Services and also other services since they added to your basket when you clicked them,<\/strong> you can click the button to see what you have all as below.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/51969-image.png?platform=QnA\" alt=\"51969-image.png\" \/>    <\/p>\n<p>Also you are selecting Reservation service, detail: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations\">https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.0,
        "Solution_reading_time":15.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":129.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634052175000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":268,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":377.9588433334,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Challenge_closed_time":1663850808295,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662490156459,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.0,
        "Challenge_reading_time":24.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":377.9588433334,
        "Challenge_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":202.0,
        "Challenge_word_count":304,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jlperla\">@jlperla<\/a> thank you for the detailed information, and great to hear that the grouping issue has been now resolved. Regarding your question using the API to filter runs, you could do that indeed with the following command:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"sweep\": \"sweep_id\"})<\/code><br>\nAlternatively you can use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/tags#how-to-add-tags\">API to tag all your runs<\/a> based on <code>my_sweep_name<\/code> identifier and then query runs as follows:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"tags\": \"my_sweep_name\"})<\/code><br>\nIs my_sweep_name defined in your config? In that case you could do <code>filters={\"config.sweep_name\": \"my_sweep_name\"}<\/code>.<\/p>\n<p>Would any of these work for you? Please let me know if you have any further questions or issues with this!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":11.58,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":22.0598044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use optuna lib in Python to optimise parameters for recommender systems' models. Those models are custom and look like standard fit-predict sklearn models (with methods get\/set params). <\/p>\n\n<p>What I do: simple objective function that selects two parameters from uniform int distribution, set these params to model, predicts the model (there no fit stage as it simple model that uses params only in predict stage) and calculates some metric. <\/p>\n\n<p>What I get: the first trial runs normal, it samples params and prints results to log. But on the second and next trial I have some strange errors (look code below) that I can't solve or google. When I run study on just 1 trial everything is okay.<\/p>\n\n<p>What I tried: to rearrange parts of objective function, put fit stage inside, try to calculate more simpler metrics - nothing helps. <\/p>\n\n<p>Here is my objective function: <\/p>\n\n<pre><code># getting train, test\n# fitting model\nself.model = SomeRecommender()\nself.model.fit(train, some_other_params)\n\ndef objective(trial: optuna.Trial):\n    # save study\n    if path is not None:\n        joblib.dump(study, some_path)\n\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # setting params to model\n    params = {'alpha': alpha,\n              'beta': beta}\n    self.model.set_params(**params)\n\n    # getting predict\n    recs = self.model.predict(some_other_params)\n\n    # metric computing\n    metric_result = Metrics.hit_rate_at_k(recs, test, k=k)\n\n    return metric_result\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>That's what I get on three trials:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>[I 2019-10-01 12:53:59,019] Finished trial#0 resulted in value: 0.1. Current best value is 0.1 with parameters: {'alpha': 59.6135986324444, 'beta': 40.714559720597585}.\n[W 2019-10-01 13:39:58,140] Setting status of trial#1 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n[W 2019-10-01 13:39:58,206] Setting status of trial#2 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n<\/code><\/pre>\n\n<p>I can't understand where is the problem and why the first trial is working. Please, help. <\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1570006754996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569926663223,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1569927339700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58183158",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":61.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.2477147222,
        "Challenge_title":"How to fix error \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\" - strange behaviour in optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":666.0,
        "Challenge_word_count":446,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Your code seems to have no problems.<\/p>\n\n<p>I ran a simplified version of your code (see below), and it worked well in my environment:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # evaluating params\n    return alpha + beta\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>Could you tell me about your environment in order to investigate the problem? (e.g., OS, Python version, Python interpreter (CPython, PyPy, IronPython or Jython), Optuna version)<\/p>\n\n<blockquote>\n  <p>why the first trial is working.<\/p>\n<\/blockquote>\n\n<p>This error is raised by <a href=\"https:\/\/github.com\/pfnet\/optuna\/blob\/389a176c8cd1c860001a7a4562670006643e5e11\/optuna\/samplers\/tpe\/sampler.py#L558\" rel=\"noreferrer\">optuna\/samplers\/tpe\/sampler.py#558<\/a>, and this line is only executed when the number of completed trials in the study is greater than zero.<\/p>\n\n<p>BTW, you might be able to avoid this problem by using <code>RandomSampler<\/code> as follows:<\/p>\n\n<pre><code>sampler = optuna.samplers.RandomSampler()\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\n<\/code><\/pre>\n\n<p>Notice that the optimization performance of <code>RandomSampler<\/code> tends to be worse than <code>TPESampler<\/code> that is the default sampler of Optuna.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":18.86,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":153.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665708717000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":583,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1226984969400,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Adelaide, Australia",
        "Answerer_reputation_count":5789.0,
        "Answerer_view_count":464.0,
        "Challenge_adjusted_solved_time":21214.2534619445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Challenge_closed_time":1570492274472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569898766770,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1569900383487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":49.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":164.8632505556,
        "Challenge_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1234.0,
        "Challenge_word_count":490,
        "Platform":"Stack Overflow",
        "Poster_created_time":1226984969400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Adelaide, Australia",
        "Poster_reputation_count":5789.0,
        "Poster_view_count":464.0,
        "Solution_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646271695950,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":80.3919036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a model quality monitor job, using the class ModelQualityMonitor from Sagemaker model_monitor, and i think i have all the import statements defined yet i get the message cannot import name error<\/p>\n<pre><code>from sagemaker import get_execution_role, session, Session\nfrom sagemaker.model_monitor import ModelQualityMonitor\n                \nrole = get_execution_role()\nsession = Session()\n\nmodel_quality_monitor = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=session\n)\n<\/code><\/pre>\n<p>Any pointers are appreciated<\/p>",
        "Challenge_closed_time":1608016613856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607727203003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65259702",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":80.3919036111,
        "Challenge_title":"AWS sagemaker model monitor- ImportError: cannot import name 'ModelQualityMonitor'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":544.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546959992036,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3<\/code> Amazon SageMaker notebook, I don't get any errors at all.<\/p>\n<p>Example screenshot output showing no errors:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined<\/code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker<\/code> and then see if this resolves your error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.9180447222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m soon going to start implementing W&amp;B for my neural network\u2019s hyperparameter tuning. This is in preparation for an academic paper I\u2019m writing on the subject. The software seems very pragmatic and well-polished, so I\u2019m quite excited to get started.<\/p>\n<p>Its visualizations in particular seem to be of a very high quality. Some present sophisticated functionality that other experiment trackers can\u2019t touch. With proper citation, can these be included for publication?<\/p>",
        "Challenge_closed_time":1638466847308,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638456342347,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/publishing-graphs-visualizations\/1457",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.9180447222,
        "Challenge_title":"Publishing Graphs\/Visualizations",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":217.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Logan,<\/p>\n<p>I\u2019m so happy you\u2019re excited to use our product! Our engineers have worked very hard in order to get it to where it is today. We would love for you to use our graphs in your paper. We have a few examples of how to do so here (<a href=\"https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases\" class=\"inline-onebox-loading\">https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases<\/a>).<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":5.79,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635429600000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.1185105556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Challenge_closed_time":1659047787071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658741360433,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.1185105556,
        "Challenge_title":"Update offline run before syncing",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.3760377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Challenge_closed_time":1573141908796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573137628147,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1890691667,
        "Challenge_title":"How to disable logs in Kedro",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479159384132,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Illinois, United States",
        "Poster_reputation_count":513.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573142581883,
        "Solution_link_count":1.0,
        "Solution_readability":12.4,
        "Solution_reading_time":12.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":195.7456172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1545210167392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544505483170,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":195.7456172222,
        "Challenge_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544503799112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":57.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2083.0769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Summary\r\n\r\nProfiling with mlflow and without an mlflow writer fails silently. \r\n\r\n### Steps to Reproduce it\r\n\r\nuse mlflow with get_or_create_session and no files are written.\r\n\r\n### Example\r\n\r\nThere are examples of how to configure mlflow writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_mlflow.yaml\r\n\r\nwhylogs should mention the missing mlflow writer in a warning. Maybe we can automatically add the mlflow writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## What is the current *bug* behavior?\r\n\r\nlogging with mlflow and default configuration appears to fail silently.\r\n\r\n### What is the expected *correct* behavior?\r\n\r\nmlflow integration should write to mlflow by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1655127386000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647628309000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/480",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2083.0769444444,
        "Challenge_title":"using mlflow without an mlflow writer configured appears to fail silently",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.5,
        "Solution_reading_time":0.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630081759000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":270.0,
        "Challenge_repo_star_count":13.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.0572022222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I need to import MatPlotLib images into WandB.  On the surface, this seems simple, since the documentation clearly shows how to ingest a <code>plt<\/code> or <code>fig<\/code> object.  However, WandB is making a mess of the plots and I don\u2019t want to recode them in plotly.<br>\nSo I next want to use MatPlotLib to save a PNG and ingest that.  Again seems easy, but I would prefer to do it using an in-memory buffer object (this avoids messing with local paths and temp directories on various instances).  Apparently I\u2019m not the first one to do this either (<a href=\"https:\/\/stackoverflow.com\/questions\/35999020\/convert-pyplot-figure-into-wand-image-image\" rel=\"noopener nofollow ugc\">link<\/a>). The instructions are clear and show someone has already done this.  But it fails when I try it:<\/p>\n<pre><code class=\"lang-auto\">fig, (ax1, ax2) = plt.subplots(2, 1, dpi=300, figsize=(10, 5))\n...\nbuf = io.BytesIO()\nplt.savefig(buf, format='png')\nbuf.seek(0)\nwandb.log(({\"chart\": wandb.Image(file=buf)}))\n<\/code><\/pre>\n<p>The error seems to be with <code>wandb.Image()<\/code>.  It returns:<br>\n<code>{TypeError}__init__() got an unexpected keyword argument 'file'<\/code><\/p>\n<p>I can remove the <code>file=<\/code> parameter so that the command is:<\/p>\n<pre><code class=\"lang-auto\">wandb.Image(buf)\n<\/code><\/pre>\n<p>And I get: <code>{AttributeError}'_io.BytesIO' object has no attribute 'ndim'<\/code><\/p>\n<p>Any recommendations?<\/p>",
        "Challenge_closed_time":1664817995860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664770989932,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/matplotlib-into-wandb\/3212",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.0,
        "Challenge_reading_time":18.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.0572022222,
        "Challenge_title":"MatPlotLib into WandB",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":790.0,
        "Challenge_word_count":179,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a>!<\/p>\n<p>The <code>BytesIO<\/code> type is not supported by <code>wandb.Image<\/code> which is why you are running into this issue. Here are a few options that would work instead:<\/p>\n<ul>\n<li><code>wandb.log({ 'chart' : wandb.Image(Image.open(buf)) })<\/code><\/li>\n<li><code>wandb.log({ 'chart' : wandb.Image(fig) })<\/code><\/li>\n<li>\n<code>wandb.log({ 'chart' : fig })<\/code> (Please note that this does not actually save an image but an interactable Plotly chart on your workspace<\/li>\n<\/ul>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":7.48,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":65.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1363369778320,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":7643.0,
        "Answerer_view_count":515.0,
        "Challenge_adjusted_solved_time":152.6492175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented a PyTorch <code>Dataset<\/code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset<\/code> implementation is as follows.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>\n\n<p>I am following this <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/master\/src\/sagemaker\/pytorch\" rel=\"noreferrer\">example<\/a> and this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_cnn_cifar10\/pytorch_local_mode_cifar10.ipynb\" rel=\"noreferrer\">one too<\/a>, and I run the <code>estimator<\/code> as follows.<\/p>\n\n<pre><code>inputs = {\n 'train': 'file:\/\/images',\n 'eval': 'file:\/\/images'\n}\nestimator = PyTorch(entry_point='pytorch-train.py',\n                            role=role,\n                            framework_version='1.0.0',\n                            train_instance_count=1,\n                            train_instance_type=instance_type)\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I get the following error.<\/p>\n\n<blockquote>\n  <p>FileNotFoundError: [Errno 2] No such file or directory: '.\/images'<\/p>\n<\/blockquote>\n\n<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.<\/p>\n\n<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data\/cifar10')\n<\/code><\/pre>\n\n<p>If I take a peek at <code>inputs<\/code>, it is just a string literal <code>s3:\/\/sagemaker-us-east-3-184838577132\/data\/cifar10<\/code>. The code to create a <code>Dataset<\/code> and a <code>DataLoader<\/code> is shown <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_mnist\/mnist.py#L41\" rel=\"noreferrer\">here<\/a>, which does not help unless I track down the source and step through the logic.<\/p>\n\n<p>I think what needs to happen inside my <code>ImageDataset<\/code> is to supply the <code>S3<\/code> path and use the <code>AWS CLI<\/code> or something to query the files and acquire their content. I do not think the <code>AWS CLI<\/code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. <\/p>\n\n<p>There must be a recipe or something to create a custom <code>Dataset<\/code> backed by <code>S3<\/code> files, right?<\/p>",
        "Challenge_closed_time":1546965697560,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546416160377,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54003052",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":38.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":152.6492175,
        "Challenge_title":"How do I implement a PyTorch Dataset for use with AWS SageMaker?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4597.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363369778320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":7643.0,
        "Poster_view_count":515.0,
        "Solution_body":"<p>I was able to create a PyTorch <code>Dataset<\/code> backed by S3 data using <code>boto3<\/code>. Here's the snippet if anyone is interested.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.s3 = boto3.resource('s3')\n        self.bucket = self.s3.Bucket(path)\n        self.files = [obj.key for obj in self.bucket.objects.all()]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        # we need to download the file from S3 to a temporary file locally\n        # we need to create the local file name\n        obj = self.bucket.Object(img_name)\n        tmp = tempfile.NamedTemporaryFile()\n        tmp_name = '{}.jpg'.format(tmp.name)\n\n        # now we can actually download from S3 to a local place\n        with open(tmp_name, 'wb') as f:\n            obj.download_fileobj(f)\n            f.flush()\n            f.close()\n            image = Image.open(tmp_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":16.51,
        "Solution_score_count":12.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":136.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1249258805300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":909.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":1.4120652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Challenge_closed_time":1533929944952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533924861517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.4,
        "Challenge_reading_time":47.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1.4120652778,
        "Challenge_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1355.0,
        "Challenge_word_count":285,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378935265347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":11.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.4565155556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created 100 runs, and would like to make two scatter plots. The first scatter plot involves the first 50 simulations, with axis limits [0,10] in both directions. The second scatter plot uses simulations 51 to 100, with different axis limits, say [10,20]. So far, I created a new panel, for both these plots. But wandb does not like that. Whatever I set the axis limits will be the same for both subsets (1-50, and 51-100). What is the recommended approach to have a plot for each of the data subsets? Must I create two different panels? If so, that means that one panel 2 might have to be turned off for the first batch of data experiments, and panel 1 would be turned off for the second batch of experiments. Is this the recommended approach? Thanks.<\/p>",
        "Challenge_closed_time":1660342284524,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660153441068,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/axis-scales\/2892",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":52.4565155556,
        "Challenge_title":"Axis scales",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a>,<\/p>\n<p>Have you tried creating reports with different panel plots? They should work here.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":2.39,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":303.4149847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Challenge_closed_time":1650950165912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649857871967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":303.4149847222,
        "Challenge_title":"How to use tensorflow hub in Azure ML",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":12.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":34.0954786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>1) According to <a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html<\/a> Amazon ML uses SGD. However I can't find how many hidden layers are used in the neural network?<\/p>\n\n<p>2) Can someone confirm that SageMaker would be able to do what Amazon ML does? i.e. SageMaker is more powerful than Amazon ML?<\/p>",
        "Challenge_closed_time":1512471655643,
        "Challenge_comment_count":1,
        "Challenge_created_time":1512348911920,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1513142314292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47625056",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":34.0954786111,
        "Challenge_title":"Amazon Machine Learning and SageMaker algorithms",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2794.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_created_time":1264406140363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2211.0,
        "Poster_view_count":176.0,
        "Solution_body":"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n\n<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).<\/p>\n\n<p>You can check out each of those containers to dive deep on how it all works.<\/p>\n\n<p>You can find an exhaustive list of available algorithms in SageMaker here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For now, as of December 2017, these algorithms are all available:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html\" rel=\"noreferrer\">Linear Learner<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines.html\" rel=\"noreferrer\">Factorization Machines<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"noreferrer\">XGBoost Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"noreferrer\">Image Classification Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq.html\" rel=\"noreferrer\">Amazon SageMaker Sequence2Sequence<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/k-means.html\" rel=\"noreferrer\">K-Means Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pca.html\" rel=\"noreferrer\">Principal Component Analysis (PCA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda.html\" rel=\"noreferrer\">Latent Dirichlet Allocation (LDA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"noreferrer\">Neural Topic Model (NTM)<\/a><\/li>\n<\/ul>\n\n<p>The general SageMaker SDK interface to these algorithms looks something like this:<\/p>\n\n<pre><code>from sagemaker import KMeans\nkmeans = KMeans(role=\"SageMakerRole\",\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                data_location=\"s3:\/\/training_data\/\",\n                output_path=\"s3:\/\/model_artifacts\/\",\n                k=10)\n<\/code><\/pre>\n\n<p>The libraries here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples<\/a>\nand here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a> are particularly useful for playing with SageMaker.<\/p>\n\n<p>You can also make use of Spark with SageMaker the Spark library here: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1512575702208,
        "Solution_link_count":19.0,
        "Solution_readability":25.2,
        "Solution_reading_time":39.62,
        "Solution_score_count":7.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":194.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0275,
        "Challenge_answer_count":0,
        "Challenge_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Challenge_closed_time":1649410238000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649410139000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0275,
        "Challenge_title":"How to get model references logged by a specific run?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.6,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":47.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.43637,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello, I want to create a grid panel containing several linecharts but the selected runs are different.<br>\nTo elaborate on my need: I have several algorithms, evaluated across timesteps on several environments. I want one line chart per environment. To illustrate, my final requirement is to get something that looks like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2059dafd12562e424e2b66a3e2aabb2b4e1b204.png\" alt=\"Screenshot from 2023-01-19 13-12-26\" data-base62-sha1=\"yx1zXviDcru4wcBIHNfwKEhCyFK\" width=\"435\" height=\"213\"><\/p>\n<p>How would you do that?<\/p>",
        "Challenge_closed_time":1674240153196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674130582264,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/different-run-sets-within-a-panel-grid\/3721",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":30.43637,
        "Challenge_title":"Different run sets within a panel grid",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":206.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.47,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0858333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1568642184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668590352780,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0858333333,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":331.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1568642184000,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.5791947222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>FLAML looks like it performs better than Azure AutoML for hyperparameter tuning (based on the benchmarking in the Arxiv paper): <a href=\"https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf\">https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf<\/a>  <\/p>\n<p>Is it now being used or is there a plan to integrate it for the hyperparameter tuning in Azure Machine Learning Services? If so, when is that expected to become available?<\/p>",
        "Challenge_closed_time":1623391032688,
        "Challenge_comment_count":1,
        "Challenge_created_time":1623298947587,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/429832\/does-azure-automl-use-(or-plan-to-use)-flaml-for-t",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":25.5791947222,
        "Challenge_title":"Does Azure AutoML use (or plan to use) FLAML for the hyperparameter tuning?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=44a7ffc5-e97c-4dec-95a0-445a9835aab3\">@Rainer Hillermann  <\/a> Thanks, We are not using the FLAML for Azure AutoML for the hyperparameter tuning, You can raise a user voice request <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\">here<\/a> so the community can vote and provide their feedback, the product team then checks this feedback and implements the feature in future releases.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":5.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4162452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/172214-image.png?platform=QnA\" alt=\"172214-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1644316889220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644315390737,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/726898\/azure-subscription-cost",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":4.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.4162452778,
        "Challenge_title":"Azure Subscription Cost",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>If you want to review your costs and what resources are being charged, then the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview#understand-cost-management\">Cost Analysis blade<\/a> will allow you to drill down work this out. Please let us know if this helps    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1501040260887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":2802.1180147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1625508026043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2802.1180147222,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588429621780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation_count":21.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.0207547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Because of network problem. The local <code>debug-internal.log<\/code> files of some runs are too large (more than 500MB). To save the disk space, is there any way to avoid the generation of these log files?<\/p>",
        "Challenge_closed_time":1672190667020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671769392303,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/the-debug-internal-log-file-is-too-large-500mb\/3589",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":117.0207547222,
        "Challenge_title":"The debug-internal.log file is too large (>500MB)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":211.0,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , thank you for writing in and happy to look into this for you.  <code>debug-internal.log<\/code> files are automatically generated and cannot be disabled by the user.  Please see this github issue thread that was raised about this issue were a user provided <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4223#issuecomment-1236304565\" rel=\"noopener nofollow ugc\">workaround<\/a> solution to address this . Do let me know if this reference helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":6.39,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1436432728608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colleferro, Italy",
        "Answerer_reputation_count":809.0,
        "Answerer_view_count":361.0,
        "Challenge_adjusted_solved_time":193.9353691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a code in Azure ML which uses the function <code>ggrepel<\/code>. That function requires the version 2.0.0 of the package <code>ggplot2<\/code>. When I try to use it I obtain the error:<\/p>\n\n<pre><code>Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage 'ggplot2' 1.0.0 was found, but &gt;= 2.0.0 is required by 'ggrepel'\n<\/code><\/pre>\n\n<p>So, what I did was: <\/p>\n\n<ol>\n<li>updated the R package <code>ggplot2<\/code> of my local version (is there a command to use to check the version of a package?);<\/li>\n<li>taken the folder related to <code>ggplot2<\/code>, and put it in the zip file I pass to Azure. So the x.zip wil contain generic functions, then ggrepel.zip and ggplot2.zip.<\/li>\n<\/ol>\n\n<p>At the end I have written:<\/p>\n\n<pre><code>install.packages(\"src\/ggplot2.zip\",lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggrepel.zip\",lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ggrepel, lib.loc=\".\", verbose=TRUE)\nlibrary(ggplot2, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>It seems working for ggrepel, but not for ggplot, because I obtain the same issue shown at the beginning. It's like the system does not see the updated package, but the default ggplot2 of Azure ML.<\/p>",
        "Challenge_closed_time":1466607112096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465908944767,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37812686",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":193.9353691667,
        "Challenge_title":"Package usage in AzureML: ggplot2 and ggrepel",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>At the end I have solved adding an additional package. The problem is in the fact that you have to check the log of the error and not only the error output (that does not insert all you need). At the end I have solved in this way:<\/p>\n\n<pre><code>install.packages(\"src\/scales_0.4.0.zip\" ,lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2_2.1.0.zip\",lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggrepel.zip\"      ,lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(scales,  lib.loc=\".\", verbose=TRUE)\nlibrary(ggplot2, lib.loc=\".\", verbose=TRUE)\nlibrary(ggrepel, lib.loc=\".\", verbose=TRUE)\n...\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":8.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job?\nIs there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Challenge_closed_time":1584346840000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584346040000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926338452,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sagemaker-spot-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2222222222,
        "Challenge_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi olivier, \nIf you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the  blog on [Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs][1] also mentions the same \n\n> To avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925558612,
        "Solution_link_count":1.0,
        "Solution_readability":21.6,
        "Solution_reading_time":7.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":708.9712822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:<\/p>\n<pre><code>study0 = optuna.create_study(study_name=study_name,\n                             storage=storage_name,\n                             direction='minimize', \n                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),\n                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',\n                             reduction_factor=4, min_early_stopping_rate=0),\n                             load_if_exists=True)\n<\/code><\/pre>\n<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate<\/code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()<\/code> really do (specially <code>min_early_stopping_rate<\/code>)?\nThanks<\/p>",
        "Challenge_closed_time":1616058414416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613506117800,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66231467",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":15.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":708.9712822222,
        "Challenge_title":"How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556636382232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p><code>min_resource<\/code>'s explanation on <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.pruners.SuccessiveHalvingPruner.html\" rel=\"nofollow noreferrer\">the documentation<\/a> says<\/p>\n<blockquote>\n<p>A trial is never pruned until it executes <code>min_resource * reduction_factor ** min_early_stopping_rate<\/code> steps.<\/p>\n<\/blockquote>\n<p>So, I suppose that we need to replace the value of <code>min_resource<\/code> with a specific number depending on <code>reduction_factor<\/code> and <code>min_early_stopping_rate<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.0,
        "Solution_reading_time":7.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":44.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.8341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Challenge_closed_time":1583654787000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583496984000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668057386500,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sagemaker-xgboost-support-parallel-training-across-multiple-machines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.8341666667,
        "Challenge_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":114.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see [Docker registry paths and example code](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html) in the Amazon SageMaker developer guide. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571827,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":248.2981122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1638931046247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":248.2981122222,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1324808381143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":9050.0,
        "Answerer_view_count":1750.0,
        "Challenge_adjusted_solved_time":2198.1168433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1623834809928,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":2203.8220902778,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":332.7862925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Challenge_closed_time":1636443694643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634804848290,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1635245663990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.3,
        "Challenge_reading_time":91.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":455.2350980556,
        "Challenge_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":404,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616589293616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation_count":67.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":7.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":77.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1467943515392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Answerer_reputation_count":173.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":67.8829583333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Challenge_closed_time":1594008525280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593764146630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594008626392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":26.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":67.8829583333,
        "Challenge_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1035.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1521856385820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":820.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":325.51976,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Challenge_closed_time":1663636318523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662464447387,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73621446",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":325.51976,
        "Challenge_title":"Log Any Type of Model in MLflow",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521856385820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sri Lanka",
        "Poster_reputation_count":820.0,
        "Poster_view_count":165.0,
        "Solution_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1598030987107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":171.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":76.2785638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Challenge_closed_time":1598032113043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597757510213,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661849871016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":76.2785638889,
        "Challenge_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1043.0,
        "Challenge_word_count":289,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486549300030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1599772735680,
        "Solution_link_count":6.0,
        "Solution_readability":16.9,
        "Solution_reading_time":22.33,
        "Solution_score_count":6.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":177.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":36.5267972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have lots of run files created by running PyTorch estimator\/ ScriptRunStep experiments that are saved in azureml blob storage container. Previously, I'd been viewing these runs in the Experiments tab of the ml.azure.com portal and associating tags to these runs to categorise and load the desired models.<\/p>\n<p>However, a coworker recently deleted my workspace. I created a new one which is connected to the previously-existing blob container, the run files therefore still exist and can be accessed on this new workspace, but they no longer show up in the Experiment viewer on ml.azure.com. Neither can I see the tags I'd associated to the runs.<\/p>\n<p><strong>Is there any way to load these old run files into the Experiment viewer or is it only possible to view runs created inside the current workspace?<\/strong><\/p>\n<p>Sample scriptrunconfig code:<\/p>\n<pre><code>data_ref = DataReference(datastore=ds,\n                         data_reference_name=&quot;&lt;name&gt;&quot;,        \n                         path_on_datastore = &quot;&lt;path&gt;&quot;)\nargs = ['--data_dir',   str(data_ref),     \n        '--num_epochs', 30,     \n        '--lr',         0.01,          \n        '--classifier', 'int_ext' ]  \n\nsrc = ScriptRunConfig(source_directory='.',                       \n                      arguments=args,                      \n                      compute_target = compute_target,                       \n                      environment = env,                       \n                      script='train.py') \nsrc.run_config.data_references = {data_ref.data_reference_name: \n                                  data_ref.to_config()} \n<\/code><\/pre>",
        "Challenge_closed_time":1616813814780,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616682318310,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1617188538763,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66801546",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":36.5267972222,
        "Challenge_title":"Load Azure ML experiment run information from datastore",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606130833532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.<\/p>\n<p>Another thing you might try is:<\/p>\n<ol>\n<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)<\/li>\n<li>copy your old workspace's data into the new workspace's storage account.<\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1362025964372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":9.3757052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm following the <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker tutorial<\/a>, but I think there's an error in the step 4a. Particularly, at line 3 I'm instructed to type:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>     s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n<\/code><\/pre>\n<p>and I get the error<\/p>\n<pre><code>----&gt; 3 s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n\nAttributeError: module 'sagemaker' has no attribute 's3_input'\n<\/code><\/pre>\n<p>Indeed, using <code>dir<\/code> shows that sagemaker has no attribute called s3_input. How can fix this so that I can keep advancing in the tutorial? I tried using <code>session.inputs<\/code>, but this redirects me to a page saying that <code>session<\/code> is deprecated and suggesting that I use <code>sagemaker.inputs.TrainingInput<\/code> instead of <code>sagemaker.s3_inputs<\/code>. Is this a good way of going forward?<\/p>\n<p>Thanks everyone for the help and patience!<\/p>",
        "Challenge_closed_time":1609450565407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609421425757,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65521556",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":16.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.0943472222,
        "Challenge_title":"Does the sagemaker official tutorial generate an AttributeError, and how to solve it?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":625.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349183356160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Oxford, UK",
        "Poster_reputation_count":3803.0,
        "Poster_view_count":240.0,
        "Solution_body":"<p>Using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/inputs.html#inputs\" rel=\"noreferrer\"><code>sagemaker.inputs.TrainingInput<\/code><\/a> instead of <code>sagemaker.s3_inputs<\/code> worked to get that code cell functioning. It is an appropriate solution, though there may be another approach.<\/p>\n<p>Step 4.b also had code which needed updating<\/p>\n<pre><code>sess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m4.xlarge',output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n<\/code><\/pre>\n<p>uses parameters <code>train_instance_count<\/code> and <code>train_instance_type<\/code> which have been changed in a later version (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes<\/a>).<\/p>\n<p>Making these changes resolved the errors for the tutorial using <code>conda_python3<\/code> kernel.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1609455178296,
        "Solution_link_count":3.0,
        "Solution_readability":28.2,
        "Solution_reading_time":16.72,
        "Solution_score_count":6.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":367.1658333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Challenge_closed_time":1657912525000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656590728000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":17.7,
        "Challenge_reading_time":99.61,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":367.1658333333,
        "Challenge_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":523,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @GillianGrayson \r\n\r\nFor **1. hydra-optuna-sweeper problem**, it has been modified in release_1.4. You can find [here](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/7e67c4692590550e7b703655845e59508eb071bb\/configs\/hparams_search\/mnist_optuna.yaml#L49)\r\n\r\n @GillianGrayson ty for reporting, the problems have been fixed on the current `main` branch.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1656061360900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":78.065125,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1656071944492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656062302777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":21.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2.6782541667,
        "Challenge_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":170.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656061360900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1656343337227,
        "Solution_link_count":1.0,
        "Solution_readability":80.7,
        "Solution_reading_time":43.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.9922386111,
        "Challenge_answer_count":1,
        "Challenge_body":"i have a async inference on SageMaker, with BYOC.  The job may take about 20 minutes and more. And i already set InvocationTimeoutSeconds to 3600 seconds.    \nthe problem is, when i start a new inference request, from CloudWatch i know the job is in progress,  and there is not \/ping request log in CloudWatch. but the after about 10 minute ,  \/ping log in CloudWatch show up again with error, which says service unavailable.  \n then after 6 minute, i found a new log stream in CloudWatch, and the older one is down.  \nhere is the log in CloudWatch:\n\n```\n...(\/ping log, until i send a request)\n\n2023-05-17T16:12:15.761+08:00\ttask type:file ( my job start)\n2023-05-17T16:22:58.223+08:00.     [error] 31#31: *389 connect() to unix:\/tmp\/gunicorn.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client: 169.254.178.2, server: , request: \"GET \/ping HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\", host: \"169.254.180.2:8080\"\n2023-05-17T16:23:02.761+08:00\t169.254.178.2 - - [17\/May\/2023:08:22:58 +0000] \"GET \/ping HTTP\/1.1\" 502 166 \"-\" \"AHC\/2.0\"\n\n...(the error and \/ping repeat for 6 minute)\n\n2023-05-17T16:28:58.133+08:00    [error] 31#31: *449 connect() to unix:\/tmp\/gunicorn.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client: 169.254.178.2, server: , request: \"GET \/ping HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\", host: \"169.254.180.2:8080\"\n```\n\nhow can i fix it?",
        "Challenge_closed_time":1684462461646,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684318489587,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1684666245268,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUInjgvtuaRNi54eIpETzQ-Q\/async-inference-docker-restart-after-less-than-20-minutes-not-helpful-log-found",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":19.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":39.9922386111,
        "Challenge_title":"async inference docker restart after less than 20 minutes, not helpful log found",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":199,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If I understand your log snippets correctly, it looks like your container is failing to respond to any `\/ping`s while processing the long-running request? Failing to respond to ping for an extended period indicates your endpoint is unhealthy so will signal SageMaker to restart the container.\n\nA likely reason for not responding might be if your request handling uses multi-processing in a way that maxes out all CPUs on the instance? This would leave no cores\/threads available to handle to incoming pings while the data is getting processed. In that case, the fix would be to identify what component(s) of your request handling might be using all available system cores at once, and re-configuring them to use `int(os.environ[\"SM_NUM_CPUS\"]) - 1` instead.\n\nA similar but less likely reason is if for some reason you're using a fully-custom serving stack or have explicitly re-configured the default one to have only one worker thread: In which case your main request handling might be blocking the server with no threads available to pick up concurrent pings (even though there are CPU resources)?",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1684462461646,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":13.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":2,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649286157000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":41.0,
        "Challenge_repo_star_count":24.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":326,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":66.8869986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Challenge_closed_time":1579951518923,
        "Challenge_comment_count":2,
        "Challenge_created_time":1579709239420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1579710725728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":47.7,
        "Challenge_reading_time":60.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":67.2998619445,
        "Challenge_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1716.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":11.6,
        "Solution_reading_time":27.59,
        "Solution_score_count":4.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":289.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460494806016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":153.4413752778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1460495281968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459942893017,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":153.4413752778,
        "Challenge_title":"Azure Recommendations API's Parameter",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459941581603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":22.87,
        "Solution_score_count":3.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":227.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1386491614716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2778.0,
        "Answerer_view_count":352.0,
        "Challenge_adjusted_solved_time":1.7360491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Challenge_closed_time":1634844298240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634838048463,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.7360491667,
        "Challenge_title":"Training Job is Stopping in Sagemaker",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386491614716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2778.0,
        "Poster_view_count":352.0,
        "Solution_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":134.9044,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>if I tune a model with the LightGBMTunerCV I always get this massive result of the cv_agg's binary_logloss. If I do this with a bigger dataset, this (unnecessary) io slows down the performance of the optimization process.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nimport optuna\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nbreast_cancer = load_breast_cancer()\n\nX_train, X_test, Y_train, Y_test = train_test_split(breast_cancer.data, breast_cancer.target)\n\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=breast_cancer.feature_names.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=breast_cancer.feature_names.tolist())\ncallbacks = [lgb.log_evaluation(period=0)]\ntuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True)\n\n\ntuner.run()\n<\/code><\/pre>\n<p>And the output:<\/p>\n<pre><code>feature_fraction, val_score: 0.327411:  43%|###################2      | 3\/7 [00:00&lt;00:00, 13.84it\/s]\n[1] cv_agg's binary_logloss: 0.609496 + 0.009315\n[2] cv_agg's binary_logloss: 0.554522 + 0.00607596\n[3] cv_agg's binary_logloss: 0.512217 + 0.0132959\n[4] cv_agg's binary_logloss: 0.479142 + 0.0168108\n[5] cv_agg's binary_logloss: 0.440044 + 0.0166129\n[6] cv_agg's binary_logloss: 0.40653 + 0.0200005\n[7] cv_agg's binary_logloss: 0.382273 + 0.0242429\n[8] cv_agg's binary_logloss: 0.363559 + 0.03312\n<\/code><\/pre>\n<p>Is there any way to get rid of this output?<\/p>\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1638229134663,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637743478823,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70093026",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":134.9044,
        "Challenge_title":"Supressing optunas cv_agg's binary_logloss output",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":200.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398509643447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dortmund, Germany",
        "Poster_reputation_count":45.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can pass verbose_eval parameter with value None in LightGBMTunerCV().<\/p>\n<p>Example:<\/p>\n<pre><code>tuner = lgb.LightGBMTunerCV({&quot;objective&quot;: &quot;binary&quot;, 'verbose': -1},\n       train_set=test_dataset, num_boost_round=10,\n       nfold=5, stratified=True, shuffle=True, verbose_eval=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":4.25,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1397589101936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dominican Republic",
        "Answerer_reputation_count":563.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":10.2489016667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im creating a model using optuna lightgbm integration, My training set has some categorical features and i pass those features to the model using the <code>lgb.Dataset<\/code> class, here is the code im using ( NOTE: X_train, X_val, y_train, y_val are all pandas dataframes ).<\/p>\n<pre><code>\nimport lightgbm as lgb \n\n        grid = {\n            \n       \n            'boosting': 'gbdt',\n            'metric': ['huber', 'rmse' , 'mape'],\n            'verbose':1\n\n        }\n        \n        X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n        cat_features = [ col for col in X_train if col.startswith('cat') ]\n\n        dval = Dataset(X_val, label=y_val, categorical_feature=cat_features)\n        dtrain = Dataset(X_train, label=y_train,  categorical_feature=cat_features)\n        \n        model = lgb.train(      \n                                    grid,\n                                    dtrain,\n                                    valid_sets=[dval],\n                                    early_stopping_rounds=100)\n                                    \n\n<\/code><\/pre>\n<p>Every time the <code>lgb.train<\/code> function is called, i get the following user warning<\/p>\n<pre><code>\n UserWarning: categorical_column in param dict is overridden.\n\n<\/code><\/pre>\n<p>I believe that lighgbm is not treating my categorical features the way it should, someone knows how to fix this issue? Am i using the parameter correctly?<\/p>",
        "Challenge_closed_time":1613830364163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613793468117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66287854",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":10.2489016667,
        "Challenge_title":"Optuna lightgbm integration giving categorical features error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":328.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586625057632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>In case of picking the name (not indexes) of those columns, add as well the <code>feature_name<\/code> parameters as the <a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Dataset.html#lightgbm.Dataset.__init__\" rel=\"nofollow noreferrer\">documentation states<\/a><\/p>\n<p>That said, your <code>dval<\/code> and <code>dtrain<\/code> will be initialized as follow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>dval = Dataset(X_val, label=y_val, feature_name=cat_features, categorical_feature=cat_features)\ndtrain = Dataset(X_train, label=y_train, feature_name=cat_features, categorical_feature=cat_features)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.8,
        "Solution_reading_time":8.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":63.8910877778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Related: <a href=\"https:\/\/community.wandb.ai\/t\/unable-to-manage-columns-in-project-run-table\/3551\/4\" class=\"inline-onebox\">Unable to manage columns in project run table - #4 by artsiom<\/a><\/p>\n<p>I was unable to make step metric columns visible in the Table view. I tried logging metrics both via <code>run.log<\/code> and <code>wandb.log<\/code>, as well as refreshing the page in my browser. When attempting to drag and drop a column name from \u201cHidden Columns\u201d to \u201cVisible Columns\u201d (see the screenshot), a gap is created, but on mouse release the column name returns to \u201cHidden Columns\u201d. Clicking on column names to move them to \u201cVisible\u201d does not work either. The logged values appear in the web interface elsewhere. Manipulation with non-metric columns (e.g. config values, name, state etc) worked flawlessly as expected.<\/p>\n<p>The problem remained <em>for a fraction of a minute<\/em> after I logged a summary metric using <code>wandb.summary[...] = ...<\/code>. In particular, I tried moving all columns by pressing \u201cShow all\u201d, but without any visible result, and I closed the pop-up (on the screenshot). Suddenly, after 10 or so seconds, all columns became visible.<\/p>\n<p>The problem is similar to the one in the linked post. Unlike there, in my case, refreshing the web-page did not seem to help. I\u2019ll take a wild guess and suggest possible reasons for the bug:<\/p>\n<ol>\n<li>Something was going on in your back-end, and I had to wait till all necessary data validation or calculations are completed that would enable adding metric columns. This is unacceptably long time (several minutes), within which I was able to read relevant reference, search issues, and do a couple of empty test runs to see what\u2019s going on.<\/li>\n<li>There is a bug which prevents conversion step metrics to summary metrics unless at least one summary metric is explicitly added via <code>wandb.summary<\/code>.<\/li>\n<\/ol>\n<p>I hope you will be able to get to the bottom of it and fix it.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" data-download-href=\"\/uploads\/short-url\/6GUslld1E38x1uAv9m6acBIMSwH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" alt=\"image\" data-base62-sha1=\"6GUslld1E38x1uAv9m6acBIMSwH\" width=\"518\" height=\"500\" data-dominant-color=\"F7F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">667\u00d7643 8.92 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676062281055,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675832273139,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/could-not-add-summary-columns-for-display-in-table\/3841",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":38.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":63.8910877778,
        "Challenge_title":"Could not add summary columns for display in Table",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":211.0,
        "Challenge_word_count":348,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a> , I\u2019ve been able to to consistently  reproduce this behavior on my end and flagged it as a bug. I will update you on a timeline for a fix once I have additional info. Thanks again for the insight!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.07,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7235097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We are experimenting with programmatic report generation with WandB.<br>\nI would like to be able to add a Confusion Matrix to a report, but this is not one of the base types (as far as I can tell). Is there a good way to do this?<br>\nI could generate a PNG\/Image and insert it, but I can\u2019t figure out how to add an Image to a report yet (see recent question).  Are there other ways?<br>\nThanks.<\/p>",
        "Challenge_closed_time":1666015569168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665987764533,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/adding-confusion-matrix-to-report-programatically\/3267",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.7235097222,
        "Challenge_title":"Adding Confusion Matrix to report programatically",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> I am also posting here <a href=\"https:\/\/colab.research.google.com\/drive\/1Fepp-JLFvK-wLL2BZ_BnkCG_fAC6HFbo#scrollTo=An_example_with_all_of_the_blocks_and_panels\" rel=\"noopener nofollow ugc\">this Colab<\/a> and the Python SDK commands of our <a href=\"https:\/\/docs.wandb.ai\/guides\/reports\/edit-a-report#add-plots\">Reports reference docs<\/a> which may be helpful.<\/p>\n<p>The confusion matrix isn\u2019t <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/main\/wandb\/apis\/reports\/panels.py\" rel=\"noopener nofollow ugc\">currently exposed<\/a> but I have increased this feature requests for our engineering team. We will reach out to you once this is implemented. I hope this helps, please let me know if you have any further questions or issues with the Reports API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":10.75,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3388152778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hey all,<\/p>\n<p>I wondered if there was a way to track system power consumption caused by model development? I\u2019ve checked the W&amp;B docs and can\u2019t see anything.<br>\nIdeally I\u2019d love to be able to keep track of runs to see how much power is used by different runs but also the whole project.<\/p>\n<p>Elsewhere I\u2019ve seen packages such as <a href=\"https:\/\/pypi.org\/project\/energyusage\/\" rel=\"noopener nofollow ugc\">energyusage<\/a> but ideally would like to use something more integrated and could be aggregated across runs for whole projects.<br>\nIf something already exists I\u2019d love to hear about it, otherwise either if W&amp;B fancied adding this functionality that would be great or if it came to it if anyone would like to help me with this project.<\/p>\n<p>Thanks,<\/p>\n<p>Jeff.<\/p>",
        "Challenge_closed_time":1658218095948,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658216876213,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/track-power-energy-consumption\/2774",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":10.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3388152778,
        "Challenge_title":"Track power\/energy consumption?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":124,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nWe have this example that shows how to do this with CodeCarbon.<br>\n<a href=\"https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases--VmlldzoxMzM1NDg3\">https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases\u2013VmlldzoxMzM1NDg3<\/a><\/p>\n<p>You would use that library and log the info yourself. I do appreciate the feature request to integrate these more tightly.<br>\nThanks, hope this helps<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":7.58,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1504001058088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2101.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":243.1042716667,
        "Challenge_answer_count":2,
        "Challenge_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Challenge_closed_time":1561730574528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560855399150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":42.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":243.1042716667,
        "Challenge_title":"MLflow Error while deploying the Model to local REST server",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1840.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504001058088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2101.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.8,
        "Solution_reading_time":17.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":134.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":3.7756897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Challenge_closed_time":1653626541543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653612949060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.7756897222,
        "Challenge_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":183.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1549041651583,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":3106.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":52.7292055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#workspace-level-quota\" rel=\"nofollow noreferrer\">link<\/a>), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?<\/p>",
        "Challenge_closed_time":1597341970283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597248542537,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597249191852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63380531",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":25.9521516667,
        "Challenge_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369863777596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation_count":335.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.<\/p>\n<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest<\/code> command in the CLI.<\/p>\n<p>There are a few commands that may interest you:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspacesandcomputes\/usages\/list\" rel=\"nofollow noreferrer\">Usage<\/a> and Quotas for a region:\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/usages?api-version=2019-05-01<\/code>\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/quotas?api-version=2020-04-01<\/code><\/p>\n<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1597439016992,
        "Solution_link_count":3.0,
        "Solution_readability":18.8,
        "Solution_reading_time":20.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":131.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":100.1738869445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have an OS stable diffusion fine tuner and use Tensorboard locally and am trying to integrate wandb with existing code that is largely just calling writer.log_scalar(\u2026).  I setup my SummaryWriter then call wandb.init, but I\u2019m having all sorts of odd behavior where most of the time only system monitors (gpu temp, memory etc) are logged to wandb and my calls to writer.log_scalar simply never get recorded to wandb.<\/p>\n<p>Everything seems to be failing silently and I don\u2019t know why nothing gets recorded.  The other day testing on two machines it works from one but not the other, and it is also now working from Colab notebook instances or docker container runs.<\/p>\n<p>The runs on <a href=\"http:\/\/wandb.com\" rel=\"noopener nofollow ugc\">wandb.com<\/a> are there and created, console output shows it fires up and links me to the run and the run URL works, etc.  But, only system monitors are showing up, none of my items logged with summarywriter, at least a vast majority of instances.<\/p>\n<p>At one point it was working fine, then started to stop working.  I had thought it was an issue with trying to pass in a dict of dicts to config={main: args, opt_cfg: optimizer_cfg} but even passing in dummy objects or simply config=args it fails.  At one point wanb.init was done before writer instantiation, and that was fixed, so I\u2019m not sure at what point things went sideways as I mostly run locally but many users use Colab\/Vast, etc and wandb is a significantly better solution for those cases.<\/p>\n<p>Is there any log file or debugging I can use to troubleshoot this?  Unfortunately it is just not working and doing so silently without any feedback.<\/p>",
        "Challenge_closed_time":1679780487814,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679419861821,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/with-tb-summarywriter-only-getting-sys-logs-no-log-scalar-shows-up\/4089",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":21.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":100.1738869445,
        "Challenge_title":"With TB SummaryWriter only getting sys logs, no log_scalar shows up",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":287,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ah I think I found the magic combination to work for my training script.  For posterity in case anyone else has the issues and stumbles on this post.  This is a raw torch trainer.<\/p>\n<p>(ex log_folder = \u201clogs\/projectname20230325_124523\u201d and contains the events.out.tfevents\u2026 file)<\/p>\n<pre><code class=\"lang-auto\">        wandb.tensorboard.patch(root_logdir=log_folder, pytorch=False, tensorboard_x=False, save=False)\n        wandb_run = wandb.init(\n            project=args.project_name,\n            config={\"main_cfg\": vars(args), \"optimizer_cfg\": optimizer_config},\n            name=args.run_name\n            )\n        log_writer = SummaryWriter(log_dir=log_folder...)\n\n        log_writer.add_scalar(...)\n<\/code><\/pre>\n<p>tensorboard 2.12.0<br>\nwandb 0.14.0<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.09,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.9586066667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi!<\/p>\n<p>I have a Pay-As-You-Go account (with monthly amounts of free services) and want to create\/deploy a machine learning model in Azure ML Studio. Is it included in my subscription or do I have to pay more? I'm asking because in order to start working in ML Studio, I need to create a 'compute instance' and there is no free option.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1673651290070,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673619039086,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160625\/is-machine-learning-studio-included-in-my-subscrip",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.9586066667,
        "Challenge_title":"Is Machine Learning Studio included in my subscription?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@Marco <\/a>, Thanks for using Microsoft Q&amp;A Platform.  <\/p>\n<p>There are no additional costs associated with using Machine Learning service under a <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/purchase-options\/pay-as-you-go\/\">Pay-as-you-go<\/a> subscription.  But, for creating a compute instance it charges since it run on Azure infrastructure. If you have any doubts, always use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/\">Azure pricing calculator<\/a> to get an estimate of the costs before you create anything.<\/p>\n<p>Please visit here to know more information on <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning\/\">Machine Learning pricing<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-plan-manage-cost\">cost management<\/a>.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,\nVasavi<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.5,
        "Solution_reading_time":12.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5819444444,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Challenge_closed_time":1657616964000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657550069000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":9.1,
        "Challenge_reading_time":23.2,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":18.5819444444,
        "Challenge_title":"[mlflow] Extra args broken",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":213,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @subramaniam20jan \r\n\r\nCould you please share your values.yaml file with me? Do you have any additional change in it? Hi @subramaniam20jan \r\n\r\nI really didn't understand the problem. Static prefix is valid argument for mlflow server. You can find more information [here](https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-static-prefix).\r\n\r\nAlso, it tested with argument and without argument in [the unit tests](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/charts\/mlflow\/unittests\/deployment_test.yaml#L65). Also, it tested without argument in the integration test which we run it with [kind here](https:\/\/github.com\/community-charts\/helm-charts\/runs\/7283774204?check_suite_focus=true#step:12:175).\r\n\r\nI'm really not able to recreate the issue. Could you please share the error message? Well, if you use `mlflow ui` command, you must change it to `mlflow server` command for production usage. You can find same explanation from here: https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui Actually it was my bad. This wasnt really an issue but I appreciate the addition of the extra parameters to the readiness and liveness probe :) btw, @burakince great job on the chart and image! I was something I have made many times in individual assignments and missed having in open source somewhere. Came across your project when I was about to create one myself. Saves me a lot of work :) Thanks @subramaniam20jan :) I just added an example usage to [here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/liveness-probe-and-readiness-probe-example).\r\n\r\nBest",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.2,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":191.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1604747085276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":24.4361597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Challenge_closed_time":1604748950752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604660980577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660057709880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.4361597222,
        "Challenge_title":"Is there a way to log the keras model summary to neptune?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":132.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":513.8669444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Challenge_closed_time":1597886708000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596036787000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925777990,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sagemaker-autopilot-doing-when-in-state-inprogress-analyzingdata",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":513.8669444444,
        "Challenge_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML.  There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1609768237540,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":247.7520736111,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Challenge_closed_time":1656556961635,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655665054170,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.6,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":247.7520736111,
        "Challenge_title":"Can't associate sweeps with project",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":815.0,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":26.3555658333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Challenge_closed_time":1656329528480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655994282873,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656234648443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":20.6,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":93.1237797222,
        "Challenge_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":182.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":15.6380441667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Challenge_closed_time":1641809697276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641753400317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":15.6380441667,
        "Challenge_title":"wandb.plot.line does not work and it just shows a table",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445719444550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Iran, Canada",
        "Poster_reputation_count":331.0,
        "Poster_view_count":40.0,
        "Solution_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.6,
        "Solution_reading_time":7.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":276.9580555556,
        "Challenge_answer_count":0,
        "Challenge_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Challenge_closed_time":1635811087000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634814038000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":276.9580555556,
        "Challenge_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1308769948883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":35204.0,
        "Answerer_view_count":4971.0,
        "Challenge_adjusted_solved_time":30.0725488889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1660669810152,
        "Challenge_comment_count":7,
        "Challenge_created_time":1658770653850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660741967556,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":10.5,
        "Challenge_reading_time":31.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":527.5434172222,
        "Challenge_title":"Hyperparameter data types and scales not being validated",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":326,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1660850228732,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1592311727163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":138.6078019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Challenge_closed_time":1603707766790,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603208778703,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":32.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":138.6078019445,
        "Challenge_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":4.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":55.4034008333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the table view of a project, is it possible to show only the columns that have different values among runs? This would be very useful to quickly explore how changing parameters affect the model.<\/p>",
        "Challenge_closed_time":1661383337880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661183885637,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/show-only-columns-with-different-values-in-experiments-table\/2972",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":3.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":55.4034008333,
        "Challenge_title":"Show only columns with different values in experiments table",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/enajx\">@enajx<\/a> , would the run comparer table work for your use case, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">here<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1510.63,
        "Challenge_answer_count":1,
        "Challenge_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Challenge_closed_time":1657547192000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652108924000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":693.0,
        "Challenge_repo_star_count":111.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1510.63,
        "Challenge_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolved in #420",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.9,
        "Solution_reading_time":0.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.2845952778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am trying to run the demo code from <a href=\"https:\/\/colab.research.google.com\/gist\/sayakpaul\/5b31ed03725cc6ae2af41848d4acee45\/demo_tensorboard.ipynb\" rel=\"noopener nofollow ugc\">Demo_tensorboard.ipynb<\/a> so that I can learn more about the use of Tensorboard in combination with W&amp;B. Unfortunately this code throws this warning:<\/p>\n<p>WARNING When using several event log directories, please call <code>wandb.tensorboard.patch(root_logdir=\"...\")<\/code> before <code>wandb.init<\/code><\/p>\n<p>When I implement the suggested change with:<\/p>\n<p><code>wandb.tensorboard.patch(root_logdir=\".\/logs\/debug\")<\/code><\/p>\n<p>I get the following warning:<br>\nFound log directory outside of given root_logdir, dropping given root_logdir for event file in i:\\tinyml\\tiny_cnn\\wandb\\run-20221016_205607-22b9tlzf\\files\\train<\/p>\n<p>So my questions is: What is a suitable root_logdir for Tensorboard?<\/p>\n<p>Thanks for your support.<\/p>",
        "Challenge_closed_time":1666012581990,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665946757447,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logging-with-tensorboard\/3265",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.2845952778,
        "Challenge_title":"Logging with Tensorboard",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Susanne,<\/p>\n<p>Thanks for writing in! The <code>root_logdir<\/code>argument is the path to the root of all tfevent files, so you can use the wandb project folder (in this case I think it is <code>I:\/tinyml\/tiny_cnn<\/code>). Could you try if setting this solves the issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":3.8,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.5133555556,
        "Challenge_answer_count":2,
        "Challenge_body":"I've configured a model for async-inference, and its working correctly - I can submit a file via `invoke_endpoint_async` and download the output from s3.\n\nI'm now trying to configure auto-scaling. I'm trying experimentation with different options, but basically I want to configure 0-1 instances, have an instance created when`invoke_endpoint_async` is called, and have the instance shutdown shortly afterwards (along the lines of batch inference)\n\nI'm struggling to get it to work - I'm experiencing similar issues to https:\/\/github.com\/boto\/boto3\/issues\/2839\n\nFirst I think there's an issue with the `console` - if I  `aws register-scalable-target ...` it works but the console doesn't like the zero for `min-capacity`\n\n![Enter image description here](\/media\/postImages\/original\/IMWZdtU68ZSXSSvr46-_1nhw)\n\nI think this is just a UI nit though, I don't understand how the policy works - I have\n\n```json\n{\n    \"TargetValue\": 1.0,\n    \"CustomizedMetricSpecification\": {\n        \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n        \"Namespace\": \"AWS\/SageMaker\",\n        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": \"***-test-endpoint-2023-03-24-04-28-06-341\"}],\n        \"Statistic\": \"Average\"\n    },\n    \"ScaleInCooldown\": 60,\n    \"ScaleOutCooldown\": 60\n}\n```\n\nThe first point of confusion was the console shows a built-in and custom policy. I was initially using the name of the built-in policy (SageMakerEndpointInvocationScalingPolicy) but `put-scaling-policy` doesn't appear to edit it - it creates a new policy with the same name.\n\nWhen I monitor the scaling activity ()\n\n```console\naws application-autoscaling describe-scaling-activities \\\n    --service-namespace sagemaker\n```\n\nI can initially see \"Successfully set desired instance count to 0. Change successfully fulfilled by sagemaker.\" \n\nBut when I involve the endpoint with \n\n```python\nresponse = sm_runtime.invoke_endpoint_async(\n    EndpointName=endpoint_name, \n    InputLocation=\"***\/input\/data.json\",\n    ContentType='application\/jsonlines',\n    Accept='application\/jsonlines')\n\noutput_location = response['OutputLocation']\n```\n\nI would expect to see the instance count increase to 1, then back to zero within a space of a few minutes. I have occasionally got it to do something but not reliably. I think the main issue is I don't understand the metric and how it interacts with the target.\n\nI've seen charts but I cannot figure out how to plot the \"ApproximateBacklogSizePerInstance\"? And how does it interact with \"TargetValue\"? What is the actual trigger for a scale in\/out?",
        "Challenge_closed_time":1679845259536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679638211456,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1679985798951,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUT4xru2SdTSqtoNyt1XV3VA\/configuring-auto-scaling-for-sagemaker-async-inference",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":32.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":57.5133555556,
        "Challenge_title":"Configuring auto-scaling for sagemaker async-inference",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":162.0,
        "Challenge_word_count":310,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A target tracking scaling policy will create 2 CloudWatch alarms (one for high and one for low usage), which you'll be able to see in the CloudWatch alarms console.  The high usage policy needs to have 3 consecutive 60 second breaching datapoints to trigger a scale-out; and the low alarm needs 15 consecutive 60 second breaching datapoints to scale-in\n\nYou may instead want to use step scaling policies, where you are able to create and control the alarms as well as the policy settings\nhttps:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/application-auto-scaling-step-scaling-policies.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679845259536,
        "Solution_link_count":1.0,
        "Solution_readability":21.0,
        "Solution_reading_time":7.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":0.4063402778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Challenge_closed_time":1464685093652,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464683630827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1465977920520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37540703",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.2,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4063402778,
        "Challenge_title":"Test multiple algorithms in one experiment",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1564790214540,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":113.7935575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Challenge_closed_time":1569427861030,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569018204223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":113.7935575,
        "Challenge_title":"AML run.log() and run.log_list() fail without error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465320834943,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":677.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1536318503563,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":150.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":52.5443647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here's question proposed at the end of the chapter in 70-774 exam reference book. <\/p>\n\n<blockquote>\n  <p>If you connect a neural network with a Tune Model Hyperparameters module configured\n  with Random Sweep and Maximum number of runs on random sweep = 1, how\n  many neural networks are trained during the execution of the experiment? Why? If you\n  connect a validation dataset to the third input of the Tune Model Hyperparameters\n  module, how many neural networks are trained now?<\/p>\n<\/blockquote>\n\n<p>And the answer is :<\/p>\n\n<blockquote>\n  <p>Without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data\n  with the best combination of hyperparameters). With the validation set only 1 neural\n  network is trained, so the best model is not trained using the validation set if you provide\n  it.<\/p>\n<\/blockquote>\n\n<p>Where does 10 come from? As far as I understand the number should be 2 and 1 respectively. Shouldn't it create n-folds where n is equal to the number of runs?<\/p>",
        "Challenge_closed_time":1539202336323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539013176610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52705769",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":52.5443647222,
        "Challenge_title":"Azure ML Tune Model Hyper Parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528790837107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":610.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":7.59,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":21.6879675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Challenge_closed_time":1539831993640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539753916957,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":15.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":21.6879675,
        "Challenge_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403185902747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation_count":169.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":71.6,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3846.9952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Challenge_closed_time":1636988013000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623138830000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3846.9952777778,
        "Challenge_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@tchaton Is this a lightning issue? Closing this issue as there is no progress nor manifestation from the Comet Team.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.018835,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I run several agents in one sweep.<br>\nI want to stop a specific agent among them, but I don\u2019t know how to stop it.<\/p>",
        "Challenge_closed_time":1658470957452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658330489646,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-kill-a-specific-agent-using-a-command-in-terminal\/2782",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.2,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":39.018835,
        "Challenge_title":"How to kill a specific agent using a command in terminal?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jeongwhanchoi\">@jeongwhanchoi<\/a> , please see this <a href=\"https:\/\/community.wandb.ai\/t\/hp-sweep-correct-way-to-stop-a-specific-agent-and-not-the-entire-sweep\/1173\">post<\/a> for stopping agents. Please let me know if you have additional questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":3.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":0.5757988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use <code>[OPTUNA][1]<\/code> with <code>sklearn<\/code> <code>[MLPRegressor][1]<\/code> model.<\/p>\n<p>For almost all hyperparameters it is quite straightforward how to set OPTUNA for them.\nFor example, to set the learning rate:\n<code>learning_rate_init = trial.suggest_float('learning_rate_init ',0.0001, 0.1001, step=0.005)<\/code><\/p>\n<p>My problem is how to set it for <code>hidden_layer_sizes<\/code> since it is a tuple. So let's say I would like to have two hidden layers where the first will have 100 neurons and the second will have 50 neurons. Without OPTUNA I would do:<\/p>\n<p><code>MLPRegressor( hidden_layer_sizes =(100,50))<\/code><\/p>\n<p>But what if I want OPTUNA to try different neurons in each layer? e.g., from 100 to 500, how can I set it? the <code>MLPRegressor<\/code> expects a tuple<\/p>",
        "Challenge_closed_time":1636650321043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636648248167,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1660311769156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931757",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":11.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5757988889,
        "Challenge_title":"How to set hidden_layer_sizes in sklearn MLPRegressor using optuna trial",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517513644076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":995.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You could set up your objective function as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport warnings\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nwarnings.filterwarnings('ignore')\n\nX, y = make_regression(random_state=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n\ndef objective(trial):\n\n    params = {\n        'learning_rate_init': trial.suggest_float('learning_rate_init ', 0.0001, 0.1, step=0.005),\n        'first_layer_neurons': trial.suggest_int('first_layer_neurons', 10, 100, step=10),\n        'second_layer_neurons': trial.suggest_int('second_layer_neurons', 10, 100, step=10),\n        'activation': trial.suggest_categorical('activation', ['identity', 'tanh', 'relu']),\n    }\n\n    model = MLPRegressor(\n        hidden_layer_sizes=(params['first_layer_neurons'], params['second_layer_neurons']),\n        learning_rate_init=params['learning_rate_init'],\n        activation=params['activation'],\n        random_state=1,\n        max_iter=100\n    )\n\n    model.fit(X_train, y_train)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=3)\n# [I 2021-11-11 18:04:02,216] A new study created in memory with name: no-name-14c92e38-b8cd-4b8d-8a95-77158d996f20\n# [I 2021-11-11 18:04:02,283] Trial 0 finished with value: 161.8347337123744 and parameters: {'learning_rate_init ': 0.0651, 'first_layer_neurons': 20, 'second_layer_neurons': 40, 'activation': 'tanh'}. Best is trial 0 with value: 161.8347337123744.\n# [I 2021-11-11 18:04:02,368] Trial 1 finished with value: 159.55535852658082 and parameters: {'learning_rate_init ': 0.0551, 'first_layer_neurons': 90, 'second_layer_neurons': 70, 'activation': 'relu'}. Best is trial 1 with value: 159.55535852658082.\n# [I 2021-11-11 18:04:02,440] Trial 2 finished with value: 161.73980822730888 and parameters: {'learning_rate_init ': 0.0051, 'first_layer_neurons': 100, 'second_layer_neurons': 30, 'activation': 'identity'}. Best is trial 1 with value: 159.55535852658082.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":28.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":174.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":5,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631000506000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":10471.2408333333,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":112.1258333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714116000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601310463000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":22.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":112.1258333333,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"0332ede5",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-3.5,
        "Solution_reading_time":0.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":1.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1352429442632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":740.0,
        "Answerer_view_count":63.0,
        "Challenge_adjusted_solved_time":35.1769344445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Challenge_closed_time":1662569956687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662443319723,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":24.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":35.1769344445,
        "Challenge_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":35.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352429442632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":740.0,
        "Poster_view_count":63.0,
        "Solution_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.6,
        "Solution_reading_time":14.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":83.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1531218624572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":78.4838436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a function which looks like this:<\/p>\n<pre><code>def fine_tuning(x,y,model1,model2,model3,trial):\n   pred1 = model1.predict(x)\n   pred2 = model2.predict(x)\n   pred3 = model3.predict(x)\n   \n   h1 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h2 = trial.suggest_float('h1', 0.0001, 1, log = True)\n   h3 = trial.suggest_float('h1', 0.0001, 1, log = True)\n\n   pred = pred1 * h1 + pred2 * h2 + pred3 * h3\n\n   return mean_absolute_error(y, pred)\n<\/code><\/pre>\n<p>The problem with this function is that h1+h2+h3 != 1. How would I change this function in order to make the sum of the hyperparmaters = 1?<\/p>",
        "Challenge_closed_time":1627290244260,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627007702423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68493392",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":78.4838436111,
        "Challenge_title":"Making hyperparameters add up to 1 when fine tuning using Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575887707992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":196.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>Basically, you're looking for a dirichlet distribution for h1, 2, 3. Here's a guide on how to implement that for Optuna: <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-do-i-suggest-variables-which-represent-the-proportion-that-is-are-in-accordance-with-dirichlet-distribution<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":38.4,
        "Solution_reading_time":6.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4928227778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to understand the difference between those two function calls:<\/p>\n<p>I am referring to the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-the-summary\">documentation of define_metric<\/a>:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>Is it the \u201cbest\u201d accuracy ever measured (during training) versus the accuracy of the \u201cbest\u201d (validation) model? I understand that wandb does not care what metric I log, but what is the intended use?<\/p>\n<p>Thank you for clarification.<\/p>",
        "Challenge_closed_time":1659448810923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659447036761,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/understanding-define-metric-parameters\/2836",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4928227778,
        "Challenge_title":"Understanding define_metric parameters",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>,<\/p>\n<p>For each metric logged, there is a summary metric that\u2019ll summarize the logged values as <em>one<\/em> value for each run. By default, W&amp;B uses the <em>latest<\/em> value, but you can update it with <code>wandb.summary['acc'] = best_acc<\/code> or using the two <code>define_metric<\/code> calls you show.<\/p>\n<p>This is then used to decide which value is displayed in plots that only use one value for each run (e.g. Scatter plots).<\/p>\n<pre><code class=\"lang-auto\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>These two calls are both functionally the same, one will show <code>acc.best<\/code> and one will show as <code>acc.max<\/code> in the summary metrics of your run. Both will be the maximum value that you log for <code>acc<\/code> like <code>wandb.log('acc':acc)<\/code> during a run.<\/p>\n<p>You can see the summary metrics of each run by clicking the <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> icon in the top left nav bar in a run.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.87,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":148.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":67.9020225,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>We can use <code>wandb.watch(model, criterion, ...)<\/code> in order to log a model + a loss function.<br>\nBut my loss function is not something simple like: <code>criterion = nn.CrossEntropyLoss()<\/code>.<\/p>\n<p>Rather, here\u2019s how I calculate my loss:<\/p>\n<pre><code class=\"lang-auto\">            # `set_to_none=True` boosts performance\n            optimizer.zero_grad(set_to_none=True)\n            masks_pred = model(imgs)\n\n            probs = F.softmax(masks_pred, dim=1).float()\n            ground_truth = F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float()\n\n            loss = criterion(masks_pred, masks) + dice_loss(probs, ground_truth)\n            loss.backward()\n            optimizer.step()\n<\/code><\/pre>\n<p>As you can see, the loss is a composition of 2 functions: the criterion and the <code>dice_loss<\/code> function.<br>\nWhat should I pass to <code>wandb.watch<\/code> for the <code>criterion<\/code> argument?<\/p>",
        "Challenge_closed_time":1657306434860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657061987579,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-custom-criterion-function\/2703",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":67.9020225,
        "Challenge_title":"How to log custom criterion function?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":111.0,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vroomerify\">@vroomerify<\/a>,<\/p>\n<p>Thanks for reaching out. <code>wandb.watch<\/code> expects a torch function as a criterion parameter. You can set up a custom criterion function by subclassing <code>torch.nn.Module<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1412515367427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":682.3236119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559693969323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557237604320,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":9.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":682.3236119445,
        "Challenge_title":"Beginners guide to Sagemaker",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":748.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554397763220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.6,
        "Solution_reading_time":18.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1599815370823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Western Australia, Australia",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":0.3898341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Challenge_closed_time":1611053713910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611052310507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65789715",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3898341667,
        "Challenge_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608633925527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dubai - United Arab Emirates",
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":10.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1502816769670,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Darmstadt, Germany",
        "Answerer_reputation_count":1998.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":54.6594472223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Challenge_closed_time":1653766025390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653569251380,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":54.6594472223,
        "Challenge_title":"Sagemaker doesn't inference in an async manner",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495477835623,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rzeszow, Poland",
        "Poster_reputation_count":148.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.4,
        "Solution_reading_time":15.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":201.2384813889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with wandb.sweep(). As I know the params are defined in sweep_id and insert into wandb.sweep() like this:<\/p>\n<pre><code class=\"lang-auto\">    sweep_configuration = {\n    'method': 'bayes',\n    'name': 'I dont believe that I can not just give you a name!',\n    'metric': {'goal': 'minimize', 'name': 'Valid\/final_ber'},\n    'parameters':\n    {\n        'batch_size': {'distribution': 'int_uniform','min': 10,'max': 12},\n        'lr': {'distribution': 'int_uniform','max': -3,'min': -4}\n    }\n    }\n    sweep_id = wandb.sweep(sweep=sweep_configuration, project=args.project, entity=args.entity)\n<\/code><\/pre>\n<p>Now I what I want to do is to extract the params <strong>batch_size<\/strong> and <strong>lr<\/strong> from each sweep into the name of <code>wand.init()<\/code>, because I need these information in name of each run to identify them.<br>\nBut in wandb frame, I cannot get access to the params in <code>wandb.config<\/code> before <code>wandb.init()<\/code>. As a result I cannot define argument <strong>name<\/strong>  in <code>wandb.init()<\/code> with params which are given during each sweep.<\/p>\n<pre><code class=\"lang-auto\">......\nwandb.init(name=f'{wandb.config.lr}_{wandb.config.batch_size}')\n......\n\nRun wnb56ush errored: Error('You must call wandb.init() before wandb.config.lr')\nwandb: ERROR Run wnb56ush errored: Error('You must call wandb.init() before wandb.config.lr')\n<\/code><\/pre>\n<p>Is there a way to get the params given by <code>wandb.sweep()<\/code> before wandb.init()?<br>\nThanks at advance<\/p>",
        "Challenge_closed_time":1680815891260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680091432727,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-can-i-extract-params-from-sweep-and-add-them-into-name-of-wandb-init\/4146",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":201.2384813889,
        "Challenge_title":"How can I extract params from sweep and add them into name of wandb.init()",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":183,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/1060111768\">@1060111768<\/a> , it\u2019s not possible to get the sweep parameters before calling <code>wandb.init()<\/code>.<\/p>\n<p>When you run <code>wandb.sweep()<\/code> to define a hyperparameter sweep, it generates a unique sweep ID that is used to link the sweep to the subsequent runs that are generated by the sweep. This sweep ID is used to retrieve the sweep parameters when you initialize WandB by calling <code>wandb.init()<\/code>. The <code>wandb.init()<\/code> function retrieves the sweep parameters from the WandB servers using the sweep ID, and uses them to configure the run. Once you have called <code>wandb.init()<\/code>, you can access the sweep parameters using the <code>config<\/code> object.<\/p>\n<p>Instead of specifying a name in wandb init, rename the run immediately after initializing the run.<br>\nExample:<\/p>\n<pre><code class=\"lang-auto\">run =  wandb.init(config=config)\nrun.name=f\"{wandb.config.lr}_{wandb.config.batch_size}\"\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":12.78,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1645548595867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1.8311227778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Challenge_closed_time":1645548706456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645542424610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":12.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.7449572222,
        "Challenge_title":"How to get wandb to pass arguments by position?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":270.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440414980200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":5.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1645549016652,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":23.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":173.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1501350003092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":1294.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":32.1489905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Challenge_closed_time":1595539712143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595423975777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":32.1489905556,
        "Challenge_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495758291316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Poster_reputation_count":138.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":5.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":688.1130463889,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554497484963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":438.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    }
]