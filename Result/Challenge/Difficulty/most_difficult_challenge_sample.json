[
    {
        "Challenge_adjusted_solved_time":24155.5917811111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Challenge_closed_time":1522603988172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1435643857760,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"time trigger http client execut workspac set timeout valu http client prevent timeout",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Challenge_link_count":0,
        "Challenge_original_content":"web servic time creat trigger http client workspac execut time trigger http client set timeout valu http client set timeout valu",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"web servic time creat trigger http client workspac execut time trigger http client set timeout valu http client set timeout valu",
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24155.5917811111,
        "Challenge_title":"Azure ML web service times out",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"accord featur request set timeout valu trigger http client msdn forum batch execut servic start asynchron web servic sampl",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"isn set timeout base featur request mark plan msdn forum batch execut servic start asynchron web servic sampl comment sampl httpclient client httpclient var request batchexecutionrequest output dictionari output azureblobdatarefer connectionstr storageconnectionstr relativeloc format outputresult file extens storagecontainernam replac locat output file file extens csv score ilearn train model globalparamet dictionari client defaultrequesthead author authenticationheadervalu bearer apikei warn await statement deadlock call thread asp net address configureawait execut resum origin context instanc replac await dosometask await dosometask configureawait consol writelin submit job submit job var respons await client postasjsonasync baseurl api version request respons issuccetatuscod await writefailedrespons respons return jobid await respons readasasync consol writelin format job jobid start job consol writelin start job respons await client postasync baseurl jobid start api version null respons issuccetatuscod await writefailedrespons respons return jobloc baseurl jobid api version stopwatch watch stopwatch startnew bool consol writelin job statu respons await client getasync jobloc respons issuccetatuscod await writefailedrespons respons return batchscorestatu statu await respons readasasync watch elapsedmillisecond timeoutinmillisecond consol writelin format time delet job jobid await client deleteasync jobloc switch statu statuscod batchscorestatuscod notstart consol writelin format job start jobid break batchscorestatuscod run consol writelin format job run jobid break batchscorestatuscod consol writelin format job jobid consol writelin format statu break batchscorestatuscod cancel consol writelin format job cancel jobid break batchscorestatuscod finish consol writelin format job finish jobid processresult statu break thread sleep wait",
        "Solution_preprocessed_content":"isn set timeout base featur request mark plan msdn forum batch execut servic start asynchron web servic sampl",
        "Solution_readability":13.3,
        "Solution_reading_time":50.86,
        "Solution_score":0,
        "Solution_sentence_count":45,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":338,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":19018.4561647222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use the RStudio IDE to R on an Amazon SageMaker instance. What I have tried so far is to run the following docker command:<\/p>\n\n<pre><code>docker run --rm -p 8787:8787 rocker\/verse\n<\/code><\/pre>\n\n<p>which appears to work successfully. What I would then do when running that command from my local computer is go to <code>http:\/\/localhost:8787<\/code> where I would be able to login and find a fully functional RStudio IDE within my browser. <\/p>\n\n<p>However, this is obviously not possible from within SageMaker as there is no <code>localhost<\/code> to visit.<\/p>\n\n<p>Is there some way I can direct my browser to capture the output to port 8787 from the SageMaker instance? <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Challenge_closed_time":1636510076040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568043633847,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"rstudio id instanc tri run docker successfulli local access rstudio id browser localhost visit direct browser captur output port instanc",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857195",
        "Challenge_link_count":1,
        "Challenge_original_content":"rstudio id instanc rstudio id instanc tri run docker docker run rocker vers successfulli run local http localhost login fulli function rstudio id browser obvious localhost visit direct browser captur output port instanc advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"rstudio id instanc rstudio id instanc tri run docker successfulli run local login fulli function rstudio id browser obvious visit direct browser captur output port instanc advanc",
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":19018.4561647222,
        "Challenge_title":"Is it possible to use RStudio IDE on an AWS SageMaker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1935.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>RStudio is now available as a managed service in SageMaker. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"rstudio servic access docker link document servic",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"rstudio servic http doc com latest rstudio html",
        "Solution_preprocessed_content":null,
        "Solution_readability":30.8,
        "Solution_reading_time":3.16,
        "Solution_score":3,
        "Solution_sentence_count":2,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":14,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":17600.8984933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Challenge_closed_time":1612305167163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548941458327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat perman login jupyt notebook github ssh rsa kei pair secret kei ssh rsa file instanc reboot file download github repo push updat guidanc",
        "Challenge_last_edit_time":1548941932587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54461730",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat perman login jupyt notebook github ssh rsa kei pair creat notebook devengin secret kei ssh rsa file instanc reboot github repo download push updat",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"creat perman login jupyt notebook github kei pair creat notebook devengin secret kei file instanc reboot repo download push updat",
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17601.0302322222,
        "Challenge_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1856.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"creat folder home ssh run ssh keygen set locat home ssh rsa clone repo git ssh ssh ssh rsa dev null git clone git domain account repo git repo set repo locat git config core sshcommand ssh ssh rsa dev null",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"filesystem notebook instanc filesystem size mount devtmpf dev tmpf dev shm dev nvmenp dev nvmen home note home save reboot ssh kei store home ssh lost reboot creat folder home ssh run ssh keygen set locat home ssh rsa clone repo git ssh ssh ssh rsa dev null git clone git domain account repo git repo set repo locat git config core sshcommand ssh ssh rsa dev null base http superus com",
        "Solution_preprocessed_content":"filesystem notebook instanc note save reboot ssh kei store lost reboot creat folder run set locat clone repo repo set repo locat base",
        "Solution_readability":10.0,
        "Solution_reading_time":14.69,
        "Solution_score":3,
        "Solution_sentence_count":11,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":123,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":15023.6469805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1634653018680,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal vowpal wabbit linux tri instal pip instal vowpalwabbit relat cmake boost librari instal vowpal wabbit bind",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_original_content":"instal vowpal wabbit linux explain instal vowpalwabbit bind linux reason onlin pip instal vowpalwabbit return cach http file pythonhost org packag fcdfdadedbefecbeeddefac vowpalwabbit tar build wheel collect packag vowpalwabbit build wheel vowpalwabbit setup exit statu home anaconda env jupytersystemenv bin import sy setuptool token sy argv tmp pip instal tvpt vowpalwabbit setup file tmp pip instal tvpt vowpalwabbit setup getattr token open open file read replac close exec compil file exec bdist wheel tmp pip wheel xjac tag cwd tmp pip instal tvpt vowpalwabbit lower cmake usr lib dist packag cmake data share cmake modul findboost cmake messag request boost librari boost version boost path usr boost librari boost boost librari instal addit boost librari set boost librarydir directori boost librari boost root locat boost",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"instal vowpal wabbit explain instal vowpalwabbit linux reason return lower",
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15023.6469805556,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"instal vowpal wabbit pip instal vowpalwabbit linux test year later notebook instanc train job vowpalwabbit txt file send scikit instal successfulli",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"test year later pip instal vowpalwabbit notebook instanc train job vowpalwabbit txt send scikit dkr ecr amazonaw com scikit cpu instal successfulli test vowpalwabbit",
        "Solution_preprocessed_content":"test year later notebook instanc train job vowpalwabbit send scikit instal successfulli test",
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":38,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617867548000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"mnist pytorch train unreason slow minut trial nearli minut entir standard singl nvidia tesla gpu",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_original_content":"train extrem slow environ nni version nni mode local remot pai remot client window server remot mode linux version pytorch tensorflow version pytorch conda virtualenv venv conda run docker log messag nnimanag log datastor initi restserv start restserv base port rest server listen http nnimanag setclustermetadata kei aml config valu subscriptionid resourcegroup workspacenam computetarget nnimanag setclustermetadata kei nni valu nnimanagerip nnimanag setclustermetadata kei trial config valu mnist codedir yanmi nni trial mnist pytorch imag msranni nni start fybaxk nnimanag statu initi run add event listen trialdispatch start channel amlcommandchannel trialdispatch copi set nnimanag receiv dispatch nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index initi environ trialdispatch run loop start submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ servic aml environ xlegg request environ nni exp fybaxk acd job nni exp fybaxk env xlegg request environ live trial live environ neededenvironmentcount requestedcount environmentinform nni exp fybaxk env xlegg statu unknown wait environmentinform nni exp fybaxk env xlegg statu wait run trialdispatch env nni exp fybaxk acd receiv initi messag runner readi env statu run assign environ nni exp fybaxk acd trial khph trial job khph statu wait run trial job khph statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial uhjk trial job uhjk statu wait run trial job uhjk statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial jqjwi trial job jqjwi statu wait run trial job jqjwi statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial ijhph trial job ijhph statu wait run trial job ijhph statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial belku trial job belku statu wait run trial job belku statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial updtw trial job updtw statu wait run trial job updtw statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial zgoq trial job zgoq statu wait run trial job zgoq statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial tcl trial job tcl statu wait run trial job tcl statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial rohbk trial job rohbk statu wait run trial job rohbk statu run succeed nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index submittrialjob sequenceid hyperparamet valu paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index index assign environ nni exp fybaxk acd trial uurlr trial job uurlr statu wait run nnimanag statu run trial trial job uurlr statu run succeed nnimanag statu trial nnimanag receiv dispatch paramet paramet sourc algorithm paramet batch size hidden size momentum paramet index environmentinform nni exp fybaxk env xlegg statu run unknown environmentinform nni exp fybaxk env xlegg statu unknown succeed dispatch log nni runtim msg dispatch base mainthread dispatch start hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss hyperopt tpe thread tpe transform took hyperopt tpe thread tpe trial loss nnictl stdout stderr start time start time node maxlistenersexceededwarn eventemitt memori leak detect messag listen emitt setmaxlisten increas limit node maxlistenersexceededwarn eventemitt memori leak detect listen emitt setmaxlisten increas limit node maxlistenersexceededwarn eventemitt memori leak detect close listen emitt setmaxlisten increas limit meet mnist pytorch train unreason slow trial entir took nearli faster standard gpu nvidia tesla reproduc doc http nni readthedoc latest trainingservic amlmod html addit tri gpunum useactivegpu config file slower trial spend time wait statu trial run trial run",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"train extrem slow environ nni version nni mode remot client window server linux version version conda run docker log messag dispatch start took tpe trial took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss took tpe trial loss nnictl stdout stderr start time start time node maxlistenersexceededwarn eventemitt memori leak detect messag listen increas limit node maxlistenersexceededwarn eventemitt memori leak detect listen increas limit node maxlistenersexceededwarn eventemitt memori leak detect close listen increas limit log file meet train unreason slow trial entir took nearli faster gpu nvidia tesla reproduc doc addit tri gpunum useactivegpu config file slower trial spend time wait statu trial run trial run",
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score":0,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":12402.8375,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"gpunum useactivegpu start trial job run speed train process slow train soon set gpunum convert circular structur json nni",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"yangmingwanli run start trial job start run sparksnail gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap sparksnail workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json sparksnail convert circular structur json nni",
        "Solution_preprocessed_content":"run start trial job start run gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json convert circular structur json nni",
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score":1,
        "Solution_sentence_count":11,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":150,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":11057.4897425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Challenge_closed_time":1567514274983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527707311910,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"creat train job relat input data config flag type paramet datasourc sdatasourc suri sdatadistributiontyp shorthand syntax inject variabl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat train job type cli run creat train job creat train job train job deploy build hyper paramet file hyperparamet json algorithm trainingimag imag traininginputmod file role arn role input data config channelnam train datasourc sdatasourc sdatatyp sprefix suri input sdatadistributiontyp fullyrepl contenttyp compressiontyp recordwrappertyp output data config soutputpath output resourc config file train resourc config json stop condit file stop condit json paramet type paramet inputdataconfig datasourc sdatasourc valu sdatatyp sprefix type type type paramet inputdataconfig datasourc sdatasourc valu suri process inbound autotag data type type type paramet inputdataconfig datasourc sdatasourc valu sdatadistributiontyp fullyrepl type type train input data config flag shorthand syntax inject variabl capit haalp",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"type cli run flag shorthand syntax inject variabl haalp",
        "Challenge_readability":24.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":11057.4897425,
        "Challenge_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1130.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"input config format sampl json document input data config flag format format input data config dictionari list channel channel dictionari paramet channelnam compressiontyp contenttyp datasourc datasourc paramet dictionari option filesystemdatasourc sdatasourc",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"input config format checkout sampl json http doc com latest api createtrainingjob html format input data config dictionari inputdataconfig channelnam compressiontyp contenttyp datasourc filesystemdatasourc directorypath filesystemaccessmod filesystemid filesystemtyp sdatasourc attributenam sdatadistributiontyp sdatatyp suri inputmod recordwrappertyp shuffleconfig seed",
        "Solution_preprocessed_content":"input config format checkout sampl json",
        "Solution_readability":21.1,
        "Solution_reading_time":11.41,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"JSON Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":62,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":5,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631000506000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"tensorboard default logger option",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_original_content":"tensorboard default logger option",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"tensorboard default logger option",
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score":0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":10471.2408333333,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"tensorboard default logger tensorboard fallback instal tensorboardx default logger pytorch lightn clear consensu forc log",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"tensorboard default extern servic data awai upload tensorboard fallback instal ragier agre tensorboardx default logger pytorch lightn thibo forc log",
        "Solution_preprocessed_content":"tensorboard default extern servic data awai upload tensorboard fallback instal agre tensorboardx default logger pytorch lightn forc",
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":75,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":10362.7869172222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Challenge_closed_time":1644193786212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606886205577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal gaapipi packag anaconda notebook tri conda instal pip instal http connect establish connect tri",
        "Challenge_last_edit_time":1606887753310,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Challenge_link_count":2,
        "Challenge_original_content":"instal packag instal gaapipi packag anaconda notebook tri conda instal gaapipi conda instal gaapipi collect packag metadata repodata json condahttperror http connect url elaps http retriev url http intermitt retri http conda anaconda org conda forg linux note restart kernel updat packag tri pip instal gaapipi text warn retri retri connect read redirect statu connect broken newconnectionerror establish connect errno network unreach gaapipi warn retri retri connect read redirect statu connect broken newconnectionerror establish connect errno network unreach gaapipi warn retri retri connect read redirect statu connect broken newconnectionerror establish connect errno network unreach gaapipi warn retri retri connect read redirect statu connect broken newconnectionerror establish connect errno network unreach gaapipi warn retri retri connect read redirect statu connect broken newconnectionerror establish connect errno network unreach gaapipi version gaapipi version match distribut gaapipi warn pip version version upgrad home anaconda env bin pip instal upgrad pip note restart kernel updat packag previou packag upd tri book import sy sy execut pip instal gaapipi import sy conda instal prefix sy prefix gaapipi",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"instal packag instal gaapi packag anaconda notebook tri tri text previou packag upd tri book",
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":10363.2168430556,
        "Challenge_title":"struggling to install python package via amazon sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":348,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"instal librari block secur reason instal gaapipi packag anaconda notebook",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"forth depart figur librari instal block secur reason",
        "Solution_preprocessed_content":"depart figur librari instal block secur reason",
        "Solution_readability":13.1,
        "Solution_reading_time":1.79,
        "Solution_score":0,
        "Solution_sentence_count":1,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":19,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609843354000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"studio jupyt lab graph tab render execut path queri graph tab render shown screenshot see messag",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_original_content":"graph tab render studio jupyt lab execut path queri jupyt lab graph tab render tab children output layout layout height overflow scroll width forc network graph reproduc step reproduc jupyt lab run queri path screenshot taken jupyterlab imag http imag githubusercont com fbf eac fdcbf png screenshot taken jupyt imag http imag githubusercont com bfe bccea png",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"graph tab render studio jupyt lab execut path queri jupyt lab graph tab render reproduc step reproduc jupyt lab run queri path screenshot taken jupyterlab screenshot taken jupyt",
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Github",
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"featur request cut widget render properli jupyterlab",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"reach haven taken jupyterlab build visual widget lab tab widget isn displai properli screenshot lab forc widget isn instal properli cut featur request widget render properli jupyterlab",
        "Solution_preprocessed_content":"reach haven taken jupyterlab build visual widget lab tab widget isn displai properli screenshot lab forc widget isn instal properli cut featur request widget render properli jupyterlab",
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score":0,
        "Solution_sentence_count":5,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":72,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":10042.3480277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1573664904092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"build neural network model nlp tensorflow plan train model predict web servic pro con emr tensorflow tensorflow integr involv distribut comput",
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_original_content":"pro con emr deploi tensorflow base model build neural network model nlp framework tensorflow plan train model predict web servic distribut comput pro con emr tensorflow tensorflow integr",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"pro con emr deploi model build neural network model nlp framework tensorflow plan train model predict web servic distribut comput pro con emr tensorflow tensorflow integr",
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10042.9761819444,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"build neural network model nlp tensorflow predict web servic emr emr process massiv amount data heavili reli spark hadoop mapreduc easier distribut run notebook rich marketplac quickli model allow distribut infer comput deploi endpoint expens enforc workflow fulli emr gener low cost compar instanc elast hive pig hbase box",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"gener term serv purpos emr process massiv amount data heavili reli spark hadoop mapreduc emr elast mapreduc essenti data larg volum effici spark hadoop hive hdf hbase pig stack emr emr pro gener low cost compar instanc elast provis hive pig hbase box emr con truli benefit emr advantag entir easier distribut marketplac box algo model quick great servic conform workflow enforc creat train job deploi infer endpoint pro run notebook rich marketplac quickli model notebook popular algorithm predefin kernel minim configur deploi model allow distribut infer comput deploi endpoint con expens enforc workflow fulli expens",
        "Solution_preprocessed_content":"gener term serv purpos emr process massiv amount data heavili reli spark hadoop mapreduc essenti data larg volum effici spark hadoop hive hdf hbase pig stack emr emr pro gener low cost compar instanc elast provis hive pig hbase box emr con truli benefit emr advantag entir easier distribut marketplac box algo model quick great servic conform workflow enforc creat train job deploi infer endpoint pro run notebook rich marketplac quickli model notebook popular algorithm predefin kernel minim configur deploi model allow distribut infer comput deploi endpoint con expens enforc workflow fulli expens",
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score":10,
        "Solution_sentence_count":11,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":229,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":9316.4808333333,
        "Challenge_answer_count":4,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173479000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634634148000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"pull remot storag publicli access messag state permiss access object bucket public read",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_original_content":"remot storag publicli access pull forbidden call headobject oper forbidden pull manti cred reader bucket public read",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"remot storag publicli access manti cred reader bucket public read",
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9316.4808333333,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Github",
        "Solution_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"bucket public read credenti set configur instal awscli acl public read flag set profil set credenti set access kei secret access kei folder",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"credenti set configur instal awscli virtualenv mantisnlp public bucket access manti profil relat acl public read public bucket grant tagger flag http github com wellrust grant tagger blob abbcbcddbfacaac makefil bucket level individu object level btw probabl badli set profil set credenti set access kei secret access kei folder decid",
        "Solution_preprocessed_content":"credenti set instal awscli bucket access manti profil relat public bucket grant tagger flag bucket level individu object level btw probabl badli set set credenti set folder decid",
        "Solution_readability":7.4,
        "Solution_reading_time":11.62,
        "Solution_score":0,
        "Solution_sentence_count":9,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":149,
        "Tool":"DVC"
    },
    {
        "Challenge_adjusted_solved_time":12.2603875,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1454387492627,
        "Challenge_comment_count":4,
        "Challenge_created_time":1421424031317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"disabl cor servic prevent access servic javascript webpag tri search",
        "Challenge_last_edit_time":1454343355232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27987910",
        "Challenge_link_count":0,
        "Challenge_original_content":"cor search hour singl creat publish servic creat endpoint servic postman rest client access javascript webpag return consol log sai cor enabl servic life figur disabl cor servic",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"cor search hour singl creat publish servic creat endpoint servic postman rest client access javascript webpag return consol log sai cor enabl servic life figur disabl cor servic",
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":12,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9156.5170305556,
        "Challenge_title":"Azure Machine Learning - CORS",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3242.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this<\/p>\n\n<p>Here are the links: <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/api-management-get-started\/\" rel=\"noreferrer\">step by step<\/a> guide, also this <a href=\"http:\/\/channel9.msdn.com\/Blogs\/AzureApiMgmt\/Last-mile-Security\" rel=\"noreferrer\">video<\/a> on setting headers, and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn894084.aspx#JSONP\" rel=\"noreferrer\">this doc<\/a> on policies.<\/p>\n\n<p>API Management service allow CORS by enabling it in the API configuration page<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"api servic disabl cor link step step guid video set header document polici api servic allow cor enabl api configur page",
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_original_content":"disabl cor api option api servic disabl cor link link step step guid video set header doc polici api servic allow cor enabl api configur page",
        "Solution_preprocessed_content":"disabl cor api option api servic disabl cor link link step step guid video set header doc polici api servic allow cor enabl api configur page",
        "Solution_readability":11.6,
        "Solution_reading_time":9.27,
        "Solution_score":4,
        "Solution_sentence_count":6,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":74,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580947939000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"importerror attributeerror autogluon tabularpredict modul notebook instanc conda kernel",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_original_content":"importerror tabularpredict notebook instanc importerror autogluon instanc xlarg kernel conda autogluon import tabularpredict task importerror traceback autogluon import tabularpredict task importerror import tabularpredict import autogluon tabularpredict dataset file path data nbc golf model train csv attributeerror traceback tabularpredict dataset file path nbc golf model train csv attributeerror modul autogluon attribut tabularpredict instal autogluon version pip notebook pip instal upgrad mxnet pip instal autogluon collect mxnet download http file pythonhost org packag ceffacececfddafcdffeddbbd mxnet manylinux whl eta upgrad directli request home anaconda env mxnet lib site packag mxnet upgrad directli graphviz home anaconda env mxnet lib site packag mxnet upgrad directli numpi home anaconda env mxnet lib site packag mxnet upgrad directli chardet home anaconda env mxnet lib site packag request mxnet upgrad directli idna home anaconda env mxnet lib site packag request mxnet upgrad directli certifi home anaconda env mxnet lib site packag request mxnet upgrad directli urllib home anaconda env mxnet lib site packag request mxnet instal collect packag mxnet successfulli instal mxnet instal step boto boto incompat awscli botocor botocor incompat collect autogluon download autogluon whl eta tornado home anaconda env mxnet lib site packag autogluon cryptographi home anaconda env mxnet lib site packag autogluon collect lightgbm download lightgbm manylinux whl eta paramiko home anaconda env mxnet lib site packag autogluon collect scipi download scipi cpm manylinux whl eta collect boto download boto whl eta cython home anaconda env mxnet lib site packag autogluon collect scikit optim download scikit optim whl eta pillow home anaconda env mxnet lib site packag autogluon panda home anaconda env mxnet lib site packag autogluon graphviz home anaconda env mxnet lib site packag autogluon collect dask download dask whl eta request home anaconda env mxnet lib site packag autogluon collect scikit download scikit cpm manylinux whl eta collect distribut download distribut whl eta matplotlib home anaconda env mxnet lib site packag autogluon collect configspac download tqdm whl eta collect gluoncv download gluoncv whl eta numpi home anaconda env mxnet lib site packag autogluon cffi home anaconda env mxnet lib site packag cryptographi autogluon home anaconda env mxnet lib site packag cryptographi autogluon bcrypt home anaconda env mxnet lib site packag paramiko autogluon pynacl home anaconda env mxnet lib site packag paramiko autogluon jmespath home anaconda env mxnet lib site packag boto autogluon stransfer home anaconda env mxnet lib site packag boto autogluon collect botocor download botocor whl eta collect pyaml download pyaml whl collect joblib download joblib whl eta plotli home anaconda env mxnet lib site packag catboost autogluon pytz home anaconda env mxnet lib site packag panda autogluon dateutil home anaconda env mxnet lib site packag panda autogluon idna home anaconda env mxnet lib site packag request autogluon chardet home anaconda env mxnet lib site packag request autogluon urllib home anaconda env mxnet lib site packag request autogluon certifi home anaconda env mxnet lib site packag request autogluon tblib home anaconda env mxnet lib site packag distribut autogluon pyyaml home anaconda env mxnet lib site packag distribut autogluon sortedcontain home anaconda env mxnet lib site packag distribut autogluon msgpack home anaconda env mxnet lib site packag distribut autogluon zict home anaconda env mxnet lib site packag distribut autogluon toolz home anaconda env mxnet lib site packag distribut autogluon cloudpickl home anaconda env mxnet lib site packag distribut autogluon click home anaconda env mxnet lib site packag distribut autogluon cycler home anaconda env mxnet lib site packag matplotlib autogluon kiwisolv home anaconda env mxnet lib site packag matplotlib autogluon pypars home anaconda env mxnet lib site packag matplotlib autogluon type home anaconda env mxnet lib site packag configspaceautogluon collect portalock download portalock whl pycpars home anaconda env mxnet lib site packag cffi cryptographi autogluon docutil home anaconda env mxnet lib site packag botocor boto autogluon retri home anaconda env mxnet lib site packag plotli catboost autogluon heapdict home anaconda env mxnet lib site packag zict distribut autogluon setuptool home anaconda env mxnet lib site packag kiwisolv matplotlib autogluon build wheel collect packag gluonnlp configspac build wheel gluonnlp setup creat wheel gluonnlp filenam gluonnlp whl size sha ebaabbddeedceaaeebfcfaddfebf store directori home cach pip wheel efbeeefcdfeeefcacdaedfb build wheel configspac setup creat wheel configspac filenam configspac cpm linux whl size sha cecfaefbbecebbcaedc store directori home cach pip wheel cacbfdedfecdcecbedb successfulli built gluonnlp configspac boto boto incompat awscli botocor botocor incompat instal collect packag scipi joblib scikit lightgbm botocor boto pyaml scikit optim catboost gluonnlp dask distribut configspac tqdm portalock gluoncv autogluon uninstal scipi instal scipi uninstal scipi successfulli uninstal scipi uninstal scikit instal scikit uninstal scikit successfulli uninstal scikit uninstal botocor instal botocor uninstal botocor successfulli uninstal botocor uninstal boto instal boto uninstal boto successfulli uninstal boto uninstal dask instal dask uninstal dask successfulli uninstal dask uninstal distribut instal distribut uninstal distribut successfulli uninstal distribut successfulli instal configspac autogluon boto botocor catboost dask distribut gluoncv gluonnlp joblib lightgbm portalock pyaml scikit scikit optim scipi tqdm",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"importerror tabularpredict notebook instanc importerror autogluon instanc kernel instal autogluon version pip notebook instal step boto incompat awscli botocor",
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score":0,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":8976.4125,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":999,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"upgrad newer version boto instal autogluon sourc wait autogluon releas",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"submit haven close boto version function autogluon newer version zhuwenzhen latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_preprocessed_content":"submit haven close boto version function autogluon newer version latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":66,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":8089.9413483334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Challenge_closed_time":1595256539003,
        "Challenge_comment_count":1,
        "Challenge_created_time":1566128809927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"sentenc classif built blaze text algorithm invok endpoint insid lambda function throw type mismatch tri byte format jsonlin format payload format predict sentenc time invok endpoint option",
        "Challenge_last_edit_time":1566133069936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Challenge_link_count":0,
        "Challenge_original_content":"insid lambda function blaze text algorithm invok endpoint input type sentenc classif build blaze text algorithm invok endpoint insid lambda function through type mismatch blaze text jsonlin json invok through accept byte bytearrai input format json event featur sensor subtyp thermostat thermal switch featur hermet seal tight toler tight logic level contact comput medic electron power suppli industri control test equip infotech descript technic technic spec closetoler dielectricstrength mil std vac termin contactresist mil std tri jsonlin transform data data featur data copi return featur except err print transform format data err rais except transform format data err lambda handler event context print receiv event json dump event indent request json load json dump event transform data str transform data request featur instanc request featur print endpoint payload transform data client invok endpoint endpointnam endpoint bodi payload encod utf contenttyp json return statuscod isbaseencod bodi modelerror call invokeendpoint oper receiv client model messag payload format log decod payload data format valueerror json object decod traceback file opt lib site packag blazingtext serv line invoc data json load payload decod utf file opt lib json init line load return default decod decod file opt lib json decod line decod obj end valueerror json object decod predict sentenc realtim invok endpoint option payload format tri byte format apllic jsonlin format",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"insid lambda function blaze text algorithm invok endpoint input type sentenc classif blaze text algorithm invok endpoint insid lambda function through type mismatch blaze text invok through accept byte bytearrai tri predict sentenc realtim option payload format tri byte format format",
        "Challenge_readability":15.8,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8091.0358544445,
        "Challenge_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"kei payload instanc keep contenttyp json blazingtext model accept payload dictionari instanc kei list data predict valu",
        "Solution_last_edit_time":1595256858790,
        "Solution_link_count":0,
        "Solution_original_content":"predict text classif blazingtext simpli kei payload keep contenttyp json sentenc sell practic brand payload instanc sentenc respons client invok endpoint endpointnam text classif bodi json dump payload contenttyp json plai littl payload blaze text model accept payload dictionari instanc kei list data predict valu predict simpli print responsemetadata respons responsemetadata print print bodi respons bodi read",
        "Solution_preprocessed_content":"predict text classif blazingtext simpli kei payload keep contenttyp plai littl payload blaze text model accept payload dictionari instanc kei list data predict valu predict simpli",
        "Solution_readability":16.0,
        "Solution_reading_time":12.21,
        "Solution_score":0,
        "Solution_sentence_count":5,
        "Solution_topic":"JSON Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":103,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7952.5209027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Challenge_closed_time":1662661497360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634032422110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pass argument docker techniqu docker imag env file input run pass env file pass ecr imag tri export kei valu statement creat variabl expos variabl tri execut run sourc file env build imag pass docker run argument estim notebook document",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Challenge_link_count":0,
        "Challenge_original_content":"pass argument docker train model techniqu model train run local docker imag env file input run pass ecr imag pass env file insid train call export kei valu statement creat variabl expos variabl tri execut run sourc file env build imag bin sourc env build imag probabl flexibl variabl run pass docker run argument estim notebook document",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"pass argument docker train model techniqu model train run local docker imag input run pass ecr imag pass insid call statement creat variabl expos variabl tri execut build imag build imag probabl flexibl variabl run pass docker run argument estim notebook document",
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7952.5209027778,
        "Challenge_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"pass environ variabl docker imag url creat train job sdk sdk option set environ variabl train job implement boto equival servic sdk",
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_original_content":"pass environ variabl docker imag url creat train job sdk document train state environ dict str str environ variabl set train job default sdk sourc sdk wrapper boto pretti implement boto equival servic sdk",
        "Solution_preprocessed_content":"pass environ variabl docker imag url creat train job sdk document state sdk sourc sdk wrapper boto pretti implement boto equival servic sdk",
        "Solution_readability":15.2,
        "Solution_reading_time":11.12,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":92,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"connect jupyt notebook rd public address notebook instanc advic connect rd notebook glue athena crawl tabl retriev queri notebook",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_original_content":"jupyt notebook connect rd tri connect notebook rd connect rd public allow secur reason run curl ifconfig notebook instanc public keep time time connect rd notebook crawl rd glue athena crawl tabl queri notebook",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"jupyt notebook connect rd tri connect notebook rd connect rd public allow secur reason run curl notebook instanc public keep time time connect rd notebook crawl rd glue athena crawl tabl queri notebook",
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"connect rd notebook instanc vpc rd instanc connect databas base client librari connect connect rd instanc public interfac glue athena crawl tabl retriev queri notebook",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"rd databas run instanc connect databas connect base client librari depend flavor postgr configur connect connect rd instanc connect rd instanc public interfac notebook instanc vpc rd instanc rd directli vpc",
        "Solution_preprocessed_content":"rd databas run instanc connect databas connect base client librari configur connect connect rd instanc connect rd instanc public interfac notebook instanc vpc rd instanc rd directli vpc",
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":105,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7728.2646775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Challenge_closed_time":1656482770812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628661017973,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"configur task titl groundtruth label job consol believ relat tasktitl configur cli configur consol gui",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Challenge_link_count":1,
        "Challenge_original_content":"set task titl grounttruth label job creat groundtruth label job consol configur task titl shown worker job list relat tasktitl configur cli configur consol configur consol gui",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"set task titl grounttruth label job creat groundtruth label job consol configur task titl shown worker job list relat tasktitl configur cli configur consol configur consol gui",
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7728.2646775,
        "Challenge_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"file github repositori file line tasktitl credit card agreement entiti",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"stuff run label job git repo http github com sampl textract transform pipelin file repo http github com sampl textract transform pipelin blob notebook util smgt line tasktitl credit card agreement entiti hope",
        "Solution_preprocessed_content":"stuff run label job git repo file repo line tasktitl credit card agreement entiti hope",
        "Solution_readability":16.7,
        "Solution_reading_time":11.15,
        "Solution_score":0,
        "Solution_sentence_count":7,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":71,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7560.4671713889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... <\/p>\n\n<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?<\/p>",
        "Challenge_closed_time":1571132773200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543915091383,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"incur unnecessari cost leav jupyt notebook instanc onlin overnight automat stop instanc period inact hour",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53609409",
        "Challenge_link_count":0,
        "Challenge_original_content":"automat stop notebook instanc inact jupyt notebook instanc leav onlin overnight unnecessarili cost monei automat stop notebook instanc activ hour",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"automat stop notebook instanc inact jupyt notebook instanc leav onlin overnight unnecessarili cost automat stop notebook instanc activ hour",
        "Challenge_readability":10.9,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":19,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7560.4671713889,
        "Challenge_title":"Automatically \"stop\" Sagemaker notebook instance after inactivity?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":12683.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>You can use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">Lifecycle configurations<\/a> to set up an automatic job that will stop your instance after inactivity.<\/p>\n\n<p>There's <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\" rel=\"noreferrer\">a GitHub repository<\/a> which has samples that you can use. In the repository, there's a <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"noreferrer\">auto-stop-idle<\/a> script which will shutdown your instance once it's idle for more than 1 hour.<\/p>\n\n<p>What you need to do is<\/p>\n\n<ol>\n<li>to create a Lifecycle configuration using the script and<\/li>\n<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.<\/li>\n<\/ol>\n\n<p>If you think 1 hour is too long you can tweak the script. <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh#L17\" rel=\"noreferrer\">This line<\/a> has the value.<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"lifecycl configur set automat job stop jupyt notebook instanc inact auto stop idl github repositori shutdown instanc idl hour creat lifecycl configur associ configur instanc hour tweak",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"lifecycl configur set automat job stop instanc inact github repositori sampl repositori auto stop idl shutdown instanc idl hour creat lifecycl configur associ configur instanc edit creat notebook instanc hour tweak line valu",
        "Solution_preprocessed_content":"lifecycl configur set automat job stop instanc inact github repositori sampl repositori shutdown instanc idl hour creat lifecycl configur associ configur instanc edit creat notebook instanc hour tweak line valu",
        "Solution_readability":17.0,
        "Solution_reading_time":17.33,
        "Solution_score":26,
        "Solution_sentence_count":10,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":110,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7532.4821652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Challenge_closed_time":1603758115467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576591296847,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"assign studio list iam organ",
        "Challenge_last_edit_time":1576641179672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Challenge_link_count":0,
        "Challenge_original_content":"assign studio successfulli creat studio statu servic assign list iam organ coupl suppos",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"assign studio successfulli creat studio statu servic assign list iam organ suppos",
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7546.3385055556,
        "Challenge_title":"How to assign users in SageMaker Studio?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":1029.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Solution_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"studio set iam authent partit studio environ access partit control iam polici iam role feder studio url access environ guid board iam page",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"setup studio sso iam authent gather studio setup iam authent partit studio environ control access partit iam polici iam role feder studio url access environ guid http doc com latest pdf num gen xyz cnull page board iam",
        "Solution_preprocessed_content":"setup studio sso iam authent gather studio setup iam authent partit studio environ control access partit iam polici iam role feder studio url access environ guid page iam",
        "Solution_readability":12.5,
        "Solution_reading_time":12.65,
        "Solution_score":1,
        "Solution_sentence_count":8,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":93,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609957275000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"instal sdk pip instal sdk messag state match distribut sdk guidanc miss",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_original_content":"instal guidanc instal default packag environ previou version packag instal run pip instal sdk version sdk version match distribut sdk miss claw document edit section doc com github link eea dbefb version independ eaac acc acbbb instal sdk http doc com api overview instal sourc docset doc ref conceptu instal http github com microsoftdoc machinelearn blob live docset doc ref conceptu instal servic sub servic core github login harneetvirk alia harnvir",
        "Challenge_participation_count":15,
        "Challenge_preprocessed_content":"instal guidanc instal default packag environ previou version packag instal run pip instal sdk miss claw document edit section github version independ sourc servic core github login alia harnvir",
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":3,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7538.9758333333,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"sdk rang version older newer version switch older version pip improv perform version updat metadata sdk core sdk updat allow sdk releas sdk preview",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"klawrawkz pip version sdk rang version older newer version repli harneetvirk pretti version version win version imag http imag githubusercont com bfd acb png ran farther pip instal upgrad upgrad strategi eager sdk stuck pip multipl version core determin version compat pip multipl version mgmt containerregistri determin version compat collect mgmt containerregistri download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl pip multipl version mgmt core determin version compat download mgmt containerregistri whl download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip hundr subpackag sdk depend come core compat sdk instal pip ignor option probabl johan environ local env local environ core unfortun core instal sdk creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon adamjstewart sdk preview http doc com concept sdk date",
        "Solution_preprocessed_content":"pip version sdk version version repli pretti version win version ran farther stuck advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip notic perform gotcha klaw sdk metadata updat hundr subpackag sdk depend come core compat sdk instal pip option probabl environ local env local environ unfortun instal creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon sdk preview sdk date",
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score":16,
        "Solution_sentence_count":77,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":608,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":7347.4097222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Challenge_closed_time":1641958092267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615507417267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"workspac context control run tri workspac config subscript resourc spec guidanc local workspac control",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Challenge_link_count":1,
        "Challenge_original_content":"local workspac run figur workspac context control doc workspac config control messag config json path parent directori path config file config json parent directori tri subscript resourc spec subscript resourc group workspac workspac workspac subscript resourc group workspac monitor log authent run local local workspac control",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"local workspac run figur workspac context control doc control messag parent directori path config file parent tri subscript resourc spec monitor log authent run local local workspac control",
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7347.4097222222,
        "Challenge_title":"Get local workspace in azureml",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"workspac context run context train core import run run context run workspac",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"month come figuerd ago haven gotten train workspac run context core import run run context run workspac",
        "Solution_preprocessed_content":"month come figuerd ago haven gotten train workspac run context",
        "Solution_readability":4.5,
        "Solution_reading_time":4.52,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":55,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":3,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600043448000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"remot test log report codebuild log statu log bear log termin abruptli commit statu codebuild log sai codebuild log print termin abruptli",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_original_content":"remot test report checklist prepend tag type attach reproduc document dlc imag dockerfil relat document test run dlc imag dlc imag list http doc com latest devguid imag html built base dlc attach build imag concis descript remot test log report observ commit http github com pull http github com pull commit dddefbfafcacadbb http github com pull commit daabdeaccdeefc dlc imag dockerfil dlc github pend statu codebuild log statu codebuild log bear log termin abruptli test session start platform linux pytest pluggi rootdir codebuild output src src github com test dlc test plugin rerunfailur fork xdist timeout cloudwatch log navig train log job ran hour end successfulli sai bench gpu node algo train toolkit report train commit statu codebuild log sai codebuild log abruptli hang print termin print log session start addit context",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"remot test report checklist prepend tag type attach reproduc document dlc relat document test run dlc imag dlc imag list built base dlc concis descript remot test log report observ commit dlc dlc github pend statu codebuild log statu codebuild log bear log termin abruptli log navig train log job ran hour end successfulli sai commit statu codebuild log sai codebuild log abruptli hang print termin print log session start addit context",
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7267.8997222222,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Platform":"Github",
        "Solution_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"remot test log report commit statu codebuild log sai codebuild log print termin abruptli",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"sidu codebuild run timeout codebuild shown statu time commit statu pend open depend close ticket",
        "Solution_preprocessed_content":"codebuild run timeout codebuild shown statu time commit statu pend open depend close ticket",
        "Solution_readability":5.4,
        "Solution_reading_time":4.17,
        "Solution_score":0,
        "Solution_sentence_count":5,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":57,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7933.4568986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1610748471692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586792396013,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"enabl toc tabl jupyt extens notebook lifecycl configur tri enabl extens combin sampl quick articl enabl extens isn",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Challenge_link_count":4,
        "Challenge_original_content":"instal toc notebook extens instanc lifecycl configur probabl obviou miss extens enabl toc tabl jupyt extens notebook lifecycl configur reason isn built combin sampl quick articl enabl extens http github com sampl notebook instanc lifecycl config sampl blob master instal extens start http towardsdatasci com jupyt notebook extens fad bin bash set sudo eof activ notebook environ sourc activ jupytersystemenv instal extens pip instal jupyt contrib nbextens jupyt contrib nbextens instal jupyt nbextens enabl toc sy prefix sourc deactiv eof",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"instal toc notebook extens instanc probabl obviou miss extens enabl toc jupyt extens notebook lifecycl configur reason isn built combin sampl quick articl enabl extens",
        "Challenge_readability":20.5,
        "Challenge_reading_time":18.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6654.4654663889,
        "Challenge_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1027.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Solution_comment_count":2,
        "Solution_gpt_summary":"miss line jupyt contrib nbextens instal copi css file jupyt search directori config updat line run toc jupyt extens successfulli enabl notebook",
        "Solution_last_edit_time":1615352840848,
        "Solution_link_count":2,
        "Solution_original_content":"miss line jupyt contrib nbextens instal copi css file jupyt search directori config updat http github com ipython contrib jupyt contrib nbextens statement bin bash set sudo eof sourc home anaconda bin activ jupytersystemenv pip instal jupyt contrib nbextens jupyt contrib nbextens instal jupyt nbextens enabl toc sourc home anaconda bin deactiv eof unnecessari run initctl restart jupyt server wait",
        "Solution_preprocessed_content":"miss line copi file jupyt search directori config updat statement",
        "Solution_readability":18.1,
        "Solution_reading_time":11.02,
        "Solution_score":2,
        "Solution_sentence_count":4,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":79,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":5,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645584371000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"open restart browser laptop clear cach environ open open button keep load indefinit attach screenshot request assist",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_original_content":"open open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account screenshot attach",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"open open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account screenshot attach img width alt screen shot",
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Github",
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"delet recreat account bypass approv process",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"quick delet account recreat pass approv process email approv bsaha open close troubl michelemonclova icoxfog glad enjoi journei",
        "Solution_preprocessed_content":"quick delet account recreat pass approv process email approv open close troubl glad enjoi journei",
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score":1,
        "Solution_sentence_count":11,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":88,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":6310.4455736111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>whenever I'm running - <code>from sklearn.ensemble import RandomForestClassifier<\/code><\/p>\n\n<p>I'm getting an error - <code>ImportError: cannot import name 'parallel_helper'<\/code>\nthe stack trace is - <\/p>\n\n<pre><code>--------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-135-d80da5c856d8&gt; in &lt;module&gt;()\n      1 # feature removal using ROC-AUC score\n----&gt; 2 from sklearn.ensemble import RandomForestClassifier\n      3 roc_values = []\n      4 for feature in diabetes_MICE_X.columns:\n      5     clf = RandomForestClassifier()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/__init__.py in &lt;module&gt;()\n      5 \n      6 from .base import BaseEnsemble\n----&gt; 7 from .forest import RandomForestClassifier\n      8 from .forest import RandomForestRegressor\n      9 from .forest import RandomTreesEmbedding\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/forest.py in &lt;module&gt;()\n     59 from ..exceptions import DataConversionWarning, NotFittedError\n     60 from .base import BaseEnsemble, _partition_estimators\n---&gt; 61 from ..utils.fixes import parallel_helper, _joblib_parallel_args\n     62 from ..utils.multiclass import check_classification_targets\n     63 from ..utils.validation import check_is_fitted\n\nImportError: cannot import name 'parallel_helper'\n\n\nNote - I'm using jupyter notebook (conda_python3) in sagemaker.\nscipy version = 1.3.1\nnumpy version = 1.17.2\nscikit version = 0.21.3 \n\n\none strange thing that i'm unable to figure out is - whenever i do \n\nimport sklearn\nsklearn.__version__\n<\/code><\/pre>\n\n<p>its gives me output as 0.22<\/p>\n\n<p>can someone help me on this issue ? <\/p>",
        "Challenge_closed_time":1600076529168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577358925103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"importerror import parallel helper run sklearn ensembl import randomforestclassifi jupyt notebook conda scipi version numpi version scikit version figur output import sklearn sklearn version",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59487643",
        "Challenge_link_count":0,
        "Challenge_original_content":"importerror import parallel helper run sklearn ensembl import randomforestclassifi importerror import parallel helper stack trace importerror traceback featur remov roc auc score sklearn ensembl import randomforestclassifi roc valu featur diabet mice column clf randomforestclassifi anaconda env lib site packag sklearn ensembl init base import baseensembl forest import randomforestclassifi forest import randomforestregressor forest import randomtreesembed anaconda env lib site packag sklearn ensembl forest except import dataconversionwarn notfittederror base import baseensembl partit estim util import parallel helper joblib parallel arg util multiclass import classif target util import fit importerror import parallel helper note jupyt notebook conda scipi version numpi version scikit version figur import sklearn sklearn version output",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"importerror import run stack trace output",
        "Challenge_readability":12.2,
        "Challenge_reading_time":22.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":6310.4455736111,
        "Challenge_title":"ImportError: cannot import name 'parallel_helper'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2241.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The best way to overcome this problem in sagemaker is to use lifecycle configuration.\nRather than doing pip install inside the notebook, write all your requirements.txt installs inside the lifecycle configurations. The notebook will take more time to spawn but the code but the libraries will be pre-installed.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"overcom importerror lifecycl configur instal librari pip insid notebook write txt instal insid lifecycl configur pre instal librari notebook time spawn",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"overcom lifecycl configur pip instal insid notebook write txt instal insid lifecycl configur notebook time spawn librari pre instal",
        "Solution_preprocessed_content":"overcom lifecycl configur pip instal insid notebook write instal insid lifecycl configur notebook time spawn librari",
        "Solution_readability":8.0,
        "Solution_reading_time":3.97,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":48,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5394.9012733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i have started a sagemaker job:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server'{'enabled':False}})\n\ntraining_data_uri ='s3:\/\/path\/to\/my\/data'\nmytraining.fit(training_data_uri,run_tensorboard_locally=True)\n<\/code><\/pre>\n\n<p>using <code>run_tesorboard_locally=True<\/code> gave me<\/p>\n\n<pre><code>Tensorboard is not supported with script mode. You can run the following command: tensorboard --logdir None --host localhost --port 6006 This can be run from anywhere with access to the S3 URI used as the logdir.\n<\/code><\/pre>\n\n<p>It seems like i cant use it script mode, but I can access the logs of tensorboard in s3? But where are the logs in s3?<\/p>\n\n<pre><code>def _parse_args():\n    parser = argparse.ArgumentParser()\n\n    # Data, model, and output directories\n    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n    parser.add_argument('--model_dir', type=str)\n    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n\n    return parser.parse_known_args()\n\nif __name__ == \"__main__\":\n    args, unknown = _parse_args()\n\n    train_data, train_labels = load_training_data(args.train)\n    eval_data, eval_labels = load_testing_data(args.train)\n\n    mymodel= model(train_data, train_labels, eval_data, eval_labels)\n\n    if args.current_host == args.hosts[0]:\n        mymodel.save(os.path.join(args.sm_model_dir, '000000002\/model.h5'))\n<\/code><\/pre>\n\n<p>similiar question is here :<a href=\"https:\/\/stackoverflow.com\/questions\/53713660\/tensorboard-without-callbacks-for-keras-docker-image-in-sagemaker\">stack<\/a><\/p>\n\n<p>EDIT i tried this new config but it doesnt work.<\/p>\n\n<pre><code> tensorboard_output_config = TensorBoardOutputConfig( s3_output_path='s3:\/\/PATH\/to\/my\/bucket')\n\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server': {'enabled':False}},\n                        tensorboard_output_config=tensorboard_output_config)\n<\/code><\/pre>\n\n<p>i added the callback in my model.py script that is actually what i use without sagemaker. As logdir i defined the default dir, where the TensoboardOutputConfig writes the data.. but it doesnt work. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_TensorBoardOutputConfig.html\" rel=\"nofollow noreferrer\">docs<\/a> I also used it without the callback.<\/p>\n\n<pre><code> tensorboardCallback = tf.keras.callbacks.TensorBoard(\n        log_dir='\/opt\/ml\/output\/tensorboard',\n        histogram_freq=0,\n        # batch_size=32,ignored tf.2.0\n        write_graph=True,\n        write_grads=False,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None,\n        embeddings_data=None,\n        update_freq='batch') \n<\/code><\/pre>",
        "Challenge_closed_time":1604514854867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585083712870,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"tensorboard tensorflow tensorboard mode access log tensorboard log locat tri tensorboardoutputconfig callback model",
        "Challenge_last_edit_time":1585093210283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60839279",
        "Challenge_link_count":2,
        "Challenge_original_content":"tensorboard tensorflow start job tensorflow import tensorflow mytrain tensorflow entri model role role train instanc count train instanc type xlarg framework version version distribut paramet server enabl train data uri path data mytrain fit train data uri run tensorboard local run tesorboard local gave tensorboard mode run tensorboard logdir host localhost port run access uri logdir mode access log tensorboard log pars arg parser argpars argumentpars data model output directori model dir pass default path default bucket parser add argument model dir type str parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument host type list default json load environ host parser add argument host type str default environ host return parser pars arg arg unknown pars arg train data train label load train data arg train eval data eval label load test data arg train mymodel model train data train label eval data eval label arg host arg host mymodel save path arg model dir model similiar stack edit tri config doesnt tensorboard output config tensorboardoutputconfig output path path bucket mytrain tensorflow entri model role role train instanc count train instanc type xlarg framework version version distribut paramet server enabl tensorboard output config tensorboard output config callback model logdir defin default dir tensoboardoutputconfig write data doesnt doc callback tensorboardcallback kera callback tensorboard log dir opt output tensorboard histogram freq batch size ignor write graph write grad write imag embed freq embed layer embed metadata embed data updat freq batch",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"tensorboard tensorflow start job gave mode access log tensorboard log similiar stack edit tri config doesnt callback logdir defin default dir tensoboardoutputconfig write doesnt doc callback",
        "Challenge_readability":16.9,
        "Challenge_reading_time":43.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":5397.5394436111,
        "Challenge_title":"how can i use tensorboard with aws sagemaker tensorflow?",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1011.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.<\/p>\n<ol>\n<li><p>Followed guide on <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook\" rel=\"noreferrer\">sagemaker debugging<\/a> to configure the <code>S3<\/code> output path for tensorboard logs.<\/p>\n<pre><code>from sagemaker.debugger import TensorBoardOutputConfig\n\ntensorboard_output_config = TensorBoardOutputConfig(\n       s3_output_path = 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n)\n\nestimator = TensorFlow(entry_point='train.py',\n               source_dir='.\/',\n               model_dir=model_dir,\n               output_path= output_dir,\n               train_instance_type=train_instance_type,\n               train_instance_count=1,\n               hyperparameters=hyperparameters,\n               role=sagemaker.get_execution_role(),\n               base_job_name='Testing-TrainingJob',\n               framework_version='2.2',\n               py_version='py37',\n               script_mode=True,\n               tensorboard_output_config=tensorboard_output_config)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<\/li>\n<li><p>Start the tensorboard with the <code>S3<\/code> location provided above via a terminal on the notebook instance.<\/p>\n<pre><code>$ tensorboard --logdir 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n<\/code><\/pre>\n<\/li>\n<li><p>Access the board via URL with <code>\/proxy\/6006\/<\/code>. You need to update the notebook instance details in the following URL.<\/p>\n<pre><code>https:\/\/myinstance.notebook.us-east-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"guid debug configur output path tensorboard log start tensorboard locat termin notebook instanc access board url proxi updat notebook instanc url",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"debug exact root step start tensorboard insid notebook instanc manual guid debug configur output path tensorboard log debugg import tensorboardoutputconfig tensorboard output config tensorboardoutputconfig output path bucket tensorboard log folder estim tensorflow entri train sourc dir model dir model dir output path output dir train instanc type train instanc type train instanc count hyperparamet hyperparamet role execut role base job test trainingjob framework version version mode tensorboard output config tensorboard output config estim fit input start tensorboard locat termin notebook instanc tensorboard logdir bucket tensorboard log folder access board url proxi updat notebook instanc url http myinstanc notebook proxi",
        "Solution_preprocessed_content":"debug exact root step start tensorboard insid notebook instanc manual guid debug configur output path tensorboard log start tensorboard locat termin notebook instanc access board url updat notebook instanc url",
        "Solution_readability":22.1,
        "Solution_reading_time":20.74,
        "Solution_score":5,
        "Solution_sentence_count":13,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":114,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5233.5912852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Challenge_closed_time":1600204151310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581363222683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"paramet server distribut train tensorflow run paramet server tensorflow decid distribut variabl paramet server paramet assign paramet server commun paramet server",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Challenge_link_count":5,
        "Challenge_original_content":"tensorflow paramet server paramet server distribut train tensorflow concret run http github com sampl mode blob master distribut option distribut train ipynb block initi estim tensorflow tensorflow import tensorflow git config repo http github com sampl mode branch master instanc type xlarg instanc count model dir opt model distribut paramet server enabl hyperparamet epoch batch size estim tensorflow git config git config sourc dir distribut option entri train base job cifar role role framework version version hyperparamet hyperparamet train instanc count instanc count train instanc type instanc type model dir model dir tag kei valu cifar kei tensorboard valu dist distribut distribut document tensorflow devic scope assign variabl worker run train job model defin http github com sampl mode blob master distribut option model model rate weight decai optim momentum size mpi hvd model sequenti model add convd pad input shape height width depth model add batchnorm model add activ relu model add convd model add flatten model add dens model add activ relu model add dropout model add dens num class model add activ softmax mpi size hvd size optim lower sgd mpi opt hvd distributedoptim opt model compil loss categor crossentropi optim opt metric accuraci return model distribut strategi mpi flag set tensorflow decid variabl layer store see distribut strategi run train model instanc decreas runtim distribut train tensorflow decid distribut variabl explicit devic distribut automat paramet assign commun",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"tensorflow paramet server paramet server distribut train tensorflow concret run block initi estim tensorflow document tensorflow devic scope assign variabl worker run train job model defin distribut strategi tensorflow decid variabl layer store see distribut strategi run train model instanc decreas runtim distribut train tensorflow decid distribut variabl explicit devic distribut automat paramet assign commun",
        "Challenge_readability":16.1,
        "Challenge_reading_time":47.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":5233.5912852778,
        "Challenge_title":"Tensorflow Parameter Servers on SageMaker",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":350,
        "Platform":"Stack Overflow",
        "Solution_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"tensorflow imag setup config launch paramet server multi train distributionstrategi default strategi distributionstrategi config disabl paramet server option launch train job set train output log cloudwatch paramet assign commun",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"tensorflow decid distribut variabl explicit devic distribut automat tensorflow imag setup config launch paramet server multi train setup node cluster worker thread configur distributionstrategi default strategi distributionstrategi config disabl paramet server option launch train job set train paramet assign commun output log cloudwatch link train job consol page http github com tensorflow train toolkit blob master src tensorflow train http tensorflow org guid distribut train default strategi",
        "Solution_preprocessed_content":"tensorflow decid distribut variabl explicit devic distribut automat tensorflow imag setup launch paramet server multi train setup node cluster worker thread configur distributionstrategi default strategi distributionstrategi disabl option launch train job set train paramet assign commun output log cloudwatch link train job consol page",
        "Solution_readability":12.6,
        "Solution_reading_time":21.32,
        "Solution_score":1,
        "Solution_sentence_count":16,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":187,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5196.6227847222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Challenge_closed_time":1600285333648,
        "Challenge_comment_count":1,
        "Challenge_created_time":1581577491623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat workspac terraform terraform accomplish task",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60202189",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat resourc terraform resourc creat workspac terraform terraform achiev",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"creat resourc terraform resourc creat workspac terraform terraform achiev",
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5196.6227847222,
        "Challenge_title":"How to create azure machine learning resource using terraform resource providers?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":1152.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"terraform resourc workspac creat workspac terraform resourc http terraform doc azurerm workspac html",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"terraform resourc workspac obsolet http terraform doc azurerm workspac html resourc azurerm workspac workspac locat azurerm resourc group locat resourc group azurerm resourc group azurerm kei vault azurerm kei vault storag account azurerm storag account ident type systemassign",
        "Solution_preprocessed_content":"terraform resourc workspac obsolet",
        "Solution_readability":25.1,
        "Solution_reading_time":11.31,
        "Solution_score":4,
        "Solution_sentence_count":10,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":46,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":5092.1609733333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to know if it's possible to get an Amazon ECR container URI for a specific image programmatically (using AWS CLI or Python). For example, if I need the URL for the latest <code>linear-learner<\/code> (built-in model) image for the <code>eu-central-1<\/code> region.<\/p>\n<p>Expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:latest\n<\/code><\/pre>\n<p>EDIT: I have found the solution with <code>get_image_uri<\/code>. It looks like this function will be depreceated and I don't know how to use <code>ImageURIProvider<\/code> instead.<\/p>",
        "Challenge_closed_time":1617813746592,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599474994217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"ecr uri model imag programmat cli function imag uri imageuriprovid deprec",
        "Challenge_last_edit_time":1599481967088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63775893",
        "Challenge_link_count":0,
        "Challenge_original_content":"ecr uri model imag ecr uri imag programmat cli url latest linear learner built model imag central region dkr ecr central amazonaw com linear learner latest edit imag uri function deprec imageuriprovid",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"ecr uri model imag ecr uri imag programmat url latest imag region edit function deprec",
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":5094.0978819444,
        "Challenge_title":"How to get an Amazon ECR container URI for a specific model image in Sagemaker?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":3017.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The newer versions of SageMaker SDK have a more centralized API for getting the URIs:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker \nsagemaker.image_uris.retrieve(&quot;linear-learner&quot;, &quot;eu-central-1&quot;)\n<\/code><\/pre>\n<p>which gives the expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:1\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"newer version sdk ecr uri model imag programmat cli function retriev imag uri modul uri",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"newer version sdk central api uri import imag uri retriev linear learner central dkr ecr central amazonaw com linear learner",
        "Solution_preprocessed_content":"newer version sdk central api uri",
        "Solution_readability":20.7,
        "Solution_reading_time":5.24,
        "Solution_score":4,
        "Solution_sentence_count":2,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":29,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5056.4061419444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Challenge_closed_time":1657872107440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639669045329,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"train compil program split multipl file maintain job run multipl gpu job tri start messag messag limit addit train written multipl file tri run singl gpu mode messag origin trainingargu",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Challenge_link_count":0,
        "Challenge_original_content":"multi file sourc dir bundl train compil distribut hope train compil hug trainer api pytorch program split multipl file maintain job run multipl gpu scale multi devic singl node accept doc distribut train launcher launcher sourc dir bundl pass train train hyperparamet job tri start traceback file opt conda lib runpi line run modul return run global file opt conda lib runpi line run exec run global file opt conda lib site packag torch xla distribut xla spawn line file opt conda lib site packag torch xla distribut xla spawn line xmp spawn mod arg nproc arg num gpu attributeerror modul train attribut idea limit addit train written multipl file tri run singl gpu mode xlarg directli call train distribut launcher origin trainingargu tensorflow compil compil run edit turn explicitli set gpu troubleshoot doc messag file opt config line init init file opt conda lib site packag transform train arg line init torch devic type cuda eval file opt conda lib site packag transform file util line wrapper return func arg kwarg file opt conda lib site packag transform train arg line devic return setup devic file opt conda lib site packag transform file util line cach fget obj file opt conda lib site packag transform file util line wrapper return func arg kwarg file opt conda lib site packag transform train arg line setup devic devic xla devic file opt conda lib site packag torch xla core xla model line xla devic devic xla devic file opt conda lib site packag torch xla core xla model line xla devic xla devic devic valu file opt conda lib site packag torch xla util util line valu valu gen file opt conda lib site packag torch xla core xla model line devic lazyproperti lambda torch xla xlac xla devic runtimeerror tensorflow compil xla xla client comput client miss xla configur",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"bundl train compil hope train compil program split multipl file maintain job run multipl gpu doc launcher bundl pass train hyperparamet job tri start traceback file line return file line exec file line file line arg attributeerror modul train attribut idea limit addit train written multipl file tri run mode directli call train distribut launcher origin trainingargu compil run edit turn explicitli set troubleshoot doc messag file line file line cuda file line wrapper return func file line devic return file line cach file line wrapper return func file line devic file line devic file line file line valu file line runtimeerror miss xla configur",
        "Challenge_readability":11.2,
        "Challenge_reading_time":43.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":5056.4061419444,
        "Challenge_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":326,
        "Platform":"Tool-specific",
        "Solution_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"defin train explicitli configur gpu offici train compil notebook enabl smtc job onlin",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"ahh ago forgot updat train defin execut run gpu time hopefulli futur explicitli configur enabl smtc job onlin offici smtc notebook",
        "Solution_preprocessed_content":"ahh ago forgot updat train defin gpu explicitli configur enabl smtc job onlin offici smtc notebook",
        "Solution_readability":12.0,
        "Solution_reading_time":5.95,
        "Solution_score":0,
        "Solution_sentence_count":3,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":85,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":3,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"messag state trainingjob regist version com scheme kubectl pkg scheme scheme",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_original_content":"trainingjob regist version com scheme kubectl pkg scheme scheme trainingjob regist version com scheme kubectl pkg scheme scheme",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"trainingjob regist version scheme trainingjob regist version scheme",
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Platform":"Github",
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"oper appli run verifi kubectl oper pod",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"oper step replic instal offici document http doc com latest oper kubernet html ran oper appli run verifi kubectl oper pod close activ dai",
        "Solution_preprocessed_content":"step replic instal offici document ran oper appli run verifi kubectl pod close activ dai",
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score":0,
        "Solution_sentence_count":3,
        "Solution_topic":"Kubernetes Service",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":60,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4538.5667388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Challenge_closed_time":1659424830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643025892210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat pipelin job servic agent messag servic agent access gcr imag gcr latest",
        "Challenge_last_edit_time":1643085989932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Challenge_link_count":0,
        "Challenge_original_content":"servic agent access gcr imag idea creat pipelin job servic agent aiplatform iam gserviceaccount com grant access imag gcr latest",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"servic agent access gcr imag idea",
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4555.2605505556,
        "Challenge_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Solution_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"artifact registri role platform servic agent start role artifactregistri reader",
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_original_content":"aiplatform iam gserviceaccount com platform servic agent servic agent access read pull docker imag gcr creat pipelin run permiss edit iam role artifact registri role servic agent start role artifactregistri reader hope",
        "Solution_preprocessed_content":"platform servic agent servic agent access docker imag gcr creat pipelin run permiss edit iam role artifact registri role servic agent start hope",
        "Solution_readability":14.8,
        "Solution_reading_time":10.14,
        "Solution_score":1,
        "Solution_sentence_count":8,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":66,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":4541.5040575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Challenge_closed_time":1660920166467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644569855830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"mechan hyperparamet tune job minim cross entropi loss hyperparamet defin hyperparametertun class copi opt input config hyperparamet json adjust train imag hyperparamet file hyperparamet pass train sampl hpo notebook argpars pass hyperparamet",
        "Challenge_last_edit_time":1644570751860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Challenge_link_count":0,
        "Challenge_original_content":"hyperparamet tune job mechan mechan hyperparamet tune job minim cross entropi loss object metric tuner defin hyper paramet hyperparametertun class copi opt input config hyperparamet json adjust train imag hyper paramet opt input config hyperparamet json edit sampl hpo notebook argpars pass hp pass train",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"hyperparamet tune job mechan mechan hyperparamet tune job minim cross entropi loss defin hyper paramet class copi adjust train imag hyper paramet edit sampl hpo notebook pass hp pass train",
        "Challenge_readability":13.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4541.7529547222,
        "Challenge_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"file opt input config hyperparamet json paramet tune static paramet metric hyperparametertun class defin hyperparamet tune metric optim hyperparamet pass train argpars",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"final figur time file opt input config hyperparamet json slightli compar regular train job param tune static param metric structur hope tune object metric metric dynam param dynam param static param valu static paramn valu",
        "Solution_preprocessed_content":"final figur time file slightli compar regular param tune static param structur hope",
        "Solution_readability":9.1,
        "Solution_reading_time":7.65,
        "Solution_score":2,
        "Solution_sentence_count":8,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":70,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4512.6045455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following this example notebook to learn SageMaker's processing jobs API: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb<\/a><\/p>\n<p>I'm trying to modify their code to avoid using the default S3 bucket, namely: <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code><\/p>\n<p>For their data processing step with the <code>.run<\/code> method:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>I was able to modify it to use my own S3 bucket by using the <code>destination<\/code> parameter like this:<\/p>\n<pre><code>sklearn_processor.run( \n    code=output_bucket_uri + &quot;preprocessing.py&quot;, \n    inputs=[ProcessingInput( \n        source=input_bucket_uri + &quot;census-income.csv&quot;, \n        destination=path+&quot;input\/&quot;, \n    )], \n    outputs=[ \n        ProcessingOutput( \n            output_name=&quot;train_data&quot;, \n            source=path+&quot;train\/&quot;, \n            destination=output_bucket_uri + &quot;train\/&quot;, \n        ), \n        ProcessingOutput( \n            output_name=&quot;test_data&quot;, \n            source=path+&quot;test\/&quot;, \n            destination=output_bucket_uri + &quot;test\/&quot;, \n        ), \n    ], \n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;], \n)\n<\/code><\/pre>\n<p>But for the <code>.fit<\/code> method:<\/p>\n<pre><code>sklearn.fit({&quot;train&quot;: preprocessed_training_data})\n<\/code><\/pre>\n<p>I have not been able to find a parameter to pass it so that the output artifacts are saved to a S3 bucket that I specify instead of the default s3 bucket <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code>.<\/p>",
        "Challenge_closed_time":1648557908696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632276763933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"modifi notebook bucket default bucket modifi data process step destin paramet paramet specifi bucket output artifact fit",
        "Challenge_last_edit_time":1632312532332,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69277390",
        "Challenge_link_count":2,
        "Challenge_original_content":"specifi bucket sklearn estim sklearn notebook process job api http github com blob master process scikit data process model evalu scikit data process model evalu ipynb modifi avoid default bucket data process step run process import processinginput processingoutput sklearn processor run preprocess input processinginput sourc input data destin opt process input output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument train test split ratio modifi bucket destin paramet sklearn processor run output bucket uri preprocess input processinginput sourc input bucket uri censu incom csv destin path input output processingoutput output train data sourc path train destin output bucket uri train processingoutput output test data sourc path test destin output bucket uri test argument train test split ratio fit sklearn fit train preprocess train data paramet pass output artifact save bucket specifi default bucket",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"specifi bucket sklearn notebook process job api modifi avoid default bucket data process step modifi bucket paramet paramet pass output artifact save bucket specifi default bucket",
        "Challenge_readability":26.2,
        "Challenge_reading_time":32.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4522.5402119444,
        "Challenge_title":"Can I specify S3 bucket for sagemaker.sklearn.estimator's SKLearn?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:<\/p>\n<pre><code>from sagemaker.session import Session    \nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role='&lt;arn-role&gt;',\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1,\n                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))\n<\/code><\/pre>\n<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"specifi bucket sklearnprocessor output artifact creat session bucket send session paramet",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"sklearnprocessor ideal specifi default bucket creat session bucket send session paramet session import session sklearn processor sklearnprocessor framework version role instanc type xlarg instanc count session session default bucket exact cleaner",
        "Solution_preprocessed_content":"sklearnprocessor ideal specifi default bucket creat session bucket send paramet exact cleaner",
        "Solution_readability":12.3,
        "Solution_reading_time":7.96,
        "Solution_score":0,
        "Solution_sentence_count":6,
        "Solution_topic":"Model Registry",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":66,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":51185.3566025,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.<\/p>",
        "Challenge_closed_time":1474621313252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458446579210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"advic implement packag studio environ potenti implement process",
        "Challenge_last_edit_time":1459352400923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36110109",
        "Challenge_link_count":0,
        "Challenge_original_content":"implement packag studio environ hope tri implement knowledg pitfal",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"implement packag studio environ hope tri implement knowledg pitfal",
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4492.9816783333,
        "Challenge_title":"Has anyone any experience on implementing the R Package XGBoost within the Azure ML Studio environment?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.<\/p>\n<p>You can follow the instructions over <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Using-XGBoost-to-build-Graduation-Admit-Model-1\" rel=\"nofollow noreferrer\">here<\/a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/archive\/blogs\/benjguin\/how-to-upload-an-r-package-to-azure-machine-learning\" rel=\"nofollow noreferrer\">This<\/a> is for any generic packages which are not preloaded in the environment<\/p>\n<p>The following is a <a href=\"https:\/\/web.archive.org\/web\/20161020215633\/https:\/\/azure.microsoft.com\/en-in\/documentation\/articles\/machine-learning-r-csharp-web-service-examples\/\" rel=\"nofollow noreferrer\">list of experiments<\/a> to publish R models as a web service<\/p>\n<p>Hope this helps!<\/p>\n<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"zip load packag window binari dataset import environ download version pre load save dataset instruct link upload gener packag preload environ list publish model web servic version open version import common librari",
        "Solution_last_edit_time":1643619684692,
        "Solution_link_count":3,
        "Solution_original_content":"zip load packag window binari dataset import environ instruct import latest version simpli download version load save dataset gener packag preload environ list publish model web servic hope edit simpli version open version import common librari",
        "Solution_preprocessed_content":"zip load packag window binari dataset import environ instruct import latest version simpli download version load save dataset gener packag preload environ list publish model web servic hope edit simpli version open import common librari",
        "Solution_readability":14.8,
        "Solution_reading_time":14.54,
        "Solution_score":1,
        "Solution_sentence_count":8,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":116,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4477.9070705556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1580113898207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"messag precondit tabl initi predict deploi univers sentenc encod larg model common tensorflow hub sentenc encod",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_original_content":"precondit tabl initi deploi univers sentenc encod deploi univers sentenc encod larg predict deploi model precondit tabl initi save model import tensorflow import tensorflow hub hub import numpi tfhub savedmodel model export path model path format export path model tfhub uri http tfhub dev univers sentenc encod larg session sess modul hub modul tfhub uri sess run global variabl initi tabl initi input param modul input dict dtype input param text dtype shape input param text shape defin model input input text placehold dtype shape text output modul input text output vector output export model save model save sess model path input input output output return model path peopl common tensorflow hub sentenc encod",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"precondit tabl initi deploi univers sentenc encod deploi predict deploi model save model peopl common sentenc encod",
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4477.9070705556,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"specifi legaci init tabl initi creat tensorflow estim run tabl initi predict github repositori",
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_original_content":"run exact earlier week modifi notebook serv model run predictor predict tensorflow estim outlin perfectli http github com awslab issuecom tabl initi run train specifi legaci init run predict",
        "Solution_preprocessed_content":"run exact earlier week modifi notebook serv model run tensorflow estim outlin perfectli run train specifi run predict",
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score":0,
        "Solution_sentence_count":9,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":78,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4461.4114333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Challenge_closed_time":1607117580710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591056499550,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"enabl debugg autopilot plot shap valu guidanc enabl debug model hack background",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Challenge_link_count":2,
        "Challenge_original_content":"enabl debugg autopilot plot shap valu automl pipelin achiev debugg accord http com blog explain debugg enabl debug model autopilot hack background",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"enabl debugg autopilot plot shap valu automl pipelin achiev debugg accord enabl debug model autopilot hack background",
        "Challenge_readability":17.8,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4461.4114333334,
        "Challenge_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"shap autopilot model blog explain shap autopilot enabl debugg autopilot hack background",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"autopilot debugg box dec hack hyperparamet tune job pass debug paramet shap autopilot model blog explain shap autopilot http com blog explain autopilot model shap",
        "Solution_preprocessed_content":"autopilot debugg box hack hyperparamet tune job pass debug paramet shap autopilot model blog explain shap autopilot",
        "Solution_readability":16.1,
        "Solution_reading_time":7.55,
        "Solution_score":0,
        "Solution_sentence_count":5,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":58,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628006476000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"intermitt connect workspac messag state null object click connect button prevent notebook window open workspac window open notic automat log shortli see window open jupyt notebook",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_original_content":"null object connect notebook start workspac click connect hand corner screen null object evalu locat littl red box screen notebook window open click connect reproduc intermitt window open notic window automat log shortli see click start workspac wait statu click connect connect log servic workbench connect workspac successfulli window open jupyt notebook window version complet",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"null object connect notebook start workspac click connect corner screen null object littl red box screen notebook window open click connect reproduc intermitt window open notic window automat log shortli see click start workspac wait statu click connect connect log servic workbench connect workspac successfulli window open jupyt notebook window version",
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4421.2883333333,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":164,
        "Platform":"Github",
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"latest version access token expir enabl popup swb domain relat popup blocker",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"tom christi believ access token expir free latest version http github com awslab servic workbench releas tag see persist time connect workspac test singl workspac config consist connect workspac instanc config workspac instanc run shown swb haven verifi instanc consol relat popup blocker report gali creat messag http sim com chamdoc yeah popup disabl swb domain enabl popup swb domain allow connect free reopen ticket enabl popup",
        "Solution_preprocessed_content":"believ access token expir free latest version see persist test singl workspac config consist connect workspac instanc config workspac instanc run shown swb haven verifi instanc consol relat popup blocker report creat messag yeah popup disabl swb domain enabl popup swb domain allow connect free reopen ticket enabl popup",
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score":0,
        "Solution_sentence_count":14,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":171,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"invok sampl mnist train job job deploi invok job algorithm hyperparamet input output data configur resourc configur stop condit text",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_original_content":"kick job deploi sampl mnist train job invok kubectl trainingjob mnist namespac default label annot kubectl kubernet appli configur apivers com trainingjob metadata annot mnist namespac default api version com trainingjob metadata creation timestamp gener resourc version link api com namespac default trainingjob mnist uid efd spec algorithm train imag dkr ecr amazonaw com latest train input mode file hyper paramet depth valu eta valu gamma valu child weight valu silent valu object valu multi softmax num class valu num round valu input data config channel train compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist train channel compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist output data config output path mnist model region resourc config instanc count instanc type xlarg volum size role arn arn iam role execut role stop condit runtim",
        "Challenge_participation_count":15,
        "Challenge_preprocessed_content":"kick job deploi sampl mnist train job invok",
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Github",
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"invok sampl mnist train job involv verifi oper run successfulli cluster step instal oper trust json cluster region oidc account updat",
        "Solution_last_edit_time":null,
        "Solution_link_count":7,
        "Solution_original_content":"charlesa replac input output bucket role arn run output kubectl trainingjob mnist kubectl trainingjob mnist gautamkmr yeah bucket executor role kubectl trainingjob statu secondari statu creation time job mnist kubectl trainingjob mnist namespac default label annot kubectl kubernet appli configur apivers com trainingjob metadata annot mnist namespac default api version com trainingjob metadata creation timestamp gener resourc version link api com namespac default trainingjob mnist uid efd spec algorithm train imag dkr ecr amazonaw com latest train input mode file hyper paramet depth valu eta valu gamma valu child weight valu silent valu object valu multi softmax num class valu num round valu input data config channel train compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist train channel compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist output data config output path mnist model region resourc config instanc count instanc type xlarg volum size role arn arn iam role execut role stop condit runtim charlesa output oper run successfulli cluster verifi kubectl pod grep step http readthedoc stabl oper kubernet html setup oper deploy instal oper yeah notic kubectl pod oper readi statu restart ag oper control fdbd pend kubectl pod oper oper control fdbd namespac oper prioriti priorityclassnam node label control plane control pod templat hash fdbd annot kubernet psp ek privileg statu pend control replicaset oper control fdbd kube rbac proxi imag gcr kubebuild kube rbac proxi port tcp host port tcp arg secur listen address upstream http logtostderr environ role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn imag dkr ecr amazonaw com oper port host port arg metric addr limit cpu memori request cpu memori environ default endpoint role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn condit type statu podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond oper default token rwdkn type secret volum popul secret secretnam oper default token rwdkn option qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event type reason ag messag warn failedschedul default schedul node schedul pod ek ecr crd artifact come ek pull imag region worker node associ cluster that messag sai warn failedschedul default schedul node schedul pod run kubectl node charlesa chanc review kubectl node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek yeah recreat cluster charlesa previou output pod cluster worker node node schedul pod base output worker node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek node oper pod node node previou comment kubectl node comput intern kubectl node comput intern kubectl node comput intern oper pod kubectl pod grep kubectl pod oper oper deploi successfulli trainingjob run attach trainingjob kubectl trainingjob mnist tri oper pod log gautamkmr kubectl log oper control fdbd dkc oper control runtim metric metric server start listen addr control runtim control start eventsourc control trainingjob sourc sourc control runtim control start eventsourc control hyperparametertuningjob sourc sourc control runtim control start eventsourc control hostingdeploy sourc sourc control runtim control start eventsourc control model sourc sourc control runtim control start eventsourc control endpointconfig sourc sourc control runtim control start eventsourc control batchtransformjob sourc sourc setup start control runtim start metric server path metric control runtim control start control control trainingjob control runtim control start control control model control runtim control start control control batchtransformjob control runtim control start control control hostingdeploy control runtim control start control control endpointconfig control runtim control start control control hyperparametertuningjob control runtim control start worker control trainingjob worker count control runtim control start worker control model worker count control runtim control start worker control endpointconfig worker count control runtim control start worker control batchtransformjob worker count control runtim control start worker control hostingdeploy worker count control runtim control start worker control hyperparametertuningjob worker count control trainingjob resourc trainingjob default mnist control trainingjob job statu set intermedi statu trainingjob default mnist statu synchronizingksjobwith control trainingjob updat job statu trainingjob default mnist statu trainingjobstatu synchronizingksjobwith lastchecktim control trainingjob resourc trainingjob default mnist control trainingjob gener spec trainingjob default mnist mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request eab bae bcdee github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request eab bae bcdee lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request cceb github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request cceb lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist charlesa share log track oper pod retriev credenti iam servic unrecognizedclientexcept secur token request trust json http readthedoc stabl oper kubernet html creat iam role trust polici updat cluster region oidc add account charlesa close activ dai open exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output ubuntu imvaria repo model train kubectl pod namespac readi statu restart ag kube node tgx run kube node kqz run kube coredn dbc cwfvj run kube coredn dbc xld run kube kube proxi run kube kube proxi rjj run kube metric server cfbd nppx run job oper control fhkvv run ubuntu imvaria repo model train kubectl pod oper control fhkvv job oper control fhkvv namespac job prioriti node comput intern start time fri jun label control plane control pod templat hash annot kubernet psp ek privileg statu run ip control replicaset oper control docker dfcbeadfabfdaaabffacebc imag dkr ecr amazonaw com oper imag docker pullabl dkr ecr amazonaw com oper sha ffbbabfdbfeabbad port host port arg metric addr namespac job state run start fri jun readi restart count limit cpu memori request cpu memori environ default endpoint default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt kube rbac proxi docker ecdaafdcdceaddbffeaaddbaabbd imag gcr kubebuild kube rbac proxi imag docker pullabl gcr kubebuild kube rbac proxi sha dbbbcbabddabdcabaccceefbed port tcp host port tcp arg secur listen address upstream http logtostderr state run start fri jun readi restart count environ default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt condit type statu initi readi containersreadi podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond kube api access jrt type volum inject data multipl sourc tokenexpirationsecond configmapnam kube root crt configmapopt downwardapi qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event ubuntu imvaria repo model train kubectl log oper control fhkvv job request throttl request took request http api extens vbeta timeout control runtim metric metric server start listen addr start namespac job setup start control runtim start metric server path metric control start eventsourc reconcilergroup com reconcilerkind endpointconfig control endpointconfig sourc sourc control start eventsourc reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici sourc sourc control start eventsourc reconcilergroup com reconcilerkind model control model sourc sourc control start eventsourc reconcilergroup com reconcilerkind trainingjob control trainingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind processingjob control processingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy sourc sourc control start control reconcilergroup com reconcilerkind model control model control start control reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici control start control reconcilergroup com reconcilerkind endpointconfig control endpointconfig control start control reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob control start control reconcilergroup com reconcilerkind processingjob control processingjob control start control reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob control start control reconcilergroup com reconcilerkind trainingjob control trainingjob control start control reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy control start worker reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy worker count control start worker reconcilergroup com reconcilerkind model control model worker count control start worker reconcilergroup com reconcilerkind endpointconfig control endpointconfig worker count control start worker reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici worker count control start worker reconcilergroup com reconcilerkind processingjob control processingjob worker count control start worker reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob worker count control start worker reconcilergroup com reconcilerkind trainingjob control trainingjob worker count control start worker reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob worker count ubuntu imvaria repo model train kubectl trainingjob statu secondari statu creation time job osic test run ubuntu imvaria repo model train kubectl trainingjob osic test run osic test run namespac default label annot api version com trainingjob metadata creation timestamp gener field api version com field type fieldsv fieldsv metadata annot kubectl kubernet appli configur spec algorithmspecif trainingimag traininginputmod inputdataconfig outputdataconfig soutputpath region resourceconfig instancecount instancetyp volumesizeingb rolearn stoppingcondit maxruntimeinsecond trainingjobnam kubectl client appli oper updat time resourc version uid baf spec algorithm train imag dkr ecr amazonaw com model train latest train input mode file input data config channel train compress type data sourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri osic overrid output data config soutputpath osic overrid region resourc config instanc count instanc type xlarg volum size role arn arn iam role model train role stop condit runtim train job osic test run event",
        "Solution_preprocessed_content":"replac input output bucket role arn run output yeah bucket executor role output oper run successfulli cluster verifi step instal oper yeah notic warn failedschedul node schedul pod kubectl node kubectl node statu role ag version readi readi readi yeah recreat cluster previou output cluster worker node base output worker node statu role ag version readi readi readi node oper pod oper deploi successfulli trainingjob run attach trainingjob tri oper pod log share log track oper pod retriev credenti iam servic trust polici updat cluster region oidc add account close activ dai exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output",
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score":0,
        "Solution_sentence_count":227,
        "Solution_topic":"Kubernetes Service",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":2188,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4407.5466925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having the error mentioned in the title when trying to upload a large file (15gb) to my s3 bucket from a Sagemaker notebook instance.<\/p>\n<p>I know that there are some similar questions here that i have already visited. I have gone through <a href=\"https:\/\/stackoverflow.com\/questions\/52541933\/accessdenied-when-calling-the-createmultipartupload-operation-in-django-using-dj\">this<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/37630635\/createmultipartupload-operation-aws-policy-items-needed\">this<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/36272286\/getting-access-denied-when-calling-the-putobject-operation-with-bucket-level-per\">this<\/a> question, but after following the steps mentioned, and applying the policies described in these questions i still have the same error.<\/p>\n<p>I have also come to <a href=\"https:\/\/aws.amazon.com\/es\/premiumsupport\/knowledge-center\/s3-access-denied-error-kms\/#:%7E:text=%22An%20error%20occurred%20(AccessDenied)%20when%20calling%20the%20CreateMultipartUpload%20operation,GenerateDataKey%20and%20kms%3ADecrypt%20actions.\" rel=\"nofollow noreferrer\">this<\/a> documentation page eventually. The problem is that when i go into my users page in the IAM section, i see no users. I can see some roles but no users and i don't know which role should i edit following the steps mentioned in the documentation page. Also, my bucket DON'T have encryption enabled so i'm not really sure that the steps in the documentation page will fix the error for me.<\/p>\n<p>This is the policy in currently using for my bucket:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Id&quot;: &quot;Policy1&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::XXXX:root&quot;\n            },\n            &quot;Action&quot;: &quot;s3:*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::bauer-bucket&quot;,\n                &quot;arn:aws:s3:::bauer-bucket\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I'm totally lost with this, i need to upload that file to my bucket. Please help.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1645238505323,
        "Challenge_comment_count":2,
        "Challenge_created_time":1629371337230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"accessdeni upload larg file bucket notebook instanc tri step document page iam section role edit bucket encrypt enabl polici allow action root upload file bucket",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68846704",
        "Challenge_link_count":4,
        "Challenge_original_content":"accessdeni call createmultipartupload oper access titl upload larg file bucket notebook instanc visit gone step appli polici come document page page iam section role role edit step document page bucket encrypt enabl step document page polici bucket version polici statement sid statement allow princip arn iam root action resourc arn bauer bucket arn bauer bucket lost upload file bucket advanc",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"call createmultipartupload oper access titl upload larg file bucket notebook instanc visit gone step appli polici come document page page iam section role role edit step document page bucket encrypt enabl step document page polici bucket lost upload file bucket advanc",
        "Challenge_readability":16.2,
        "Challenge_reading_time":29.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4407.5466925,
        "Challenge_title":"An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The access is dictated by the execution role that is attached to the SageMaker notebook. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> goes through how add additional s3 permissions to a SageMaker execution role.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"execut role attach notebook instanc add addit permiss document explain",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"access dictat execut role attach notebook http doc com latest role html goe add addit permiss execut role",
        "Solution_preprocessed_content":"access dictat execut role attach notebook goe add addit permiss execut role",
        "Solution_readability":20.3,
        "Solution_reading_time":4.72,
        "Solution_score":2,
        "Solution_sentence_count":3,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":31,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"pipelin compil kubeflow notebook workshop",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_original_content":"compil kubeflow notebook workshop receiv pipelin compil imag http imag githubusercont com afceba png",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"compil kubeflow notebook workshop receiv pipelin compil",
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"short term lower version kfp term updat leverag latest featur kfp plan file compat latest version kfp potenti plan updat merg close",
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_original_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version pip instal http storag googleapi com pipelin releas kfp tar upgrad term updat leverag latest featur kfp http github com sampl ek kubeflow workshop blob dfeacbfcebea notebook kubeflow pipelin pipelin ipynb file leverag latest featur kfp master kubeflow pipelin http github com kubeflow pipelin tree master sampl contrib sampl mnist kmean master yaml file http github com kubeflow pipelin tree master compon latest version kfp compat potenti http github com kubeflow pipelin http github com kubeflow pipelin descript time team concentr updat merg close",
        "Solution_preprocessed_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version term updat leverag latest featur kfp file leverag latest featur kfp master latest version kfp compat potenti descript time team concentr updat merg close",
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score":0,
        "Solution_sentence_count":17,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":157,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4337.3736888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Challenge_closed_time":1662537228663,
        "Challenge_comment_count":3,
        "Challenge_created_time":1646922683383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run notebook instanc leav run overnight output cell print statement plot jupyterlab tab open close tab open later notebook stop notebook intend tab open",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Challenge_link_count":0,
        "Challenge_original_content":"run notebook instanc close tab notebook instanc studio run notebook hour finish leav overnight output cell output combin print statement plot howevev start run notebook initi cell run close jupyterlab tab browser minut open notebook stop notebook output cell print plot jupyterlab tab open turn laptop",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"run notebook instanc close tab notebook instanc run notebook hour finish leav overnight output cell output combin print statement plot howevev start run notebook initi cell run close jupyterlab tab browser minut open notebook stop notebook output cell jupyterlab tab open",
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4337.3736888889,
        "Challenge_title":"Run Sagemaker notebook instance and be able to close tab",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1154.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"process job librari call run notebook librari notebook structur cell run bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant librari fork modifi base creat docker base",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"end process job initi librari month ago run notebook notebook structur cell run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant librari fork creat docker base",
        "Solution_preprocessed_content":"end process job initi librari month ago run notebook notebook structur cell run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant librari fork creat docker base",
        "Solution_readability":8.3,
        "Solution_reading_time":9.86,
        "Solution_score":0,
        "Solution_sentence_count":8,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":126,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"resourcelimitexceed run cell creat endpoint host model studio tour time account account level servic limit xlarg endpoint usag instanc util instanc request delta instanc prerequisit section address proactiv link servic limit increas page notebook instanc type endpoint default servic limit",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_original_content":"resourcelimitexceed xlarg run studio account walk studio tour http doc com latest studio end end html time account servic limit hit run cell creat endpoint host model resourcelimitexceed resourcelimitexceed call createendpoint oper account level servic limit xlarg endpoint usag instanc util instanc request delta instanc contact request increas limit prerequist section address proactiv link servic limit increas page notebook instanc type endpoint default servic limit lmk prefer submit",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"resourcelimitexceed run studio account walk studio tour time account servic limit hit run cell creat endpoint host model prerequist section address proactiv link servic limit increas page notebook instanc type endpoint default servic limit lmk prefer submit",
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"link servic limit increas page prerequisit section notebook instanc type endpoint default servic limit notebook valu child weight resourcelimitexceed account level servic limit xlarg endpoint usag instanc util instanc request delta instanc default instanc train job account increas tour notebook updat contact limit increas account run notebook",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"cell call child weight tri child weight default instanc train job account increas tour add prerequsit section notebook valu child weight resourcelimitexceed resourcelimitexceed call createtrainingjob oper account level servic limit instanc train job instanc util instanc request delta instanc contact request increas limit doc notebook updat http doc com gener latest html state default limit xlarg typic account run notebook administr contact limit increas account run notebook",
        "Solution_preprocessed_content":"cell call child weight tri default instanc train job account increas tour add prerequsit section notebook valu doc notebook updat state default limit typic account run notebook administr contact limit increas account run notebook",
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score":0,
        "Solution_sentence_count":9,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":176,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566233711000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"pipelinestep automl step null messag consist dataset approxim featur fewer featur interpret refin limit featur",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_original_content":"automl null vagu exactli interpret refin pipelinestep automl step link http mlworkspac portal subscript ffeae cbd bbdcae resourcegroup itsmlteam dev machinelearningservic workspac avadevitsmlsvc deal deal nema run ab fdf dataset featur consist fewer featur limit",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"automl null vagu exactli interpret refin pipelinestep automl step dataset featur consist fewer featur limit",
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4111.2283333333,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"team member request run log team member automl setup time automl team random occurr consist dataset approxim featur",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"nema unfortun access workspac pipelin log sonni run ebfd ecfbbfcf runid automl setup time time automl team random occurr lost track",
        "Solution_preprocessed_content":"nema unfortun access workspac log sonni run runid automl setup time time automl team random occurr lost track",
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score":1,
        "Solution_sentence_count":8,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":96,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4025.8450966667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Challenge_closed_time":1639574747348,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625081705000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"pass data endpoint deepar chicago traffic violat notebook creat model endpoint load endpoint pass data messag byte data serial encod data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_original_content":"pass data endpoint deepar chicago traffic violat notebook model endpoint creat forecast http github com blob master introduct appli deepar chicago traffic violat deepar chicago traffic violat ipynb howevr haven delet model endpoint order extern creat tri load endpoint pass data predict load csv exactli notebook pars csv notebook predictor predict predictor predict invok endpoint load endpoint pass data previou respons notebook messag type type file object byte data guess send data list endpoint serial encod convert json byte",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"pass data endpoint deepar chicago traffic violat notebook model endpoint creat forecast howevr haven delet model endpoint order extern creat tri load endpoint pass data predict load csv exactli notebook pars csv notebook load endpoint pass data previou respons notebook messag type type object byte guess send data list endpoint serial encod convert json byte",
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":4025.8450966667,
        "Challenge_title":"How to pass data to an endpoint",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":201,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"serial encod payload json dump payload json dump payload encod predictor class serial option encod decod data serial import identityseri deseri import jsondeseri serial identityseri type json serial option link http readthedoc stabl api infer serial html",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"predictor predict convert data format endpoint serial encod payload json dump payload byte arrai json dump payload encod predictor class taken care serial option serial encod decod data simpli endpoint predictor class serial import identityseri deseri import jsondeseri serial identityseri type json hope serial option link serial http readthedoc stabl api infer serial html edit rvegira jul edit rvegira jul",
        "Solution_preprocessed_content":"convert data format endpoint serial encod payload byte arrai predictor class taken care serial option serial data simpli endpoint predictor class serial import identityseri deseri import jsondeseri hope serial option link serial edit jul edit jul",
        "Solution_readability":13.1,
        "Solution_reading_time":13.16,
        "Solution_score":0,
        "Solution_sentence_count":11,
        "Solution_topic":"JSON Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":143,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"import librari notebook load train automl modul messag tensorflow modul attribut log",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_original_content":"load train automl receiv notebook import librari import json import pickl import numpi import panda train automl import automlconfig sklearn extern import joblib core model import model import json import pickl import numpi import panda train automl import automlconfig sklearn extern import joblib core model import model attributeerror traceback import numpi import panda train automl import automlconfig sklearn extern import joblib core model import model anaconda env lib site packag train automl init suppress warn import phase warn simplefilt ignor automl import fit pipelin automlconfig import automlconfig automl step import automlstep automlsteprun anaconda env lib site packag train automl automl automl client core runtim cach store import cachestor automl client core runtim import log util runtim log util automl core import data transform fit pipelin fit pipelin helper automl core automl pipelin import automlpipelin automl core data context import rawdatacontext transformeddatacontext anaconda env lib site packag automl core fit pipelin automl client core common limit function except import timeoutexcept automl client core runtim dataset import datasetbas import packag util pipelin run helper train util automl base set import automlbaseset automl pipelin import automlpipelin anaconda env lib site packag automl core pipelin run helper automl client core common except import clientexcept automl client core runtim import metric automl client core runtim import pipelin spec pipelin spec modul automl client core runtim dataset import datasetbas automl client core runtim execut context import executioncontext anaconda env lib site packag automl core vendor automl client core runtim pipelin spec automl client core common import constant automl client core runtim import model wrapper wrapper automl client core runtim nimbu wrapper import averagedperceptronbinaryclassifi averagedperceptronmulticlassclassifi nimbusmlclassifiermixin nimbusmlregressormixin anaconda env lib site packag automl core vendor automl client core runtim wrapper environ cpp log level log set verbos log optim attributeerror modul tensorflow attribut log",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"load receiv notebook import librari import json import pickl import numpi import panda import automlconfig import joblib import model import json import pickl import numpi import panda import automlconfig import joblib import model attributeerror traceback import numpi import panda import automlconfig import joblib import model suppress warn import phase import automlconfig import automlconfig import automlstep automlsteprun import cachestor import import import automlpipelin import rawdatacontext transformeddatacontext import timeoutexcept import datasetbas import import automlbaseset import automlpipelin import clientexcept import metric import import datasetbas import executioncontext import constant import import averagedperceptronbinaryclassifi averagedperceptronmulticlassclassifi nimbusmlclassifiermixin nimbusmlregressormixin optim attributeerror modul tensorflow attribut log",
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"upgrad downgrad tensorflow version",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"version tensorflow http stackoverflow com modul tensorflow attribut log tensorflow stock notebook creat week librari sorri inconveni relat instal default notebook instanc broke api option upgrad downgrad tensorflow pip instal tensorflow gpu tensorflow estim",
        "Solution_preprocessed_content":"version tensorflow tensorflow stock notebook creat week librari sorri inconveni relat instal default notebook instanc broke api option upgrad downgrad tensorflow pip instal",
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score":0,
        "Solution_sentence_count":14,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":109,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3886.8446766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Challenge_closed_time":1633922581063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619929277080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"figur run hyperparamet train step pipelin sdk successfulli creat hyperparamet configur pipelin step hyperparamet configur pipelin",
        "Challenge_last_edit_time":1619929940227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Challenge_link_count":0,
        "Challenge_original_content":"combin pipelin hyperparamet sdk train step short figur run hyperparam train step train step pythonscriptstep pipelin config hyperdr gener regist environ diabet env regist workspac regist env environ diabet pipelin env creat runconfig object pipelin run config runconfigur comput creat run config target computertarget crea assign environ run configur run config environ regist env hyperparam config scriptrunconfig sourc directori folder diabet train add hyperparamet argument train dataset argument input data diabet input train data environ sklearn env comput target train cluster sampl rang paramet valu param gridparametersampl hyperdr combin argument rate choic estim choic configur hyperdr set hyperdr hyperdriveconfig run config config hyperparamet sampl param polici earli stop polici primari metric auc highest auc metric primari metric goal primarymetricgo maxim run restict iter concurr run run iter parallel run run hyperparam pipelin workspac mslearn diabet hyperdr run submit config hyperdr pipelin prep step pythonscriptstep prepar data sourc directori folder prep diabet argument input data diabet input raw data prep data prep data folder output prep data folder comput target computertarget crea runconfig run config allow reus step run train train step pythonscriptstep train regist model sourc directori folder train diabet argument train folder prep data folder input prep data folder comput target computertarget crea runconfig run config allow reus construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step print pipelin built creat run pipelin line hyperdr workspac mslearn diabet pipelin pipelin run submit pipelin regener output config hyperdr pipelin section",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"combin pipelin hyperparamet sdk train step short figur run hyperparam train step pipelin config hyperdr gener hyperparam pipelin config hyperdr pipelin section",
        "Challenge_readability":20.0,
        "Challenge_reading_time":41.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3887.0288841667,
        "Challenge_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":280,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"combin hyperparamet aml pipelin sdk hyperdrivestep class document link sampl notebook demonstr hyperparamet aml pipelin",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"combin hyperparamet aml pipelin http doc com api pipelin step pipelin step hyperdrivestep sampl notebook http github com machinelearningnotebook blob master pipelin intro pipelin aml pipelin paramet tune hyperdr ipynb",
        "Solution_preprocessed_content":"combin hyperparamet aml pipelin sampl notebook",
        "Solution_readability":57.4,
        "Solution_reading_time":11.41,
        "Solution_score":1,
        "Solution_sentence_count":4,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":22,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3708.3794758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1533111465720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1519761299607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"exclud controlno column process output score label column select column dataset exclud controlno column output",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Challenge_link_count":4,
        "Challenge_original_content":"bypass column train model output input data model column controlno column process select column dataset exclud controlno column output column controlno score label score probabl column controlno process output score label column",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"bypass column train model output input data model column controlno column process exclud column output column column process output column",
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3708.3794758333,
        "Challenge_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"edit metadata modul clear featur flag controlno column studio algorithm ignor train controlno column output score label column",
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_original_content":"remov controlno column dataset edit metadata modul clear featur flag select column set field clear featur studio algorithm ignor train return output",
        "Solution_preprocessed_content":"remov controlno column dataset edit metadata modul clear featur flag select column set field clear featur studio algorithm ignor train return output",
        "Solution_readability":12.2,
        "Solution_reading_time":8.65,
        "Solution_score":3,
        "Solution_sentence_count":5,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":69,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3670.8803794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Challenge_closed_time":1659435013616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646219844250,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"build environ packag devop set workspac connect access packag access packag access token option authtyp valueformat argument packag set target argument connect devop repositori potenti authent",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Challenge_link_count":3,
        "Challenge_original_content":"workspac connect argument option build environ packag devop workspac connect devop packag publish artifact feed access sdk access token set connect connectionnam pythonfe target http pkg dev com authtyp pat valu pat token packag git repositori devop document sdk underli rest api option argument link option argument authtyp valueformat set target argument connect devop repositori potenti authent",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"workspac connect argument option build environ packag devop workspac connect devop packag publish artifact feed access sdk access token packag git repositori devop document sdk underli rest api option argument option argument authtyp valueformat set target argument connect devop repositori potenti authent",
        "Challenge_readability":13.0,
        "Challenge_reading_time":18.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3670.8803794444,
        "Challenge_title":"What are valid Azure ML Workspace connection argument options?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"target repositori url packag devop git repositori set connect target authtyp valu argument authtyp valueformat argument packag",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"packag devop git repositori target repositori url set connect connectionnam pythonfe target http dev com git authtyp pat valu note specifi url standard clone url devop devop option authtyp valueformat",
        "Solution_preprocessed_content":"packag devop git repositori target repositori url note specifi url option authtyp valueformat",
        "Solution_readability":14.3,
        "Solution_reading_time":8.53,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":64,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641941774000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"runtim log data qualiti metric batch whylog messag session close prevent creation logger",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_original_content":"close session summari http github com whylab whylog blob mainlin integr ipynb step reproduc binder run notebook runtimeerror traceback tmp ipykernel whylog log data qualiti metric batch whylog log panda batch wait run creat time seri predict srv conda env notebook lib site packag whylog patcher log panda dataset dataset timestamp param dataset dataset option specifi ylog creat logger dataset dataset timestamp dataset timestamp ylog srv conda env notebook lib site packag whylog patcher creat logger dataset dataset timestamp ylog logger dataset ylog ylog creat logger dataset dataset timestamp dataset timestamp logger dataset ylog return ylog srv conda env notebook lib site packag whylog patcher creat logger dataset dataset timestamp tag logger session logger run run session timestamp session timestamp dataset timestamp dataset timestamp tag tag return logger srv conda env notebook lib site packag whylog app session logger dataset dataset timestamp session timestamp tag metadata segment profil dataset rotat time cach size constraint activ rais runtimeerror session close creat logger explicitli set default timezon utc equal test runtimeerror session close creat logger",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"close session summari summar concis step reproduc binder run notebook",
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3662.6730555556,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"initi session creation updat creat session whylog session depend instal writer configur binder writer allow implicitli specifi config throw except option disabl choos cell",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"close default session later whylog wrapper close session creat logger updat initi session creation whylog import creat session session creat session summari session profil datafram train train data flat summari summari summari updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer obviou reason http github com whylab whylog blob mainlin whylog yaml imag http imag githubusercont com cbccd png notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason took bit debug config relat vote throw except option disabl except choos path ignor drop cell config pipelin pipelin verbos writer save format protobuf output path type cfg file config yaml echo config cfg file whylog import creat session session creat session cfg file assert whylog version later integr whylog enabl session stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"close default session later whylog wrapper close session creat logger updat initi session creation updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason vote throw except option disabl except choos path ignor drop cell stale remov stale label close tomorrow",
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score":1,
        "Solution_sentence_count":15,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":261,
        "Tool":"MLflow"
    },
    {
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"clarif scriptprocessor processor api sdk notic api achiev goal scriptprocessor docker paramet processor docker entrypoint paramet api",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_original_content":"processor scriptprocessor sdk sdk api scriptprocessor processor achiev goal notic scriptprocessor docker paramet hand processor docker entrypoint paramet",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"processor scriptprocessor sdk sdk api scriptprocessor processor achiev goal notic scriptprocessor docker paramet hand processor docker paramet",
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3630.1468475,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"scriptprocessor api write process subclass processor api creat customprocessor class complex api",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"process scriptprocessor subclass process processor scriptprocessor write process processor subclass creat customprocessor class complex",
        "Solution_preprocessed_content":"subclass write process subclass creat class complex",
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score":1,
        "Solution_sentence_count":3,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":28,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3592.1438852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Challenge_closed_time":1466402236047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453470518060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"plot dimension graph label idea add label graph",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Challenge_link_count":0,
        "Challenge_original_content":"plot label suppos datafram mydatafram column valu fourth plot dimension graph return graph label run idea plot mydatafram mydatafram titl xlab ylab col blue pch cex lty solid lwd line col yellow add label text relativo relativo label relativo cex po",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"plot label suppos datafram column valu plot graph return graph label run idea",
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3592.1438852778,
        "Challenge_title":"Azure: plot without labels",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"ggplot plot dimension graph label cortana intellig galleri",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"ggplot bit plot label cortana intellig galleri task",
        "Solution_preprocessed_content":"ggplot bit plot label cortana intellig galleri task",
        "Solution_readability":11.9,
        "Solution_reading_time":3.66,
        "Solution_score":-1,
        "Solution_sentence_count":4,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":28,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3574.5224019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Challenge_closed_time":1486144727963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1473271529687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run tensorflow window paa vm workaround run tensorflow",
        "Challenge_last_edit_time":1473276447316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Challenge_link_count":2,
        "Challenge_original_content":"tensorflow peopl tensorflow link advantag ubuntu window tensorflow run window link convers window engin hai ning came paa vm window tensorflow window direct run tensorflow figur allow run tensorflow",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"tensorflow peopl tensorflow link advantag ubuntu window tensorflow run window link convers window engin hai ning came paa vm window tensorflow window direct run tensorflow figur allow run tensorflow",
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3575.88841,
        "Challenge_title":"How to call Tensorflow in Azure ML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1743.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"nativ tensorflow packag window releas tensorflow packag instal run window laptop",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"quick updat tensorflow nativ tensorflow packag window run successfulli window laptop blog",
        "Solution_preprocessed_content":"quick updat tensorflow nativ tensorflow packag window run successfulli window laptop blog",
        "Solution_readability":9.4,
        "Solution_reading_time":4.47,
        "Solution_score":1,
        "Solution_sentence_count":6,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3418.0769397223,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Challenge_closed_time":1614800344830,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602495267847,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"list index rang send data model deploi studio rest api infer schema allow compat power deploy design healthi messag entri predict",
        "Challenge_last_edit_time":1615561798207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Challenge_link_count":1,
        "Challenge_original_content":"infer schema list index rang model deploi studio updat infer schema allow compat power send data model rest api infer schema return schema instruct link personalis data data sent rest api return list index rang deploy goe ahead design healthi messag greatli edit entri import numpi import panda import joblib core model import model infer schema schema decor import input schema output schema infer schema paramet type standard paramet type import standardpythonparametertyp infer schema paramet type numpi paramet type import numpyparametertyp infer schema paramet type panda paramet type import pandasparametertyp init global model model model regist workspac model path model model path model databricksmodelpowerbi model joblib load model path sampl input schema gener row data numpi sampl input numpyparametertyp arrai dtype panda sampl input pandasparametertyp datafram standard sampl input standardpythonparametertyp nest input sampl item wrap parametertyp schema sampl input standardpythonparametertyp input numpi sampl input input panda sampl input input standard sampl input sampl global paramet standardpythonparametertyp option sampl output standardpythonparametertyp input schema input sampl input input schema global paramet sampl global paramet option output schema sampl output run input global paramet data input input data convert target format assert isinst data ndarrai model predict data return tolist except str return predict import request import json ast import liter eval url web servic score uri servic authent set kei token kei set data score data data convert json input data json dump data set type header type json authent enabl set author header header author bearer kei request displai respons resp request score uri input data header header print resp text liter eval resp text",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"infer schema list index rang model deploi studio updat infer schema allow compat power send data model rest api return schema instruct link personalis data data sent rest api return list index rang deploy goe ahead design healthi messag greatli edit entri predict",
        "Challenge_readability":13.9,
        "Challenge_reading_time":48.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":3418.0769397223,
        "Challenge_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":785.0,
        "Challenge_word_count":386,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"score run function api shape paramet type standardpythonparametertyp input nest option paramet type standardpythonparametertyp globalparamet return dictionari type standardpythonparametertyp nest modifi score run function conform api shape modifi score",
        "Solution_last_edit_time":1614801695372,
        "Solution_link_count":1,
        "Solution_original_content":"document order gener conform swagger autom web servic consumpt score run function api shape paramet type standardpythonparametertyp input nest option paramet type standardpythonparametertyp globalparamet return dictionari type standardpythonparametertyp nest test sensit import numpi import panda import joblib core model import model infer schema schema decor import input schema output schema infer schema paramet type standard paramet type import standardpythonparametertyp infer schema paramet type numpi paramet type import numpyparametertyp infer schema paramet type panda paramet type import pandasparametertyp init global model model model regist workspac model path model model path model databricksmodelpowerbi model joblib load model path sampl input schema gener row data numpi sampl input numpyparametertyp arrai dtype panda sampl input pandasparametertyp datafram valu delai percent valu delai num invoic deal dai delai streak prompt streak standard sampl input standardpythonparametertyp nest input sampl item wrap parametertyp schema sampl input standardpythonparametertyp input numpi sampl input input panda sampl input input standard sampl input sampl global paramet standardpythonparametertyp option numpi sampl output numpyparametertyp arrai sensit sampl output standardpythonparametertyp numpi sampl output input sensit input schema input sampl input input schema global paramet sampl global paramet option output schema sampl output run input global paramet data input input data convert target format assert isinst data ndarrai model predict data return tolist except str return",
        "Solution_preprocessed_content":"document order gener conform swagger autom web servic consumpt score run function api shape paramet type standardpythonparametertyp input nest option paramet type standardpythonparametertyp globalparamet return dictionari type standardpythonparametertyp test sensit",
        "Solution_readability":19.0,
        "Solution_reading_time":40.74,
        "Solution_score":1,
        "Solution_sentence_count":25,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":253,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3528.4438297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Challenge_closed_time":1627578025127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615324569760,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"referenc file path local file path workflow pipelin pipelin object modifi step scriptprocessor pipelin object artifact creat codebuild codebuild copi local file posit run pipelin permiss referenc artifact bucket sync local directori bucket buildspec yml",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Challenge_link_count":0,
        "Challenge_original_content":"workflow pipelin store artifact creat codebuild creat workflow pipelin pipelin object coupl process step file path local file path upload file everytim pipelin run modifi step scriptprocessor pipelin object artifact creat codebuild codebuild copi local file posit permiss run pipelin step data ingest processingstep dataingest processor sklearn data ingest processor input processinginput input input train data sourc input data destin opt process input data train processinginput input input test data sourc test data destin opt process input data test processinginput input file sourc path dir txt destin opt process input output processingoutput output train sourc opt process output train destin dir dir dataset train processingoutput output sourc opt process output destin dir dir dataset processingoutput output test sourc opt process output test destin dir dir dataset test processingoutput output sampl sourc opt process output sampl destin dir dir dataset sampl path dir data ingest dir data ingest job argument countri train val split percentag process step processor processingstep data ingest locat artifact bucket buildartifact fdskz zip process step processor processingstep artifact bucket buildartifact fdsix data ingest buildspec yml codebuild sync dir dir",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"workflow pipelin store artifact creat codebuild creat object coupl process step file path local file path upload file everytim pipelin run modifi object artifact creat codebuild codebuild copi local file posit run pipelin",
        "Challenge_readability":21.6,
        "Challenge_reading_time":38.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3403.7376019445,
        "Challenge_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"uri locat processingstep workflow pipelin pipelin object link",
        "Solution_last_edit_time":1628026967547,
        "Solution_link_count":1,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":18.6,
        "Solution_reading_time":4.2,
        "Solution_score":0,
        "Solution_sentence_count":2,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":24,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3320.1937519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the different data sources we can import data into Azure Machine Learning Services storage or notebook. I mean from Salesforce or any ERP or any website? As of now I have seen importing data using URL or getting it from data location in storage where notebook will also be stored.<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Challenge_closed_time":1566330717407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554359325097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import data sourc salesforc erp system servic storag notebook relev onlin unsuccess search",
        "Challenge_last_edit_time":1554378019900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55509207",
        "Challenge_link_count":0,
        "Challenge_original_content":"data sourc servic data sourc import data servic storag notebook salesforc erp websit import data url data locat storag notebook store relev link",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"data sourc servic data sourc import data servic storag notebook salesforc erp websit import data url data locat storag notebook store relev link",
        "Challenge_readability":6.7,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3325.3867527778,
        "Challenge_title":"Data sources in Azure Machine Learning Services",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. \nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data<\/a><\/p>\n\n<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.\nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"import data sourc blob file adl gen adl gen sql postgresql creat dataset train creat data store public url relev document",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"import data blob file adl gen adl gen sql postgresql http doc com servic access data creat dataset train dataset creat data store public url http doc com servic creat regist dataset",
        "Solution_preprocessed_content":"import data blob file adl gen adl gen sql postgresql creat dataset train dataset creat data store public url",
        "Solution_readability":15.7,
        "Solution_reading_time":10.46,
        "Solution_score":1,
        "Solution_sentence_count":8,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":61,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"logger metric modifi logger log metric metric call logger submit tensor convers switch val item updat logger accept metric dict str union torch tensor convert dict str",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_original_content":"logger modifi log metric logger log metric metric call logger metric modifi train step batch batch idx loss loss batch logger log metric loss return loss loss tensor move cpu gradient detach runtimeerror element tensor grad grad backprop logger metric log metric call accept metric dict str tensorboard logger torch tensor type csvlogger tensor valu valu isinst valu torch tensor return valu item return valu metric valu metric dict item tensorboardlogg similarli metric item isinst torch tensor item add scalar step logger tensor convers kei val metric item tensor val metric kei val cpu detach entir metric dictionari copi later function sens modif copi happi submit logger modifi origin metric dictionari coupl tensor convers logger val cpu detach switch val item prefer end updat logger accept metric dict str union torch tensor probabl import logger base convert dict str logger tensor type annot isn precis convers parti val cpu detach val item sort tensor element log tensor throw anybodi purpos logger log metric test torch tensor warn safe convert arrai dtype object scalar valu represent log metric web interfac access queri directli api",
        "Challenge_participation_count":10,
        "Challenge_preprocessed_content":"logger modifi log metric call modifi tensor move cpu gradient detach backprop logger call accept type tensor similarli tensor convers entir dictionari copi later function sens modif copi happi submit modifi origin dictionari coupl tensor convers detach prefer updat logger accept convert logger tensor type annot isn precis convers sort tensor element log tensor throw anybodi purpos metric web interfac access queri directli api",
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Github",
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"submit logger metric modifi logger log metric metric call val cpu detach val item updat logger accept metric dict str union torch tensor convert dict str contributor taken submit approv test valu aren modifi log metric trainer",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"welcom great observ btw believ directli logger log metric val cpu detach val item accept scalar tensor tensor convers val cpu detach logger neighthan send automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team awaelchli open sourc contribut hand dear sohamtiwari free open tchaton review determin exact sincer soham sohamtiwari approv test prevent regress tchaton love time write test explain test prevent regress beginn write test sincer soham dear sohamtiwari document http github com pytorchlightn pytorch lightn blob master github contribut test valu aren modifi log metric trainer",
        "Solution_preprocessed_content":"welcom great observ btw believ directli accept scalar tensor convers logger send automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team open sourc contribut hand dear free open review determin exact sincer soham sohamtiwari approv test prevent regress love time write test explain test prevent regress beginn write test sincer soham dear document test valu aren modifi log metric trainer",
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score":1,
        "Solution_sentence_count":25,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":296,
        "Tool":"Comet"
    },
    {
        "Challenge_adjusted_solved_time":3240.8772311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Challenge_closed_time":1510651584203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1498984762217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run softwar mongodb background tri write set mongodb creat pickl file guidanc manual import pickl file mongodb",
        "Challenge_last_edit_time":1498985131100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Challenge_link_count":2,
        "Challenge_original_content":"import pickl file connect mongodb softwar run mongodb background configur mongo observ tri write set mongodb creat file tmp mongo eerwu pickl messag warn save mongodb store entri tmp mongo eerwu pickl traceback call intern file usr local lib dist packag observ mongo line start event run entri sourc save sourc file usr local lib dist packag observ mongo line save sourc file filenam ab path file usr local lib dist packag gridf init line filter arg kwarg file usr local lib dist packag gridf grid file line file gridoutcursor file usr local lib dist packag pymongo cursor line len data refresh file usr local lib dist packag pymongo cursor line refresh collat file usr local lib dist packag pymongo cursor line send messag kwarg file usr local lib dist packag pymongo mongo client line send messag respons server topolog select server selector file usr local lib dist packag pymongo topolog line select server address file usr local lib dist packag pymongo topolog line select server messag selector serverselectiontimeouterror localhost errno connect refus pickl file import mongodb manual",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"import pickl file connect mongodb softwar run mongodb background configur tri write set mongodb creat file messag pickl file import mongodb manual",
        "Challenge_readability":17.5,
        "Challenge_reading_time":29.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":3240.783885,
        "Challenge_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Solution_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Solution_comment_count":2,
        "Solution_gpt_summary":"manual import pickl file mongodb load pickl file set insert mongodb",
        "Solution_last_edit_time":1510652289132,
        "Solution_link_count":1,
        "Solution_original_content":"load pickl file set insert pymongo mongocli entri pickl load open tmp mongo eerwu pickl entri list run run insert entri quick dirti depend list object order cleanest item iter list",
        "Solution_preprocessed_content":"load pickl file set insert quick dirti depend list object order cleanest item iter",
        "Solution_readability":10.0,
        "Solution_reading_time":8.51,
        "Solution_score":1,
        "Solution_sentence_count":7,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":57,
        "Tool":"Sacred"
    },
    {
        "Challenge_adjusted_solved_time":3215.9254886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Challenge_closed_time":1626975605176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615398273417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"resourc limit set studio run process share environ step video tutori proce regist domain rout instanc",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Challenge_link_count":3,
        "Challenge_original_content":"setup resourc limit set studio team run process share environ step video http youtub com watch widhcwvrjcu channel amazonwebservic select standard setup select ident access iam permiss creat select execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase greet resourc limit regist domain rout sai domain displai instanc idea domain util dashboard imag notebook setup sage maker account",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"setup resourc limit set studio team run process share environ step select standard setup select ident access permiss creat select execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase greet resourc limit regist domain rout sai domain displai instanc idea domain util dashboard imag notebook setup sage maker account",
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3215.9254886111,
        "Challenge_title":"How to setup AWS sagemaker - Resource limit Error",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"delet domain cli recreat consol note consol visual studio domain",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"maximum studio domain region default limit domain provis delet domain cli recreat consol unfortun consol visual studio domain",
        "Solution_preprocessed_content":"maximum studio domain region default limit domain provis delet domain cli recreat consol unfortun consol visual studio domain",
        "Solution_readability":8.3,
        "Solution_reading_time":4.1,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":51,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2983.5262194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Challenge_closed_time":1545063624710,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534309681237,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"creat imag classifi messag miss pictur lst file directori list",
        "Challenge_last_edit_time":1534322930320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Challenge_link_count":0,
        "Challenge_original_content":"track creat imag classifi clienterror data download nosuchkei specifi kei pictur lst file miss directori lst list troubl",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"track creat imag classifi pictur file miss directori list troubl",
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2987.2065202778,
        "Challenge_title":"Error Tracking in Amazon SageMaker",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"imag file verifi order",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"examin log file lie lst file imag file referenc leav sai lst file corrupt imag file verifi hopefulli",
        "Solution_preprocessed_content":"examin lie lst file imag file referenc imag file verifi hopefulli",
        "Solution_readability":7.2,
        "Solution_reading_time":4.57,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":64,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2971.3031783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Challenge_closed_time":1638951877412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628255185970,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"role attach notebook instanc insid docker forc network host mode role default network option docker bridg role run insid docker network host role network bridg",
        "Challenge_last_edit_time":1639139946700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Challenge_link_count":0,
        "Challenge_original_content":"notebook instanc role docker default network mode leverag notebook environ data scienc team notebook essenti instanc rel nice id cloud addit run docker instanc forc network host mode role assign notebook instanc insid docker host account datascientist role attach notebook instanc st caller ident userid aroaupvgymmvxxj account arn arn st role datascientist run insid docker network host produc docker run network host cli st caller ident userid aroaupvgymmvxxj account arn arn st role datascientist docker network bridg docker run cli st caller ident userid aroaimgppfpttnbyx badedcb account arn arn st role basenotebookinstanceecinstancerol badedcb complet role notic account role arn sth intern default network option docker bridg time role attach notebook instanc datascientist attach host notebook hack iptabl rule achiev",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"notebook instanc role docker default network mode leverag notebook environ data scienc team notebook essenti instanc nice id addit run docker instanc forc mode role assign notebook instanc insid docker host run insid docker produc docker complet role notic account role arn sth intern default network option docker time role attach host hack achiev",
        "Challenge_readability":14.2,
        "Challenge_reading_time":30.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2971.3031783333,
        "Challenge_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":244,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"attach bridg network local role attach notebook instanc insid docker decemb default bridg network attach local anymor",
        "Solution_last_edit_time":1639131288447,
        "Solution_link_count":1,
        "Solution_original_content":"network creat clean notebook instanc notic defin bridg network local docker network network driver scope fdaac bridg bridg local host host local adfbfa null local decaa local bridg local attach bridg role attach notebook instanc docker run network local cli st caller ident userid aroaupvgymmvxxj account arn arn st role datascientist updat write dec attach local bridg network anymor default bridg note network bridg implicit docker run cli st caller ident userid aroaupvgymmvxxj account arn arn st role datascientist restart notebook instanc manual patch iptabl updat patch",
        "Solution_preprocessed_content":"network creat clean notebook instanc notic bridg network attach bridg role updat write attach bridg network anymor default restart notebook instanc manual patch updat patch",
        "Solution_readability":21.9,
        "Solution_reading_time":24.61,
        "Solution_score":0,
        "Solution_sentence_count":5,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":178,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2874.6210316667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a requirement to use azure machine learning to develop a pipeline. In this pipeline we don't pass data as inputs\/outputs but variables (for example a list or an int). I have looked on the Microsoft documentation but could not seem to find something fitting my case. Also tried to use the PipelineData class but could not retrieve my variables.<\/p>\n<ol>\n<li>Is this possible?<\/li>\n<li>Is this a good approach?<\/li>\n<\/ol>\n<p>Thanks for your help.<\/p>",
        "Challenge_closed_time":1658826630380,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648478111533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pass variabl step pipelin search document pipelinedata class retriev variabl advic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71649163",
        "Challenge_link_count":0,
        "Challenge_original_content":"pass variabl step pipelin pipelin pass data input output variabl list document fit tri pipelinedata class retriev variabl",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"pass variabl step pipelin pipelin pass data variabl document fit tri pipelinedata class retriev variabl",
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2874.5885686111,
        "Challenge_title":"Can azureml pass variables from one step to another?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":399.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I know I'm a bit late to the party but here we go:<\/p>\n<p><strong>Passing variables between AzureML Pipeline Steps<\/strong><\/p>\n<p>To directly answer your question, to my knowledge it is not possible to pass variables directly between PythonScriptSteps in an AzureML Pipeline.<\/p>\n<p>The reason for that is that the steps are executed in isolation, i.e. the code is run in different processes or even computes. The only interface a PythonScriptStep has is (a) command line arguments that need to be set prior to submission of the pipeline and (b) data.<\/p>\n<p><strong>Using datasets to pass information between PythonScriptSteps<\/strong><\/p>\n<p>As a workaround you can use PipelineData to pass data between steps.\nThe previously posted blog post may help: <a href=\"https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/\" rel=\"nofollow noreferrer\">https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/<\/a><\/p>\n<p>As for your concrete problem:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pipeline.py\n\n# This will make Azure create a unique directory on the datastore everytime the pipeline is run.\nvariables_data = PipelineData(&quot;variables_data&quot;, datastore=datastore)\n\n# `variables_data` will be mounted on the target compute and a path is given as a command line argument\nwrite_variable = PythonScriptStep(\n    script_name=&quot;write_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    outputs=[variables_data],\n)\n\nread_variable = PythonScriptStep(\n    script_name=&quot;read_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    inputs=[variables_data],\n)\n\n<\/code><\/pre>\n<p>In your script you'll want to serialize the variable \/ object that you're trying to pass between steps:<\/p>\n<p>(You could of course use JSON or any other serialization method)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># write_variable.py\n\nimport argparse\nimport pickle\nfrom pathlib import Path\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\nobj = [1, 2, 3, 4]\n\nPath(args.data_path).mkdir(parents=True, exist_ok=True)\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;wb&quot;) as f:\n    pickle.dump(obj, f)\n<\/code><\/pre>\n<p>Finally, you can read the variable in the next step:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># read_variable.py\n\nimport argparse\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\n\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;rb&quot;) as f:\n    obj = pickle.load(f)\n\nprint(obj)\n<\/code><\/pre>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"pass variabl directli pythonscriptstep pipelin pipelinedata class pass data step workaround involv creat uniqu directori datastor time pipelin run pipelinedata class mount data target comput data serial pass step mount directori pythonscriptstep write read serial data",
        "Solution_last_edit_time":1658826747247,
        "Solution_link_count":2,
        "Solution_original_content":"bit late parti pass variabl pipelin step directli knowledg pass variabl directli pythonscriptstep pipelin reason step execut isol run process comput interfac pythonscriptstep line argument set prior submiss pipelin data dataset pass pythonscriptstep workaround pipelinedata pass data step previous blog http vladiliescu net pass data pipelin step concret pipelin creat uniqu directori datastor everytim pipelin run variabl data pipelinedata variabl data datastor datastor variabl data mount target comput path line argument write variabl pythonscriptstep write variabl argument data path variabl data output variabl data read variabl pythonscriptstep read variabl argument data path variabl data input variabl data serial variabl object pass step cours json serial write variabl import argpars import pickl pathlib import path parser argpars argumentpars parser add argument data path arg parser pars arg obj path arg data path mkdir parent open arg data path obj pkl pickl dump obj final read variabl step read variabl import argpars import pickl parser argpars argumentpars parser add argument data path arg parser pars arg open arg data path obj pkl obj pickl load print obj",
        "Solution_preprocessed_content":"bit late parti pass variabl pipelin step directli knowledg pass variabl directli pythonscriptstep pipelin reason step execut isol run process comput interfac pythonscriptstep line argument set prior submiss pipelin data dataset pass pythonscriptstep workaround pipelinedata pass data step previous blog concret serial variabl object pass step cours json serial final read variabl step",
        "Solution_readability":11.5,
        "Solution_reading_time":34.54,
        "Solution_score":1,
        "Solution_sentence_count":23,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":274,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621374020000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"upgrad pytorch lightn pytorch logger produc multipl gpu run ddp multi gpu slurm cluster log metric singl singl trainer fit reproduc inabl multi gpu ddp colab authent",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_original_content":"upgrad pytorch logger produc multipl run ddp multi gpu slurm cluster pytorch lightn creat multipl gpu log metric sit gpu log metric http bwsz everglad syqjplzxsbwvfgmojvbp run gpu announc log http bwsz everglad dbdffbeabdbc reproduc boringmodel reproduc reproduc multi gpu ddp colab authent past singl singl trainer fit lightn environ note faster colab notebook public id report model http github com pytorchlightn pytorch lightn blob master report model templat colab notebook copi past output environ collect http raw githubusercont com pytorchlightn pytorch lightn master test collect env checklist manual run wget http raw githubusercont com pytorchlightn pytorch lightn master test collect env secur purpos collect env run collect env pytorch version torch pytorch lightn linux linux instal pytorch conda pip sourc pip build compil sourc version cuda cudnn version gpu model configur geforc relev slurm hpc cluster singl node addit context upgrad believ relat http stackoverflow com log pytorch lightn ddp",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"upgrad pytorch logger produc multipl clear concis descript run ddp slurm cluster creat multipl gpu log metric sit img width alt screen shot img width alt screen shot gpu log metric run gpu announc log reproduc boringmodel reproduc reproduc boringmodel reproduc ddp colab authent past singl singl lightn environ note faster templat copi past output run pytorch version linux instal pytorch pip build version version gpu model configur geforc relev slurm hpc cluster singl node addit context upgrad believ relat",
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":1,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":2840.7811111111,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":325,
        "Platform":"Github",
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"workaround kei concret multipl produc pytorch logger upgrad pytorch lightn report slurm ddp multi gpu reproduc mark stale",
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_original_content":"bwsz report sourc workaround kei logger logger api kei environ api kei workspac environ workspac option save dir option default option rest api kei environ rest api kei option kei environ kei option default option setup slurm ddp automat mark stale hasn activ close dai activ contribut pytorch lightn team bit slurm ddp local duplic run slurm slurm tough reproduc maintain forward thu aug andr costa wrote slurm receiv repli email directli github unsubscrib triag notif github mobil io android ben weinstein postdoctor fellow univers florida http benweinstein weebli com automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"report sourc workaround setup automat mark stale hasn activ close dai activ contribut pytorch lightn team bit slurm ddp local duplic run slurm slurm tough reproduc maintain forward thu aug andr costa wrote slurm receiv repli email directli github unsubscrib triag notif github mobil io android ben weinstein postdoctor fellow univers florida automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score":1,
        "Solution_sentence_count":28,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":260,
        "Tool":"Comet"
    },
    {
        "Challenge_adjusted_solved_time":2833.1668325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Challenge_closed_time":1661350818332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651142988007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"push docker imag cloud platform registri defin train job directli insid notebook execut docker build imag uri notebook cell tri magic cell bash import subprocess librari execut bash present jupyterlab workaround execut push insid jupyt notebook train process insid notebook",
        "Challenge_last_edit_time":1651151719750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Challenge_link_count":0,
        "Challenge_original_content":"run docker build notebook cell workbench push docker imag cloud platform registri defin train job directli insid notebook prepar dockerfil uri push imag train push imag directli notebook cell exact execut docker build imag uri imag uri environment variabl previous defin run bin bash docker tri execut magic cell bash import subprocess librari execut store file unfortun return run bash present jupyterlab workaround push execut insid jupyt notebook train process insid notebook",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"run docker build notebook cell workbench push docker imag cloud platform registri defin train job directli insid notebook prepar dockerfil uri push imag push imag directli notebook cell exact execut environment variabl previous defin run tri execut magic cell bash import subprocess librari execut store file unfortun return run bash present jupyterlab workaround push execut insid jupyt notebook train process insid notebook",
        "Challenge_readability":8.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2835.5084236111,
        "Challenge_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"creat notebook workbench select come docker allow docker docker build insid notebook",
        "Solution_last_edit_time":1661351120347,
        "Solution_link_count":3,
        "Solution_original_content":"guid creat notebook workbench select come docker docker docker build insid notebook",
        "Solution_preprocessed_content":"guid creat notebook workbench select come docker docker insid notebook",
        "Solution_readability":12.3,
        "Solution_reading_time":8.08,
        "Solution_score":2,
        "Solution_sentence_count":7,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":57,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":2802.1180147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1625508026043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"hyperparamet tune train job relat keyerror channel train step",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_original_content":"keyerror channel train tune hyperparamet hyperparamet tune unexpectedstatusexcept hyperparametertun job imageclassif job reason train job succeed train job log cloudwatch train job end traceback file usr lib runpi line run modul mod spec file usr lib runpi line run exec run global file opt train line parser add argument data dir type str default environ channel train file usr lib line getitem rais keyerror kei keyerror channel train step http github com petrooha deploi lstm blob ipynb hihgli hint",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"keyerror tune hyperparamet hyperparamet tune log cloudwatch train job end step hihgli hint",
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2802.1180147222,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"relat keyerror channel train environ variabl train file channel train channel train torch framework version version affect link",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"train file environ variabl parser add argument data dir type str default environ channel train parser add argument data dir type str default environ channel train address torch framework version version affect link",
        "Solution_preprocessed_content":"file environ variabl address torch version affect link",
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score":2,
        "Solution_sentence_count":10,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":45,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2777.7798802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Challenge_closed_time":1600448991412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590448983843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi model member environ constraint map valu length equal deploi model successfulli reduc featur pass schema attribut",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Challenge_link_count":0,
        "Challenge_original_content":"sparkml schema eroor member environ constraint deploi model set json schema import json schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output featur type doubl struct vector schema json json dump schema print schema json deploi model import model pipelin import pipelinemodel sparkml model import sparkmlmodel sparkml data format model bucket model kei prefix model tar pass schema defin environ variabl sparkml serv sparkml model sparkmlmodel model data sparkml data env sparkml schema schema json xgb model model model data xgb model model data imag train imag model infer pipelin timestamp prefix model pipelinemodel model role role model sparkml model xgb model endpoint infer pipelin timestamp prefix model deploi initi instanc count instanc type xlarg endpoint endpoint clienterror validationexcept call createmodel oper detect valu sparkml schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output type doubl featur struct vector member environ constraint map valu constraint member length equal member length greater equal member regular express pattern reduc featur deploi pass schema attribut",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"sparkml schema eroor constraint deploi model set json schema deploi clienterror call createmodel oper detect valu output constraint map valu constraint reduc featur deploi pass schema attribut",
        "Challenge_readability":17.0,
        "Challenge_reading_time":51.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2777.7798802778,
        "Challenge_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":418,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"rebuild spark sparkml schema env var",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"environ length limit increas short time rebuild spark sparkml schema env var http github com sparkml serv blob master readm run imag local",
        "Solution_preprocessed_content":"environ length limit increas short time rebuild spark env var",
        "Solution_readability":17.8,
        "Solution_reading_time":6.48,
        "Solution_score":0,
        "Solution_sentence_count":3,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":39,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2502.7853702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing an issue while invoking the Pytorch model Endpoint. Please check the below error for detail.<\/p>\n<p>Error Message:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4.<\/p>\n<\/blockquote>\n<p>Automatically restarted the Endpoint after this error. No specific log in cloud watch.<\/p>",
        "Challenge_closed_time":1626976733376,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617966706043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok pytorch model endpoint internalfailur messag request contact endpoint automat restart log cloud watch",
        "Challenge_last_edit_time":1631708929472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67020040",
        "Challenge_link_count":0,
        "Challenge_original_content":"pytorch model internalfailur call invokeendpoint oper reach retri invok pytorch model endpoint messag internalfailur call invokeendpoint oper reach retri except send request model contact request dfb ccbc automat restart endpoint log cloud watch",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"pytorch model call invokeendpoint oper invok pytorch model endpoint messag call invokeendpoint oper except send request model contact request automat restart endpoint log cloud watch",
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2502.7853702778,
        "Challenge_title":"Sagemaker Pytorch model - An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4):",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":407.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>There may be a few issues here we can explore the paths and ways to resolve.<\/p>\n<ol>\n<li>Inference Code Error\nSometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.<\/li>\n<\/ol>\n<pre><code>from sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)\nprint(rf_pred.predict(payload).decode('utf-8'))\n<\/code><\/pre>\n<p>For more information about the different serializers based off the type of data you are feeding in check the following link.\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html<\/a><\/p>\n<ol start=\"2\">\n<li>Throttling Limits Reached\nSometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/<\/a><\/p>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"infer payload data fed endpoint format serial encod data properli send format throttl limit reach payload larg api request rate exceed endpoint increas retri boto configur comput heavi instanc bias summari",
        "Solution_last_edit_time":1626979329227,
        "Solution_link_count":4,
        "Solution_original_content":"explor path infer payload feed endpoint format invok endpoint data format encod properli serial creat endpoint serial care encod send data format predictor import csv serial pred deploi xlarg serial csv serial print pred predict payload decod utf serial base type data feed link http readthedoc stabl api infer serial html throttl limit reach payload feed larg api request rate exceed endpoint comput heavi instanc increas retri boto configur link retri configur endpoint http com premiumsupport knowledg center throttlingexcept",
        "Solution_preprocessed_content":"explor path infer payload feed endpoint format invok endpoint data properli serial creat endpoint serial care encod send data format serial base type data feed link throttl limit reach payload feed larg api request rate exceed endpoint comput heavi instanc increas retri boto configur link retri configur endpoint",
        "Solution_readability":12.9,
        "Solution_reading_time":22.27,
        "Solution_score":0,
        "Solution_sentence_count":16,
        "Solution_topic":"JSON Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":201,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2447.0429822222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Challenge_closed_time":1551699854476,
        "Challenge_comment_count":2,
        "Challenge_created_time":1542890499740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pass variabl config function clutter deal dozen variabl pass variabl dictionari object attribut",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Challenge_link_count":0,
        "Challenge_original_content":"pass paramet pass variabl config function iri rbf svm config cfg gamma automain run gamma iri dataset load iri permut iri target size iri data iri data iri target iri target clf svm svc rbf gamma gamma clf fit iri data iri target return clf score iri data iri target variabl gamma pass function dozen variabl pass function clutter pass dictionari mayb object attribut automain run config config option config option",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"pass paramet pass variabl config function variabl pass function dozen variabl pass function clutter pass dictionari mayb object attribut",
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2447.0429822222,
        "Challenge_title":"Sacred - pass all parameters as one",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":735.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"valu config pass variabl dictionari defin variabl config access function config variabl",
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_original_content":"valu config valu iri rbf svm config cfg gamma automain run config config gamma config gamma",
        "Solution_preprocessed_content":null,
        "Solution_readability":16.5,
        "Solution_reading_time":5.35,
        "Solution_score":4,
        "Solution_sentence_count":5,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":33,
        "Tool":"Sacred"
    },
    {
        "Challenge_adjusted_solved_time":0.0373211111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Challenge_closed_time":1503919410423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1495124614783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"tracker git awar gitignor exclud file save commit hash run directli github commit review finish job",
        "Challenge_last_edit_time":1503919276067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Challenge_link_count":1,
        "Challenge_original_content":"git tracker git awar gitignor exclud file save commit hash run review finish job directli github commit",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"git exclud file save commit hash run review finish job directli github commit",
        "Challenge_readability":7.9,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2442.9987888889,
        "Challenge_title":"Can I make Neptune talk to git?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"integr git start version integr allow gitignor exclud file save commit hash run directli github commit review finish job document integr http doc advanc git integr",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"start version integr git http doc advanc git integr",
        "Solution_preprocessed_content":"start version integr git",
        "Solution_readability":28.7,
        "Solution_reading_time":3.2,
        "Solution_score":0,
        "Solution_sentence_count":3,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":14,
        "Tool":"Neptune"
    },
    {
        "Challenge_adjusted_solved_time":2355.5938513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Challenge_closed_time":1439539745912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431059608047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"run web servic web servic publish throw http modul debug limit function web servic",
        "Challenge_last_edit_time":1446192965568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Challenge_link_count":0,
        "Challenge_original_content":"run web servic creat modul run publish web servic throw http believ modul modul run web servic debug http statu timestamp fri gmt limit function wont web servic",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"run web servic creat modul run publish web servic throw http believ modul modul run web servic debug http statu timestamp fri gmt limit function wont web servic",
        "Challenge_readability":13.7,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2355.5938513889,
        "Challenge_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"modul variabl data type produc output output loop http",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"modul variabl data type produc output loop",
        "Solution_preprocessed_content":"modul variabl data type produc output loop",
        "Solution_readability":8.9,
        "Solution_reading_time":3.0,
        "Solution_score":1,
        "Solution_sentence_count":2,
        "Solution_topic":"JSON Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":47,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2331.6441666667,
        "Challenge_answer_count":6,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png)\r\n\r\nThis issue wasn't present until a few days ago. Issue shows up when we submit an experiment to azure ml workspace in the image build logs. We are using mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Challenge_closed_time":1623755236000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615361317000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"sdk imag build layer present dai ago submit workspac imag build log mcr com base intelmpi ubuntu base imag",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1387",
        "Challenge_link_count":1,
        "Challenge_original_content":"sdk imag build layer imag http imag githubusercont com daee deebaaf png wasn present dai ago submit workspac imag build log mcr com base intelmpi ubuntu base imag",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"sdk imag build layer wasn present dai ago submit workspac imag build log base imag",
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2331.6441666667,
        "Challenge_title":"azure ml Python SDK 1.24.0 image build fails with the error failed to get layer was working fine before",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"I would think that is some transient issue. \r\nSide note, mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 was deprecated in 2019, please use \r\nmcr.microsoft.com\/azureml\/intelmpi2018.3-ubuntu16.04 or better default cpu image from your client version that is pinned to a versioned tag Still facing the same issue. Kind of blocked is their some way i could fix this. Updated the image thanks for that is it local build? if yes, try to remove the image Nope it is remote compute build. It is in AML Compute cluster The issue got resolved we were earlier creating an image and storing it in Azure Container Registry. Now we don't pass it to RunConfiguration() object. We create it directly in the AML Build process and that has fixed the issue though now the image is not cached anymore so that is problematic. @MAQ-Ravijit-Ramana it would be great to get some details of your scenario, like the script you running",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"creat imag directli build process creat store registri pass runconfigur object imag cach anymor problemat",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"transient note mcr com base intelmpi ubuntu deprec mcr com intelmpi ubuntu default cpu imag client version pin version tag block updat imag local build remov imag nope remot comput build aml comput cluster earlier creat imag store registri pass runconfigur object creat directli aml build process imag cach anymor problemat maq ravijit ramana great run",
        "Solution_preprocessed_content":"transient note deprec default cpu imag client version pin version tag block updat imag local build remov imag nope remot comput build aml comput cluster earlier creat imag store registri pass runconfigur object creat directli aml build process imag cach anymor problemat great run",
        "Solution_readability":7.4,
        "Solution_reading_time":11.27,
        "Solution_score":0,
        "Solution_sentence_count":11,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":148,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633792312000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"train resnet model multi core tpu crash print messag dump comput start loop logger",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_original_content":"dump comput start loop logger multi core tpu train train resnet model multi core tpu kaggl dump comput tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util compar pred compar constant constant direct type unsign tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util convert convert constant tensorflow compil xla xla client xla util divid divid constant convert tensorflow compil xla xla client xla util constant constant nan tensorflow compil xla xla client xla util select select pred compar divid constant tensorflow compil xla xla client xla util multipli multipli reduc select tensorflow compil xla xla client xla util convert convert multipli tensorflow compil xla xla client xla util reshap reshap convert tensorflow compil xla xla client xla util reshap reshap tensorflow compil xla xla client xla util concaten concaten reshap dimens tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util reduc reduc concaten constant dimens appli addcomput tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util compar pred compar constant constant direct type unsign tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util convert convert constant tensorflow compil xla xla client xla util divid divid constant convert tensorflow compil xla xla client xla util constant constant nan tensorflow compil xla xla client xla util select select pred compar divid constant tensorflow compil xla xla client xla util multipli multipli reduc select tensorflow compil xla xla client xla util convert convert multipli tensorflow compil xla xla client xla util reshap reshap convert tensorflow compil xla xla client xla util reshap reshap tensorflow compil xla xla client xla util concaten concaten reshap dimens tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util reduc reduc concaten constant dimens appli addcomput tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util compar pred compar constant constant direct type unsign tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util convert convert constant tensorflow compil xla xla client xla util divid divid constant convert tensorflow compil xla xla client xla util constant constant nan tensorflow compil xla xla client xla util select select pred compar divid constant tensorflow compil xla xla client xla util multipli multipli reduc select tensorflow compil xla xla client xla util convert convert multipli tensorflow compil xla xla client xla util reshap reshap convert tensorflow compil xla xla client xla util reshap reshap tensorflow compil xla xla client xla util concaten concaten reshap dimens tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util reduc reduc concaten constant dimens appli addcomput tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util compar pred compar constant constant direct type unsign tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util convert convert constant tensorflow compil xla xla client xla util divid divid constant convert tensorflow compil xla xla client xla util constant constant nan tensorflow compil xla xla client xla util select select pred compar divid constant tensorflow compil xla xla client xla util multipli multipli reduc select tensorflow compil xla xla client xla util convert convert multipli tensorflow compil xla xla client xla util reshap reshap convert tensorflow compil xla xla client xla util reshap reshap tensorflow compil xla xla client xla util concaten concaten reshap dimens tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util reduc reduc concaten constant dimens appli addcomput tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util compar pred compar constant constant direct type unsign tensorflow compil xla xla client xla util constant constant tensorflow compil xla xla client xla util convert convert constant tensorflow compil xla xla client xla util divid divid constant convert tensorflow compil xla xla client xla util constant constant nan tensorflow compil xla xla client xla util select select pred compar divid constant tensorflow compil xla xla client xla util multipli multipli reduc select tensorflow compil xla xla client xla util convert convert multipli text goe page epoch run loop start train crash text print output note logger print normal print evid notebook http kaggl com rustyelectron documentclassif pytorch tpu log tri small batch size probabl isn memori reproduc notebook http kaggl com rustyelectron documentclassif pytorch tpu resnetd http kaggl com rustyelectron documentclassif pytorch tpu train run normal log environ cuda gpu version packag numpi pytorch debug pytorch version cpu pytorch lightn tqdm pytorch xla linux architectur bit processor addit context kaushikb rohitgr awaelchli morganmcg ayushexel borisdayma scottir",
        "Challenge_participation_count":11,
        "Challenge_preprocessed_content":"dump comput start loop logger tpu train train resnet model multi core tpu kaggl text goe page epoch run loop start train crash text print output note logger normal evid tri small batch size probabl isn memori reproduc train run normal log environ cuda gpu version packag numpi tqdm linux architectur bit processor addit context",
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":2330.3280555556,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":786,
        "Platform":"Github",
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"pytorch lightn team team larg hopefulli ongo pr updat multiprocess progress edg bias respons",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"rusti electron open line dump comput output log logger progressbar creat tqdm dear rusti electron team larg hopefulli updat tchaton tchaton track updat borisdayma idea ongo week automat mark stale hasn activ close dai activ contribut pytorch lightn team borisdayma end late progress thx advanc process updat multiprocess progress edg automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"open line dump comput output log logger progressbar creat dear team larg hopefulli updat track updat idea ongo week automat mark stale hasn activ close dai activ contribut pytorch lightn team end late progress thx advanc process updat multiprocess progress edg automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score":1,
        "Solution_sentence_count":23,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":238,
        "Tool":"Comet"
    },
    {
        "Challenge_adjusted_solved_time":8936.4835138889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Challenge_closed_time":1596561017227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588239469173,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train pytorch model studio local mode instanc classic notebook local mode studio messag state file directori docker docker instal docker pip termin",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Challenge_link_count":0,
        "Challenge_original_content":"file directori docker docker run studio local mode train pytorch model studio train estim pytorch entri train role role session sess train instanc count train instanc type xlarg framework version sourc dir git config git config estim fit stockdata data path local mode classic notebook studio estim pytorch entri train role role train instanc count train instanc type local framework version sourc dir git config git config estim fit stockdata data path train instanc type local studio file directori docker docker tri instal docker pip instal docker termin",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"file directori docker docker run studio local mode train pytorch model studio train local mode classic notebook studio file directori docker docker tri instal docker pip instal docker termin",
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2311.5411261111,
        "Challenge_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1884.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"instal docker studio default docker instal studio respons github ticket",
        "Solution_last_edit_time":1620410809823,
        "Solution_link_count":1,
        "Solution_original_content":"docker servic default docker instal studio github ticket respons",
        "Solution_preprocessed_content":"docker servic default docker instal studio",
        "Solution_readability":14.2,
        "Solution_reading_time":3.98,
        "Solution_score":7,
        "Solution_sentence_count":3,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":29,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":8,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612300739000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"run pipelin local target remot comput target document request explan pipelin run local target",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_original_content":"local execut pipelin valueerror specifi remot comput target run pipelin target local valueerror specifi remot comput target end page target section specifi pipelin run local target peopl wast time realiz shortcom sdk updat document page soon imag http imag githubusercont com fccddf png document edit section doc com github link fcec decf version independ acb fbbdbce core runconfig runconfigur class http doc com api core core runconfig runconfigur sourc docset stabl doc ref autogen core core runconfig runconfigur yml http github com microsoftdoc machinelearn blob live docset stabl doc ref autogen core core runconfig runconfigur yml servic sub servic core github login debfro alia debfro",
        "Challenge_participation_count":8,
        "Challenge_preprocessed_content":"local execut pipelin valueerror specifi remot run pipelin target local valueerror specifi remot end page target section specifi pipelin run local target peopl wast time realiz shortcom sdk updat document page soon document edit section github version independ sourc servic core github login alia debfro",
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Github",
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"team fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev miss featur local pipelin eta releas date run pipelin local target",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note http doc com api core core workspac class core workspac comput target comput target section http doc com api core core runconfig runconfigur target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession lostmygithubaccount meghalv block alloc remot comput target local featur deliv lostmygithubaccount statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin data flow core variabl access pipelin run",
        "Solution_preprocessed_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note section target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession block alloc remot comput target local featur deliv statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin core variabl access pipelin run",
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score":4,
        "Solution_sentence_count":22,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":333,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2198.1168433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1623834809928,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"implement distribut train tensorflow import smdistribut receiv modulenotfounderror messag",
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_original_content":"distribut train tensorflow implement distribut train explain blogpost http doc com latest model parallel train html model parallel train import smdistribut import tensorflow import smdistribut modelparallel tensorflow smp traceback file temp line import smdistribut modelparallel tensorflow smp modulenotfounderror modul smdistribut miss",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"distribut train tensorflow implement distribut train explain blogpost import smdistribut miss",
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":2203.8220902778,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"enabl smdistribut estim distribut smdistribut dataparallel enabl enabl smdistribut tensorflow version",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"smdistribut tensorflow version add distribut smdistribut dataparallel enabl estim order enabl",
        "Solution_preprocessed_content":"smdistribut tensorflow version add estim order enabl",
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score":0,
        "Solution_sentence_count":2,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":33,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596320174000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"version cli compat packag creat workspac linux ubuntu test platform version cli core",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_original_content":"ver cli compat packag descript sdk ver cli core version cli compat packag throw creat workspac creat workspac invalidrequestcont messag request deseri member templat object type deploymentdefinit path templat line posit open cli http github com cli extens platform linux ubuntu haven test platform replic instal reco pyspark run operation notebook version cli cli core comment cli core version okai address",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"ver cli compat packag descript sdk ver version compat packag throw creat workspac open cli platform linux ubuntu haven test platform replic instal run operation notebook version comment version okai address",
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"addit version cli core version mgmt cosmosdb",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"mgmt cosmosdb version attributeerror modul mgmt cosmosdb attribut cosmosdb",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score":0,
        "Solution_sentence_count":3,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":16,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":9,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647978773000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"open databas file save file instal librari pip creat file start kernel aros delet unwant notebook studio lab file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_original_content":"open databas file save file pytorch studio lab dash untitl ipynb open databas file imag http imag githubusercont com ada efdf png reproduc delet unwant notebook studio lab file instal librari pip creat file start kernel",
        "Challenge_participation_count":9,
        "Challenge_preprocessed_content":"open databas file save file open databas file reproduc delet unwant notebook studio lab file instal librari pip creat file start kernel",
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"recreat account option delet open account report open databas file save file instal librari pip creat file start kernel",
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_original_content":"dev sorri wanna start gpu runtim kindli lorazabora launch gpu instanc runtim environ imag http imag githubusercont com faf png experienc week cpu runtim arent workload sens there free cloud cpu option hope soon someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv messag someshfengd delet account imag http imag githubusercont com abba ceb png dear someshfengd delet account close icoxfog delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_preprocessed_content":"dev sorri wanna start gpu runtim kindli launch gpu instanc runtim environ experienc week cpu runtim arent workload sens there free cloud cpu option hope soon report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv delet account dear delet account close delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score":2,
        "Solution_sentence_count":25,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":349,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2121.3562466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Challenge_closed_time":1582590253856,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574954456480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat model store locat bucket creat multi model endpoint model built linear learner model type regressor deploi",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat multi model endpoint boto creat model complex renam locat bucket creat multi model endpoint model end model built linear learner model type regressor stuck deploi",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"creat multi model endpoint boto creat model complex renam locat bucket creat multi model endpoint model end model built model type regressor stuck deploi",
        "Challenge_readability":6.7,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2121.0548266667,
        "Challenge_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"request forum implement multi model endpoint linear learner algorithm build meet multi model endpoint implement version linear learner algorithm sampl notebook bias summari",
        "Solution_last_edit_time":1582591338968,
        "Solution_link_count":4,
        "Solution_original_content":"linear learner algorithm implement multi model endpoint request forum build version linear learner algorithm deploi model multi model endpoint build meet multi model endpoint implement version linear learner algorithm sampl notebook creat multi model compat serv mxnet model adapt implement linear learner algorithm http github com awslab blob master advanc function multi model multi model endpoint ipynb",
        "Solution_preprocessed_content":"linear learner algorithm implement endpoint request forum build version linear learner algorithm deploi model endpoint build meet endpoint implement version linear learner algorithm sampl notebook creat compat serv mxnet model adapt implement linear learner algorithm",
        "Solution_readability":21.5,
        "Solution_reading_time":16.66,
        "Solution_score":0,
        "Solution_sentence_count":8,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":107,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2094.66748,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>My search didn't yield anything useful so I was wondering if there is any easy way to copy notebooks from one instance to another instance on Sagemaker? Of course other than manually downloading the notebooks on one instance and uploading to the other one!<\/p>",
        "Challenge_closed_time":1544938600760,
        "Challenge_comment_count":1,
        "Challenge_created_time":1537397018090,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"copi notebook instanc manual download upload",
        "Challenge_last_edit_time":1537397797832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52415136",
        "Challenge_link_count":0,
        "Challenge_original_content":"copi notebook instanc search yield copi notebook instanc instanc cours manual download notebook instanc upload",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"copi notebook instanc search yield copi notebook instanc instanc cours manual download notebook instanc upload",
        "Challenge_readability":9.9,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2094.884075,
        "Challenge_title":"How to copy notebooks between different Sagemaker instances?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5303.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The recommended way to do this (as of 12\/16\/2018) would be to use the newly- launched Git integration for SageMaker Notebook Instances.<\/p>\n\n<ol>\n<li>Create a Git repository for your notebooks<\/li>\n<li>Commit and push changes from Notebook Instance #1 to your Git repo<\/li>\n<li>Start Notebook Instance #2 using the same Git repo<\/li>\n<\/ol>\n\n<p>This way your notebooks are persisted in the Git repo rather than on the  instance, and the Git repo can be shared by multiple instances. <\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"newli launch git integr notebook instanc creat git repositori notebook commit push notebook instanc git repo start notebook instanc git repo notebook persist git repo instanc git repo share multipl instanc",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"newli launch git integr notebook instanc creat git repositori notebook commit push notebook instanc git repo start notebook instanc git repo notebook persist git repo instanc git repo share multipl instanc http com blog notebook git integr increas persist collabor reproduc",
        "Solution_preprocessed_content":"newli launch git integr notebook instanc creat git repositori notebook commit push notebook instanc git repo start notebook instanc git repo notebook persist git repo instanc git repo share multipl instanc",
        "Solution_readability":19.5,
        "Solution_reading_time":11.4,
        "Solution_score":3,
        "Solution_sentence_count":4,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":82,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2073.4166358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Challenge_closed_time":1621508759232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614044459343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"specifi sourc directori entri train job boto sdk start train job lambda gatewai lambda sdk forc client boto lambda handler argument config specifi sourc directori entri train job data gener fly sdk boto sdk field allow set sourc directori entri boto sdk",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Challenge_link_count":1,
        "Challenge_original_content":"specifi sourc directori entri train job boto sdk start train lambda run train job sdk notebook instanc local iam credenti start train job lambda gatewai lambda sdk high level sdk forc client boto lambda handler boto client supposedli boto servic level sdk control argument config specifi sourc directori entri run train job data gener kera gener flight sdk estim tensorflow base job tensorflow train role execut role sourc dir train src path locat train path output path train output path depend txt entri mode instanc count instanc type gdn xlarg framework version version hyperparamet model model epoch batch size rate train split patienc input path inject call fit input channel train train input path estim fit input input channel sourc dir uri src zip model perform train entri train begin tensorflow simpli run locat prefix train sourc upload run train local local model output path uri train job upload model artifact went document client creat train job field allow set sourc directori entri boto client creat train job trainingjobnam train job lambda hyperparamet dictionari algorithmspecif trainingimag dkr ecr amazonaw com tensorflow train gpu ubuntu traininginputmod file enablemetricstimeseri rolearn execut role goe inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri train input path sdatadistributiontyp fullyrepl compressiontyp recordwrappertyp inputmod file outputdataconfig soutputpath train output path resourceconfig instancetyp gdn xlarg instancecount volumesizeingb stoppingcondit maxruntimeinsecond minut test config sdk accept train input output locat config field allow specifi sourc directori entri",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"specifi sourc directori entri train job boto sdk start train lambda run train job sdk notebook instanc local iam credenti start train job lambda gatewai lambda sdk forc client lambda handler supposedli boto sdk control argument config specifi sourc directori entri run train job data gener flight sdk input path inject call uri model perform train train begin tensorflow simpli run prefix train sourc upload run train local local model uri train job upload model artifact went document field allow set sourc directori entri config sdk accept train input output locat config field allow specifi sourc directori entri",
        "Challenge_readability":15.8,
        "Challenge_reading_time":49.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":2073.4166358333,
        "Challenge_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":374,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"pass sourc directori hyperparamet boto sdk specifi train program field uri sourc directori submit directori field sourc directori tar format throw link",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"pass sourc dir hyperparamet respons boto creat train job trainingjobnam job hyperparamet model model epoch batch size rate train split patienc program specifi train submit directori bucket sourc uri tensorflow sourc sourcedir tar algorithmspecif trainingimag train imag note tar throw http github com blob master sdk scikit randomforest sklearn endend ipynb",
        "Solution_preprocessed_content":"pass hyperparamet note throw",
        "Solution_readability":20.2,
        "Solution_reading_time":15.17,
        "Solution_score":1,
        "Solution_sentence_count":10,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":71,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5616.0211319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pre-trained model which I am loading in AWS SageMaker Notebook Instance from S3 Bucket and upon providing a test image for prediction from S3 bucket it gives me the accurate results as required. I want to deploy it so that I can have an endpoint which I can further integrate with AWS Lambda Function and AWS API GateWay so that I can use the model with real time application.\nAny idea how can I deploy the model from AWS Sagemaker Notebook Instance and get its endpoint?\nCode inside the <code>.ipynb<\/code> file is given below for reference.<\/p>\n<pre><code>import boto3\nimport pandas as pd\nimport sagemaker\n#from sagemaker import get_execution_role\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport json\n#role = get_execution_role()\nrole = sagemaker.get_execution_role()\n\nbucketname = 'bucket' # bucket where the model is hosted\nfilename = 'test_model.h5' # name of the model\ns3 = boto3.resource('s3')\nimage= s3.Bucket(bucketname).download_file(filename, 'test_model_new.h5')\nmodel= 'test_model_new.h5'\n\nmodel = load_model(model)\n\nbucketname = 'bucket' # name of the bucket where the test image is hosted\nfilename = 'folder\/image.png' # prefix\ns3 = boto3.resource('s3')\nfile= s3.Bucket(bucketname).download_file(filename, 'image.png')\nfile_name='image.png'\n\ntest=np.array([resize(imread(file_name), (137, 310, 3))])\n\ntest_predict = model.predict(test)\n\nprint ((test_predict &gt; 0.5).astype(np.int))\n<\/code><\/pre>",
        "Challenge_closed_time":1616234632040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608787488573,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pre train model load notebook instanc bucket accur test imag predict bucket deploi model endpoint integr lambda function api gatewai time",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65434323",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi pre train model notebook instanc pre train model load notebook instanc bucket test imag predict bucket accur deploi endpoint integr lambda function api gatewai model time idea deploi model notebook instanc endpoint insid ipynb file import boto import panda import import execut role skimag import imread skimag transform import resiz import numpi kera model import load model import import time import json role execut role role execut role bucketnam bucket bucket model host filenam test model model boto resourc imag bucket bucketnam download file filenam test model model test model model load model model bucketnam bucket bucket test imag host filenam folder imag png prefix boto resourc file bucket bucketnam download file filenam imag png file imag png test arrai resiz imread file test predict model predict test print test predict astyp",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"deploi model notebook instanc model load notebook instanc bucket test imag predict bucket accur deploi endpoint integr lambda function api gatewai model time idea deploi model notebook instanc endpoint insid file",
        "Challenge_readability":9.8,
        "Challenge_reading_time":20.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2068.6509630556,
        "Challenge_title":"How to deploy a Pre-Trained model using AWS SageMaker Notebook Instance?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1387.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Here is the solution that worked for me. Simply follow the following steps.<\/p>\n<p>1 - Load your model in the SageMaker's jupyter environment with the help of<\/p>\n<pre><code>from keras.models import load_model\n\nmodel = load_model (&lt;Your Model name goes here&gt;) #In my case it's model.h5\n<\/code><\/pre>\n<p>2 - Now that the model is loaded convert it into the <code>protobuf format<\/code> that is required by <code>AWS<\/code> with the help of<\/p>\n<pre><code>def convert_h5_to_aws(loaded_model):\n\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\n\nmodel_version = '1'\nexport_dir = 'export\/Servo\/' + model_version\n# Build the Protocol Buffer SavedModel at 'export_dir'\nbuilder = builder.SavedModelBuilder(export_dir)\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\nfrom keras import backend as K\n\nwith K.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\nimport tarfile\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add('export', recursive=True)\nimport sagemaker\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\nconvert_h5_to_aws(model):\n<\/code><\/pre>\n<p>3 - And now you can deploy your model with the help of<\/p>\n<pre><code>!touch train.py\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.15.2',\n                                  entry_point = 'train.py')\n%%timelog\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This will generate the endpoint which can be seen in the Inference section of the Amazon SageMaker and with the help of that endpoint you can now make predictions from the jupyter notebook as well as from web and mobile applications.\nThis <a href=\"https:\/\/www.youtube.com\/watch?v=RPnvfxR5DY8\" rel=\"nofollow noreferrer\">Youtube tutorial<\/a> by Liam and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">AWS blog<\/a> by Priya helped me alot.<\/p>",
        "Solution_comment_count":2,
        "Solution_gpt_summary":"load pre train model jupyt environ kera librari convert load model protobuf format tensorflow librari deploi model tensorflowmodel class tensorflow model librari gener endpoint time",
        "Solution_last_edit_time":1629005164648,
        "Solution_link_count":2,
        "Solution_original_content":"simpli step load model jupyt environ kera model import load model model load model model model load convert protobuf format convert load model tensorflow save model import builder tensorflow save model signatur util import predict signatur tensorflow save model import tag constant model version export dir export servo model version build protocol buffer savedmodel export dir builder builder savedmodelbuild export dir creat predict signatur tensorflow serv predict api signatur predict signatur input input load model input output score load model output kera import backend session sess save meta graph variabl builder add meta graph variabl sess sess tag tag constant serv signatur map serv default signatur builder save import tarfil tarfil open model tar mode archiv archiv add export recurs import session session input session upload data path model tar kei prefix model convert model deploi model touch train tensorflow model import tensorflowmodel model tensorflowmodel model data session default bucket model model tar role role framework version entri train timelog predictor model deploi initi instanc count instanc type xlarg gener endpoint infer section endpoint predict jupyt notebook web mobil youtub tutori liam blog priya alot",
        "Solution_preprocessed_content":"simpli step load model jupyt environ model load convert deploi model gener endpoint infer section endpoint predict jupyt notebook web mobil youtub tutori liam blog priya alot",
        "Solution_readability":13.8,
        "Solution_reading_time":34.23,
        "Solution_score":3,
        "Solution_sentence_count":28,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":244,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592335014000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"build type miss type common manual deepcopi packag type type gener deepcopi miss import type client build",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_original_content":"build type miss type common manual deepcopi build type miss type common manual deepcopi base afccd creat mod modul tmp sig control cmd control gen sig control devel tutori bin control gen object headerfil hack boilerpl txt path fmt control guestbook control vet github com oper api common tutori pkg mod github com oper api common manual deepcopi tag deepcopi undefin type tag field deepcopi vet packag type type gener deepcopi miss reproduc minim precis import type client build import trainingjobv github com oper api trainingjob environ kubernet version kubectl version oper version control imag tag cat releas kernel unam instal",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"build type miss type build type miss type base afccd creat modul tmp fmt vet undefin packag type type miss reproduc import type client build import environ kubernet version oper version kernel instal",
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2039.8183333333,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"pin master branch build type api backwardli compat master",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"believ gener deepcopi branch backport previous modul pick reason pin master branch api backwardli compat master",
        "Solution_preprocessed_content":"believ gener deepcopi branch backport previous modul pick reason pin branch api backwardli compat",
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score":0,
        "Solution_sentence_count":4,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":67,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi pre train tensorflow model endpoint deploi successfulli call predictor predict predict messag receiv modelerror modelerror call invokeendpoint oper receiv server model messag gone cloud watch log relat nodedef attr explicit pad convd wast dai tensorflow version notebook instanc",
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_original_content":"deploi pre train tensorflow model modelerror modelerror call invokeendpoint oper time web servic deploi pre train model deploi pre train tensorflow model deploi endpoint successfulli predictor predict data predict invok endpoint throw modelerror modelerror call invokeendpoint oper receiv server model messag http consol com cloudwatch home region logeventview group endpoint tensorflow account cloud watch log nodedef attr explicit pad output attr type allow half bfloat doubl attr stride list attr cudnn gpu bool default attr pad allow attr data format default nhwc allow nhwc nchw attr dilat list default nodedef node conv conv convolut convd output shape data format nhwc dilat explicit pad pad stride cudnn gpu devic job localhost replica task devic cpu conv pad pad conv conv kernel read graphdef interpret binari date graphdef gener binari wast dai log share tensorflow version notebook instanc",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"deploi tensorflow model modelerror call invokeendpoint oper time web servic deploi model deploi tensorflow model deploi endpoint successfulli predict invok endpoint throw cloud watch log wast dai log share tensorflow version notebook instanc",
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2016.3332602778,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"tensorflow version tensorflowmodel appar tensorflowmodel allow version tensorflow serv sdk class represent document model object defin final model tensorflow version",
        "Solution_last_edit_time":null,
        "Solution_link_count":14,
        "Solution_original_content":"search tensorflow version deploi endpoint tensorflowmodel exactli model tensorflowmodel model data model data role role framework version entri train appar tensorflowmodel allow version tensorflow serv sdk class represent document shown tensorflowmodel http github com sdk blob master src tensorflow model doc http github com sdk tree src tensorflow deploi directli model artifact kei proxi grpc client send request impl http github com tensorflow blob master src serv model http github com sdk blob master src tensorflow serv doc http github com sdk blob master src tensorflow deploi tensorflow serv rst kei util tensorflow serv rest api impl http github com tensorflow serv blob master serv isn tensorflowmodel object tensorflow serv api librari conjunct grpc client infer tensorflow serv api isn offici version tensorflowmodel object model object defin final model tensorflow version model model model data model data role role framework version entri train",
        "Solution_preprocessed_content":"search tensorflow version deploi endpoint exactli appar allow version tensorflow serv sdk class represent document shown tensorflowmodel doc kei proxi grpc client send request impl model doc kei util tensorflow serv rest api impl isn object tensorflow serv api librari conjunct grpc client infer tensorflow serv api isn offici version object object defin final tensorflow version",
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score":4,
        "Solution_sentence_count":29,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":271,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1978.3835261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console<\/a> docs to create device fleet. In this console, Role ARN is optional but it throws <code>RoleARN is required<\/code>. If I provide proper RoleArn it throws <code>Failed to create\/modify RoleAlias. Check your IAM role permission<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have no idea what is going wrong. Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1614723172687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607600991993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat devic fleet consol throw messag state role arn option rolearn throw messag state creation modif rolealia",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65233943",
        "Challenge_link_count":6,
        "Challenge_original_content":"creat devic fleet http doc com latest edg devic fleet creat html edg devic fleet creat consol doc creat devic fleet consol role arn option throw rolearn rolearn throw creat modifi rolealia iam role permiss idea hint",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"creat devic fleet doc creat devic fleet consol role arn option throw rolearn throw idea hint",
        "Challenge_readability":13.9,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1978.3835261111,
        "Challenge_title":"Unable to create Device Fleet",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"attach amazonedgedevicefleetpolici rolearn trust iot core messag creat devic fleet",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"moham edg rolealia action creat devicefleet amazonedgedevicefleetpolici attach permiss grant trust iot core",
        "Solution_preprocessed_content":"moham edg rolealia action creat devicefleet amazonedgedevicefleetpolici attach trust iot core",
        "Solution_readability":13.4,
        "Solution_reading_time":3.95,
        "Solution_score":1,
        "Solution_sentence_count":2,
        "Solution_topic":"Role Permission",
        "Solution_topic_macro":"Security Management",
        "Solution_word_count":46,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1972.6603797222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Challenge_closed_time":1637782762627,
        "Challenge_comment_count":5,
        "Challenge_created_time":1630681185260,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"pass depend file sklearnprocessor object modulenotfounderror import function preprocess instal librari txt file set paramet depend sourc dir scriptprocessor sklearnprocessor processor set processor workflow step processingstep workflow pipelin pipelin",
        "Challenge_last_edit_time":1637936310430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Challenge_link_count":2,
        "Challenge_original_content":"pass depend file sklearnprocessor pipelin import function insid preprocess file pass depend file sklearnprocessor object modulenotfounderror sklearn process import sklearnprocessor process import processinginput processingoutput sklearn processor sklearnprocessor framework version role role instanc type xlarg instanc count sklearn processor run preprocess input processinginput sourc input data destin opt process input output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument train test split ratio pass depend file file file txt preprocess access depend modul instal librari txt file share updat frameworkprocessor run arg function depend sourc dir paramet frameworkprocessor set paramet scriptprocessor sklearnprocessor processor set processor workflow step processingstep workflow pipelin pipelin pipelin mandatori creat pipelin directli",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"pass depend file sklearnprocessor pipelin import function insid file pass depend file object pass access depend modul instal librari file share function paramet frameworkprocessor set paramet set mandatori creat directli",
        "Challenge_readability":16.5,
        "Challenge_reading_time":34.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":11,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":1972.6603797222,
        "Challenge_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2139.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Solution_comment_count":5,
        "Solution_gpt_summary":"add addit file folder send entir folder input opt process input txt add preprocess file subprocess",
        "Solution_last_edit_time":1637783228436,
        "Solution_link_count":0,
        "Solution_original_content":"coupl option accomplish addit file folder packag file file txt preprocess send entir folder input opt process input sklearn process import sklearnprocessor process import processinginput processingoutput sklearn processor sklearnprocessor framework version role role instanc type xlarg instanc count sklearn processor run preprocess upload opt process input preprocess input processinginput sourc input data destin opt process input send packag opt process input packag processinginput sourc packag destin opt process input packag output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument train test split ratio sdk argument preprocess opt process input packag directori edit txt add preprocess import sy import subprocess subprocess sy execut pip instal opt process input packag txt",
        "Solution_preprocessed_content":"coupl option accomplish addit file folder send entir folder input argument directori edit add",
        "Solution_readability":23.3,
        "Solution_reading_time":27.94,
        "Solution_score":17,
        "Solution_sentence_count":15,
        "Solution_topic":"Remote Persistence",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":134,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576464430000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"trainer fit pickl logger distribut backend ddp gpu slurm multiprocess pickl nest function function rest store trainer fit run sampl environ",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_original_content":"pickl trainer fit logger distribut data parallel slurm trainer fit pickl logger logger distribut backend ddp gpu slurm reproduc step reproduc instanti logger pytorch pytorch execut environ environ variabl track uri track usernam track password connect track server http authent track server instanti trainer logger instanc logger distribut backend ddp gpu paramet nvidia gpu slurm run trainer fit output multiprocess pickl nest function function rest store http github com blob track track servic util ayla khan gpu photosynthet test traceback file test line trainer fit model file mnt unihom home corp ayla khan miniconda env photosynthet lib site packag pytorch lightn trainer trainer line fit spawn ddp train nproc num gpu arg model file mnt unihom home corp ayla khan miniconda env photosynthet lib site packag torch multiprocess spawn line spawn process start file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess process line start popen popen file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess context line popen return popen process obj file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess popen spawn posix line init init process obj file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess popen fork line init launch process obj file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess popen spawn posix line launch reduct dump process obj file mnt unihom home corp ayla khan miniconda env photosynthet lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object rest store default host cred sampl sampl test test model gist http gist github com khan dbabafdcec test hparam namespac model xorgatemodel test hparam logger logger test lightn logger track uri environ track uri trainer trainer logger logger distribut backend ddp gpu trainer fit model trainer fit run environ photosynthet ayla khan gpu photosynthet collect env collect environ pytorch version debug build cuda build pytorch cento linux core gcc version gcc red hat cmake version collect version cuda cuda runtim version gpu model configur gpu geforc gtx gpu geforc gtx gpu geforc gtx gpu geforc gtx gpu geforc gtx gpu geforc gtx gpu geforc gtx gpu geforc gtx nvidia driver version cudnn version usr local cuda lib libcudnn version relev librari pip numpi pip pytorch lightn pip pytorch toolbelt pip torch pip torchsummari pip torchvis conda bla mkl conda mkl conda mkl servic pyhebf conda mkl fft pyhadb conda mkl random pyhdbf conda pytorch cuda cudnn pytorch conda pytorch lightn pypi pypi conda pytorch toolbelt pypi pypi conda torchsummari pypi pypi conda torchvis pytorch",
        "Challenge_participation_count":9,
        "Challenge_preprocessed_content":"pickl logger distribut data parallel slurm pickl logger logger gpu slurm reproduc step reproduc instanti logger pytorch pytorch execut environ environ variabl connect track server http authent track server instanti trainer logger instanc logger gpu paramet nvidia gpu slurm run output multiprocess pickl nest function function sampl sampl test test model run environ",
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":409,
        "Platform":"Github",
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"logger distribut backend ddp gpu slurm multiprocess pickl nest function function rest store implement git temporari review merg version instal master pickl merg master branch coupl dai ago",
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_original_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu lt core gpu pytorch pytorch lightn tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard ttdummyfilewrit logdir pickl thread lock copi process report build dbczumar smurch neggert open pytorch lightn ident pickl ddp logger reproduc gave led lightn version gist reproduc pytorch lightn http gist github com polyphenolx efcffaeb easili reproduc output home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn log packag renam logger deprec packag remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn logger modul renam deprec modul remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn data loader decor deprec remov warn warn arg kwarg gpu environ variabl node rank defin set cuda visibl devic traceback file test line trainer fit model file home anaconda env pytorch lib site packag pytorch lightn trainer trainer line fit spawn ddp train nproc num process arg model file home anaconda env pytorch lib site packag torch multiprocess spawn line spawn return start process arg nproc daemon start spawn file home anaconda env pytorch lib site packag torch multiprocess spawn line start process process start file home anaconda env pytorch lib multiprocess process line start popen popen file home anaconda env pytorch lib multiprocess context line popen return popen process obj file home anaconda env pytorch lib multiprocess popen spawn posix line init init process obj file home anaconda env pytorch lib multiprocess popen fork line init launch process obj file home anaconda env pytorch lib multiprocess popen spawn posix line launch reduct dump process obj file home anaconda env pytorch lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object rest store default host cred environ cuda gpu geforc gtx geforc gtx geforc gtx geforc gtx version packag numpi pytorch debug pytorch version pytorch lightn tensorboard tqdm linux architectur bit processor version ubuntu smp mon utc add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari review merg pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run http github com minimaxir aitextgen imag http imag githubusercont com cac fdd png imag http imag githubusercont com facb bfde png strategi version version",
        "Solution_preprocessed_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu core gpu pytorch tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard pickl copi process report build ident pickl ddp logger reproduc gave led gist reproduc easili reproduc output environ add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run strategi version version",
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score":1,
        "Solution_sentence_count":72,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":675,
        "Tool":"MLflow"
    },
    {
        "Challenge_adjusted_solved_time":1955.6086208333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Challenge_closed_time":1539631654852,
        "Challenge_comment_count":1,
        "Challenge_created_time":1532591463817,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"algorithm run docker imag instanc end train process tri save model output directori mount model big receiv space left devic advic prevent size limit model store mount directori",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Challenge_link_count":2,
        "Challenge_original_content":"space left devic model train algorithm run ship docker imag instanc bit http github com awslab blob master advanc function scikit scikit ipynb end train process write model output directori mount tutori model path opt model model save path model path model unluckili appar model big time runtimeerror close file file write time thu jul filenam model file descriptor errno messag space left devic buf xadd write hour gpu time wast prevent size limit model store mount directori",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"space left devic model train algorithm run ship docker imag instanc end train process write model output directori mount unluckili appar model big time runtimeerror close file file write time thu jul filenam file descriptor errno messag space left devic buf hour gpu time wast prevent size limit model store mount directori",
        "Challenge_readability":14.3,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1955.6086208333,
        "Challenge_title":"No space left on device in Sagemaker model training",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2721.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Solution_comment_count":2,
        "Solution_gpt_summary":"increas default storag limit train volum size paramet constructor larger valu tune valu closer subsequ job alloc extra storag space cheap insur polici run storag cost storag month provis storag",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"train model estim default storag train volum size param constructor increas valu larg ish big model subsequ job tune valu closer storag cost month provis storag partial usag prorat extra room cheap insur polici run storag",
        "Solution_preprocessed_content":"train model default storag param constructor increas valu big model subsequ job tune valu closer storag cost provis storag partial usag prorat extra room cheap insur polici run storag",
        "Solution_readability":8.7,
        "Solution_reading_time":9.56,
        "Solution_score":1,
        "Solution_sentence_count":9,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":97,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655132901000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"trainer instanti instal instal creat trainer report trainingargu step reproduc trainer successfulli creat callback enabl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_original_content":"instal trainer instal shell transform version platform linux amd glibc version huggingfac hub version pytorch version gpu tensorflow version gpu flax version cpu gpu tpu cpu jax version jaxlib version gpu distribut parallel set offici modifi task offici task folder glue squad task dataset reproduct instal creat trainingargu report instanti trainer reproduc report train argument notebook http github com nielsrogg transform tutori blob master bert tune bert friend multi label text classif ipynb creat trainer runtimeerror traceback tmp ipykernel trainer trainer model arg train dataset encod dataset train eval dataset encod dataset opt conda lib site packag transform trainer init model arg data collat train dataset eval dataset token model init comput metric callback optim preprocess logit metric default callback default callback report integr callback arg report callback default callback callback default callback callback callback handler callbackhandl callback model token optim schedul opt conda lib site packag transform trainer callback init callback model token optim schedul callback callback add callback model model token token opt conda lib site packag transform trainer callback add callback callback add callback callback callback isinst callback type callback class callback isinst callback type callback class class class callback opt conda lib site packag transform integr init init rais runtimeerror callback instal run pip instal initi log asset runtimeerror callback instal run pip instal shell trainer successfulli creat callback enabl",
        "Challenge_participation_count":8,
        "Challenge_preprocessed_content":"instal trainer offici modifi task offici task folder task dataset reproduct instal creat trainingargu train argument notebook creat trainer",
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score":0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":1943.8975,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":298,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"messag instal report run pip instal instal persist instal set api kei mode environ variabl persist open team",
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_original_content":"sgugger messag instal report runtimeerror callback instal run pip instal exactli run pip instal instal pip edit edit titl clear mon jul sylvain gugger wrote messag instal report runtimeerror callback instal run pip instal exactli run pip instal repli email directli github unsubscrib receiv author thread messag properli initi api kei automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor sgugger http huggingfac doc transform class callback doc api kei callback tri set api kei mode insid function run spawn luck downgrad till environ api kei environ mode onlin environ mayb open write integr maintain written team automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"messag instal exactli run instal edit edit titl clear mon jul sylvain gugger wrote messag instal runtimeerror callback instal run exactli run pip instal repli email directli github unsubscrib receiv author properli initi api kei automat mark stale activ address comment thread note ignor doc api kei callback tri set insid function run spawn luck downgrad till mayb open write integr maintain written team automat mark stale activ address comment thread note ignor",
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score":0,
        "Solution_sentence_count":29,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":312,
        "Tool":"Comet"
    },
    {
        "Challenge_adjusted_solved_time":1833.7168480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Challenge_closed_time":1531369355823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524767975170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train model built resnet docker imag deploi model endpoint classifi million imag store recordio format imag classif algorithm imag infer perform infer train data recordio format copi raw jpg imag jupyt notebook instanc perform infer time took time run infer imag",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Challenge_link_count":2,
        "Challenge_original_content":"imag classif perform infer imag train model built resnet docker imag deploi model endpoint classifi million imag train test imag store recordio format convert imrec accord doc imag classif algorithm recordio recordio imag imag type train algorithm onlyappl imagefor infer perform infer train data recordio format overcom copi raw jpg imag jupyt notebook instanc perform infer time img list listdir temp data list imag img list open temp data payload read payload bytearrai payload respons runtim invok endpoint endpointnam endpoint contenttyp imag bodi payload needless transfer data notebook instanc took time prefer run infer imag classif recordio infer importantli run infer imag",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"imag classif perform infer imag train model resnet docker imag deploi model endpoint classifi million imag train test imag store recordio format accord doc imag classif algorithm recordio imag type train algorithm infer perform infer train data recordio format overcom copi raw jpg imag jupyt notebook instanc perform infer time needless transfer data notebook instanc took time prefer run infer imag classif recordio infer importantli run infer imag",
        "Challenge_readability":12.1,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1833.7168480556,
        "Challenge_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2600.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1,
        "Solution_gpt_summary":"sdk predictor predict api sdk boto api invok endpoint perform http base predict credenti build servic perform pre process process lambda retriev train model write line hoc mxnet load run predict batch predict bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_original_content":"recordio format design pack larg imag singl file predict singl imag come predict definit copi imag notebook instanc load inlin predict request http base predict option sdk predictor predict api credenti http github com sdk sdk aka boto api invok endpoint credenti build servic perform pre process process lambda http medium com julsimon chalic serv predict acb batch predict simplest retriev train model write line hoc mxnet load run predict http mxnet incub apach org tutori predict imag html hope",
        "Solution_preprocessed_content":"recordio format design pack larg imag singl file predict singl imag come predict definit copi imag notebook instanc load inlin predict request predict option sdk api sdk api build servic perform lambda batch predict simplest retriev train model write line mxnet load run predict hope",
        "Solution_readability":12.1,
        "Solution_reading_time":20.35,
        "Solution_score":2,
        "Solution_sentence_count":13,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":177,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1803.6491758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1659992618003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653499480970,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"execut task remot repo access worker option execut singl file clone instal repo docker execut worker pack local repo send remot execut pass directori file execut reproduc state repo access worker",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Challenge_link_count":0,
        "Challenge_original_content":"remot execut task local repo execut task remot accord doc option execut singl file identifi repo repo clone instal docker execut worker repo remot url access worker isn pack local repo send remot execut somewhat extend singl file pass execut directori file reproduc repo access worker advanc",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"remot execut task repo execut task remot accord doc option execut singl file identifi repo repo clone instal docker execut worker repo remot url access worker isn pack local repo send remot execut somewhat extend singl file pass execut directori file reproduc repo access worker advanc",
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1803.6491758334,
        "Challenge_title":"Remotely execute ClearML task using local-only repo",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"execut singl file clone instal repo docker execut worker option git store entir base free host github bitbucket gitlab pack local repo send remot execut theori doabl submit featur request store entir folder artifact auto zip agent unzip artifact",
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_original_content":"disclaim team member repo remot url access worker isn pack local repo send remot execut singl store entir worker reproduc remot base compos singl file git free host github bitbucket gitlab theori doabl urg featur store entir folder artifact auto zip agent unzip artifact run clone task clone artifact",
        "Solution_preprocessed_content":"disclaim team member repo remot url access worker isn pack local repo send remot execut singl store entir worker reproduc remot base compos singl file git free host github bitbucket gitlab theori doabl urg featur store entir folder artifact agent unzip artifact run clone task clone",
        "Solution_readability":6.8,
        "Solution_reading_time":10.74,
        "Solution_score":0,
        "Solution_sentence_count":9,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":162,
        "Tool":"ClearML"
    },
    {
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal hash zip packag step blog receiv messag state zip file assist",
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_original_content":"instal addit packag step instal hash zip packag upload zip file dataset creat add execut drag drop zip file dataset connect dataset step execut step run instal packag zip file src hash zip clear step articl http blog technet com saketbi archiv amp languag extens aspx greatli",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"instal addit packag step instal packag upload zip file dataset creat add execut drag drop zip file dataset connect dataset step execut step run instal packag clear step articl greatli",
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1787.9299130556,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2765.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"creat zip zip instal hash zip packag outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0,
        "Solution_original_content":"instal packag creat zip zip outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_preprocessed_content":"instal packag creat zip zip outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score":5,
        "Solution_sentence_count":3,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":47,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1733.7563858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Challenge_closed_time":1598820674152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592493163510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"detect memori leak neural network mxnet written tri mxnet profil tracemalloc modul potenti memori leak guid detect memori leak mxnet tri execut notebook import identifi miss",
        "Challenge_last_edit_time":1592579151163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Challenge_link_count":2,
        "Challenge_original_content":"memori leak mxnet afraid neural network mxnet written memori leak tri mxnet profil tracemalloc modul memori profil potenti memori leak valgrind detect memori leak buffer overflow mxnet build section asan build mxnet replac ubuntu cpu docker dockerfil build ubuntu cpu mxnetci build ubuntu cpu ubuntu cpu tri execut notebook root eef mxnet nosetest verbos home run predict import importerror modul run predict run predict usr bin env run predict call mxnet infer run predict tri put miss link rnn test run analog import",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"memori leak mxnet afraid neural network mxnet written memori leak tri mxnet profil tracemalloc modul memori profil potenti memori leak valgrind detect memori leak buffer overflow mxnet build section asan build mxnet replac tri execut notebook import importerror miss link run analog import",
        "Challenge_readability":10.9,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1757.641845,
        "Challenge_title":"How to find memory leak in Python MXNet?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Solution_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Solution_comment_count":0,
        "Solution_gpt_summary":"automat test memori leak mxnet examin garbag collect record implement link",
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_original_content":"mxnet automat test examin garbag collect record implement http github com apach incub mxnet blob caffdedcfbdc test conftest",
        "Solution_preprocessed_content":"mxnet automat test examin garbag collect record implement",
        "Solution_readability":21.7,
        "Solution_reading_time":5.82,
        "Solution_score":1,
        "Solution_sentence_count":3,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":24,
        "Tool":"Amazon SageMaker"
    }
]