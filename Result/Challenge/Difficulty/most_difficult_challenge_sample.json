[
    {
        "Answerer_created_time":1518533097007,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":896.0,
        "Answerer_view_count":177.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was using the code <code>pd.read_json('s3:\/\/example2020\/kaggle.json')<\/code> to access S3 bucket data, but it threw the error of <code>FileNotFoundError: example2020\/kaggle.json<\/code>. <\/p>\n\n<p>The methods I tried:<\/p>\n\n<p><strong>[Region]<\/strong>\nThe s3 bucket is in Ohio region while the SageMaker notebook instance is in Singapore. Not sure if this matters. I tried to recreate a s3 bucket in Singapore region but I still cannot access it and got the same file not found error. <\/p>\n\n<p><strong>[IAM Role]<\/strong>\nI checked the permission of IAM-SageMaker Execution role\n<a href=\"https:\/\/i.stack.imgur.com\/st4AR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/st4AR.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1581359496750,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581359496750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60156370",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0,
        "Challenge_title":"how SageMaker to access s3 bucket data",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518533097007,
        "Poster_location":"Australia",
        "Poster_reputation_count":896.0,
        "Poster_view_count":177.0,
        "Solution_body":"<p>The problem is still IAM permission. <\/p>\n\n<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket<\/code>. Then the problem solved. \n<a href=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><br>\n<br>\n<strong>[Solution]<\/strong>\nIn Resource tab, check whether bucket name is general.\n <a href=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.2,
        "Solution_reading_time":9.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Challenge_closed_time":1643643837027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643643837027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0,
        "Challenge_title":"AzureML Model Register",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.6,
        "Solution_reading_time":6.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Model Registry",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1351080779276,
        "Answerer_location":"Leuven, Belgium",
        "Answerer_reputation_count":3126.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My training data looks like <\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>I have trained a model in AWS Sagemaker and I deployed the model behind an endpoint.\nThe endpoint accepts the payload as \"text\/csv\".<\/p>\n\n<p>to invoke the endpoint using boto3 you can do:<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>\n\n<p>How do i construct the payload \"my_payload_as_csv\" from my Dataframe in order to invoke the Sagemaker Endpoint correctly?<\/p>",
        "Challenge_closed_time":1590329636843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590329636843,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61987233",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to construct a \"text\/csv\" payload when invoking a sagemaker endpoint",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2627.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351080779276,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":3126.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>if you start from the dataframe example<\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>you take a row<\/p>\n\n<pre><code>df_1_record = df[:1]\n<\/code><\/pre>\n\n<p>and convert <code>df_1_record<\/code> to a csv like this:<\/p>\n\n<pre><code>import io\nfrom io import StringIO\ncsv_file = io.StringIO()\n# by default sagemaker expects comma seperated\ndf_1_record.to_csv(csv_file, sep=\",\", header=False, index=False)\nmy_payload_as_csv = csv_file.getvalue()\n<\/code><\/pre>\n\n<p><code>my_payload_as_csv<\/code> looks like<\/p>\n\n<pre><code>'2,1\\n'\n<\/code><\/pre>\n\n<p>then you can invoke the sagemaker endpoint<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":10.94,
        "Solution_score_count":7.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are currently working on making an Azure MachineLearning Studio experiment operational.<\/p>\n\n<p>Our most recent iteration has a webjob that accepts a queue message, gets some data to train the model, and consumes the ML Experiment webservice to put a trained model in a blob location.<\/p>\n\n<p>A second webjob accepts a queue message, pulls the data to be used in the predictive experiment, gets the location path of the trained .ilearner model, and then consumes THAT ML Experiment webservice.<\/p>\n\n<p>The data used to make the predictions is passed in as an input parameter, and the storage account name, key, and .ilearner path are all passed in as global parameters--Dictionary objects defined according to what the data scientist provided.<\/p>\n\n<p>Everything <em>appears<\/em> to work correctly--except in some cases, the predictive experiment fails, and the error message makes it clear the wrong .ilearner file is being used.<\/p>\n\n<p>When a non-existent blob path is passed to the experiment webservice, the error message reflects there is no such blob, so it's clear the webservice is at least validating the .ilearner's existence. <\/p>\n\n<p>The data scientist can run it locally, but has to change the name of the .ilearner file when he exports it locally through PowerShell. Ensuring each trained model has a unique file name did not resolve this issue.<\/p>\n\n<p>All files, when I view them in the Azure Storage Explorer, appear to be getting updated as expected based on last-modified dates. It's almost like there's a cached version of the .ilearner somewhere that isn't being overridden properly.<\/p>",
        "Challenge_closed_time":1535139575903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535139575903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52010761",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":20.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.0,
        "Challenge_title":"Azure MachineLearning WebService Not Using Passed .ilearner Model",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":260,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.<\/p>\n\n<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.<\/p>\n\n<p><strong>The webservice was accepting my global parameters<\/strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything<\/strong>--the hardcoded .ilearner was being applied no matter what.<\/p>\n\n<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that<\/em> happened, we would have gotten to the bottom of it much more quickly.<\/p>\n\n<p>tl\/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":17.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On Azure ML Workspace Notebook, I'm trying to get my workspace instance, as seen at<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-auto-train-models#configure-workspace.<\/a><\/p>\n<p>I have a config file and I am running the notebook in an Azure compute instance.<\/p>\n<p>I tried to execute Workspace.from_config().<\/p>\n<p>As a result, I'm getting the 'MSIAuthentication' object has no attribute 'get_token' error.<\/p>\n<p>I tried to submit both <code>MsiAuthentication<\/code> and <code>InteractiveLoginAuthentication<\/code>, as suggested in<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb.<\/a><\/p>",
        "Challenge_closed_time":1625753450150,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625753450150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68303285",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":28.1,
        "Challenge_reading_time":16.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0,
        "Challenge_title":"'MSIAuthentication' object has no attribute 'get_token'",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1670.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>There are 2 solutions I've found:<\/strong><\/p>\n<p>1.- Use the kernel &quot;Python 3.6 - AzureML&quot;<\/p>\n<p>2.- <code>pip install azureml-core --upgrade<\/code><\/p>\n<p>This will <strong>upgrade<\/strong><\/p>\n<blockquote>\n<p>azureml-core to 1.32.0<\/p>\n<\/blockquote>\n<p>But will <strong>downgrade<\/strong>:<\/p>\n<blockquote>\n<p>azure-mgmt-resource to 13.0.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>azure-mgmt-storage down to 11.2.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>urllib3 to 1.26.5 (was 1.26.6)<\/p>\n<\/blockquote>\n<p>This upgrade \/ downgrade allows the same package versions as in the python 3.6 anaconda install<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":8.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565697423932,
        "Answerer_location":"Uzbekistan",
        "Answerer_reputation_count":602.0,
        "Answerer_view_count":117.0,
        "Challenge_adjusted_solved_time":8037.6370116666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Challenge_closed_time":1630917852487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630917852487,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1630917952870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to optimize for multiple metrics in Optuna",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1887.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565697423932,
        "Poster_location":"Uzbekistan",
        "Poster_reputation_count":602.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659853446112,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":18.47,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":113.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1457555855467,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a big pipeline, taking a few hours to run. A small part of it needs to run quite often, how do I run it without triggering the entire pipeline?<\/p>",
        "Challenge_closed_time":1574848206347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574848206347,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59067349",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to run parts of your Kedro pipeline conditionally?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":4724.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457555855467,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":86.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>There are multiple ways to specify which nodes or parts of your pipeline to run. <\/p>\n\n<ol>\n<li><p>Use <code>kedro run<\/code> parameters like <code>--to-nodes<\/code>\/<code>--from-nodes<\/code>\/<code>--node<\/code> to explicitly define what needs to be run.<\/p><\/li>\n<li><p>In <code>kedro&gt;=0.15.2<\/code> you can define multiple pipelines, and then run only one of them with <code>kedro run --pipeline &lt;name&gt;<\/code>. If no <code>--pipeline<\/code> parameter is specified, the default pipeline is run. The default pipeline might combine several other pipelines. More information about using modular pipelines: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines<\/a><\/p><\/li>\n<li><p>Use tags. Tag a small portion of your pipeline with something like \"small\", and then do <code>kedro run --tag small<\/code>. Read more here: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":16.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":107.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Challenge_closed_time":1562042063000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562042043000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":8.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.0055401804,
        "Challenge_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.0226744444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Challenge_closed_time":1617256809808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617256728180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0224212002,
        "Challenge_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":560.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378039539503,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1327234712912,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Challenge_adjusted_solved_time":0.0405816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Challenge_closed_time":1519929928907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1519929782813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1519930038023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0397798517,
        "Challenge_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324654920387,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Solution_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1640731722280,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0418091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Challenge_closed_time":1640732355180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640732204667,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0409587852,
        "Challenge_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":30,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596727881047,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395230906503,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":129.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":0.0431730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a <code>csv<\/code> file that looks like<\/p>\n\n<pre><code>a,b,c,d\n1,2,3,4\n5,6,7,8\n<\/code><\/pre>\n\n<p>and I want to load it in as a Kedro <code>CSVLocalDataSet<\/code>, but I don't want to read the entire file. I only want a few columns (say <code>a<\/code> and <code>b<\/code> for example).<\/p>\n\n<p>Is there any way for me to specify the list of columns to read\/load?<\/p>",
        "Challenge_closed_time":1573216728420,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573216572997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1573220304527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58766724",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0422670833,
        "Challenge_title":"How do I select which columns to load in a Kedro CSVLocalDataSet?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":306.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395230906503,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":129.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>CSVLocalDataSet uses <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html\" rel=\"nofollow noreferrer\">pandas.read_csv<\/a>, which takes \"usecols\" parameter. It can easily be proxied by using <code>load_args<\/code> dataset parameter (all datasets support additional parameters passing via <code>load_args<\/code> and <code>save_args<\/code>):<\/p>\n\n<pre><code>my_cool_data:\n  type: CSVLocalDataSet\n  filepath: data\/path.csv\n  load_args: \n    usecols: ['a', 'b']\n<\/code><\/pre>\n\n<p>Also note the same parameters would work for any pandas-based dataset.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":7.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":51.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1378136257732,
        "Answerer_location":"Budapest, Hungary",
        "Answerer_reputation_count":8162.0,
        "Answerer_view_count":283.0,
        "Challenge_adjusted_solved_time":0.0478905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Challenge_closed_time":1471040769603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471040597197,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0467791488,
        "Challenge_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48200.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1439246522636,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0595647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to test my Azure ML model, I get the following error: \u201cError code: InternalError, Http status code: 500\u201d, so it appears something is failing inside of the machine learning service. How do I get around this error?<\/p>",
        "Challenge_closed_time":1439848076380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1439847861947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1439906222223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32060196",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0578581844,
        "Challenge_title":"Azure ML Internal Error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1408.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439847618300,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I've run into this error before, and unfortunately, the only workaround I found was to create a new ML workspace backed by a storage account that you know is online. Then copy your experiment over to the new workspace, and things should work. It can be a bit cumbersome, but it should get rid of your error message. With the service being relatively new, things sometimes get corrupted as updates are being made, so I recommend checking the box labeled \"disable updates\" within your experiment.  Hope that helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":6.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468845236196,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0645166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>so I've been poking around with sacred a little bit and it seems great. unfortunately I did not find any multiple files use-cases examples like I am trying to implement.<\/p>\n\n<p>so i have this file called configuration.py, it is intended to contain different variables which will (using sacred) be plugged in to the rest of the code (laying in different files):<\/p>\n\n<pre><code>from sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.config\ndef configure_analysis_default():\n    \"\"\"Initializes default  \"\"\"\n    generic_name = \"C:\\\\basic_config.cfg\" # configuration filename\n    message = \"This is my generic name: %s!\" % generic_name\n    print(message)\n\n@ex.automain #automain function needs to be at the end of the file. Otherwise everything below it is not defined yet\n#  when the experiment is run.\ndef my_main(message):\n    print(message)\n<\/code><\/pre>\n\n<p>This by itself works great. sacred is working as expected. However, when I'm trying to introduce a second file named Analysis.py:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.capture\ndef what_is_love(generic_name):\n    message = \" I don't know\"\n    print(message)\n    print(generic_name)\n\n@ex.automain\ndef my_main1():\n    what_is_love()\n<\/code><\/pre>\n\n<p>running Analysis.py yields:<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>TypeError: what_is_love is missing value(s) for ['generic_name']<\/p>\n<\/blockquote>\n\n<p>I expected that the 'import configuration' statement to include the configuration.py file, thus importing everything that was configured in there including configure_analysis_default() alongside its decorator @ex.config and then inject it to what_is_love(generic_name).\nWhat am I doing wrong? how can i fix this?<\/p>\n\n<p>Appreciate it!<\/p>",
        "Challenge_closed_time":1513160793190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513160560930,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1513161714983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47790619",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":22.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.0625208621,
        "Challenge_title":"sacred, python - ex.config in one file and ex",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1446.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>So, pretty dumb, but I'll post it here in favour of whoever will have similar issue...<\/p>\n\n<p>My issue is that I have created a different instance of experiment. I needed simply to import my experiment from the configuration file.<\/p>\n\n<p>replacing this:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n<\/code><\/pre>\n\n<p>with this:<\/p>\n\n<pre><code>import configuration\nex = configuration.ex\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0708333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Challenge_closed_time":1516633097000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516632842000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0684371615,
        "Challenge_title":"LightGBM on SageMaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":388.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1501772252047,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":922.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":0.0029661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Following the pandas documentation for visualization (<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist\" rel=\"nofollow noreferrer\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist<\/a>) I am trying to create the following graphics:<\/p>\n\n<pre><code>import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n## A data set in my AzureML workplace experiment \ndf = ds.to_dataframe()\nplt.figure(); \ndf.plot.hist(stacked=True, bins=20) \nplt.figure();df.boxplot()\n<\/code><\/pre>\n\n<p>However, the output is limited to <code>\"&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e15dc18&gt;\"<\/code> (for the histogram(=) and <code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e0ce828&gt;\"<\/code> (to the box plot), but no image appearing. Can anyone help me to identify what I'm missing out? Thanks!<\/p>\n\n<p>I'm using Python 3 in Jupyter Notebook in AzureML. <\/p>\n\n<p>The <code>df.describe()<\/code> method works properly (there is a dataFrame)<\/p>",
        "Challenge_closed_time":1516035353070,
        "Challenge_comment_count":3,
        "Challenge_created_time":1516035090403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1516035342392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48267427",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0704240321,
        "Challenge_title":"Trouble in creating graphics with matplotlib in a Jupyter notebook",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":378.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316565559336,
        "Poster_location":"Brazil",
        "Poster_reputation_count":2563.0,
        "Poster_view_count":503.0,
        "Solution_body":"<p>Have you set the backend?<\/p>\n\n<pre><code>%matplotlib inline\n<\/code><\/pre>\n\n<p>Worth reading about what this does for a notebook here too\n<a href=\"https:\/\/stackoverflow.com\/questions\/43027980\/purpose-of-matplotlib-inline\/43028034\">Purpose of &quot;%matplotlib inline&quot;<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0783333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Challenge_closed_time":1556295681000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295399000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0754166393,
        "Challenge_title":"Greengrass for data processing and ML model training",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":2068.9864736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Challenge_closed_time":1506066553020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506066265147,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1506066568087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.076928376,
        "Challenge_title":"How to configure comet (comet.ml) to track Keras?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1208.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1513514919392,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":33.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1365750215796,
        "Answerer_location":"Graz, Austria",
        "Answerer_reputation_count":3186.0,
        "Answerer_view_count":204.0,
        "Challenge_adjusted_solved_time":0.64708,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Folks,\nI am trying to access a json value from Azure ML rest service but i get a null value all the time, i tried different options but it did not work. Can u please provide ideas.<\/p>\n\n<p><em>Json String<\/em><\/p>\n\n<pre><code>{\n    \"Results\": {\n        \"output1\": {\n            \"type\": \"table\",\n            \"value\": {\n                \"ColumnNames\": [\"Sentiment\",\n                \"Score\"],\n                \"ColumnTypes\": [\"String\",\n                \"Double\"],\n                \"Values\": [[\"negative\",\n                \"0.20\"],\n                **[\"negative\",\n                \"0.03\"]**]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>Trying to fetch the value between **<\/p>\n\n<p>Tried the below.<\/p>\n\n<pre><code>using Newtonsoft.Json.Linq;\nJObject o = JObject.Parse(sentimentvalue);\nstring valv = (string)o.SelectToken(\"Results[0].output1[0].Values[0]\");\n<\/code><\/pre>",
        "Challenge_closed_time":1473068287320,
        "Challenge_comment_count":4,
        "Challenge_created_time":1473067994950,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1473068420572,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39327655",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":9.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0780843812,
        "Challenge_title":"Reading JSON Object Value in C# code",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":947.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Your JSON path in the SelectToken seems to be wrong. \nTry this:<\/p>\n\n<pre><code>string valv = (string)o.SelectToken(\"$.Results.output1.value.Values[0][1]\");\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1473070750060,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":2.28,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1357215436787,
        "Answerer_location":"Netherlands",
        "Answerer_reputation_count":12551.0,
        "Answerer_view_count":1671.0,
        "Challenge_adjusted_solved_time":0.4334583333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to upload my <code>Excel<\/code> Workbook into Azure Machine Learning Studio. The reason is I have some data that I would like to join into my other <code>.csv<\/code> files to create a training data set. \nWhen I upload my <code>Excel<\/code>, I don't get <code>.xlsx<\/code>, or <code>.xls<\/code>, but other extensions such as <code>.csv<\/code>, <code>.txt<\/code> etc.. <\/p>\n\n<p>This is how it looks,\n<a href=\"https:\/\/i.stack.imgur.com\/SrSon.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SrSon.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I uploaded anyways and now, I am getting weird characters. How can I get excel workbook uploaded and get my sheets, so, I can join data and do, data preparation. Any suggestions?<\/p>",
        "Challenge_closed_time":1530199230416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530198936390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51086377",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":10.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.0785097384,
        "Challenge_title":"Upload Microsoft Excel Workbook with Many Sheets into Azure ML Studio",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":577.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519936486960,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation_count":1113.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You could save the workbook as a (set of) CSV file(s) and upload them separately.<\/p>\n\n<p>A CSV file, a '<a href=\"https:\/\/en.wikipedia.org\/wiki\/Comma-separated_values\" rel=\"nofollow noreferrer\">Comma Separated Values<\/a>' file, is exactly that. A flat file with some values separated by a comma. If you load an Excel file it will mess up since there's way more information in an Excel file than just values separated by comma's. Have a look at <code>File<\/code> -> <code>Save as<\/code> -> <code>Save as type<\/code> where you can select 'CSV (comma delimited) (*.csv)'<\/p>\n\n<p><em>Disclaimer: no, it's not always a comma...<\/em>  <\/p>\n\n<blockquote>\n  <p>In addition, the term \"CSV\" also denotes some closely related delimiter-separated formats that use different field delimiters. These include tab-separated values and space-separated values. A delimiter that is not present in the field data (such as tab) keeps the format parsing simple. These alternate delimiter-separated files are often even given a .csv extension despite the use of a non-comma field separator.<\/p>\n<\/blockquote>\n\n<p><strong>Edit<\/strong><br>\nSo apparently Excel files <em>are<\/em> supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/data-prep-appendix2-supported-data-sources\" rel=\"nofollow noreferrer\">Supported data sources for Azure Machine Learning data preparation<\/a>  <\/p>\n\n<p><em>Excel (.xls\/.xlsx)<\/em><br>\nRead an Excel file one sheet at a time by specifying sheet name or number.<\/p>\n\n<p>But also, only UTF-8 is supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-data#bkmk_Notes\" rel=\"nofollow noreferrer\">Import Data - Technical notes<\/a><\/p>\n\n<blockquote>\n  <p>Azure Machine Learning requires UTF-8 encoding. If the data you are importing uses a different encoding, or was exported from a data source that uses a different default encoding, various problems might appear in the text.<\/p>\n<\/blockquote>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1530200496840,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":25.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":241.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1545311054088,
        "Answerer_location":null,
        "Answerer_reputation_count":170.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":8.4162855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro pipeline, nodes (something like python functions) are declared sequentially. In some cases, the input of one node is the output of the previous node. However, sometimes, when kedro run API is called in the commandline, the nodes are not run sequentially.<\/p>\n\n<p>In kedro documentation, it says that by default the nodes are ran in sequence. <\/p>\n\n<p>My run.py code:<\/p>\n\n<pre><code>def main(\ntags: Iterable[str] = None,\nenv: str = None,\nrunner: Type[AbstractRunner] = None,\nnode_names: Iterable[str] = None,\nfrom_nodes: Iterable[str] = None,\nto_nodes: Iterable[str] = None,\nfrom_inputs: Iterable[str] = None,\n):\n\nproject_context = ProjectContext(Path.cwd(), env=env)\nproject_context.run(\n    tags=tags,\n    runner=runner,\n    node_names=node_names,\n    from_nodes=from_nodes,\n    to_nodes=to_nodes,\n    from_inputs=from_inputs,\n)\n<\/code><\/pre>\n\n<p>Currently my last node is sometimes ran before my first few nodes.<\/p>",
        "Challenge_closed_time":1572835437910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572835098980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58686533",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0899752674,
        "Challenge_title":"How to run the nodes in sequence as declared in kedro pipeline?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1741.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The answer that I recieved from Kedro github:<\/p>\n\n<blockquote>\n  <p>Pipeline determines the node execution order exclusively based on\n  dataset dependencies (node inputs and outputs) at the moment. So the\n  only option to dictate that the node A should run before node B is to\n  put a dummy dataset as an output of node A and an input of node B.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1572865397608,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":61.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1577554446867,
        "Answerer_location":"Ramallah Street, Ramallah",
        "Answerer_reputation_count":3947.0,
        "Answerer_view_count":576.0,
        "Challenge_adjusted_solved_time":0.0951986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an angular application that takes an image as input, the image is then passed to a python script that performs a neural style transfer and returns the stylized image. I have created the python file and the angular frontend seperately and I'm stuck on the integration. I am using aws sagemaker to run the python script (due to its computation speed) but I have no idea how to call the python script with the image passed to it from angular. Any suggestions would be really appreciated. Thank you<\/p>",
        "Challenge_closed_time":1632665752712,
        "Challenge_comment_count":1,
        "Challenge_created_time":1632665409997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69335737",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.0909357268,
        "Challenge_title":"How to call a python file in aws sagemaker from angular application?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601448674660,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>You can create a lambda function and expose it using API gateway to be called by your angular app. this lambda in return will call the  Sagemaker function you have<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.2,
        "Solution_reading_time":6.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.1047952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Challenge_closed_time":1596811524960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596811147697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1596823164336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0996600488,
        "Challenge_title":"sudo: not found on AWS Sagemaker Studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":980.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596815281143,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1102777778,
        "Challenge_answer_count":1,
        "Challenge_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Challenge_closed_time":1602771143000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602770746000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":2.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1046102343,
        "Challenge_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":24,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.4,
        "Solution_reading_time":2.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload.\n\nQuestion:\n\nHow to express in CF a following logic:\n\nIf Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\nIf Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Challenge_closed_time":1607357193000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607356793000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloud-formation",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1053605156,
        "Challenge_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic. One alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied. I know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n\n\nHere the Vpc input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the Condition to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":15.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":204.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_closed_time":1590501508000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590501108000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":1.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1053605156,
        "Challenge_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":15,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":20.5,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1487197909143,
        "Answerer_location":"Brussels, Belgium",
        "Answerer_reputation_count":2206.0,
        "Answerer_view_count":169.0,
        "Challenge_adjusted_solved_time":0.3434,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Challenge_closed_time":1597994468467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597994063423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":180.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":143,
        "Challenge_solved_time":0.1066207212,
        "Challenge_title":"using gunicorn for nested folders",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":1101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597993100672,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1597995299663,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1080384269,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1310438510470,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":33615.0,
        "Answerer_view_count":1772.0,
        "Challenge_adjusted_solved_time":27861.6479,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I have a number of large csv (tab delimited) data stored as azure blobs, and I want to create a pandas dataframe from these. I can do this locally as follows:<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\nimport pandas as pd\nimport os.path\n\nSTORAGEACCOUNTNAME= 'account_name'\nSTORAGEACCOUNTKEY= \"key\"\nLOCALFILENAME= 'path\/to.csv'        \nCONTAINERNAME= 'container_name'\nBLOBNAME= 'bloby_data\/000000_0'\n\nblob_service = BlobService(account_name=STORAGEACCOUNTNAME, account_key=STORAGEACCOUNTKEY)\n\n# Only get a local copy if haven't already got it\nif not os.path.isfile(LOCALFILENAME):\n    blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\n\ndf_customer = pd.read_csv(LOCALFILENAME, sep='\\t')\n<\/code><\/pre>\n\n<p>However, when running the notebook on azure ML notebooks, I can't 'save a local copy' and then read from csv, and so I'd like to do the conversion directly (something like pd.read_azure_blob(blob_csv) or just pd.read_csv(blob_csv) would be ideal).<\/p>\n\n<p>I can get to the desired end result (pandas dataframe for blob csv data), if I first create an azure ML workspace, and then read the datasets into that, and finally using <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"noreferrer\">https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python<\/a> to access the dataset as a pandas dataframe, but I'd prefer to just read straight from the blob storage location.<\/p>",
        "Challenge_closed_time":1444693139743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1444692718393,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1444814426276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33091830",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":11.6,
        "Challenge_reading_time":19.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1106838217,
        "Challenge_title":"How best to convert from azure blob csv format to pandas dataframe while running notebook in azure ml",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":22789.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346359012420,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I think you want to use <code>get_blob_to_bytes<\/code>, <code>or get_blob_to_text<\/code>; these should output a string which you can use to create a dataframe as<\/p>\n\n<pre><code>from io import StringIO\nblobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)\ndf = pd.read_csv(StringIO(blobstring))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1545116358716,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":17.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342628508448,
        "Answerer_location":"United States",
        "Answerer_reputation_count":5147.0,
        "Answerer_view_count":1739.0,
        "Challenge_adjusted_solved_time":0.1219261111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If I have data in a Hadoop Cluster or SQL Elastic DB, is ML bringing that data onto ML servers, or leaving it on Hadoop\/sql and running its analysis there?<\/p>",
        "Challenge_closed_time":1440686498027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1440686059093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1440686537416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32252282",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1150469503,
        "Challenge_title":"Where does AzureML run its analytics?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":35,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384802035143,
        "Poster_location":"Miami Beach, FL",
        "Poster_reputation_count":2682.0,
        "Poster_view_count":1006.0,
        "Solution_body":"<p>Currently, Azure Machine Learning will bring that data onto ML servers.  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.0,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1398051491376,
        "Answerer_location":"Massachusetts",
        "Answerer_reputation_count":459.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":0.1220702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Challenge_closed_time":1657516175783,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657515736330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1657607287156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":3.4,
        "Challenge_reading_time":3.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1151754413,
        "Challenge_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498814861883,
        "Poster_location":null,
        "Poster_reputation_count":37.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":31.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1540309037063,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":72.6951308334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Challenge_closed_time":1561274854928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561274387650,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1561274899232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1220401341,
        "Challenge_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1621.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540309037063,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1561536601703,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":5.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1320822222,
        "Challenge_answer_count":2,
        "Challenge_body":"Experts,\n\nI just try studio classic which is good but retired soon\n\nI am moving to the new studio in azure portal. Any guidance for newbie?",
        "Challenge_closed_time":1653988525023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653988049527,
        "Challenge_favorite_count":11.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871064\/migrate-to-portal-studio.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.7,
        "Challenge_reading_time":1.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1240586116,
        "Challenge_title":"Migrate to portal studio",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello @Alexandre-4252\n\nWelcome to Microsoft Q&A Platform,\n\nI would start checking the docs below:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/overview-what-is-machine-learning-studio\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/migrate-rebuild-experiment\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/migrate-register-dataset\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.9,
        "Solution_reading_time":8.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1576948018896,
        "Answerer_location":"Portugal",
        "Answerer_reputation_count":580.0,
        "Answerer_view_count":135.0,
        "Challenge_adjusted_solved_time":0.1342122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey guys so recently i started working with sagemaker and I was testing autopilot and it got a fairly good accuracy and I wanted to test it on some more data so I chose the one with best ACC and created an endpoint. The problem now is that I don't know how to use the endpoit properly. I tried using AWS CLI but I keep getting the following errors:<\/p>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'SW0gaGFwcHk=' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from container-1 with message &quot;'application\/json' is an unsupported content type.&quot;. See https:\/\/eu-west-2.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/autopilottest in account 288240193481 for more information.\n<\/code><\/pre>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'Im happy!' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Invalid base64: &quot;Im happy!&quot;\n<\/code><\/pre>\n<p>Endpoit configuration:\n<a href=\"https:\/\/i.stack.imgur.com\/11qAt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/11qAt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1633716252927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633715769763,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69499960",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1259383326,
        "Challenge_title":"Sagemaker Endpoint returning strange error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":521.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576948018896,
        "Poster_location":"Portugal",
        "Poster_reputation_count":580.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Ended up fixing the issue by adding <code>--content-type text\/csv<\/code> and using base64 and it worked like a charm.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":0.1342155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Challenge_closed_time":1606928528563,
        "Challenge_comment_count":6,
        "Challenge_created_time":1606928045387,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1608739608112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":11.3,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1259412715,
        "Challenge_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2146.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568658032912,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":133.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.12,
        "Solution_score_count":12.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1396913889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi team,\nI am learning about the quota for machine learning service and I have a general doubt.\n\nI can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.\n\nso that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.\n\nI am able to find out the- get details query but this one doesnt give me details specific to each resource-group or the workspace.\n\nHTTP query -> https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01",
        "Challenge_closed_time":1667069947336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667069444447,
        "Challenge_favorite_count":21.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":10.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1307575143,
        "Challenge_title":"how to set Quota at resource group level?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @JA-9673 ,\n\nquotas can be set on Azure Subscription level only.\nThere is no option to apply quotas for different Azure Resource Groups.\nThere are 2 options I can see for your requirement:\nUse 2 Azure Subscriptions for each Resource Group\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1340895239552,
        "Answerer_location":"Cambridge, MA, United States",
        "Answerer_reputation_count":1401.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":0.1411905556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a dataframe like bellow, where <code>ID<\/code> is numeric value, and <code>comment1<\/code> and <code>comment2<\/code> string that I am importing as a csv. But the data frame is giving result something like this bellow, where <code>fifth comment<\/code> should be in the <code>comment2<\/code> and the original <code>ID<\/code> value is replaced by this. This is happening randomly for only few rows. Moreover, this problem is only occurring when I am importing my <strong>R<\/strong> code in <strong>Azure ML<\/strong> studio, in <strong>RStudio<\/strong> no data misplace is occurring. So what I was thinking, just delete the entire row where the first column <code>ID<\/code> is not a numeric value. As the misplace string value is random long sentence, I can not do string matching to delete the row. And the dataframe is big enough that I just cannot delete the rows manually. Suggestion please. <\/p>\n\n<pre><code>  ID                 Comment1                  comment2\n 123             This is first comment        this is second\n 234              third comment               fourth comment\nfifth comment                                                  \n 345               sixth comment              seventh comment\n<\/code><\/pre>\n\n<p>You will find a sample of the dataframe here,<\/p>\n\n<pre><code>    df &lt;-\n  read.csv(\n    \"https:\/\/docs.google.com\/spreadsheets\/d\/171YXjzm3FsapXSkqgOSos6UGXNRcd1yxmLyvaRnCX5E\/pub?output=csv\"\n  )\ndf &lt;- df[-1,]\ndf &lt;- df[, 1:12]\ncolnames(df) &lt;-\n  c(\n    \"ID\",\"Created\",\"Comments\",\"Liked_By\",\"Disliked_By\", \"Recipient_Number\",\n    \"Sender\",\"Recipients\",\"Read_By\", \"Subject\",\"Introduction\",\"Body\"\n  )\n<\/code><\/pre>",
        "Challenge_closed_time":1457377932163,
        "Challenge_comment_count":5,
        "Challenge_created_time":1457377423877,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1483518911967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35851851",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":12.3,
        "Challenge_reading_time":19.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1320720645,
        "Challenge_title":"How to delete all non-numeric rows in R?",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5339.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455672951607,
        "Poster_location":"Germany",
        "Poster_reputation_count":473.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>Subset to numeric IDs:<\/p>\n\n<pre><code>subset(df, grepl('^\\\\d+$', df$ID))\n<\/code><\/pre>\n\n<p>The pattern should match values of ID that start and end with digits, and only contain digits.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.45,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1422222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Challenge_closed_time":1668738306000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668737794000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":15.87,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1329756827,
        "Challenge_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":160,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":0.1452175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Having issues with kedro. The 'register_pipelines' function doesn't seem to be running or creating the <strong>default<\/strong> Pipeline that I'm returning from it.<\/p>\n<p>The error is<\/p>\n<pre><code>(kedro-environment) C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files&gt;kedro run\n2021-03-22 13:30:28,201 - kedro.framework.session.store - INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\nfatal: not a git repository (or any of the parent directories): .git\n2021-03-22 13:30:28,447 - kedro.framework.session.session - WARNING - Unable to git describe C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\n2021-03-22 13:30:28,476 - root - INFO - ** Kedro project dcs_files\n2021-03-22 13:30:28,486 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step.\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 304, in _get_pipeline\n    return pipelines[name]\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\dynaconf\\utils\\functional.py&quot;, line 17, in inner\n    return func(self._wrapped, *args)\nKeyError: '__default__'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\Scripts\\kedro-script.py&quot;, line 9, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 228, in main\n    cli_collection(**cli_context)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\\src\\dcs_package\\cli.py&quot;, line 240, in run\n    pipeline_name=pipeline,\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\session\\session.py&quot;, line 344, in run\n    pipeline = context._get_pipeline(name=pipeline_name)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 310, in _get_pipeline\n    ) from exc\nkedro.framework.context.context.KedroContextError: Failed to find the pipeline named '__default__'. It needs to be generated and returned by the 'register_pipelines' function.\n<\/code><\/pre>\n<p>My src\\dcs_package\\pipeline_registry.py looks like this:<\/p>\n<pre><code># Copyright 2021 QuantumBlack Visual Analytics Limited\n#\n# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND\n# NONINFRINGEMENT. IN NO EVENT WILL THE LICENSOR OR OTHER CONTRIBUTORS\n# BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF, OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n# The QuantumBlack Visual Analytics Limited (&quot;QuantumBlack&quot;) name and logo\n# (either separately or in combination, &quot;QuantumBlack Trademarks&quot;) are\n# trademarks of QuantumBlack. The License does not grant you any right or\n# license to the QuantumBlack Trademarks. You may not use the QuantumBlack\n# Trademarks or any confusingly similar mark as a trademark for your product,\n# or use the QuantumBlack Trademarks in any other manner that might cause\n# confusion in the marketplace, including but not limited to in advertising,\n# on websites, or on software.\n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n&quot;&quot;&quot;Project pipelines.&quot;&quot;&quot;\nfrom typing import Dict\nfrom kedro.pipeline import Pipeline, node\nfrom .pipelines.data_processing.pipeline import create_pipeline\nimport logging\n\ndef register_pipelines() -&gt; Dict[str, Pipeline]:\n    &quot;&quot;&quot;Register the project's pipelines.\n\n    Returns:\n        A mapping from a pipeline name to a ``Pipeline`` object.\n    &quot;&quot;&quot;\n    log = logging.getLogger(__name__)\n    log.info(&quot;Start register_pipelines&quot;) \n    data_processing_pipeline = create_pipeline()\n    log.info(&quot;create pipeline done&quot;) \n    \n\n    return {\n        &quot;__default__&quot;: data_processing_pipeline,\n        &quot;dp&quot;: data_processing_pipeline\n    }\n\n<\/code><\/pre>\n<p>Then I have a &quot;src\\dcs_package\\pipelines\\data_processing\\pipeline.py&quot; file with a real simple function that outputs &quot;test string&quot; and nothing else.<\/p>\n<p>I was able to read a few items from my catalog (a csv and a xlsx) so I think all the dependencies are working fine.<\/p>",
        "Challenge_closed_time":1616435908600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616435385817,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66751310",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":72.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":0.1355945753,
        "Challenge_title":"Kedro : Failed to find the pipeline named '__default__'",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1486.0,
        "Challenge_word_count":536,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398987181710,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>What version of kedro are you on? There is a bit of a problem with kedro 0.17.2 where the true error is masked and will return the exception that you're seeing instead. It's possible that the root cause of the error is actually some other <code>ModuleNotFoundError<\/code> or <code>AttributeError<\/code>. Try doing a <code>kedro install<\/code> before <code>kedro run<\/code> and see if that fixes it.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":5.05,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1545360696800,
        "Answerer_location":"Earth",
        "Answerer_reputation_count":1011.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":6002.4414652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Challenge_closed_time":1597108074440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597107545867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597107925232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63350039",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.1369979812,
        "Challenge_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":395.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618716714507,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Challenge_closed_time":1597251737000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597251203000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1383116158,
        "Challenge_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":11.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":137.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442334437952,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":2272.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":0.1509027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use the azure ML designer (preview).<\/p>\n\n<p>referencing this - <a href=\"https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>using my own input sheet which has four columns and some decimal values. nothing fancy and identical to the sample datasets provided. <\/p>\n\n<p>I do this step (from the linked document above)<\/p>\n\n<p><em>Select the Train Model module.\nIn the module details pane to the right of the canvas, select Edit column selector.\nIn the Label column dialog box, expand the drop-down menu and select Column names.\nIn the text box, enter price to specify the value that your model is going to predict.<\/em><\/p>\n\n<p>and I get this (but there are no errors in the actual designer window.<\/p>\n\n<p>\"Failed to parse column picker rules\"<\/p>",
        "Challenge_closed_time":1582126496627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582125953377,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1582126556240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60303714",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1405466586,
        "Challenge_title":"Failed to parse column picker rules - azure ML designer",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>Okay, I found an answer myself. Hope that is okay.<\/p>\n\n<p>In my input sheet, the title was something like this \"Interest Rate %\". Looks like azure was trying to say that it does not like special characters it the column names.<\/p>\n\n<p>I edited my original csv file in excel, and removed the % in all the titles. <\/p>\n\n<p>Then, created a new data store. problem solved. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":4.52,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444418094503,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":0.1511591667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. \nThis might sound so simple and easy, but I could not find a way to do it. Appreciate any help.<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1551375926143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551375381970,
        "Challenge_favorite_count":8.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54931270",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":17.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.1407694058,
        "Challenge_title":"Download an entire folder from AWS sagemaker to laptop",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":13015.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366530725212,
        "Poster_location":"California, USA",
        "Poster_reputation_count":440.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it<\/p>\n\n<pre><code>zip -r -X archive_name.zip folder_to_compress\n<\/code><\/pre>\n\n<p>You will find the zipped folder. You can then select it and download it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.45,
        "Solution_score_count":35.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":133.8006258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Challenge_closed_time":1658387166296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658386596203,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658442017223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":23.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.1470044923,
        "Challenge_title":"Terraform - Optional Nested Variable",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1658923699476,
        "Solution_link_count":4.0,
        "Solution_readability":23.4,
        "Solution_reading_time":18.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1499682655627,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation_count":193.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.7683222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Challenge_closed_time":1655983680023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983093280,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1509892593,
        "Challenge_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1655985859240,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":26.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":182.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1647222222,
        "Challenge_answer_count":1,
        "Challenge_body":"We would like to make algorithms shareable and re-usable across teams. Is it possible to create a private Amazon SageMaker algorithm marketplace?",
        "Challenge_closed_time":1556296138000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295545000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCu2f_chpRL2mDtFiGqwVNg\/private-marketplace-for-sage-maker-algorithms",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":2.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1524826227,
        "Challenge_title":"Private Marketplace for SageMaker algorithms",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Private Marketplace is a feature of the AWS Marketplace platform that AWS customers can use to create a private catalog of products containing both algorithms and models which are available in AWS Marketplace. To create a private catalog containing algorithms which have not been published in AWS Marketplace, you can use AWS Service Catalog. AWS Service Catalog lets users create and share algorithms packaged via a CloudFormation template. However, you do have to make the container image and model artifacts available in the destination account outside AWS Service Catalog. Here is a sample template for sharing a model - https:\/\/github.com\/aws-samples\/aws-service-catalog-reference-architectures\/blob\/master\/sagemaker\/sagemaker_vend_endpoint.yml",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.6,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1225669466307,
        "Answerer_location":"Cambridge, United Kingdom",
        "Answerer_reputation_count":236107.0,
        "Answerer_view_count":18730.0,
        "Challenge_adjusted_solved_time":0.1705583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1410733358503,
        "Challenge_comment_count":3,
        "Challenge_created_time":1410732744493,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1446025307110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":17.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.157480843,
        "Challenge_title":"C# async\/await to F# using Azure ML example",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":607.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349689794400,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation_count":4174.0,
        "Poster_view_count":396.0,
        "Solution_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":18.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":156.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1347532849952,
        "Answerer_location":"Tel Aviv, Israel",
        "Answerer_reputation_count":2529.0,
        "Answerer_view_count":172.0,
        "Challenge_adjusted_solved_time":1.6824691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to download a file to sagemaker from my S3 bucket.<\/p>\n\n<p>the path of the file is\n<code>s3:\/\/vemyone\/input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>The path of that file is stored as a list element at <code>train_fns[0]<\/code>.<\/p>\n\n<p>the value of <code>train_fns[0]<\/code> is <\/p>\n\n<p><code>input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>I used the following code:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0][:], train_fns[0])\n<\/code><\/pre>\n\n<p>but I get the following error:<\/p>\n\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: 'input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm.5b003ba1'<\/code><\/p>\n\n<p>I notice that some characters have appended itself at the end of the path.<\/p>\n\n<p>how do I solve this problem?<\/p>",
        "Challenge_closed_time":1562759076923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562758462203,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56969859",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":18.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1576493144,
        "Challenge_title":"AWS: FileNotFoundError: [Errno 2] No such file or directory",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":14766.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737834360,
        "Poster_location":"Pondicherry, Puducherry, India",
        "Poster_reputation_count":1303.0,
        "Poster_view_count":139.0,
        "Solution_body":"<p>please see <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file<\/a><\/p>\n\n<p>by the doc, first argument is file key, second argument is path for local file:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0], '\/path\/to\/local\/file')\n<\/code><\/pre>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":1562764519092,
        "Solution_link_count":2.0,
        "Solution_readability":30.3,
        "Solution_reading_time":6.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1615216681047,
        "Answerer_location":null,
        "Answerer_reputation_count":43.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.174105,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a local environment for the ML Studio using the Python SDK, following\n<a href=\"https:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/environment\/\" rel=\"nofollow noreferrer\">this official cheatsheet<\/a>. The result should be a conda-like environment that can be used for local testing. However, I am running into an error when importing the Numpy package with the <code>add_conda_package()<\/code> method of the <code>CondaDependencies()<\/code> class. Where I've tried not specifying, as well as specifying package versions, like:\n<code>add_conda_package('numpy')<\/code> or <code>add_conda_package('numpy=1.21.2')<\/code>, but it does not seem to make a difference.<\/p>\n<p>Numpy's error message is extensive, and I've tried many of the suggestions, without success nonetheless. I'm grateful for any tips on what might resolve my issues!<\/p>\n<hr \/>\n<h2>Full code<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>\n<hr \/>\n<h2>Detailed error message:<\/h2>\n<p>User program failed with ImportError:<\/p>\n<p>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!<\/p>\n<p>Importing the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.<\/p>\n<p>We have compiled some common reasons and troubleshooting tips at:<\/p>\n<pre><code>https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\n<\/code><\/pre>\n<p>Please note and check the following:<\/p>\n<ul>\n<li>The Python version is: Python3.8 from &quot;&lt;LOCAL_DIR&gt;.azureml\\envs\\azureml_&gt;\\python.exe&quot;<\/li>\n<li>The NumPy version is: &quot;1.19.1&quot;<\/li>\n<\/ul>\n<p>and make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.<\/p>\n<p>Original error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.<\/p>\n<hr \/>\n<h2>System specifications:<\/h2>\n<ul>\n<li>Local OS: Windows 10<\/li>\n<li>ML studio OS: Linux Ubuntu 18<\/li>\n<li>Python version: 3.8<\/li>\n<\/ul>",
        "Challenge_closed_time":1637743033888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637742407110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70092793",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":38.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":0.1605061552,
        "Challenge_title":"Azure ML Studio Local Environment \u2014 Numpy package import failure using the Azure ML Python SDK",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1615216681047,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I was finally able to resolve the issue by using the pip method instead of the conda method:\n<code>add_pip_package('numpy')<\/code> instead of <code>add_conda_package('numpy')<\/code>\nI can imagine this being the reason for other packages as well.<\/p>\n<hr \/>\n<h2>Full solution<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    #conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # Add pip packages\n    conda.add_pip_package('numpy') # &lt;--- Fixes import error\n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.6,
        "Solution_reading_time":16.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1572812561640,
        "Answerer_location":null,
        "Answerer_reputation_count":3502.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":0.1742652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1660922530248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660921902893,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1606426565,
        "Challenge_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":5.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1638396070903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1607580885,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":716.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.1744530556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?<\/p>",
        "Challenge_closed_time":1652333522488,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652332894457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72210450",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1608025546,
        "Challenge_title":"Is there any way to create or delete workspaces in AML studio using powershell?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652331444420,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>According to the requirements, there is no procedure developed to create\/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using <em><strong>Az<\/strong><\/em><\/p>\n<p>Here is the table link to check PowerShell support table<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.9,
        "Solution_reading_time":10.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1746036111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house. But it seems python script 'transform_titanic.py' was executed on Azure.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\n\nI executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.\n\nfrom azureml.core import Environment, Experiment, ScriptRunConfig, Workspace\nfrom dotenv import load_dotenv\nload_dotenv()\nws = Workspace(\n    os.environ['SUBSCRIPTION_ID']\n    os.environ['RESOURCE_GROUP']\n    os.environ['WORKSPACE_NAME']\n)\nexp = Experiment(workspace=ws, name='experiment')\nenv = Environment('user-managed-env')\nenv.python.user_managed_dependencies = True\nscript_run_config = ScriptRunConfig(\n    source_directory='src\/transform',\n    script='transform_titanic.py',\n    arguments=['--input_dataset_name1', 'titanic'],\n)\nscript_run_config.run_config.target = 'local'\nscript_run_config.run_config.environment = env\nrun = exp.submit(config=script_run_config)\nprint(run.get_portal_url())\nrun.wait_for_completion()",
        "Challenge_closed_time":1600495830720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600495202147,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/99901\/what-does-34local34-mean-in-compute-target.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":18.1,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.1609307384,
        "Challenge_title":"What does \"local\" mean in compute target?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sorry, I found it was run on my local computer. Some artifact created in the script was in C:\\Users{username}\\AppData\\Local\\Temp\\azureml_runs\\local_experiment_XXXXXXXXXX",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.1789469444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Challenge_closed_time":1591906575912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591905931703,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609771005756,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.16462162,
        "Challenge_title":"Tracking separate train\/test processes with Trains",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592833417200,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":202.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1808333333,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done. Any links or pointers would be greatly appreciated.",
        "Challenge_closed_time":1594232347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594231696000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1662204039,
        "Challenge_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":519.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\nhttps:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E: https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here:\n\nhttps:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.0,
        "Solution_reading_time":12.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606378353316,
        "Answerer_location":"Huskvarna, Sverige",
        "Answerer_reputation_count":275.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":3.7086241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Challenge_closed_time":1643284402368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643283742143,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":24.0,
        "Challenge_reading_time":28.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1683881303,
        "Challenge_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":391.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452981020536,
        "Poster_location":"UK",
        "Poster_reputation_count":311.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1643297093190,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1255292583096,
        "Answerer_location":"Udaipur, Rajasthan, India",
        "Answerer_reputation_count":119996.0,
        "Answerer_view_count":13669.0,
        "Challenge_adjusted_solved_time":0.1888275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've created some <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow\">Azure Machine Learning<\/a> Workspaces and associated them with \"classic\" storage accounts; but would like to have them associated with \"not-classic\" (or whatever the term is) storage accounts.<\/p>\n\n<p>Is there a way to convert the storage accounts from \"classic\", or to change the storage account associated with a Machine Learning Workspace?<\/p>",
        "Challenge_closed_time":1463238035912,
        "Challenge_comment_count":3,
        "Challenge_created_time":1463237356133,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37228077",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1729675273,
        "Challenge_title":"Converting Azure \"classic\" storage accounts",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1254.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299959670312,
        "Poster_location":"United States",
        "Poster_reputation_count":41475.0,
        "Poster_view_count":1912.0,
        "Solution_body":"<p>As of today, there's no automatic way of converting a \"Classic\" storage account into \"Azure Resource Manager (ARM)\" storage account. Today, you would need to copy data from a classic storage account to a new storage account.<\/p>\n\n<p>Having said that, there's no difference in how the data is stored in both kinds of storage accounts. Both of them support connecting via account name\/key and\/or shared access signature. The difference is how these storage account themselves are managed. In ARM storage accounts, you can assign granular role-based access control (RBAC) to control what a user can do as far as managing the storage accounts (like updating, deleting, viewing\/regenerating keys).<\/p>\n\n<p>Regarding your question about using new storage accounts with ML workspace, I don't think it's possible today (I may be wrong though). Reason being, ML is still managed via old portal which doesn't have the capability to manage ARM storage accounts.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":11.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1954663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Challenge_closed_time":1596781453976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596780750297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.8,
        "Challenge_reading_time":16.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.1785363928,
        "Challenge_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513841518107,
        "Poster_location":"China",
        "Poster_reputation_count":71.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2003183333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nno practice exam for the Azure certification DP-100 seems to be available in the official channels. It would, however, be very helpful for preparing.\nBy any chance, do you plan to introduce such a resource any time soon?\n\nThanks and best regards\nTim",
        "Challenge_closed_time":1593083339536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593082618390,
        "Challenge_favorite_count":4.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/39948\/practice-exam-for-dp-100.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1825867994,
        "Challenge_title":"Practice Exam for DP-100",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nMicrosoft Certification \/ Exams are currently not supported in the Q&A forums, the supported products are listed over here https:\/\/docs.microsoft.com\/en-us\/answers\/products (more to be added later on).\n\nYou can ask the experts in the dedicated Microsoft Certification - Preparation Resources forum over here:\nhttps:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_exams-mcp_prep\n\n(Please don't forget to accept helpful replies as answer)\n\nBest regards,\nLeon",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":5.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.2018694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am managing a data pipeline using Kedro and at the last step I have a huge csv file stored in a S3 bucket and I need to load it back to SQL Server.<\/p>\n<p>I'd normally go about that with a <a href=\"https:\/\/towardsdatascience.com\/use-python-and-bulk-insert-to-quickly-load-data-from-csv-files-into-sql-server-tables-ba381670d376\" rel=\"nofollow noreferrer\">bulk insert<\/a>, but not quite sure how to fit that into the <strong>kedro<\/strong> templates. This are the destination table and the S3 Bucket as configured in the <code>catalog.yml<\/code><\/p>\n<pre><code>flp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    schema: 'dwschema'\n  save_args:\n    schema: 'dwschema'\n    if_exists: 'replace'\n\nbulk_insert_input:\n   type: pandas.CSVDataSet\n   filepath: s3:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n   credentials: dev_s3\n\n\ndef insert_data(self, conn, csv_file_nm, db_table_nm):\n    qry = &quot;BULK INSERT &quot; + db_table_nm + &quot; FROM '&quot; + csv_file_nm + &quot;' WITH (FORMAT = 'CSV')&quot;\n    # Execute the query\n    cursor = conn.cursor()\n    success = cursor.execute(qry)\n    conn.commit()\n    cursor.close\n<\/code><\/pre>\n<ul>\n<li>How do I point <code>csv_file_nm<\/code> to my <code>bulk_insert_input<\/code> S3 catalog?<\/li>\n<li>Is there a proper way to indirectly access <code>dw_dev_credentials<\/code> to do the insert?<\/li>\n<\/ul>",
        "Challenge_closed_time":1626182900783,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626182174053,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1626187242808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68363180",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.183878215,
        "Challenge_title":"How to use SQL Server Bulk Insert in Kedro Node?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271930452580,
        "Poster_location":null,
        "Poster_reputation_count":5469.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">pandas.SQLTableDataSet.html<\/a> uses the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.to_sql<\/a> method as is. To use this as is you would need one <code>pandas.CSVDataSet<\/code> into a <code>node<\/code> which then writes to a target <code>pandas.SQLDataTable<\/code> dataset in order to write it to SQL. If you have Spark available this will be faster than Pandas.<\/p>\n<p>In order to use the built in <code>BULK INSERT<\/code> query I think you will need to define a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":11.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":76.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2032822222,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\nWe have resource group(RG1) created in Germany region. Resource created are\n1) Vnet\n2) VPN gateway\n3) Disks - HDD\n4) VM (database installed)\n\nWe wanted to use Azure machine learning service to read data from database(present in resource group RG1 in German region). We later found that azure machine learning service is not available in Germany region and in order to create end points for Azure ML service both the networks(vnets of the DB and Azure ML ) should be in the same region.\n\nSo we tried migrating the resource and resource group (RG1) from German region to west Europe by using wizard on the portal. But we got prompted that disk , vnet and vpn gateway cannot be moved to different region ) .\n\nIs there anyway we could move them ? or any alternate solutions ? .Else, we would end up recreating every thing in westeurope which i would like to avoid.\n\nRegards,\nSuman",
        "Challenge_closed_time":1606984258243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606983526427,
        "Challenge_favorite_count":9.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/184565\/moving-resources-across-the-regions.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.6,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1850530081,
        "Challenge_title":"Moving resources across the regions",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Maybe this is helpful:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":0.2048830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1629887227052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1863825129,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":13.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.1185691666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Challenge_closed_time":1637159193836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637158421443,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637158766987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.194376611,
        "Challenge_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586517832390,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":72.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1314094431590,
        "Answerer_location":"Ghent, Belgium",
        "Answerer_reputation_count":2648.0,
        "Answerer_view_count":181.0,
        "Challenge_adjusted_solved_time":0.2174413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm following the automate_model_retraining_workflow example from SageMaker examples, and I'm running that in AWS SageMaker Jupyter notebook. I followed all the steps given in the example for creating the roles and policies.<\/p>\n<p>But when I try to run the following block of code to creat a Glue job, I ran into an error:<\/p>\n<pre><code>glue_script_location = S3Uploader.upload(\n    local_path=&quot;.\/code\/glue_etl.py&quot;,\n    desired_s3_uri=&quot;s3:\/\/{}\/{}&quot;.format(bucket, project_name),\n    sagemaker_session=session,\n)\nglue_client = boto3.client(&quot;glue&quot;)\n\nresponse = glue_client.create_job(\n    Name=job_name,\n    Description=&quot;PySpark job to extract the data and split in to training and validation data sets&quot;,\n    Role=glue_role,  # you can pass your existing AWS Glue role here if you have used Glue before\n    ExecutionProperty={&quot;MaxConcurrentRuns&quot;: 2},\n    Command={&quot;Name&quot;: &quot;glueetl&quot;, &quot;ScriptLocation&quot;: glue_script_location, &quot;PythonVersion&quot;: &quot;3&quot;},\n    DefaultArguments={&quot;--job-language&quot;: &quot;python&quot;},\n    GlueVersion=&quot;1.0&quot;,\n    WorkerType=&quot;Standard&quot;,\n    NumberOfWorkers=2,\n    Timeout=60,\n)\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDeniedException) when calling the CreateJob\noperation: User:\narn:aws:sts::############:assumed-role\/AmazonSageMaker-ExecutionRole-############\/SageMaker is not authorized to perform: iam:PassRole on resource:\narn:aws:iam::############:role\/AWS-Glue-S3-Bucket-Access<\/p>\n<\/blockquote>\n<p>This is how AmazonSageMaker-ExecutionPolicy-############ looks like:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;############&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;lambda:CreateFunction&quot;,\n                &quot;glue:UpdateCrawler&quot;,\n                &quot;glue:UpdateTrigger&quot;,\n                &quot;lambda:DeleteFunction&quot;,\n                &quot;glue:DeleteCrawler&quot;,\n                &quot;glue:UpdateSchema&quot;,\n                &quot;lambda:UpdateFunctionCode&quot;,\n                &quot;glue:DeleteConnection&quot;,\n                &quot;glue:UseMLTransforms&quot;,\n                &quot;glue:BatchDeleteConnection&quot;,\n                &quot;lambda:PutProvisionedConcurrencyConfig&quot;,\n                &quot;glue:StartCrawlerSchedule&quot;,\n                &quot;glue:UpdateMLTransform&quot;,\n                &quot;lambda:PublishVersion&quot;,\n                &quot;lambda:DeleteEventSourceMapping&quot;,\n                &quot;glue:CreateMLTransform&quot;,\n                &quot;glue:CreateRegistry&quot;,\n                &quot;glue:StartMLEvaluationTaskRun&quot;,\n                &quot;glue:DeleteTableVersion&quot;,\n                &quot;glue:CreateTrigger&quot;,\n                &quot;glue:BatchDeletePartition&quot;,\n                &quot;glue:StopTrigger&quot;,\n                &quot;glue:CreateUserDefinedFunction&quot;,\n                &quot;glue:StopCrawler&quot;,\n                &quot;lambda:InvokeAsync&quot;,\n                &quot;glue:DeleteJob&quot;,\n                &quot;glue:DeleteDevEndpoint&quot;,\n                &quot;glue:DeleteMLTransform&quot;,\n                &quot;glue:CreateJob&quot;,\n                &quot;glue:ResetJobBookmark&quot;,\n                &quot;glue:CreatePartition&quot;,\n                &quot;lambda:PutFunctionCodeSigningConfig&quot;,\n                &quot;glue:UpdatePartition&quot;,\n                &quot;glue:RegisterSchemaVersion&quot;,\n                &quot;glue:ResumeWorkflowRun&quot;,\n                &quot;lambda:UpdateEventSourceMapping&quot;,\n                &quot;lambda:UpdateFunctionCodeSigningConfig&quot;,\n                &quot;lambda:UpdateFunctionConfiguration&quot;,\n                &quot;glue:StartMLLabelingSetGenerationTaskRun&quot;,\n                &quot;lambda:UpdateCodeSigningConfig&quot;,\n                &quot;glue:CreateDatabase&quot;,\n                &quot;glue:BatchDeleteTableVersion&quot;,\n                &quot;lambda:DeleteAlias&quot;,\n                &quot;glue:DeleteSchemaVersions&quot;,\n                &quot;glue:BatchCreatePartition&quot;,\n                &quot;glue:CreateClassifier&quot;,\n                &quot;glue:UpdateTable&quot;,\n                &quot;lambda:DeleteProvisionedConcurrencyConfig&quot;,\n                &quot;glue:DeleteTable&quot;,\n                &quot;glue:DeleteWorkflow&quot;,\n                &quot;glue:DeleteSchema&quot;,\n                &quot;glue:UpdateWorkflow&quot;,\n                &quot;glue:CreateScript&quot;,\n                &quot;glue:StartWorkflowRun&quot;,\n                &quot;glue:StopCrawlerSchedule&quot;,\n                &quot;lambda:UpdateFunctionEventInvokeConfig&quot;,\n                &quot;lambda:DeleteFunctionCodeSigningConfig&quot;,\n                &quot;glue:UpdateDatabase&quot;,\n                &quot;glue:CreateTable&quot;,\n                &quot;lambda:InvokeFunction&quot;,\n                &quot;glue:BatchStopJobRun&quot;,\n                &quot;glue:DeleteUserDefinedFunction&quot;,\n                &quot;glue:CreateConnection&quot;,\n                &quot;glue:CreateCrawler&quot;,\n                &quot;lambda:UpdateAlias&quot;,\n                &quot;glue:DeleteSecurityConfiguration&quot;,\n                &quot;glue:CreateSchema&quot;,\n                &quot;glue:StartJobRun&quot;,\n                &quot;glue:BatchDeleteTable&quot;,\n                &quot;glue:UpdateClassifier&quot;,\n                &quot;glue:CreateWorkflow&quot;,\n                &quot;glue:DeletePartition&quot;,\n                &quot;lambda:CreateAlias&quot;,\n                &quot;glue:CreateSecurityConfiguration&quot;,\n                &quot;glue:PutWorkflowRunProperties&quot;,\n                &quot;glue:DeleteDatabase&quot;,\n                &quot;glue:RemoveSchemaVersionMetadata&quot;,\n                &quot;lambda:PublishLayerVersion&quot;,\n                &quot;lambda:CreateEventSourceMapping&quot;,\n                &quot;glue:StartTrigger&quot;,\n                &quot;glue:DeleteRegistry&quot;,\n                &quot;lambda:PutFunctionConcurrency&quot;,\n                &quot;lambda:DeleteCodeSigningConfig&quot;,\n                &quot;glue:ImportCatalogToGlue&quot;,\n                &quot;glue:PutDataCatalogEncryptionSettings&quot;,\n                &quot;glue:UpdateRegistry&quot;,\n                &quot;glue:StartCrawler&quot;,\n                &quot;lambda:DeleteLayerVersion&quot;,\n                &quot;lambda:PutFunctionEventInvokeConfig&quot;,\n                &quot;glue:UpdateJob&quot;,\n                &quot;lambda:DeleteFunctionEventInvokeConfig&quot;,\n                &quot;lambda:CreateCodeSigningConfig&quot;,\n                &quot;glue:StartImportLabelsTaskRun&quot;,\n                &quot;glue:DeleteClassifier&quot;,\n                &quot;glue:StartExportLabelsTaskRun&quot;,\n                &quot;glue:UpdateUserDefinedFunction&quot;,\n                &quot;glue:CancelMLTaskRun&quot;,\n                &quot;glue:StopWorkflowRun&quot;,\n                &quot;glue:PutSchemaVersionMetadata&quot;,\n                &quot;glue:UpdateCrawlerSchedule&quot;,\n                &quot;glue:UpdateConnection&quot;,\n                &quot;glue:CreateDevEndpoint&quot;,\n                &quot;glue:UpdateDevEndpoint&quot;,\n                &quot;lambda:DeleteFunctionConcurrency&quot;,\n                &quot;glue:DeleteTrigger&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Sid&quot;: &quot;VisualEditor1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;iam:PassRole&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*&quot;,\n                &quot;arn:aws:iam::############:role\/query_training_status-role&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1628669577192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628668794403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68738148",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":46.2,
        "Challenge_reading_time":87.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.1967514343,
        "Challenge_title":"SageMaker is not authorized to perform: iam:PassRole",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1684.0,
        "Challenge_word_count":274,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450889293150,
        "Poster_location":"Finland",
        "Poster_reputation_count":398.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>It's clear from the IAM policy that you've posted that you're only allowed to do an <code>iam:PassRole<\/code> on <code>arn:aws:iam::############:role\/query_training_status-role<\/code> while Glue is trying to use the <code>arn:aws:iam::############:role\/AWS-Glue-S3-Bucket-Access<\/code>. So you'll just need to update your IAM policy to allow <code>iam:PassRole<\/code> role as well for the other role.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.2190208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1595162985808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1980479408,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job? Is there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Challenge_closed_time":1584346840000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584346040000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sage-maker-spot-training",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2006706954,
        "Challenge_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi olivier, If you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the blog on Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs also mentions the same\n\nTo avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.9,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":0.2273408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Challenge_closed_time":1573497881147,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573497062720,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.204849905,
        "Challenge_title":"Create a predictor from an endpoint in a different region",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553808322940,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450242937020,
        "Answerer_location":"Syria",
        "Answerer_reputation_count":235.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":0.2286694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a python program using Pycharm IDE but unable to do so without stumbling into \"Your system has run out of application memory\". After some research I came across a suggestion of using Microsoft Azure ML. Can anyone point me to some helpful links that can get me started or any other suggestions at all?<\/p>\n\n<p>Edit: I am working with a data that has 400,000 samples and ~5000 samples and I want to use chi2 feature selection but I am unable to run the program.<\/p>",
        "Challenge_closed_time":1454732632400,
        "Challenge_comment_count":1,
        "Challenge_created_time":1454731809190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1483522935080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35237226",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2059318313,
        "Challenge_title":"Cannot run huge Python program",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1454200887396,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You can use: PyPy to run your program with less memory usage and more speed. see this <a href=\"http:\/\/pypy.org\/\" rel=\"nofollow\">pypy site<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408356046196,
        "Answerer_location":"Bonn, Deutschland",
        "Answerer_reputation_count":594.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.2372688889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559043123528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559042269360,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":22.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.2129064416,
        "Challenge_title":"Docker image not found during local deployment (\"no such image\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":3685.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408356046196,
        "Poster_location":"Bonn, Deutschland",
        "Poster_reputation_count":594.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1521276815912,
        "Answerer_location":null,
        "Answerer_reputation_count":731.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":0.2378675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to download the best performing model for a certain ClearlML project. I have the following content in my ClearML experiment platform:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>According to: <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models<\/a> I can get a list of models for a specific project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list = Model.query_models(\n    # Only models from `examples` project\n    project_name='YOLOv5', \n    # Only models with input name\n    model_name=None,\n    # Only models with `demo` tag but without `TF` tag\n    tags=['demo', '-TF'],\n    # If `True`, only published models\n    only_published=False,\n    # If `True`, include archived models\n    include_archived=True,\n    # Maximum number of models returned\n    max_results=5\n)\n\nprint(model_list)\n<\/code><\/pre>\n<p>Which prints:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[&lt;clearml.model.Model object at 0x7fefbaf22130&gt;, &lt;clearml.model.Model object at 0x7fefbaf22340&gt;]\n<\/code><\/pre>\n<p>So I can run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list[0].get_local_copy()\n<\/code><\/pre>\n<p>and get this specific model. But how do I download the best performing one for this project on a certain metric (in this case mAP_0.5:0.95 MAX)?<\/p>",
        "Challenge_closed_time":1662538633323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662537777000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662645914387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73632015",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2133901411,
        "Challenge_title":"ClearML, how to query the best performing model for a specific project and metric",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521276815912,
        "Poster_location":null,
        "Poster_reputation_count":731.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>I ended up doing the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    import clearml\n    from clearml import Dataset, Task, Model, OutputModel\n    assert hasattr(clearml, '__version__')  # verify package import not local dir\nexcept (ImportError, AssertionError):\n    clearml = None\n\ntasks = Task.get_tasks(project_name='YOLOv5', task_name='exp', task_filter={'status': ['completed']})\n\nresults = {}\nbest_task = None\nfor task in tasks:\n    results[task.id] = task.get_last_scalar_metrics()['metrics']['mAP_0.5:0.95']['max']\n\nbest_model_task_id = max(results, key=results.get)\nmodel_list = Task.get_task(best_model_task_id).get_models()\ndest = model_list['output'][0].get_local_copy()\nprint('Saved model at:', dest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1662553432007,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":9.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":58.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1446841002932,
        "Answerer_location":"Trondheim, Norway",
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.2383613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an Azure ML Web service which outputs JSON response on request, and the structure of the sample request is as following:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"gender\",\n        \"age\",\n        \"income\"\n      ],\n      \"Values\": [\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ],\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>And the input parameters are supposedly like this:<\/p>\n\n<p>gender  String<br>\nage Numeric<br>\nincome  Numeric     <\/p>\n\n<p>My Post method looks like this:<\/p>\n\n<pre><code>    [HttpPost]\n        public ActionResult GetPredictionFromWebService()\n        {\n            var gender = Request.Form[\"gender\"];\n            var age = Request.Form[\"age\"];\n\n\n            if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n            {\n                var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = Int32.Parse(result[0, 2])\n                    };\n                }\n            }\n\n\n\n\n            return RedirectToAction(\"index\");\n        }\n<\/code><\/pre>\n\n<p>But for whatever reason; the Azure ML Webservice doesn\u2019t seem to respond anything to my request.\nDoes anyone know what the reason might be? I see no error or anything, just an empty response.<\/p>",
        "Challenge_closed_time":1456314151448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1456313293347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1456839908307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35600907",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2137890452,
        "Challenge_title":"Azure ML Web Service request not working in C#",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":480.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The answer to your problem is that the \u201cNumeric\u201d datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the \u201cadequate\u201d information needed in the right format for it to respond correctly, resulting in it not giving you any response.<\/p>\n\n<p>I believe your model would look something similar to this based on your input parameters:<\/p>\n\n<pre><code>public class Person\n    {\n        public string Gender { get; set; }\n        public int Age { get; set; }\n        public int Income { get; set; }\n\n\n        public override string ToString()\n        {\n            return Gender + \",\" + Age + \",\" + Income;\n        }\n    }\n<\/code><\/pre>\n\n<p>You would have to change your Income datatype into float like so:<\/p>\n\n<pre><code>public class Person\n{\n    public string Gender { get; set; }\n    public int Age { get; set; }\n    public float Income { get; set; }\n\n    public override string ToString()\n    {\n        return Gender + \",\" + Age + \",\" + Income;\n    }\n}\n<\/code><\/pre>\n\n<p>And then your post-method would look something like this:<\/p>\n\n<pre><code>    [HttpPost]\n    public ActionResult GetPredictionFromWebService()\n    {\n        var gender = Request.Form[\"gender\"];\n        var age = Request.Form[\"age\"];\n\n        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n        {\n            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n                };\n            }\n        }\n\n        ViewBag.myData = PersonResult.Income.ToString();\n        return View(\"Index\");\n    }\n<\/code><\/pre>\n\n<p>The key here is simply:<\/p>\n\n<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n<\/code><\/pre>\n\n<p>Rather than your legacy <\/p>\n\n<pre><code>Income = Int32.Parse(result[0, 2])\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":221.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.2422272222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to reproduce 2 tutorials below using my own dataset instead of MNIST dataset.\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>\n\n<p>About\n'\/notebooks\/tutorials\/03.auto-train-models.ipynb'\nthere's no problem. I've got 'model.pkl'.<\/p>\n\n<p>However, \n'\/notebooks\/tutorials\/02.deploy-models.ipynb'\nhas an error below in 'Predict test data' cell.\nI guess it's a matter of 'pickle' and 'import'.<\/p>\n\n<p>Tell me solutions, please.<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-6-11cf888b622f&gt; in &lt;module&gt;\n      2 from sklearn.externals import joblib\n      3 \n----&gt; 4 clf = joblib.load('.\/model.pkl')\n      5 # clf = joblib.load('.\/sklearn_mnist_model.pkl')\n      6 y_hat = clf.predict(X_test)\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in load(filename, mmap_mode)\n    576                     return load_compatibility(fobj)\n    577 \n--&gt; 578                 obj = _unpickle(fobj, filename, mmap_mode)\n    579 \n    580     return obj\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in _unpickle(fobj, filename, mmap_mode)\n    506     obj = None\n    507     try:\n--&gt; 508         obj = unpickler.load()\n    509         if unpickler.compat_mode:\n    510             warnings.warn(\"The file '%s' has been generated with a \"\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load(self)\n   1048                     raise EOFError\n   1049                 assert isinstance(key, bytes_types)\n-&gt; 1050                 dispatch[key[0]](self)\n   1051         except _Stop as stopinst:\n   1052             return stopinst.value\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load_global(self)\n   1336         module = self.readline()[:-1].decode(\"utf-8\")\n   1337         name = self.readline()[:-1].decode(\"utf-8\")\n-&gt; 1338         klass = self.find_class(module, name)\n   1339         self.append(klass)\n   1340     dispatch[GLOBAL[0]] = load_global\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in find_class(self, module, name)\n   1386             elif module in _compat_pickle.IMPORT_MAPPING:\n   1387                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1388         __import__(module, level=0)\n   1389         if self.proto &gt;= 4:\n   1390             return _getattribute(sys.modules[module], name)[0]\n\nModuleNotFoundError: No module named 'automl'\n<\/code><\/pre>",
        "Challenge_closed_time":1543816325648,
        "Challenge_comment_count":1,
        "Challenge_created_time":1543815453630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53588040",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":16.5,
        "Challenge_reading_time":33.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.2169059154,
        "Challenge_title":"No module named 'automl' when unpickle auto-trained model",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4788.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1543814686768,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>you have to include azureml-train-automl package. and you have to do this:<\/p>\n\n<p>import azureml.train.automl<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.51,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2438519445,
        "Challenge_answer_count":1,
        "Challenge_body":"How could I request quota for the NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs\n\nThey both do not appear as an option on the usual form to request quota increase in any of the 4 US regions I looked\n\nThanks\n\nManuel",
        "Challenge_closed_time":1629922551630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629921673763,
        "Challenge_favorite_count":22.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/528034\/access-to-ncast4-v3-series-and-nd-a100-v4-series-v.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.2182129716,
        "Challenge_title":"Access to NCasT4_v3-series and ND A100 v4-series VMs",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @ManuelReyesGomez-1028 ,\n\nthe VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)\n\nSource: https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.5,
        "Solution_reading_time":6.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.2453072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Challenge_closed_time":1587756432003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587755548897,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1587809992912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":0.2193822643,
        "Challenge_title":"Log metrics in PythonScriptStep",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":10.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2472222222,
        "Challenge_answer_count":1,
        "Challenge_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Challenge_closed_time":1603455348000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603454458000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2209188563,
        "Challenge_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":426.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460436951967,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2497888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Challenge_closed_time":1460437058280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460436159040,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36563769",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2229746482,
        "Challenge_title":"How to download the entire scored dataset from Azure machine studio?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5296.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":14.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2554944445,
        "Challenge_answer_count":6,
        "Challenge_body":"I tried to deploy a VM to Azure Machine Learning, but I get the error message \"You do not have enough quota for the following VM sizes. Click here to view and request quota.\" And the VM cannot be deployed.\n\nBut I have enough quota (24 CPUs).\n\nWhat is causing the problem?\n\nI'm using Azure's Free trial plan.",
        "Challenge_closed_time":1623772934643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":19.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/437136\/can39t-deploy-a-vm-on-the-azure-machine-learning.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":3.4,
        "Challenge_reading_time":4.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2275294747,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @Sss-1842 ,\n\nthere are different quotas in Azure:\n\nThere are quotas for vCPUs per Azure Region\n\n\nIn addition there are quotas for vCPUs per VM Series\n\nBoth quotas (for Azure Region and VM Series) must fit the requirements.\n\nIt seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.\nYou can check your quotas by the link you marked with the red line in your screenshot.\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":6.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1540534565632,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.4110722223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS Sagemaker for model training and deployment, this is sample example for model training <\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nhyperparameters = {'train-steps': 10}\ninstance_type = 'ml.m4.xlarge'\n\nestimator = Estimator(role=role,\n                      train_instance_count=1,\n                      train_instance_type=instance_type,\n                      image_name=ecr_image,\n                      hyperparameters=hyperparameters)\n\nestimator.fit(data_location)\n<\/code><\/pre>\n\n<p>The docker image mentioned here is a tensorflow system. <\/p>\n\n<p>Suppose it will take 1000 seconds to train the model, now I will increase the instance count to 5 then the training time will increase 5 times i.e. 5000 seconds. As per my understanding the training job will be distributed to 5 machines so ideally it will take 200 seconds per machine but seems its doing separate training on each machine. Can someone please let me know its working over distributed system in general or with Tensorflow.<\/p>\n\n<p>I tried to find out the answer on this documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf<\/a> but seems the way of working on distributed machines is not mentioned here.<\/p>",
        "Challenge_closed_time":1546585143583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546584216850,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54034172",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2290666418,
        "Challenge_title":"AWS Sagemaker | Why multiple instances training taking time multiplied to instance number",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Are you using <a href=\"https:\/\/www.tensorflow.org\/guide\/estimators\" rel=\"nofollow noreferrer\">TensorFlow estimator APIs<\/a> in your script? If yes, I think you should run the script by wrapping it in <code>sagemaker.tensorflow.TensorFlow<\/code> class as described <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#training-with-tensorflow-estimator\" rel=\"nofollow noreferrer\">in the documentation here<\/a>. If you run training that way, parallelization and communication between instances should work out-of-the-box.<\/p>\n\n<p>But note that scaling will not be linear when you increase the number of instances. Communicating between instances takes time and there could be non-parallelizable bottlenecks in your script like loading data to memory.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1546585696710,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2574272222,
        "Challenge_answer_count":2,
        "Challenge_body":"I repeat the example from https:\/\/docs.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-onnx \"Deploy and make predictions with an ONNX model and SQL machine learning\" In this quickstart, you'll learn how to train a model, convert it to ONNX, deploy it to Azure SQL Edge, and then run native PREDICT on data using the uploaded ONNX model.\n\nSuccessfully create a model using Python, convert to onnx format, I test the model using Python, save the model to the database, load the necessary data and try to execute the SQL query\nUSE onnx\nDECLARE @model VARBINARY(max) = (\nSELECT DATA\nFROM dbo.models\nWHERE id = 1\n);\nWITH predict_input\nAS (\nSELECT TOP (1000) [id]\n, CRIM\n, ZN\n, INDUS\n, CHAS\n, NOX\n, RM\n, AGE\n, DIS\n, RAD\n, TAX\n, PTRATIO\n, B\n, LSTAT\nFROM [dbo].[features]\n)\nSELECT predict_input.id\n, p.variable1 AS MEDV\nFROM PREDICT(MODEL = @model, DATA = predict_input, RUNTIME=ONNX) WITH (variable1 FLOAT) AS p;\n\nAs a result I get an error Msg 102, Level 16, State 5, Line 27 Incorrect syntax near 'RUNTIME'.\n\nI can't figure out what's wrong. The documentation clearly says \"The RUNTIME = ONNX argument is only available in Azure SQL Edge, Azure Synapse Analytics, and is in Preview in Azure SQL Managed Instance.\"",
        "Challenge_closed_time":1650461393248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650460466510,
        "Challenge_favorite_count":10.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/819485\/azure-sql-managed-instance-predict-with-an-onnx-mo.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2290677463,
        "Challenge_title":"Azure SQL Managed Instance PREDICT with an ONNX model",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think there is a misunderstanding here. The quickstart article named \"Deploy and make predictions with an ONNX model and SQL machine learning\" can be successfully implemented only with Azure SQL Edge and cannot be implemented with Azure SQL Managed Instance.\n\nYou cannot have an ONNX model and make predictions with it on Azure SQL Managed Instance. Please deploy Azure SQL Edge on an IoT device using this documentation.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1367338176003,
        "Answerer_location":"San Diego, CA, USA",
        "Answerer_reputation_count":271801.0,
        "Answerer_view_count":20314.0,
        "Challenge_adjusted_solved_time":22.9453238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Challenge_closed_time":1557849422180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557848494950,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":10.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.229176428,
        "Challenge_title":"Render hyperlink in pandas df in jupyterlab",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376684625670,
        "Poster_location":null,
        "Poster_reputation_count":1105.0,
        "Poster_view_count":222.0,
        "Solution_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1557931098116,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":11.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1469978721363,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":0.2587683333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build a Docker image from SageMaker studio using the CLI described here: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a><\/p>\n<p>This should be straight forward but I'm missing something because when running the command <code>sm-docker build .<\/code> I have a &quot;invalid syntax&quot; error. Any idea what I'm doing wrong?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1620075508956,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620074577390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67375607",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2301337296,
        "Challenge_title":"Using the Amazon SageMaker Studio Image Build CLI to build container images from Studio notebooks",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":901.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469978721363,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>No actually I was missing the &quot;<code>!<\/code>&quot; in front of <code>sm-docker build .<\/code>\nSo <code>!sm-docker build .<\/code> is working.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":1.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1402536093248,
        "Answerer_location":null,
        "Answerer_reputation_count":817703.0,
        "Answerer_view_count":106500.0,
        "Challenge_adjusted_solved_time":0.2592397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Challenge_closed_time":1554572339496,
        "Challenge_comment_count":1,
        "Challenge_created_time":1554571406233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2305081438,
        "Challenge_title":"Azure ML and r scripts",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553239567403,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646758945343,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.2594925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)<\/code> the same code runs fine on my laptop.<\/p>\n<ol>\n<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))<\/code> from within my training script (right before it throws the error) it prints<\/li>\n<\/ol>\n<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|===========================================================================|\n<\/code><\/pre>\n<p>but I have no idea what it means or how to interpret it<\/p>\n<ol start=\"2\">\n<li>when i run <code>!df -h<\/code> I can see:<\/li>\n<\/ol>\n<pre><code>Filesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         30G   72K   30G   1% \/dev\ntmpfs            30G     0   30G   0% \/dev\/shm\n\/dev\/xvda1      109G   93G   16G  86% \/\n\/dev\/xvdf       196G   61M  186G   1% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>how is this memory different from the GPU? if theres 200GB in \/dev\/xvdf is there anyway I can just use that..? in my test script I tried<br \/>\n<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)<\/code>\nbut that just gives the same error<\/p>",
        "Challenge_closed_time":1646760128676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646759194503,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646767755128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71398882",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.230708862,
        "Challenge_title":"CUDA: RuntimeError: CUDA out of memory - BERT sagemaker",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1983.0,
        "Challenge_word_count":456,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597997723910,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>A <code>CUDA out of memory<\/code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h<\/code> command).<\/p>\n<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).<\/p>\n<p>Things you can try:<\/p>\n<ul>\n<li>Provision an instance with more GPU memory<\/li>\n<li>Decrease batch size<\/li>\n<li>Use a different (smaller) model<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1646760473952,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Challenge_closed_time":1593108137000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593107198000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sage-maker-studio",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":7.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.231772878,
        "Challenge_title":"Notebook Instance Types for SageMaker Studio",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":343.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2627777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Challenge_closed_time":1609955532000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609954586000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.23331388,
        "Challenge_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":6.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579536819712,
        "Answerer_location":null,
        "Answerer_reputation_count":18.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4.6947408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1604344726272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1604343780117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1604344440696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2333479753,
        "Challenge_title":"No Module named pyarrow",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604361341763,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1547395160296,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":30365.0,
        "Answerer_view_count":5514.0,
        "Challenge_adjusted_solved_time":0.2671719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Challenge_closed_time":1594554603092,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594553641273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594581031423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.236787602,
        "Challenge_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1210.0,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.84,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2683527778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have imported packages:\n\nlibrary(dplyr)\n\nUploaded my dataset:\n\nbike <- readRDS(\"bike.rds\")\n\nBut when I try simple \"filter\" it is not working:",
        "Challenge_closed_time":1597097170387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597096204317,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/63901\/simple-filter-is-not-working-in-azure-notebook-for.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2377190333,
        "Challenge_title":"Simple filter is not working in Azure notebook for R",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed.\n\nIt looks azure notebook clean the session after some period of inactivity, there the package dplyr was not loaded after some time",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.2689961111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to organize the node functions by Classes in the nodes.py file. For example, functions related to cleaning data are in the \"CleanData\" Class, with a @staticmethod decorator, while other functions will stay in the \"Other\" Class, without any decorator (the names of these classes are merely representative). In the pipeline file, I tried importing the names of the classes, the names of the nodes and the following way: CleanData.function1 (which gave an error) and none of them worked. How can I call the nodes from the classes, if possible, please?<\/p>",
        "Challenge_closed_time":1573209953536,
        "Challenge_comment_count":2,
        "Challenge_created_time":1573208985150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58764792",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2382261242,
        "Challenge_title":"How to run functions from a Class in the nodes.py file?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I'm not entirely certain what the error you're getting is. If you're literally trying to do <code>from .nodes import CleanData.function1<\/code> that won't work. Imports don't work like that in Python. If you do something like this:<\/p>\n\n<p><code>nodes.py<\/code> has:<\/p>\n\n<pre><code>class CleanData:\n    def clean(arg1):\n        pass\n<\/code><\/pre>\n\n<p>and <code>pipeline.py<\/code> has:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import CleanData\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                CleanData.clean,\n                \"example_iris_data\",\n                None,\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>that should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":68.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2697222222,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Challenge_closed_time":1603286522000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603285551000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2387981539,
        "Challenge_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":33.0,
        "Challenge_word_count":119,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"See the performance efficiency and cost optimization pillars in Machine Learning Lens. Additionally this is an EC2 based right sizing best practices guide.\nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.\nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":6.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.2737619444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I launched ipython session and trying to load a dataset.<br \/>\nI am running<br \/>\ndf = catalog.load(&quot;test_dataset&quot;)<br \/>\nFacing the below error<br \/>\n<code>NameError: name 'catalog' is not defined<\/code><\/p>\n<p>I also tried %reload_kedro but got the below error<\/p>\n<p><code>UsageError: Line magic function `%reload_kedro` not found.<\/code><\/p>\n<p>Even not able to load context either.\nI am running the kedro environment from a Docker container.\nI am not sure where I am going wrong.<\/p>",
        "Challenge_closed_time":1637671420496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637670434953,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70080915",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2419746829,
        "Challenge_title":"kedro context and catalog missing from ipython session",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":617.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>new in 0.17.5 there is a fallback option, please run the following commands in your Jupyter\/IPython session:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext kedro.extras.extensions.ipython\n%reload_kedro &lt;path_to_project_root&gt;\n<\/code><\/pre>\n<p>This should help you get up and running.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":4.1,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":32.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Challenge_closed_time":1604679222000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604678225000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2444700713,
        "Challenge_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":3650.1353344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was looking at <code>iris<\/code> project example provided by kedro. Apart from logging the accuracy I also wanted to save the <code>predictions<\/code> and <code>test_y<\/code> as a csv.<\/p>\n<p>This is the example node provided by kedro.<\/p>\n<pre><code>def report_accuracy(predictions: np.ndarray, test_y: pd.DataFrame) -&gt; None:\n    &quot;&quot;&quot;Node for reporting the accuracy of the predictions performed by the\n    previous node. Notice that this function has no outputs, except logging.\n    &quot;&quot;&quot;\n    # Get true class index\n    target = np.argmax(test_y.to_numpy(), axis=1)\n    # Calculate accuracy of predictions\n    accuracy = np.sum(predictions == target) \/ target.shape[0]\n    # Log the accuracy of the model\n    log = logging.getLogger(__name__)\n    log.info(&quot;Model accuracy on test set: %0.2f%%&quot;, accuracy * 100)\n<\/code><\/pre>\n<p>I added the following to save the data.<\/p>\n<pre><code>data = pd.DataFrame({&quot;target&quot;: target , &quot;prediction&quot;: predictions})\ndata_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)\ndata_set.save(data)\n<\/code><\/pre>\n<p>This works as intended, however, my question is &quot;is it the kedro way of doing thing&quot; ? Can I provide the <code>data_set <\/code> in <code>catalog.yml<\/code> and later save <code>data<\/code> to it? If I want to do it, how do I access the <code>data_set<\/code> from <code>catalog.yml<\/code> inside a node.<\/p>\n<p>Is there a way to save data without creating a catalog inside a node like this <code>data_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)<\/code> ? I want this in <code>catalog.yml<\/code>, if possible and if it follows kedro convention!.<\/p>",
        "Challenge_closed_time":1629898742263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629897723887,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1629897818943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68923747",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":0.2491092827,
        "Challenge_title":"Saving data with DataCatalog",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519724643532,
        "Poster_location":null,
        "Poster_reputation_count":453.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>Kedro actually abstracts this part for you. You don't need to access the datasets via their Python API.<\/p>\n<p>Your <code>report_accuracy<\/code> method does need to be tweaked to return the <code>DataFrame<\/code> instead of <code>None<\/code>.<\/p>\n<p>Your node needs to be defined as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n  func=report_accuracy,\n  inputs='dataset_a',\n  outputs='dataset_b'\n)\n<\/code><\/pre>\n<p>Kedro then looks at your catalog and will load\/save <code>dataset_a<\/code> and <code>dataset_b<\/code> as required:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset_a:\n   type: pandas.CSVDataSet\n   path: xxxx.csv\n\ndataset_b:\n   type: pandas.ParquetDataSet\n   path: yyyy.pq\n<\/code><\/pre>\n<p>As you run the node\/pipeline Kedro will handle the load\/save operations for you. You also don't need to save every dataset if it's only used mid-way in a pipeline, you can read about <code>MemoryDataSet<\/code>s <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/11_tools_integration\/01_pyspark.html#use-memorydataset-for-intermediary-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643038306147,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":14.51,
        "Solution_score_count":7.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1494171603136,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Challenge_adjusted_solved_time":0.2887638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Challenge_closed_time":1644935036627,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644933997077,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1644960360047,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.2536835333,
        "Challenge_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":219.0,
        "Challenge_word_count":225,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":6.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1357747047007,
        "Answerer_location":"Moscow",
        "Answerer_reputation_count":3962.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":0.0331805555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>The <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow\">documentation<\/a> for the Azure Machine Learning Python script module describes using a ZIP file containing code as a resource, but I don't see how to create and upload such a ZIP file in the first place.<\/p>\n\n<p>How do get my custom Python code into Azure Machine Learning for use as a ZIP resource?<\/p>",
        "Challenge_closed_time":1463238158720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463237105637,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1463238039270,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37228027",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2565961652,
        "Challenge_title":"How do get my custom Python code into Azure Machine Learning for use a a ZIP resource?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":664.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299959670312,
        "Poster_location":"United States",
        "Poster_reputation_count":41475.0,
        "Poster_view_count":1912.0,
        "Solution_body":"<p>Just upload it as a dataset. <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow\">Reference.<\/a> (search for it, as it is not on the first page).<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-walkthrough-2-upload-data\/#upload-the-dataset-to-machine-learning-studio\" rel=\"nofollow\">Reference<\/a> on how to upload the dataset. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.6,
        "Solution_reading_time":6.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2939025,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI'm aware that running compute Instance costs us for the number of hours we used. So, we stop it when ever we don't need it.\n\nSimilarly, does compute cluster & Endpoints also costs us if we do not delete them after use?\n\n\n\n\nThanks\nBhaskar",
        "Challenge_closed_time":1617896011616,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617894953567,
        "Challenge_favorite_count":7.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349617\/does-compute-cluster-amp-endpoints-costs-if-we-don.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.1,
        "Challenge_reading_time":3.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2576628455,
        "Challenge_title":"Does compute cluster & Endpoints costs if we dont delete after use ?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @Bhaskar11-9991\n\nWhen a compute cluster is idle, it autoscales to 0 nodes, so you don't pay when it's not in use. A compute instance is always on and doesn't autoscale. You should stop the compute instance when you aren't using it to avoid extra cost.\nrefer - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.02458,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I currently work with Kedro (from quantum black <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html<\/a>) as a framework for deployment oriented framework to code collaboratively. It is a great framework to develop machine learning in a team.<\/p>\n<p>I am looking for an R equivalent.<\/p>\n<p>My main issue is that I have teams of data scientists that develop in R, but each team is developing in different formats.<\/p>\n<p>I wanted to make them follow a common framework to develop deployment ready R code, easy to work on in 2 or 3-people teams.<\/p>\n<p>Any suggestions are welcome<\/p>",
        "Challenge_closed_time":1639581358648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639580284993,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639581270160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70365836",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2610075754,
        "Challenge_title":"Is there a package in R that mimics KEDRO as a modular collaborative framework for development?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460063521156,
        "Poster_location":"S\u00e3o Paulo - SP, Brasil",
        "Poster_reputation_count":2472.0,
        "Poster_view_count":186.0,
        "Solution_body":"<p>member of the Kedro team here. We've heard good things about the <a href=\"https:\/\/github.com\/ropensci\/targets\" rel=\"nofollow noreferrer\">Targets<\/a> library doing similar things in the R world.<\/p>\n<p>It would be remiss for me to not try and covert you and your team to the dark side too :)<\/p>\n<p>Before Kedro our teams internally were writing a mix of Python, SQL, Scala and R. Part of the drive to write the framework was to get our teams internally speaking the same language. Python felt like the best compromise available at the time and I'd argue this still holds. We also had trouble productionising R projects and felt Python is more manageable in that respect.<\/p>\n<p>Whilst not officially documented - I've also seen some people on the Kedro <a href=\"https:\/\/discord.com\/channels\/778216384475693066\/778998585454755870\/901111920290070588\" rel=\"nofollow noreferrer\">Discord play with r2py<\/a> so that they can use specific R functionality within their Python pipelines.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":141.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1597855076910,
        "Answerer_location":"Delft, Netherlands",
        "Answerer_reputation_count":60.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.3015636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Challenge_closed_time":1608634938236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608633852607,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2635663195,
        "Challenge_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":510.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597855076910,
        "Poster_location":"Delft, Netherlands",
        "Poster_reputation_count":60.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1429262032907,
        "Answerer_location":"Manchester, United Kingdom",
        "Answerer_reputation_count":11490.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.3111347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Challenge_closed_time":1633709785048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633708664963,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1634116759467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2708929624,
        "Challenge_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623330365063,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3139213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>following the answers to this question <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a> I tried to load data from S3 bucket to SageMaker Jupyter Notebook.<\/p>\n<p>I used this code:<\/p>\n<pre><code>import pandas as pd\n\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n<\/code><\/pre>\n<p>I replaced <code>'my-bucket'<\/code> by the ARN (Amazon Ressource name) of my S3 bucket (e.g. &quot;<code>arn:aws:s3:::name-of-bucket<\/code>&quot;) and replaced <code>'train.csv'<\/code> by the csv-filename which is stored in the S3 bucket. Regarding the rest I did not change anything at all. What I got was this <code>ValueError<\/code>:<\/p>\n<pre><code>ValueError: Failed to head path 'arn:aws:s3:::name-of-bucket\/name_of_file_V1.csv': Parameter validation failed:\nInvalid bucket name &quot;arn:aws:s3:::name-of-bucket&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>What did I do wrong? Do I have to modify the name of my S3 bucket?<\/p>",
        "Challenge_closed_time":1613558457267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613557327150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66239966",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":19.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.2730160925,
        "Challenge_title":"Loading data from S3 bucket to SageMaker Jupyter Notebook - ValueError - Invalid bucket name",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":345.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>The path should be:<\/p>\n<pre><code>data_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n<p>where <code>bucket<\/code> is <code>&lt;bucket-name&gt;<\/code> <strong>not ARN<\/strong>. For example <code>bucket=my-bucket-333222<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":3.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3159602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am designing a learning management system and inflow for the website is more in some cases and  less in another time. I would like to know about the getting the vCPU's which are scaled up to make it down after the stipulated time. I found a document regarding <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/best-practices\/auto-scaling\" rel=\"nofollow noreferrer\">scaling up<\/a> but didn't find a way to scale it down.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1655984305567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983168110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729267",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2745666484,
        "Challenge_title":"Degrading the services automatically by autoscaling in azure services - vCPU",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652123310643,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>There is a chance of auto scaling for the normal services in azure cloud services, that means for stipulated time you can increase or decrease as mentioned in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cloud-services\/cloud-services-how-to-scale-portal\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>When it comes for vCPU which is cannot be performed automatically. vCPU can be scaled up based on the request criteria and in the same manner we need to request the support team to scale those down to the normal.<\/p>\n<p><strong>There is no specific procedure to make the auto scaling for vCPU operations. We can increase the capacity of core, but to reduce to the normal, we need to approach the support system for manual changing. You can change it from 10 cores to next level 16 cores, but cannot be performed automatic scaling down from 16 cores to 10 cores.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":11.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Kubernetes Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":14.6893280556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Challenge_closed_time":1589225919163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589224779627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2750053952,
        "Challenge_title":"Is there IO functionality to store trained models in kedro?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":795.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1589277661208,
        "Solution_link_count":5.0,
        "Solution_readability":16.1,
        "Solution_reading_time":21.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":137.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.3237788889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Challenge_closed_time":1628038438487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628037272883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1628038626927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2804904412,
        "Challenge_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1312330872123,
        "Answerer_location":null,
        "Answerer_reputation_count":20988.0,
        "Answerer_view_count":1015.0,
        "Challenge_adjusted_solved_time":0.324335,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to set up <a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"nofollow noreferrer\"><code>sacred<\/code><\/a> for Python and I am going through the <a href=\"http:\/\/sacred.readthedocs.org\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">tutorial<\/a>. I was able to set up sacred using <code>pip install sacred<\/code> with no issues. I am having trouble running the basic code:<\/p>\n\n<pre><code>from sacred import Experiment\n\nex = Experiment(\"hello_world\")\n<\/code><\/pre>\n\n<p>Running this code returns the a <code>ValueError<\/code>:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-25-66f549cfb192&gt; in &lt;module&gt;()\n      1 from sacred import Experiment\n      2 \n----&gt; 3 ex = Experiment(\"hello_world\")\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/experiment.pyc in __init__(self, name, ingredients)\n     42         super(Experiment, self).__init__(path=name,\n     43                                          ingredients=ingredients,\n---&gt; 44                                          _caller_globals=caller_globals)\n     45         self.default_command = \"\"\n     46         self.command(print_config, unobserved=True)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/ingredient.pyc in __init__(self, path, ingredients, _caller_globals)\n     48         self.doc = _caller_globals.get('__doc__', \"\")\n     49         self.sources, self.dependencies = \\\n---&gt; 50             gather_sources_and_dependencies(_caller_globals)\n     51 \n     52     # =========================== Decorators ==================================\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in gather_sources_and_dependencies(globs)\n    204 def gather_sources_and_dependencies(globs):\n    205     dependencies = set()\n--&gt; 206     main = Source.create(globs.get('__file__'))\n    207     sources = {main}\n    208     experiment_path = os.path.dirname(main.filename)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in create(filename)\n     61         if not filename or not os.path.exists(filename):\n     62             raise ValueError('invalid filename or file not found \"{}\"'\n---&gt; 63                              .format(filename))\n     64 \n     65         mainfile = get_py_file_if_possible(os.path.abspath(filename))\n\nValueError: invalid filename or file not found \"None\"\n<\/code><\/pre>\n\n<p>I am not sure why this error is returning. The documentation does not say anything about setting up an Experiment file prior to running the code. Any help would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1459298810363,
        "Challenge_comment_count":5,
        "Challenge_created_time":1459297642757,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1505634379180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36297520",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":16.5,
        "Challenge_reading_time":32.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.2809104467,
        "Challenge_title":"Using Sacred Module with iPython",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1818.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435168587747,
        "Poster_location":"Santa Monica, CA",
        "Poster_reputation_count":308.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>The traceback given indicates that the constructor for <code>Experiment<\/code> searches its namespace to find the file in which its defined.<\/p>\n\n<p>Thus, to make the example work, place the example code into a file and run that file directly.<\/p>\n\n<p>If you are using <code>ipython<\/code>, then you could always try using the <code>%%python<\/code> command, which will effectively capture the code you give it into a file before running it (in a separate python process).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":5.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":72.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3261111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda.\n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Challenge_closed_time":1605020138000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605018964000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":10.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2822506824,
        "Challenge_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.3264786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm just testing out AWS Sagemaker notebook and created an endpoint using a partial script below:<\/p>\n\n<pre><code>endpoint_name = 'engine' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nendpoint_config_name = 'engine_config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmodel_name = 'engine_model' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\nwhile status=='Creating':\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(\"Status: \" + status)\n\n<\/code><\/pre>\n\n<p>I'm trying to remove that endpoint by using:\nsm_client.delete_endpoint(EndpointName=endpoint_name)<\/p>\n\n<p>However, it didn't work because I naively used timestamps for the endpoint_name and I didn't remember them. The original variable values were overriden when I re-run the code. As a result, I can't delete the existing endpoint.\nI went to the Sagemaker management dashboard --> inference --> endpoints, but it's empty. I don't even know if I'm currently having any active endpoints or not. Please advise how to delete my endpoint in this case. Thank you in advance.<\/p>",
        "Challenge_closed_time":1574200291200,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574199115877,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1574275275427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58943117",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2825277702,
        "Challenge_title":"Confirming endpoints were deleted in SageMaker notebook",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":661.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521994074812,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":338.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p><strong>If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.<\/strong><\/p>\n\n<p>If this is the case, your Endpoints tab should look like the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Endpoint Configurations<\/strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the \"Endpoint Configurations\" tab. You do not need to remove these configurations when tearing down an endpoint.<\/p>\n\n<p><strong>Important note:<\/strong> Double check that you are checking in the console for the <em>region you would have deployed to<\/em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1<\/code>, but check the SageMaker console for <code>us-west-2<\/code>, it would not be displaying endpoints from the other region.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":148.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3284980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using .NII file format which represents, the neuroimaging dataset. I need to use Auto ML to label the dataset of images that are nearly 2GB in size per patient. The main issue is with using Auto ML to label the dataset of images with.NII file extension and classify whether the patient is having dementia or not.<\/p>\n<p><strong>Requirement:<\/strong> Forget about the problem domain of implementation like dementia. I would like to know about the procedure of using Auto ML for Computer vision applications through ML studio to use.NII file format dataset images.<\/p>\n<p>Any help would be thankful.<\/p>",
        "Challenge_closed_time":1652091549923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652090367330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72170169",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2840490226,
        "Challenge_title":"Implementing Computer Vison on AutoML to classify dementia using MRI images in .NII file format",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>The requirement of using .nii or other file formats in Azure auto ML is a challenging task. Unfortunately, Auto ML image input format will be using in only JSON format. Kindly check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-automl-images-schema\" rel=\"nofollow noreferrer\">document<\/a><\/p>\n<p>Answering regarding requirement of .nii format of dataset, there are different file format convertors available like &quot;<em><strong><a href=\"http:\/\/medicalimageconverter.com\/?msclkid=403b597ecf8111ecac65b3422c7b95b5\" rel=\"nofollow noreferrer\">Medical Image Convertor<\/a><\/strong><\/em>&quot;. This software is commercial and can be used for 10days for free. Convert .nii file formats into JPG and proceed with the general documentation provided in the top of the answer.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3294444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Challenge_closed_time":1605281179000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605279993000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.284761144,
        "Challenge_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":149,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0127177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Amazon SageMaker and I am closely following this tutorial <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/<\/a> to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker<\/p>\n\n<p>when I run the following command on terminal (<strong>Step 2 of the Tutorial<\/strong> )<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name &lt;endpoint-name&gt; \\\n  --body '{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}' \\\n  --content-type application\/json \\\n  --accept application\/json \\\n  results\n<\/code><\/pre>\n\n<p>I get the following <strong>Error:<\/strong> <code>Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\"<\/code>\nMy endpoint is <code>InService<\/code> on the SageMaker console and the example Jupyter notebook run successfully. (I also substituted <code>&lt;endpoint-name&gt;<\/code> with the actual name - same error received with\/without quotations around the name) <\/p>\n\n<p>Using <strong>zsh<\/strong> here is the aws cli version:<\/p>\n\n<pre><code>aws --version\naws-cli\/2.0.15 Python\/3.7.4 Darwin\/19.4.0 botocore\/2.0.0dev19\n<\/code><\/pre>\n\n<p>Wondering what the problem could be. Any help is appreciated<\/p>",
        "Challenge_closed_time":1590314994627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590313799700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1590314948843,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61984217",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":21.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2866246386,
        "Challenge_title":"Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\" when testing Amazon SageMaker model endpoint using the AWS CLI",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1117.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578221300168,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":117.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>The problem is that the body contents is being expected to be base 64 encoded, try base64 encoding the body before passing it to the invoke statement.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.6,
        "Solution_reading_time":1.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1486312694643,
        "Answerer_location":"M\u00fcnchen, Germany",
        "Answerer_reputation_count":668.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":7705.2959355556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Challenge_closed_time":1601639323903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601638106830,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1601641259400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2912326368,
        "Challenge_title":"Pyathena is super slow compared to querying from Athena",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3188.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486312694643,
        "Poster_location":"M\u00fcnchen, Germany",
        "Poster_reputation_count":668.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1629380324768,
        "Solution_link_count":2.0,
        "Solution_readability":16.7,
        "Solution_reading_time":10.36,
        "Solution_score_count":13.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3388152778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hey all,<\/p>\n<p>I wondered if there was a way to track system power consumption caused by model development? I\u2019ve checked the W&amp;B docs and can\u2019t see anything.<br>\nIdeally I\u2019d love to be able to keep track of runs to see how much power is used by different runs but also the whole project.<\/p>\n<p>Elsewhere I\u2019ve seen packages such as <a href=\"https:\/\/pypi.org\/project\/energyusage\/\" rel=\"noopener nofollow ugc\">energyusage<\/a> but ideally would like to use something more integrated and could be aggregated across runs for whole projects.<br>\nIf something already exists I\u2019d love to hear about it, otherwise either if W&amp;B fancied adding this functionality that would be great or if it came to it if anyone would like to help me with this project.<\/p>\n<p>Thanks,<\/p>\n<p>Jeff.<\/p>",
        "Challenge_closed_time":1658218095948,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658216876213,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/track-power-energy-consumption\/2774",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":10.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2917851019,
        "Challenge_title":"Track power\/energy consumption?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":124,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nWe have this example that shows how to do this with CodeCarbon.<br>\n<a href=\"https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases--VmlldzoxMzM1NDg3\">https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases\u2013VmlldzoxMzM1NDg3<\/a><\/p>\n<p>You would use that library and log the info yourself. I do appreciate the feature request to integrate these more tightly.<br>\nThanks, hope this helps<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":7.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3402777778,
        "Challenge_answer_count":1,
        "Challenge_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Challenge_closed_time":1592313864000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592312639000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.2928768893,
        "Challenge_title":"SageMaker Model Spend",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":4.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.3402952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say we have multiple long running pipeline nodes.\nIt seems quite straight forward to checkpoint or cache the intermediate results, so when nodes after a checkpoint are changed or added only these nodes must be executed again.<\/p>\n\n<p>Does Kedro provide functionality to make sure, that when I run the pipeline only those steps are \nexecuted that have changed?\nAlso the reverse, is there a way to make sure, that all steps that have changed are executed?<\/p>\n\n<p>Let's say a pipeline producing some intermediate result changed, will it be executed, when i execute a pipeline depending on the output of the first?<\/p>\n\n<p><strong>TL;DR:<\/strong> Does Kedro have <code>makefile<\/code>-like tracking of what needs to be done and what not?<\/p>\n\n<p>I think my question is similar to <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/341\" rel=\"nofollow noreferrer\">issue #341<\/a>, but I do not require support of cyclic graphs.<\/p>",
        "Challenge_closed_time":1591362515296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591361290233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62215724",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2928899463,
        "Challenge_title":"Does Kedro support Checkpointing\/Caching of Results?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1494502461176,
        "Poster_location":null,
        "Poster_reputation_count":83.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You might want to have a look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.IncrementalDataSet.html\" rel=\"nofollow noreferrer\">IncrementalDataSet<\/a> alongside the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">partitioned dataset<\/a> documentation, specifically the section on <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#incremental-loads-with-incrementaldataset\" rel=\"nofollow noreferrer\">incremental loads with the incremental dataset<\/a> which has a notion of \"checkpointing\", although checkpointing is a manual step and not automated like <code>makefile<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":23.6,
        "Solution_reading_time":9.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":51.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":0.3459813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Challenge_closed_time":1656143487270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656142241737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656265981920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2971234042,
        "Challenge_title":"Load existing data catalog programmatically",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1258185382660,
        "Poster_location":null,
        "Poster_reputation_count":333.0,
        "Poster_view_count":33.0,
        "Solution_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1656143791536,
        "Solution_link_count":4.0,
        "Solution_readability":19.5,
        "Solution_reading_time":23.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":135.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1361156607787,
        "Answerer_location":"Bergen, Norway",
        "Answerer_reputation_count":6168.0,
        "Answerer_view_count":334.0,
        "Challenge_adjusted_solved_time":0.3461716667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS SageMaker domain in my account created via Terraform. The resource was modified outside of Terraform. The modification was the equivalent of the following:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: { &quot;CustomImages&quot;: [ { ... } ] } }'\n<\/code><\/pre>\n<p>Ever since, all <code>terraform plan<\/code> operations want to replace the AWS SageMaker domain:<\/p>\n<pre><code>  # module.main.aws_sagemaker_domain.default must be replaced\n-\/+ resource &quot;aws_sagemaker_domain&quot; &quot;default&quot; {\n      ~ arn                                            = &quot;arn:aws:sagemaker:eu-central-1:000111222333:domain\/d-domainid123&quot; -&gt; (known after apply)\n      ...\n        # (6 unchanged attributes hidden)\n      ~ default_user_settings {\n            # (2 unchanged attributes hidden)\n          - kernel_gateway_app_settings { # forces replacement\n               - custom_images = [ ... ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>My goal is to reconcile the situation without Terraform or me needing to create a new domain. I can't modify the Terraform sources to match the state of the SageMaker domain because that would force the recreation of domains in other accounts provisioned from the same Terraform source code.<\/p>\n<p><strong>I want to issue an <code>aws<\/code> CLI command that updates the domain and removes the <code>&quot;KernelGatewayAppSettings&quot;: { ... }<\/code> key completely from the <code>&quot;DefaultUserSettings&quot;<\/code> of the SageMaker domain. Is there a way to do this?<\/strong><\/p>\n<p>I tried the following, but the empty object is still there, so they did not work.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: {} }'\naws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: null }'\n\n# Still:\naws sagemaker describe-domain --domain-id d-domainid123\n{\n    &quot;DomainArn&quot;: ...,\n    &quot;DomainId&quot;: ...,\n    ...\n    &quot;DefaultUserSettings&quot;: {\n        &quot;ExecutionRole&quot;: &quot;arn:aws:iam::0001112233444:role\/SageMakerStudioExecutionRole&quot;,\n        &quot;SecurityGroups&quot;: [\n            &quot;...&quot;\n        ],\n        &quot;KernelGatewayAppSettings&quot;: {\n            &quot;CustomImages&quot;: []\n        }\n    },\n    ...\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1662394449848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662393203630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662455073636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73611956",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":31.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.2972647615,
        "Challenge_title":"Remove JSON object via AWS Update* API to prevent Terraform from recreating the resource",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1219760368600,
        "Poster_location":"Miskolc, Hungary",
        "Poster_reputation_count":3743.0,
        "Poster_view_count":178.0,
        "Solution_body":"<p>One option you have is to use the <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/lifecycle\" rel=\"nofollow noreferrer\">lifecycle meta argument<\/a> to ignore out-of-band changes to the resource.<\/p>\n<pre><code>  lifecycle {\n    ignore_changes = [\n      default_user_settings\n    ]\n  }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":3.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3468794445,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to connect my MSA with my certification profile.\n\nMy MSA is julia.loef@hotmail.com and my work account is julia.loef-bleiksch@nl.abnamro.com\n\nCan you please help me with this?",
        "Challenge_closed_time":1661435443443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661434194677,
        "Challenge_favorite_count":11.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/981835\/a-request-to-change-the-personal-account-msa-assoc.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2977903942,
        "Challenge_title":"a request to change the personal account (MSA) associated with my Certification profile",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @JuliaLoefBleiksch-4662\n\nPlease post this on Microsoft Certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp and someone will gladly assist.\n\nUnfortunately MS Certifications is not supported on this forum.\n\nIf this was helpful please accept answer.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1340063804276,
        "Answerer_location":null,
        "Answerer_reputation_count":298.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":0.3479,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So our team created a new Azure <strong>Machine Learning<\/strong> resource, but whenever I try to add a new notebook and try to edit it using &quot;JUPYTERLAB&quot; i get <code>ERR_HTTP2_PROTOCOL_ERROR<\/code> error, but the same notebook, when edited using <code>EDIT IN JUPYTER<\/code> works perfectly.<\/p>\n<p>This is a blank and clean notebook, I also tried 2 different laptops and multiple browsers per laptop, same error. I also tried incognito and clearing cookies, but to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IevSG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IevSG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>update: I seem to have accidentally replicated the issue and I now know what is causing it, the situation is that Im using my work laptop and constantly switching VPN connections, and some times, connecting to the AZURE PORTAl OUTSIDE the VPN. So, when you've worked on a notebook while inside a VPN, then you disconnected, and tried loading the notebook sometime later, you will encounter this<\/p>",
        "Challenge_closed_time":1596643705983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596642453543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597056501367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63268849",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":14.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2985478258,
        "Challenge_title":"ERR_HTTP2_PROTOCOL_ERROR when opening Notebook in JUPYTERLAB Azure ML Studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340063804276,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;<\/p>\n<p><a href=\"https:\/\/imgur.com\/aRB8GWS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/imgur.com\/aRB8GWS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IceQO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IceQO.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1596669031916,
        "Solution_link_count":4.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0890694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Challenge_closed_time":1593895412640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593894159730,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593895091990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2986446796,
        "Challenge_title":"How to update pandas version in SageMaker notebook terminal?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1158.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.3501991667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to get AWS SageMaker to call AWS Comprehend. I'm getting this message in SageMaker:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the StartTopicsDetectionJob operation: User:\n  arn:aws:sts::545176143103:assumed-role\/access-aws-services-from-sagemaker\/SageMaker\n  is not authorized to perform: iam:PassRole on resource:\n  arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>When creating the Jupyter notebook, I used this role:<\/p>\n\n<blockquote>\n  <p>arn:aws:sagemaker:us-east-2:545176143103:notebook-instance\/access-comprehend-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>...with the following policies attached:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/t6feB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t6feB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm using the same IAM role in SageMaker:<\/p>\n\n<pre><code> data_access_role_arn = \"arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker\"\n<\/code><\/pre>\n\n<p>It looks like I'm giving the role all the access it needs. How can I correct this error?<\/p>",
        "Challenge_closed_time":1556151395660,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556150134943,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1586235828680,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55840023",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3002521125,
        "Challenge_title":"IAM Roles for Sagemaker?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":10244.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>Based on your error, it looks like there's a permissions issue with the SageMaker notebook trying to change IAM settings from within a notebook that does not explicitly have permission to do so.<\/p>\n\n<hr>\n\n<p>You have a few options here to remedy this:<\/p>\n\n<p><strong>Option 1: Granting the SageMaker notebook permissions to define IAM role within the notebook during runtime.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can add policies such as <code>IAMFullAccess<\/code> or <code>IAMReadOnlyAccess<\/code>. This should solve for the permissions error when you try to attach an IAM role from within the notebook.<\/p>\n\n<p><strong>Option 2: Explicitly define the permissions you want SageMaker to have in the console.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can directly add policies for resource permissions (such as Comprehend). Without attaching explicit IAM access policies to this role, you wouldn't be able to change permissions during runtime.<\/p>\n\n<p><strong>Option 3: Both<\/strong><\/p>\n\n<p>If you'd like to pre-define access for some resources, but also potentially add other resource permissions during experimentation, you can do both steps 1 and 2 (Add IAM + other resource permissions to the hosted notebook in console, with the ability to change your SageMaker IAM role inline during experimentation).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1556152415310,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":19.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":222.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935217000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664933940000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":558.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":42.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3035964312,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":18.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.3555730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1641488156140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641486876077,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3042242842,
        "Challenge_title":"Why doesn't my Kedro starter prompt for input?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416091573812,
        "Poster_location":"Texas, USA",
        "Poster_reputation_count":197.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":38.0,
        "Solution_reading_time":7.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":21.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.4647708333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Challenge_closed_time":1611661436392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611660152193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3050714552,
        "Challenge_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":294.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500383313376,
        "Poster_location":"London, UK",
        "Poster_reputation_count":851.0,
        "Poster_view_count":86.0,
        "Solution_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611661825368,
        "Solution_link_count":4.0,
        "Solution_readability":12.2,
        "Solution_reading_time":16.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":129.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Challenge_closed_time":1533318766000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533317474000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3066673724,
        "Challenge_title":"Sagemaker and Data on Databases",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":208.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.8,
        "Solution_reading_time":9.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.361325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I don't know where else to ask this question so would appreciate any help or feedback. I've been reading the SDK documentation for azure machine learning service (in particular <code>azureml.core<\/code>). There's a class called <code>Pipeline<\/code> that has methdods <code>validate()<\/code> and <code>publish()<\/code>. Here are the docs for this:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py<\/a><\/p>\n<p>When I call <code>validate()<\/code>, everything validates and I call publish but it seems to only create an API endpoint in the workspace, it doesn't register my pipeline under Pipelines and there's obviously nothing in the designer.<\/p>\n<p>My question: I want to publish my pipeline so I just have to launch from the workspace with one click. I've built it already using the SDK (Python code). I don't want to work with an API. Is there any way to do this or would I have to rebuild the entire pipeline using the designer (drag and drop)?<\/p>",
        "Challenge_closed_time":1595468157440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1595466856670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1595544102140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63045395",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":16.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3084584902,
        "Challenge_title":"Machine learning in Azure: How do I publish a pipeline to the workspace once I've already built it in Python using the SDK?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595292020127,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines<\/code> still confused me initially because:<\/p>\n<ul>\n<li>what the SDK calls a <code>PublishedPipeline<\/code> is called as a <code>Pipeline Endpoint<\/code> in the Studio UI, and<\/li>\n<li>it is semi-related to <code>Dataset<\/code> and <code>Model<\/code>'s <code>.register()<\/code> method, but fundamentally different.<\/li>\n<\/ul>\n<p><code>TL;DR<\/code>: all <code>Pipeline.publish()<\/code> does is create an endpoint that you can use to:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\" rel=\"nofollow noreferrer\">schedule<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb\" rel=\"nofollow noreferrer\">version<\/a> Pipelines, and<\/li>\n<li>re-run the pipeline from other services via a REST API call (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\" rel=\"nofollow noreferrer\">via Azure Data Factory<\/a>).<\/li>\n<\/ul>\n<p>You can see <code>PublishedPipelines<\/code> in the Studio UI in two places:<\/p>\n<ul>\n<li>Pipelines page :: Pipeline Endpoints tab<\/li>\n<li>Endpoints page :: Pipeline Endpoints tab<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595468750207,
        "Solution_link_count":5.0,
        "Solution_readability":17.5,
        "Solution_reading_time":22.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.3643536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Challenge_closed_time":1619085494700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619084183027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3106807715,
        "Challenge_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":32.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":227.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":0.365185,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Sagemaker instance that's linked to a github repo <code>my-repo<\/code>, and every time I open a new terminal, I see this immediately at startup: <\/p>\n\n<pre><code>sh-4.2$ cd \"my-repo\"\nsh: cd: my-repo: No such file or directory\n<\/code><\/pre>\n\n<p>I assumed something was in the .bashrc or .bash_profile that prompted this (failed) <code>cd<\/code> but it's not in there. Any ideas where I should look for what's causing this behavior? <\/p>",
        "Challenge_closed_time":1567699632856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567698318190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57808963",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3112899506,
        "Challenge_title":"Sagemaker instance does automatic cd at startup",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":304.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386023479736,
        "Poster_location":null,
        "Poster_reputation_count":725.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>The issue is not specific to SageMaker Notebook instances. Rather, it is a bug in the Git extension of JupyterLab. You can find details around this here: <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346\" rel=\"nofollow noreferrer\">https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":4.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3654761111,
        "Challenge_answer_count":1,
        "Challenge_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Challenge_closed_time":1652798353412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652797037698,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3115031672,
        "Challenge_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":327.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like Kernel: Idle | Instance MEM. You can click on that text to show the kernel and instance usage metrics.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1603263564903,
        "Answerer_location":null,
        "Answerer_reputation_count":488.0,
        "Answerer_view_count":59.0,
        "Challenge_adjusted_solved_time":0.3718936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to read multiple csv files from S3 bucket with boto3 in python and finally combine those files in single dataframe in pandas.However, in some of the folders there are some empty files which results in the error &quot;No columns to parse from file&quot;. Can we skip those empty files in the below codes?<\/p>\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\n    prefix_df = []\n\nfor obj in prefix_objs:\n    key = obj.key\n    body = obj.get()['Body'].read()\n    temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n    prefix_df.append(temp)\n<\/code><\/pre>\n<p>I have used this ans [https:\/\/stackoverflow.com\/questions\/52855221\/reading-multiple-csv-files-from-s3-bucket-with-boto3][1]<\/p>",
        "Challenge_closed_time":1613737607800,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613736268983,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66277250",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":10.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3161919834,
        "Challenge_title":"Reading multiple CSV files from S3 using Python with Boto3",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3048.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486529134640,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\nprefix_df = []\n\nfor obj in prefix_objs:\n    try:\n        key = obj.key\n        body = obj.get()['Body'].read()\n        temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n        prefix_df.append(temp)\n    except:\n        continue\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3852777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Challenge_closed_time":1598914035000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598912648000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3259006811,
        "Challenge_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1403804251956,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":609.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":0.3923111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Challenge_closed_time":1603941358963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603939946643,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":31.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.3309650363,
        "Challenge_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":827.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524603494496,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.4,
        "Solution_reading_time":4.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.3938825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1589565622150,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.332093019,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":14416.4976566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Challenge_closed_time":1609345139523,
        "Challenge_comment_count":2,
        "Challenge_created_time":1609343679217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609427009848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.3404931103,
        "Challenge_title":"Can ClearML (formerly Trains) work a local server?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383083414230,
        "Poster_location":null,
        "Poster_reputation_count":2801.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661326401412,
        "Solution_link_count":4.0,
        "Solution_readability":14.4,
        "Solution_reading_time":35.39,
        "Solution_score_count":6.0,
        "Solution_sentence_count":20.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":265.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4162452778,
        "Challenge_answer_count":1,
        "Challenge_body":"my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.",
        "Challenge_closed_time":1644316889220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644315390737,
        "Challenge_favorite_count":19.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/726898\/azure-subscription-cost.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.3480091991,
        "Challenge_title":"Azure Subscription Cost",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you want to review your costs and what resources are being charged, then the Cost Analysis blade will allow you to drill down work this out. Please let us know if this helps",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":2.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.4167916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Challenge_closed_time":1616771138300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616769637850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66819026",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3483949257,
        "Challenge_title":"Output model metrics to Cloudwatch",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":475.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1533753580750,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":31.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":31.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":239.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1326951949648,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":788.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":0.4201786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to have a multi-node Dask cluster be the compute for a <code>PythonScriptStep<\/code> with AML Pipelines?<\/p>\n<p>We have a <code>PythonScriptStep<\/code> that uses <code>featuretools<\/code>'s, deep feature synthesis (<code>dfs<\/code>) (<a href=\"https:\/\/docs.featuretools.com\/en\/stable\/generated\/featuretools.dfs.html\" rel=\"nofollow noreferrer\">docs<\/a>). <code>ft.dfs()<\/code> has a param, <code>n_jobs<\/code> which allows for parallelization. When we run on a single machine, the job takes three hours, and runs much faster on a Dask. How can I operationalize this within an Azure ML pipeline?<\/p>",
        "Challenge_closed_time":1596823714436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596822201793,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63306816",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3507826462,
        "Challenge_title":"Use a Dask Cluster in a PythonScriptStep",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":362.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>We've been working and recently released a <code>dask_cloudprovider.AzureMLCluster<\/code> that might be of interest to you: <a href=\"https:\/\/github.com\/dask\/dask-cloudprovider\" rel=\"noreferrer\">link to repo<\/a>. You can install it via <code>pip install dask-cloudprovider<\/code>.<\/p>\n<p>The <code>AzureMLCluster<\/code> instantiates Dask cluster on AzureML service with elasticity of scaling up to 100s of nodes should you require that. The only required parameter is the <code>Workspace<\/code> object, but you can pass your own <code>ComputeTarget<\/code> should you choose to.<\/p>\n<p>An example of how to use it you can <a href=\"https:\/\/github.com\/drabastomek\/GTC\/blob\/master\/SJ_2020\/workshop\/1_Setup\/Setup.ipynb\" rel=\"noreferrer\">found here<\/a>. In this example I use my custom GPU\/RAPIDS docker image but you can use any images within the <code>Environment<\/code> class.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":11.44,
        "Solution_score_count":6.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Kubernetes Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":101.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.4228347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Challenge_closed_time":1618568305232,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618566783027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1618585069787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":25.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3526511649,
        "Challenge_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1216.0,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.4279386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1568928522076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3562318736,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1356359759000,
        "Answerer_location":"Dubai - United Arab Emirates",
        "Answerer_reputation_count":436.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":0.4312602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The code below works fine in a sagemaker notebook in the cloud. Locally I also have aws credentials created via the aws cli. Personally, I do not like notebooks (unless I do some EDA or the like). So I wonder if one can also fire this code up from a local machine (e.g. in visual studio code) as it only tells sagemaker what to do anyway? I guess it is only a question of authenticating and getting the session object? Thanks!<\/p>\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker import image_uris\n\nregion_name = boto3.session.Session().region_name\ns3_bucket_name = 'bucket_name'\n# this image cannot be used below !!! there must be an issue with sagemaker ?\ntraining_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\nrole = get_execution_role()\ns3_prefix = 'my_model'\ntrain_file_name = 'sagemaker_train.csv'\nval_file_name = 'sagemaker_val.csv'\nsagemaker_session = sagemaker.Session()\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>",
        "Challenge_closed_time":1650001048760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649999496223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651138032863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71880336",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.3585553693,
        "Challenge_title":"can one run sagemaker notebook code locally in visual studio code",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On a local machine,<\/p>\n<ul>\n<li>Make sure to install AWS CLI: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html<\/a><\/li>\n<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/li>\n<li>Set these credentials using <code>aws configure<\/code> command locally.<\/li>\n<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4346333334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi , i would like to know if it is possible to change the location of AzureML workspace after creating it ?\nRight now i do not find any option to change it manually on the UI. We want to move the server location to a different country.\nAny leads would be helpful. Thanks",
        "Challenge_closed_time":1643796022067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643794457387,
        "Challenge_favorite_count":9.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/719406\/change-location-of-azure-ml-workspace.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":3.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3609092998,
        "Challenge_title":"change location of Azure ML workspace?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Based on the below document, ML workspace can't be moved across region. Probably, you will have to create a new resource in target region and move artifacts \/ pipelines \/ child resources to it (not so familiar with ML)\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations\n\nPlease don't forget to Accept Answer and Up-vote if the response helped -- Vaibhav",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":6.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1297232105368,
        "Answerer_location":null,
        "Answerer_reputation_count":27164.0,
        "Answerer_view_count":3016.0,
        "Challenge_adjusted_solved_time":0.6067841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Challenge_closed_time":1653863593763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653862009460,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":40.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":0.364701561,
        "Challenge_title":"Questions on json and GCP",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632597456472,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653864193883,
        "Solution_link_count":3.0,
        "Solution_readability":19.8,
        "Solution_reading_time":27.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":135.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4416666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217619000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565216029000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3657998517,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1506221099907,
        "Answerer_location":"Kolkata, West Bengal, India",
        "Answerer_reputation_count":374.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":0.4425261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Challenge_closed_time":1650898428827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650896835733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":13.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.3663958205,
        "Challenge_title":"How to prevent Weights & Biases from saving best model parameters",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Poster_created_time":1514243890900,
        "Poster_location":null,
        "Poster_reputation_count":167.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":3.69,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4425733334,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have some variables in my sweeps that i want to be able to group by at the same time in my charts.<\/p>\n<p>In this specific case it\u2019s 3 hyperparameters of the architecture: \u201clevels\u201d, \u201cconvolutions per level\u201d and \u201cstarting features\u201d.<\/p>\n<p>I can have multiple charts, grouping by one at a time, and see how each individual variable affects the runs, but it would be much more beneficial to see the effects of all three together.<\/p>\n<p>The \u201ccustom chart\u201d seemed the way to go, but i couldn\u2019t make it work so far. Any help would be really appreciated!<\/p>",
        "Challenge_closed_time":1668699023534,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668697430270,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/group-by-multiple-variables-in-charts\/3435",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3664285558,
        "Challenge_title":"Group by multiple variables in charts",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mateoballa\">@mateoballa<\/a> thank you for writing in! In the Project level, you can group your Runs by all these three hyperparameters from the Group button as in the attachment.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/40fde472a1d51962372411528e99e65a7c90656d.png\" alt=\"Screenshot 2022-11-17 at 15.26.35\" data-base62-sha1=\"9gWweVkvwN7ym0GpqkZ8aunFcq9\" width=\"596\" height=\"303\"><\/p>\n<p>Then the Charts will adjust to this grouping. Would this help, or you wanted something different to achieve? Could you share a screenshot of your current custom chart?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":8.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4452663889,
        "Challenge_answer_count":1,
        "Challenge_body":"When I tried invoking an Azure ML pipeline from an Azure DevOps pipeline, I keep running into errors, Can you please share any sample that works.",
        "Challenge_closed_time":1658317648816,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658316045857,
        "Challenge_favorite_count":10.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/934296\/error-invoking-the-azure-ml-pipeline-from-azure-de.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.3682936568,
        "Challenge_title":"Error invoking the azure ML pipeline from Azure Devops",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@Srin-4824 Thanks for the question. yes this is possible just use the Azure CLI task - Azure Pipelines step and run command line or Python scripts inside that to submit your pipelines.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426685176247,
        "Answerer_location":"Athens, Greece",
        "Answerer_reputation_count":54268.0,
        "Answerer_view_count":22884.0,
        "Challenge_adjusted_solved_time":0.4555380556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Looks like I have 672 mission values, according to statistics. \nThere are NULL value in QuotedPremium column.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I implemented Clean Missing Data module where it should substitute missing values with 0, but for some reason I'm still seeing NULL values as QuotedPremium, but...it says that missing values are = 0<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PDg97.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PDg97.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Here you see it tells me that missing values = 0, but there are still NULLs <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So what really happened after I ran Clean Missing Data module? Why it ran succesfully but there are still NULL values, even though it tells that number of missing values are 0. <\/p>",
        "Challenge_closed_time":1515030180700,
        "Challenge_comment_count":2,
        "Challenge_created_time":1515028540763,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48087407",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":17.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3753756299,
        "Challenge_title":"How to deal with missing values in Azure Machine Learning Studio",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1556.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p><code>NULL<\/code> is indeed a value; entries containing NULLs are <em>not<\/em> missing, hence they are neither cleaned with the 'Clean Missing Data' operator nor reported as missing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1780152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building an Azure ML pipeline with the azureml Python SDK. The pipeline calls a PythonScriptStep which stores data on the workspaceblobstore of the AML workspace. <\/p>\n\n<p>I would like to extend the pipeline to export the pipeline data to an Azure Data Lake (Gen 1). Connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML as far as I understand. Therefore, I added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. According to the Microsoft documentation this should be possible.<\/p>\n\n<p>So far I have built this solution, only this results in a file of 0 bytes on the Gen 1 Data Lake. I think the output_export_blob PipelineData does not correctly references the test.csv, and therefore the DataTransferStep cannot find the input. How can I connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep?<\/p>\n\n<p>Example I followed:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb<\/a><\/p>\n\n<p>pipeline.py<\/p>\n\n<pre><code>input_dataset = delimited_dataset(\n    datastore=prdadls_datastore,\n    folderpath=FOLDER_PATH_INPUT,\n    filepath=INPUT_PATH\n)\n\noutput_export_blob = PipelineData(\n    'export_blob',\n    datastore=workspaceblobstore_datastore,\n)\n\ntest_step = PythonScriptStep(\n    script_name=\"test_upload_stackoverflow.py\",\n    arguments=[\n        \"--output_extract\", output_export_blob,\n    ],\n    inputs=[\n        input_dataset.as_named_input('input'),\n    ],\n    outputs=[output_export_blob],\n    compute_target=aml_compute,\n    source_directory=\".\"\n)\n\noutput_export_adls = DataReference(\n    datastore=prdadls_datastore, \n    path_on_datastore=os.path.join(FOLDER_PATH_OUTPUT, 'test.csv'),\n    data_reference_name='export_adls'        \n)\n\nexport_to_adls = DataTransferStep(\n    name='export_output_to_adls',\n    source_data_reference=output_export_blob,\n    source_reference_type='file',\n    destination_data_reference=output_export_adls,\n    compute_target=adf_compute\n)\n\npipeline = Pipeline(\n    workspace=aml_workspace, \n    steps=[\n        test_step, \n        export_to_adls\n    ]\n)\n<\/code><\/pre>\n\n<p>test_upload_stackoverflow.py<\/p>\n\n<pre><code>import os\nimport pathlib\nfrom azureml.core import Datastore, Run\n\nparser = argparse.ArgumentParser(\"train\")\nparser.add_argument(\"--output_extract\", type=str)\nargs = parser.parse_args() \n\nrun = Run.get_context()\ndf_data_all = (\n    run\n    .input_datasets[\"input\"]\n    .to_pandas_dataframe()\n)\n\nos.makedirs(args.output_extract, exist_ok=True)\ndf_data_all.to_csv(\n    os.path.join(args.output_extract, \"test.csv\"), \n    index=False\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1591813300647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591811660073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1591812659792,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62310010",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":38.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.3754971889,
        "Challenge_title":"Azure ML PipelineData with DataTransferStep results in 0 bytes file",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":917.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509387489888,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData<\/code>. Working initially even without the <code>DataTransferStep<\/code>.<\/p>\n\n<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:<\/p>\n\n<ol>\n<li>Does your <code>PipelineData<\/code>,  <code>export_blob<\/code>, contain the \"test.csv\" file? I would verify that before troubleshooting the <code>DataTransferStep<\/code>. You can verify this using the SDK, or more easily with the UI.\n\n<ol>\n<li>Go to the PipelineRun page, click on the <code>PythonScriptStep<\/code> in question.<\/li>\n<li>On \"Outputs + Logs\" page, there's a \"Data Outputs\" Section (that is slow to load initially)<\/li>\n<li>Open it and you'll see the output PipelineDatas then click on \"View Output\"<\/li>\n<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.\n<a href=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" alt=\"enter image description here\"><\/a><\/li>\n<\/ol><\/li>\n<li>In <code>test_upload_stackoverflow.py<\/code> you are treating the <code>PipelineData<\/code> as a directory when call <code>.to_csv()<\/code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)<\/code>. Perhaps try defining the <code>PipelineData<\/code> with <code>is_directory=True<\/code>. Not sure if this is required though.<\/li>\n<\/ol>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.0,
        "Solution_reading_time":21.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":184.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4736111111,
        "Challenge_answer_count":7,
        "Challenge_body":"> From https:\/\/github.com\/iterative\/dvc.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$ dvc fetch\r\nERROR: failed to fetch data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n```",
        "Challenge_closed_time":1606074573000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606072868000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":16.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3877159266,
        "Challenge_title":"example-get-started is broken with latest DVC",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"What DVC version do you use? It should be be fixed in the most recent one. 1.9.1 on Windows (latest) the latest version is 1.10 something. if it's not updated on Windows then we have a problem with Win releases cc @efiop  Ah I was wrong, you're right. Works with 1.10.1 which I got from https:\/\/github.com\/iterative\/dvc\/releases\/\r\n\r\nThe problem is that the dvc.org home page download button is stuck at 1.9.1 for all platforms, it seems. Opened iterative\/dvc.org\/issues\/1964, resolving here. I use the latest dvc version [DVC version: 1.11.16 (pip)] and have got the same issue while following the [installation](https:\/\/github.com\/iterative\/example-get-started) steps:\r\nOS: Mac OS Mojave 10.14.6\r\n```\r\n$ dvc pull\r\nEverything is up to date.                                             \r\nERROR: failed to pull data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n``` @yakushechkin example-get-started is migrating to dvc 2.0, so it is no longer compatible with older dvc versions. We plan on releasing 2.0 on Wednesday. You could try `pip install --pre dvc` to install dvc pre-release. Sorry for the inconvenience.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":5.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":163.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1441651557140,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation_count":851.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.9014630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Challenge_closed_time":1576464384063,
        "Challenge_comment_count":1,
        "Challenge_created_time":1576462664100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":0.390532503,
        "Challenge_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2197.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539556112483,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":2954.0,
        "Poster_view_count":1143.0,
        "Solution_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1576465909367,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1379265931347,
        "Answerer_location":"Rotterdam, Netherlands",
        "Answerer_reputation_count":6502.0,
        "Answerer_view_count":561.0,
        "Challenge_adjusted_solved_time":0.4821711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a python script that is written in different files (one for importing, one for calculations, et cetera). These are all in the same folder, and when I need a function from another function I do something like<\/p>\n\n<pre><code>import file_import\nfile_import.do_something_usefull()\n<\/code><\/pre>\n\n<p>where, of course, in the <code>file_import<\/code> there is a function <code>do_something_usefull()<\/code> that, uhm, does something usefull. How can I accomplish the same in Azure?<\/p>",
        "Challenge_closed_time":1457451133736,
        "Challenge_comment_count":4,
        "Challenge_created_time":1457449397920,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1457540530820,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35870839",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3935079798,
        "Challenge_title":"Is it possible to import python scripts in Azure?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1473.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379265931347,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":6502.0,
        "Poster_view_count":561.0,
        "Solution_body":"<p>I found it out myself. It is documenten on Microsoft's site <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The steps, very short, are:<\/p>\n\n<ol>\n<li>Include all the python you want in a .zip<\/li>\n<li>Upload that zip as a dataset<\/li>\n<li>Drag the dataset as the third option parameter in the 'execute python'-block (example below)<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" alt=\"Example dragging zip to Python script\"><\/a><\/p>\n\n<ol start=\"4\">\n<li>execute said function by importing <code>import Hello<\/code> (the name of the file, not the zip) and running <code>Hello.do_something_usefull()<\/code><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1457451850803,
        "Solution_link_count":3.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1483548930012,
        "Answerer_location":null,
        "Answerer_reputation_count":1875.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4867144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Challenge_closed_time":1617311610732,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617309858560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1617383727407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.5,
        "Challenge_reading_time":88.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":0.3965686143,
        "Challenge_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1398.0,
        "Challenge_word_count":615,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526288719836,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1617312029390,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4928227778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to understand the difference between those two function calls:<\/p>\n<p>I am referring to the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-the-summary\">documentation of define_metric<\/a>:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>Is it the \u201cbest\u201d accuracy ever measured (during training) versus the accuracy of the \u201cbest\u201d (validation) model? I understand that wandb does not care what metric I log, but what is the intended use?<\/p>\n<p>Thank you for clarification.<\/p>",
        "Challenge_closed_time":1659448810923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659447036761,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/understanding-define-metric-parameters\/2836",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4006688094,
        "Challenge_title":"Understanding define_metric parameters",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>,<\/p>\n<p>For each metric logged, there is a summary metric that\u2019ll summarize the logged values as <em>one<\/em> value for each run. By default, W&amp;B uses the <em>latest<\/em> value, but you can update it with <code>wandb.summary['acc'] = best_acc<\/code> or using the two <code>define_metric<\/code> calls you show.<\/p>\n<p>This is then used to decide which value is displayed in plots that only use one value for each run (e.g. Scatter plots).<\/p>\n<pre><code class=\"lang-auto\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>These two calls are both functionally the same, one will show <code>acc.best<\/code> and one will show as <code>acc.max<\/code> in the summary metrics of your run. Both will be the maximum value that you log for <code>acc<\/code> like <code>wandb.log('acc':acc)<\/code> during a run.<\/p>\n<p>You can see the summary metrics of each run by clicking the <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/information_source.png?v=12\" title=\":information_source:\" class=\"emoji\" alt=\":information_source:\" loading=\"lazy\" width=\"20\" height=\"20\"> icon in the top left nav bar in a run.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":148.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1582574311347,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":0.4946644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretty big CSV that would not fit into memory, and I need to convert it into .parquet file to work with vaex.<\/p>\n\n<p>Here is my catalog:<\/p>\n\n<pre><code>raw_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    filepath: data\/01_raw\/data.csv\n    file_format: csv\n\nparquet_data:\n    type: ParquetLocalDataSet\n    filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>node:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def convert_to_parquet(data: SparkDataSet) -&gt; ParquetLocalDataSet:\n    return data.coalesce(1)\n<\/code><\/pre>\n\n<p>and a pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=convert_to_parquet,\n                inputs=\"raw_data\",\n                outputs=\"parquet_data\",\n                name=\"data_to_parquet\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<p>But if I do <code>kedro run<\/code> I receive this error <code>kedro.io.core.DataSetError: Failed while saving data to data set ParquetLocalDataSet(engine=auto, filepath=data\/02_intermediate\/data.parquet, save_args={}).\n'DataFrame' object has no attribute 'to_parquet'<\/code><\/p>\n\n<p>What should I fix to get my dataset converted?<\/p>",
        "Challenge_closed_time":1582574347452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582572566660,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583421031536,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60382704",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":15.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.4019017297,
        "Challenge_title":"Convert csv into parquet in kedro",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":498.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>You could try the following. This has worked for me in the past.<\/p>\n\n<pre><code>parquet_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    file_format: 'parquet'\n    filepath: data\/02_intermediate\/data.parquet\n    save_args:\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":22.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.4960861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>i have added \"versioned: true\" in the \"catalog.yml\" file of the \"hello_world\" tutorial.<\/p>\n\n<pre><code>example_iris_data:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/iris.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>Then when I used \n\"kedro run\" to run the tutorial, it has error as below:\n\"VersionNotFoundError: Did not find any versions for CSVDataSet\".<\/p>\n\n<p>May i know what is the right way for me to do versioning for the \"iris.csv\" file? thanks!<\/p>",
        "Challenge_closed_time":1592218684390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592216898480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62386291",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4028524388,
        "Challenge_title":"Data versioning of \"Hello_World\" tutorial",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517455831447,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Try versioning one of the downstream outputs. For example, add this entry in your <code>catalog.yml<\/code>, and run <code>kedro run<\/code><\/p>\n\n<pre><code>example_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/02_intermediate\/example_iris_data.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>And you will see <code>example_iris.data.csv<\/code> directory (not a file) under <code>data\/02_intermediate<\/code>. The reason <code>example_iris_data<\/code> gives you an error is that it's the starting data and there's already <code>iris.csv<\/code> in <code>data\/01_raw<\/code> so, Kedro cannot create <code>data\/01_raw\/iris.csv\/<\/code> directory because of the name conflict with the existing <code>iris.csv<\/code> file. <\/p>\n\n<p>Hope this helps :) <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":9.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":78.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.0008886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I initially had a notebook in one directory in AWS SageMaker JupyterLab, say <code>\/A<\/code>, but then moved it into <code>\/A\/B<\/code>. However, when I run <code>!pwd<\/code> in a jupyter notebook cell, I still get <code>\/A<\/code>. This happens even when I press 'restart kernel'. How does the notebook remember this, and is there a way to prevent or reset this?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1597057792807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597055996810,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597057789608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63338577",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4047235369,
        "Challenge_title":"Jupyter notebook seems to remember previous path (!pwd) after being moved to a different directory?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I was actually using AWS SageMaker, and restarting the kernel from the toolbar was not enough. I needed to restart the kernel session, by pressing 'shut down' in the &quot;Running terminals and kernels&quot; section on the left navigation.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/934Fe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/934Fe.png\" alt=\"Shut down button seen on left navigation of JupterLab\" \/><\/a><\/p>\n<p>They are currently <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab\/issues\/5989\" rel=\"nofollow noreferrer\">discussing<\/a> warning users about the need to restart the kernel when a notebook is moved.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.5003305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure AI and initially had been using Machine Learning Studio Classic.<\/p>\n\n<p>It was working well but was slow training my models. From looking around, it seems that if I use Azure Machine Learning Studio, I can control the hardware used to run the experiments, so this is what I am trying.<\/p>\n\n<p>My issue is that Azure Machine Learning Studio is extremely slow in starting the experiments-it can take 10 minutes to even start.<\/p>\n\n<p>Is this as expected or am I missing something?<\/p>\n\n<p>Incidentally, NC24 was actually slower than NC6 - is this because of the configuration of my experiment?<\/p>\n\n<p>GPU Training    Whole run\nNC6 2m 36s  10m 48s\nNC24    2m 52s  16m 48s<\/p>",
        "Challenge_closed_time":1581956292927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581954491737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60265992",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.4056854542,
        "Challenge_title":"Azure Machine Learning - slow to start",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1143.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420792743172,
        "Poster_location":null,
        "Poster_reputation_count":3531.0,
        "Poster_view_count":312.0,
        "Solution_body":"<p>I'm assuming that you're:<\/p>\n\n<ul>\n<li>using Azure ML Studio (i.e. <a href=\"https:\/\/ml.azure.com\/\" rel=\"nofollow noreferrer\"><code>ml.azure.com\/<\/code><\/a>)'s Pipeline Designer, and <\/li>\n<li>creating a new compute target before running?<\/li>\n<\/ul>\n\n<p>If so, then 10 minutes is normal for the first run, given that a cluster of VMs has to be created and provisioned with a Docker container and Conda environment. After the run first completes the compute target is configured to stay on and available for two hours so future runs should execute without the 10 minute delay (provided you don't change the Conda dependencies or choose a new compute target).<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#amlcompute\" rel=\"nofollow noreferrer\">more information about AMLCompute<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1318047784840,
        "Answerer_location":null,
        "Answerer_reputation_count":6665.0,
        "Answerer_view_count":926.0,
        "Challenge_adjusted_solved_time":0.5016727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Challenge_closed_time":1625324240192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625322434170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.7,
        "Challenge_reading_time":39.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.406579672,
        "Challenge_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318047784840,
        "Poster_location":null,
        "Poster_reputation_count":6665.0,
        "Poster_view_count":926.0,
        "Solution_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.9,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1365701399963,
        "Answerer_location":"California, USA",
        "Answerer_reputation_count":1279.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4401530556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Challenge_closed_time":1532031297043,
        "Challenge_comment_count":9,
        "Challenge_created_time":1532029428013,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1532029712492,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":7.8,
        "Challenge_reading_time":15.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.4181674244,
        "Challenge_title":"split dataframe column header and values into multiple columns",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1086.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338678668792,
        "Poster_location":"Eindhoven, Netherlands",
        "Poster_reputation_count":11479.0,
        "Poster_view_count":887.0,
        "Solution_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":3.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5291666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly.\n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated.\n\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\nSkipping further processing because of column count mismatch.\n\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Challenge_closed_time":1586976185000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586974280000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sage-maker-model-monitor-missing-columns-constraint-violation",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":37.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.4247229247,
        "Challenge_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"That violation fires when, for example, input to your endpoint has fewer columns than baseline input does. This is helpful to flag data quality issues. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-interpreting-violations.html\n\nIn this case, however, this is an artifact of how we perform the analysis. We concatenate output and input CSVs into a single CSV to analyze the whole thing in one go. E.g. it would look like:\n\noutput_col,input_col_1,input_col_2,...,input_col_n\n\n\nIn this case, however, your output has a trailing newline which means that after concatenating this looks like:\n\noutput_col # embedded newline in your output\n,input_col_1,input_col_2,...,input_col_n\n\n\nTriggering the code to think there is only one column in dataset and hence failing the job.\n\nWe have a fix flowing through the pipeline now, while that goes out you can add a preprocessing script to your schedule to strip out the trailing newline from the output. We will create a sample notebook for this, in the meantime docs are at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":152.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5291666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs, https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n\nModel files in the top-level directory\nInference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\nOptional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Challenge_closed_time":1594981418000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594979513000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sage-maker-py-torch-deployment-process-change",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.4247229247,
        "Challenge_title":"Did the SageMaker PyTorch deployment process change?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":134.0,
        "Challenge_word_count":161,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When AWS Sample - BERT sample using torch 1.4 was published, advance knowledge of the inference seems to be necessary. If you use the PyTorch SageMaker SDK to create or deploy the model after it is trained, it automatically re-packages the model.tar.gz to include the code files and the inference files. As an example, when you use the following script, the model.tar.gz is repackaged so the contents of the src directory is automatically added to the code directory model.tar.gz, which initially only contains model files. You don't need to know the inference code in advance.\n\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\n\nmodel_uri = estimator.model_data\n\nmodel = PyTorchModel(model_data=model_uri,\n                     role=role,\n                     framework_version='1.4.0',\n                     entry_point='serve.py',\n                     source_dir='src')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n\nFor the older versions, you couldn't include additional files \/dependencies during inference unless you built a custom container. The source.tar.gz was only used during training.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":14.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":140.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.5330241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1645069539320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645067620433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645073711768,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":14.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.4272423641,
        "Challenge_title":"how to create a serverless endpoint in sagemaker?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5344444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Challenge_closed_time":1592471900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592469976000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4281683901,
        "Challenge_title":"Custom packages in Sagemaker studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":266.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Short answer: [Studio UI] > File > New > Terminal > sudo yum install unzip\nThen unzip away...\n\nLong answer:\nYou can open a terminal in two different types of compute environment:\n\nOn a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\nOn the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal\n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.\nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":8.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":110.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5526797223,
        "Challenge_answer_count":1,
        "Challenge_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of smdistributed-modelparallel module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Challenge_closed_time":1660807962126,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660805972479,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.2,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4399822913,
        "Challenge_title":"How to check smdistributed-modelparallel version?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for PyTorch 1.10.2 with HuggingFace transformers DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":2,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581438052000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.4404031594,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5544444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation. I have some initialization to do at creation time (clone a github repo, etc.). That all works fine. The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a Stopped state. A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige. I would have thought it would let me do the same in the lifecycle config. Unfortunately, SageMaker still has the notebook in the Pending state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Challenge_closed_time":1539777191000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539775195000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloud-formation-with-sage-maker-life-cycle-config-without-leaving-the-instance-running",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.4411182113,
        "Challenge_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":306.0,
        "Challenge_word_count":208,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One solutions will be to create a CFN custom resource backed by lambda. You can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":2.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Challenge_closed_time":1606709124000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606707121000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":2.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4423683231,
        "Challenge_title":"Sagemaker Jupyter notebook",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":179.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1434117836363,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.2549266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Challenge_closed_time":1598632128583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598630122480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1598631210847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.4429219803,
        "Challenge_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434117836363,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.2,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":0.5689572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Challenge_closed_time":1610637215396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610635167150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611071837132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":36.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.450411209,
        "Challenge_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1209.0,
        "Challenge_word_count":288,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Solution_comment_count":23.0,
        "Solution_last_edit_time":1610641519768,
        "Solution_link_count":1.0,
        "Solution_readability":24.6,
        "Solution_reading_time":43.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5754191667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say I have some text data that has already been labeled in SageMaker. This data could have either been labeled by humans or an ner model. Then let's say I want to have a human go back over the dataset, either to label new entity class or correct existing labels. How would I set up a labeling job to allow this? I tried using an output manifest from another labeling job, but all of the documents that were already labeled cannot be accessed by workers to re-label.<\/p>",
        "Challenge_closed_time":1609890680276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609888608767,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65587939",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.4545213745,
        "Challenge_title":"Can I use pre-labeled data in AWS SageMaker Ground Truth NER?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588952728736,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Yes, this is possible you are looking for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates.html\" rel=\"nofollow noreferrer\">Custom Labelling worklflows<\/a> you can also apply either Majority Voting (MV) or MDS to evaluate the accuracy of the job<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":0.5756725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Challenge_closed_time":1606832097008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606830024587,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4546821653,
        "Challenge_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510064331503,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":5537.0,
        "Poster_view_count":215.0,
        "Solution_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.6,
        "Solution_reading_time":2.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566993998790,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":6883.0,
        "Answerer_view_count":2734.0,
        "Challenge_adjusted_solved_time":0.5757988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use <code>[OPTUNA][1]<\/code> with <code>sklearn<\/code> <code>[MLPRegressor][1]<\/code> model.<\/p>\n<p>For almost all hyperparameters it is quite straightforward how to set OPTUNA for them.\nFor example, to set the learning rate:\n<code>learning_rate_init = trial.suggest_float('learning_rate_init ',0.0001, 0.1001, step=0.005)<\/code><\/p>\n<p>My problem is how to set it for <code>hidden_layer_sizes<\/code> since it is a tuple. So let's say I would like to have two hidden layers where the first will have 100 neurons and the second will have 50 neurons. Without OPTUNA I would do:<\/p>\n<p><code>MLPRegressor( hidden_layer_sizes =(100,50))<\/code><\/p>\n<p>But what if I want OPTUNA to try different neurons in each layer? e.g., from 100 to 500, how can I set it? the <code>MLPRegressor<\/code> expects a tuple<\/p>",
        "Challenge_closed_time":1636650321043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636648248167,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1660311769156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931757",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":11.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.4547623747,
        "Challenge_title":"How to set hidden_layer_sizes in sklearn MLPRegressor using optuna trial",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517513644076,
        "Poster_location":"Israel",
        "Poster_reputation_count":995.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You could set up your objective function as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport warnings\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nwarnings.filterwarnings('ignore')\n\nX, y = make_regression(random_state=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n\ndef objective(trial):\n\n    params = {\n        'learning_rate_init': trial.suggest_float('learning_rate_init ', 0.0001, 0.1, step=0.005),\n        'first_layer_neurons': trial.suggest_int('first_layer_neurons', 10, 100, step=10),\n        'second_layer_neurons': trial.suggest_int('second_layer_neurons', 10, 100, step=10),\n        'activation': trial.suggest_categorical('activation', ['identity', 'tanh', 'relu']),\n    }\n\n    model = MLPRegressor(\n        hidden_layer_sizes=(params['first_layer_neurons'], params['second_layer_neurons']),\n        learning_rate_init=params['learning_rate_init'],\n        activation=params['activation'],\n        random_state=1,\n        max_iter=100\n    )\n\n    model.fit(X_train, y_train)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=3)\n# [I 2021-11-11 18:04:02,216] A new study created in memory with name: no-name-14c92e38-b8cd-4b8d-8a95-77158d996f20\n# [I 2021-11-11 18:04:02,283] Trial 0 finished with value: 161.8347337123744 and parameters: {'learning_rate_init ': 0.0651, 'first_layer_neurons': 20, 'second_layer_neurons': 40, 'activation': 'tanh'}. Best is trial 0 with value: 161.8347337123744.\n# [I 2021-11-11 18:04:02,368] Trial 1 finished with value: 159.55535852658082 and parameters: {'learning_rate_init ': 0.0551, 'first_layer_neurons': 90, 'second_layer_neurons': 70, 'activation': 'relu'}. Best is trial 1 with value: 159.55535852658082.\n# [I 2021-11-11 18:04:02,440] Trial 2 finished with value: 161.73980822730888 and parameters: {'learning_rate_init ': 0.0051, 'first_layer_neurons': 100, 'second_layer_neurons': 30, 'activation': 'identity'}. Best is trial 1 with value: 159.55535852658082.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":28.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":174.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.5786791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Challenge_closed_time":1597010598252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597008515007,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597058318400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.456588527,
        "Challenge_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415053264667,
        "Poster_location":"USA",
        "Poster_reputation_count":11166.0,
        "Poster_view_count":653.0,
        "Solution_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.2,
        "Solution_reading_time":25.97,
        "Solution_score_count":4.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":227.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":0.5847786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1618410091900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":0.46044472,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1218.0,
        "Challenge_word_count":510,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1638293279416,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":0.5860544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1638293279416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638291169620,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639486727367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":74.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":0.4612494507,
        "Challenge_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":372,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463607987528,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":143.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1362230894870,
        "Answerer_location":null,
        "Answerer_reputation_count":165.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":0.5939086111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Challenge_closed_time":1506026797448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506024659377,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1506491876827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.4661892457,
        "Challenge_title":"comet (comet-ml) fails to run with Keras",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":601.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1506066589407,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":7.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":76.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":0.3772533333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML studio, how to import images dataset, for image recognition algorithms. As zip file?<\/p>",
        "Challenge_closed_time":1460058230132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460056088943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1460056872020,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36485084",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.1,
        "Challenge_reading_time":1.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4667324863,
        "Challenge_title":"Azure ML Studio: How to import images dataset?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2420.0,
        "Challenge_word_count":23,
        "Platform":"Stack Overflow",
        "Poster_created_time":1282073536110,
        "Poster_location":null,
        "Poster_reputation_count":4529.0,
        "Poster_view_count":1142.0,
        "Solution_body":"<p>You can use \"<strong>import images<\/strong>\" module in Azure ML Studio that can read images from Azure blob storage - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Face-detection-2\" rel=\"nofollow\">here<\/a> is the sample experiment <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":3.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1662621266503,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.6069663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Challenge_closed_time":1662652096572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662649911493,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4743481711,
        "Challenge_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662649653072,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":3.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1585824581960,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":0.6095908333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <em>Training Job<\/em> using the Sagemaker API. The code for configuring the estimator looks as follows (I shrinked the full path names a bit):<\/p>\n<pre><code>s3_input = &quot;s3:\/\/sagemaker-studio-****\/training-inputs&quot;.format(bucket)\ns3_images = &quot;s3:\/\/sagemaker-studio-****\/dataset&quot;\ns3_labels = &quot;s3:\/\/sagemaker-studio-****\/labels&quot;\ns3_output = 's3:\/\/sagemaker-studio-****\/output'.format(bucket)\n\ncfg='{}\/input\/models\/'.format(s3_input)\nweights='{}\/input\/data\/weights\/'.format(s3_input)\noutpath='{}\/'.format(s3_output)\nimages='{}\/'.format(s3_images)\nlabels='{}\/'.format(s3_labels)\n\nhyperparameters = {\n    &quot;epochs&quot;: 1,\n    &quot;batch-size&quot;: 2\n}\n\ninputs = {\n    &quot;cfg&quot;: TrainingInput(cfg),\n    &quot;images&quot;: TrainingInput(images),\n    &quot;weights&quot;: TrainingInput(weights),\n    &quot;labels&quot;: TrainingInput(labels)\n}\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='s3:\/\/sagemaker-studio-****\/input\/input.tar.gz',\n    image_uri=container,\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    input_mode='File',\n    output_path=outpath,\n    train_output=outpath,\n    base_job_name='visualsearch',\n    hyperparameters=hyperparameters,\n    framework_version='1.9',\n    py_version='py38'\n)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>Everything runs fine and I get the success message:<\/p>\n<pre><code>Results saved to #033[1mruns\/train\/exp#033[0m\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n2022-07-08 08:38:35,767 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2022-07-08 08:39:08 Uploading - Uploading generated training model\n2022-07-08 08:39:08 Completed - Training job completed\nProfilerReport-1657268881: IssuesFound\nTraining seconds: 558\nBillable seconds: 558\nCPU times: user 1.34 s, sys: 146 ms, total: 1.48 s\nWall time: 11min 20s\n<\/code><\/pre>\n<p>When I call <code>estimator.model_data<\/code> I get a path poiting to a model.tar.gz file <code>s3:\/\/sagemaker-studio-****\/output\/...\/model.tar.gz<\/code><\/p>\n<p>Sagemaker generated subfoldes into the output folder (which in turn contain a lot of json files and other artifacts):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WymlH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WymlH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But the file <code>model.tar.gz<\/code> is missing. This file is nowhere to be found. Is there anything I need to change or to add, in order to obtain my model?<\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1657273188600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657270994073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72909085",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":36.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":0.4759800059,
        "Challenge_title":"Sagemaker creates output folders but no model.tar.gz after successful completion of the Training Job",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":248,
        "Platform":"Stack Overflow",
        "Poster_created_time":1640612109920,
        "Poster_location":"Zedtwitz, Germany",
        "Poster_reputation_count":215.0,
        "Poster_view_count":46.0,
        "Solution_body":"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)\n<\/code><\/pre>\n<p>Normally it is set to <code>opt\/ml\/model<\/code><\/p>\n<p>Ref:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.3,
        "Solution_reading_time":12.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3125277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1629024434667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629022235437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1629023309567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":60.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.4767913046,
        "Challenge_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":499.0,
        "Challenge_word_count":370,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.7,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6132516667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI tried to use the pipe operation %>% of R in an azure notebook without success ...\n\n\n\n\n\nis possible to use it or it is a limitation in azure notebooks ?",
        "Challenge_closed_time":1597097474163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597095266457,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/63863\/pipe-ampgt-for-r-is-not-working-in-azure-notebooks.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.478251811,
        "Challenge_title":"Pipe %&gt;% for R is not working in azure notebooks",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nFixed.\n\nAzure Notebook release the session after some time of inactivity, therefore the dplyr package wasn\u00b4t loaded in the session",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6197222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Challenge_closed_time":1592579742000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592577511000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4822546668,
        "Challenge_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.8,
        "Solution_reading_time":11.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6205213889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I don\u2019t have admin access to a team and want to remove myself from the said team. There are other admin users, but say I can\u2019t communicate with them, how to remove myself?<\/p>",
        "Challenge_closed_time":1649424480723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649422246846,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-remove-myself-from-a-team\/2201",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4827479425,
        "Challenge_title":"How to remove myself from a team?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/sai_prasanna\">@sai_prasanna<\/a> no problem! You\u2019ve been removed from \u201cagara\u201d. Is there anything else I can help with?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":21.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6311111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Challenge_closed_time":1607684202000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607681930000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":8.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4892614458,
        "Challenge_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":5.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":69.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":0.6330175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Challenge_closed_time":1623878015320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623875736457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68009703",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.4904295304,
        "Challenge_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622063222448,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.0,
        "Solution_reading_time":7.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6372222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Challenge_closed_time":1588843302000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588841008000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4930010388,
        "Challenge_title":"Training a classifier on parquet with SageMaker ?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example notebook).\nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see source)\n\nAdditionally:\nUber Petastorm for reading parquet into Tensorflow, Pytorch, and PySpark inputs.\nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.6372286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Challenge_closed_time":1597702121696,
        "Challenge_comment_count":5,
        "Challenge_created_time":1597699827673,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1599771558392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":17.3,
        "Challenge_reading_time":77.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.4930049411,
        "Challenge_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1768.0,
        "Challenge_word_count":509,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432406490590,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation_count":381.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Solution_comment_count":9.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6409055556,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi There,\nI am running a very simple pipeline that contains a dataset and a SQL transformation task. When i run the two tasks i get an error : 2021\/09\/07 17:49:47 Wrapper cmd failed with err: exit status 143 which i can't seem to find anywhere. I am running a compute VM DS1.\nany direction?\nThanks,",
        "Challenge_closed_time":1631040059280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631037752020,
        "Challenge_favorite_count":8.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543071\/azure-machine-learning-exit-code-143.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.6,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.4952482575,
        "Challenge_title":"Azure Machine Learning Exit Code 143",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Incase anyone is wondering, you must increase the compute with more memory to avoid this...",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":0.6438841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Challenge_closed_time":1596232881236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596230563253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.4970618359,
        "Challenge_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":278.0,
        "Challenge_word_count":196,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.4,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":120.6901825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Challenge_closed_time":1646329084320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646326739290,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.5016217309,
        "Challenge_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578932319743,
        "Poster_location":"Ireland",
        "Poster_reputation_count":1012.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646761223947,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1379362192207,
        "Answerer_location":"Embrach, Schweiz",
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":0.6535952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do a sales prediction and I'm evaluation using a ML.NET solution hosted in a virtual machine(in Azure) vs using Azure ML Studio. The data may change once or twice per month. Which solutions should I choose? Also, for my use case, pricing might be a factor.\nThank you. <\/p>",
        "Challenge_closed_time":1575557093136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575554740193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1575622440107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59196919",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5029518737,
        "Challenge_title":"ML.NET vs Azure ML Studio",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":510.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406013299956,
        "Poster_location":null,
        "Poster_reputation_count":450.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>In short:\nIf you are building a .NET application and want to integrate ML, use ML.NET.\nIf you don't do .NET, use Azure ML.\nDocs are helpful here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":5.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395422283667,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.6555202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Challenge_closed_time":1452007973623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1452005613750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.5041153267,
        "Challenge_title":"Send request as Json on UWP",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3194.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352139399460,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Solution_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":7.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.6604344445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to overwrite properties taken from the parameters.yaml file within a Kedro notebook?<\/p>\n\n<p>I am trying to dynamically change parameter values within a notebook. I would like to be able to give users the ability to run a standard pipeline but with customizable parameters. I don't want to change the YAML file, I just want to change the parameter for the life of the notebook.<\/p>\n\n<p>I have tried editing the params within the context but this has no affect.<\/p>\n\n<pre><code>context.params.update({\"test_param\": 2})\n<\/code><\/pre>\n\n<p>Am I missing something or is this not an intended use case?<\/p>",
        "Challenge_closed_time":1582114523347,
        "Challenge_comment_count":1,
        "Challenge_created_time":1582112145783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583420995670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60299478",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.5070792817,
        "Challenge_title":"Setting parameters in Kedro Notebook",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582111403048,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Kedro supports specifying <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/03_configuration.html#specifying-extra-parameters\" rel=\"nofollow noreferrer\">extra parameters<\/a> from the command line by running<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>kedro run --params \"key1:value1,key2:value2\"\n<\/code><\/pre>\n\n<p>which solves your second use case.<\/p>\n\n<p>As for the notebook use case, updating <code>context.params<\/code> does not have any effect since the context does not store the parameters on <code>self<\/code> but rather pulls them from the config every time the property is being called.<\/p>\n\n<p>However you can still add extra parameters to the context object after it being instantiated:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_params = context._extra_params or {}\nextra_params.update({\"test_param\": 2})\ncontext._extra_params = extra_params\n<\/code><\/pre>\n\n<p>This will update extra parameters that are applied on top of regular parameters coming from the config.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":13.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":106.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":0.6262913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Challenge_closed_time":1654270724136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654268345127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1654268469487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.5073209897,
        "Challenge_title":"SageMaker Inference for a video input",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.6,
        "Solution_reading_time":6.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":0.6647269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Challenge_closed_time":1598979186260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598976793243,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1599032818923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":37.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":0.5096611126,
        "Challenge_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":793.0,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502454917960,
        "Poster_location":"Milano, MI, Italia",
        "Poster_reputation_count":132.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":6.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5792555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Challenge_closed_time":1610631020420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610628627223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1610628935100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":4.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.5096911471,
        "Challenge_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":715.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512023194592,
        "Poster_location":null,
        "Poster_reputation_count":547.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.6663211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write delta tables in Kedro. Changing file format to delta makes the write as delta tables with mode as overwrite.<\/p>\n<p>Previously, a node in the raw layer (meta_reload) creates a dataset that determines what's the start date for incremental load for each dataset. each node uses that raw dataset to filter the working dataset to apply the transformation logic and write partitioned parquet tables incrementally.<\/p>\n<p>But now writing delta with mode as overwrite with just file type change to delta makes current incremental data overwrite all the past data instead of just those partitions. So I need to use replaceWhere option in save_args in the catalog.\nHow would I determine the start date for replaceWhere in the catalog when I need to read the meta_reload raw dataset to determine the date.\nIs there a way to dynamically pass the save_args from inside the node?<\/p>\n<pre><code>my_dataset:\n  type: my_project.io.pyspark.SparkDataSet\n  filepath: &quot;s3:\/\/${bucket_de_pipeline}\/${data_environment_project}\/${data_environment_intermediate}\/my_dataset\/&quot;\n  file_format: delta\n  layer: intermediate\n  save_args:\n    mode: &quot;overwrite&quot;\n    replaceWhere: &quot;DATE_ID &gt; xyz&quot;  ## what I want to implement dynamically\n    partitionBy: [ &quot;DATE_ID&quot; ]\n<\/code><\/pre>",
        "Challenge_closed_time":1632930071223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632927672467,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69378898",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.5106182689,
        "Challenge_title":"How to dynamically pass save_args to kedro catalog?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477986647030,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":171.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I've answered this on the GH <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/discussions\/910\" rel=\"nofollow noreferrer\">discussion<\/a>. In short you would need to subclass and define your own <code>SparkDataSet<\/code> we avoid changing the underlying API of the datasets at a Kedro level, but you're encouraged to alter and remix this for your own purposes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":4.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":47.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.6761936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand what the entry script\/scoring script is and does. See <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/deploying-models.html\" rel=\"nofollow noreferrer\">here<\/a> as an example. As I struggle to expose my deployed model via code as described <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">here<\/a> (see also <a href=\"https:\/\/stackoverflow.com\/questions\/67535014\/deploy-model-to-azure-machine-learning-via-azuremlsdk\">here<\/a>), I am trying to use the UI ml.azure.com instead. I am a bit puzzled by the mandatory dependency: conda dependencies file:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have an R model but clearly this is a Python thing. What shall I use in this case?<\/p>",
        "Challenge_closed_time":1621007147567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621004713270,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67536581",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.5165255151,
        "Challenge_title":"conda dependencies file r model in azure machine learning",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":609.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>conda is actually not just a Python thing, you might be thinking of pip?<\/p>\n<p>Conda is a package &amp; environment manager for nearly any kind of package, provided that it has been uploaded to anaconda. So you <em>can<\/em> use anaconda (and conda environment files) for R projects.<\/p>\n<p>The trouble is that the <code>azuremlsdk<\/code> CRAN package is not hosted as an anaconda package, but is probably needed for the scoring service. Worth using a file like below to see what it works.<\/p>\n<p>If it doesn't work, then I agree that this UI needs to generalized to better support R model deployment scenarios.<\/p>\n<p>It is also possible to add the <code>azuremlsdk<\/code> CRAN package to anaconda, but that requires <a href=\"https:\/\/stackoverflow.com\/a\/36653411\/3842610\">some extra work<\/a>, but ideally you shouldn't have to require this much manual effort.<\/p>\n<code>environment.yml<\/code>\n<p>Here's an example conda dependencies file for R.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: scoring_environment\nchannels:\n  - defaults\ndependencies:\n  - r-base=3.6.1\n  - r-essentials=3.6.0\n  # whatever other dependencies you have\n  - r-tidyverse=1.2.1\n  - r-caret=6.0_83\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":15.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":157.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":0.6793758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Challenge_closed_time":1657053745440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657051299687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5184221966,
        "Challenge_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":66.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":25.5,
        "Solution_reading_time":7.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    }
]