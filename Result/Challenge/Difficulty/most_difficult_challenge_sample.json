[
    {
        "Answerer_age":null,
        "Answerer_created_time":1373922677212,
        "Answerer_location":null,
        "Answerer_reputation":1350.0,
        "Answerer_views":56.0,
        "Challenge_adjusted_solved_time":24155.5917811111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Challenge_closed_time":1522603988172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1435643857760,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":10.0923125734,
        "Challenge_title":"Azure ML web service times out",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1313488868532,
        "Poster_location":null,
        "Poster_reputation":209.0,
        "Poster_views":42.0,
        "Solution_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":50.86,
        "Solution_score":0.0,
        "Solution_sentence_count":45.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":338.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1409077455067,
        "Answerer_location":"Mountain View, CA",
        "Answerer_reputation":171.0,
        "Answerer_views":11.0,
        "Challenge_adjusted_solved_time":19018.4561647222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use the RStudio IDE to R on an Amazon SageMaker instance. What I have tried so far is to run the following docker command:<\/p>\n\n<pre><code>docker run --rm -p 8787:8787 rocker\/verse\n<\/code><\/pre>\n\n<p>which appears to work successfully. What I would then do when running that command from my local computer is go to <code>http:\/\/localhost:8787<\/code> where I would be able to login and find a fully functional RStudio IDE within my browser. <\/p>\n\n<p>However, this is obviously not possible from within SageMaker as there is no <code>localhost<\/code> to visit.<\/p>\n\n<p>Is there some way I can direct my browser to capture the output to port 8787 from the SageMaker instance? <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Challenge_closed_time":1636510076040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568043633847,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857195",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.8532177429,
        "Challenge_title":"Is it possible to use RStudio IDE on an AWS SageMaker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1935.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1437507107363,
        "Poster_location":null,
        "Poster_reputation":553.0,
        "Poster_views":13.0,
        "Solution_body":"<p>RStudio is now available as a managed service in SageMaker. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":30.8,
        "Solution_reading_time":3.16,
        "Solution_score":3.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1454593215100,
        "Answerer_location":null,
        "Answerer_reputation":56.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":17600.8984933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Challenge_closed_time":1612305167163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548941458327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1548941932587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54461730",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.7757695285,
        "Challenge_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1856.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1541484812983,
        "Poster_location":"London, UK",
        "Poster_reputation":33.0,
        "Poster_views":3.0,
        "Solution_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":14.69,
        "Solution_score":3.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Challenge_adjusted_solved_time":15023.6469805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1634653018680,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":9.6174472637,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1442180190107,
        "Poster_location":null,
        "Poster_reputation":3203.0,
        "Poster_views":400.0,
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617867548000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":9.4257611795,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1457261731840,
        "Answerer_location":"Vancouver, BC",
        "Answerer_reputation":584.0,
        "Answerer_views":270.0,
        "Challenge_adjusted_solved_time":11057.4897425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Challenge_closed_time":1567514274983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527707311910,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":24.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":9.3109537144,
        "Challenge_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1130.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1443201378360,
        "Poster_location":null,
        "Poster_reputation":749.0,
        "Poster_views":49.0,
        "Solution_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.1,
        "Solution_reading_time":11.41,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":5,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631000506000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":9.2564833052,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1474967309676,
        "Answerer_location":null,
        "Answerer_reputation":59.0,
        "Answerer_views":66.0,
        "Challenge_adjusted_solved_time":10362.7869172222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Challenge_closed_time":1644193786212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606886205577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1606887753310,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":9.2461144642,
        "Challenge_title":"struggling to install python package via amazon sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":348,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1474967309676,
        "Poster_location":null,
        "Poster_reputation":59.0,
        "Poster_views":66.0,
        "Solution_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":1.79,
        "Solution_score":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609843354000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.2332544391,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1333391842272,
        "Answerer_location":"California, United States",
        "Answerer_reputation":1405.0,
        "Answerer_views":151.0,
        "Challenge_adjusted_solved_time":10042.3480277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1573664904092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.2147283489,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1341967360208,
        "Poster_location":null,
        "Poster_reputation":2832.0,
        "Poster_views":368.0,
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score":10.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":9316.4808333333,
        "Challenge_answer_count":4,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173479000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634634148000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9.1396475743,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.4,
        "Solution_reading_time":11.62,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":149.0,
        "Tool":"DVC"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation":1237.0,
        "Answerer_views":116.0,
        "Challenge_adjusted_solved_time":12.2603875,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1454387492627,
        "Challenge_comment_count":4,
        "Challenge_created_time":1421424031317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1454343355232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27987910",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":12.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.1223303544,
        "Challenge_title":"Azure Machine Learning - CORS",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3242.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1403948655636,
        "Poster_location":null,
        "Poster_reputation":135.0,
        "Poster_views":15.0,
        "Solution_body":"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this<\/p>\n\n<p>Here are the links: <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/api-management-get-started\/\" rel=\"noreferrer\">step by step<\/a> guide, also this <a href=\"http:\/\/channel9.msdn.com\/Blogs\/AzureApiMgmt\/Last-mile-Security\" rel=\"noreferrer\">video<\/a> on setting headers, and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn894084.aspx#JSONP\" rel=\"noreferrer\">this doc<\/a> on policies.<\/p>\n\n<p>API Management service allow CORS by enabling it in the API configuration page<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":9.27,
        "Solution_score":4.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580947939000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":9.1024669795,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":999,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1569423384323,
        "Answerer_location":null,
        "Answerer_reputation":96.0,
        "Answerer_views":6.0,
        "Challenge_adjusted_solved_time":8089.9413483334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Challenge_closed_time":1595256539003,
        "Challenge_comment_count":1,
        "Challenge_created_time":1566128809927,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1566133069936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":15.8,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.9986356291,
        "Challenge_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1541220234400,
        "Poster_location":"Singapore",
        "Poster_reputation":13.0,
        "Poster_views":5.0,
        "Solution_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595256858790,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":12.21,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1276712500692,
        "Answerer_location":"Tandil, Buenos Aires Province, Argentina",
        "Answerer_reputation":7226.0,
        "Answerer_views":637.0,
        "Challenge_adjusted_solved_time":7952.5209027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Challenge_closed_time":1662661497360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634032422110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":8.9813699905,
        "Challenge_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1633433905427,
        "Poster_location":null,
        "Poster_reputation":43.0,
        "Poster_views":5.0,
        "Solution_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.12,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1406731060412,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation":139.0,
        "Answerer_views":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.9626214768,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1509559597047,
        "Poster_location":null,
        "Poster_reputation":313.0,
        "Poster_views":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1351085228507,
        "Answerer_location":null,
        "Answerer_reputation":98.0,
        "Answerer_views":41.0,
        "Challenge_adjusted_solved_time":7728.2646775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Challenge_closed_time":1656482770812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628661017973,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.9527690112,
        "Challenge_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1354705336800,
        "Poster_location":"Pittsburgh",
        "Poster_reputation":2517.0,
        "Poster_views":78.0,
        "Solution_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.7,
        "Solution_reading_time":11.15,
        "Solution_score":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1291082954112,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation":20306.0,
        "Answerer_views":1991.0,
        "Challenge_adjusted_solved_time":7560.4671713889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... <\/p>\n\n<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?<\/p>",
        "Challenge_closed_time":1571132773200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543915091383,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53609409",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":19.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.9308205206,
        "Challenge_title":"Automatically \"stop\" Sagemaker notebook instance after inactivity?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":12683.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1299752760990,
        "Poster_location":null,
        "Poster_reputation":2563.0,
        "Poster_views":167.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">Lifecycle configurations<\/a> to set up an automatic job that will stop your instance after inactivity.<\/p>\n\n<p>There's <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\" rel=\"noreferrer\">a GitHub repository<\/a> which has samples that you can use. In the repository, there's a <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"noreferrer\">auto-stop-idle<\/a> script which will shutdown your instance once it's idle for more than 1 hour.<\/p>\n\n<p>What you need to do is<\/p>\n\n<ol>\n<li>to create a Lifecycle configuration using the script and<\/li>\n<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.<\/li>\n<\/ol>\n\n<p>If you think 1 hour is too long you can tweak the script. <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh#L17\" rel=\"noreferrer\">This line<\/a> has the value.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.0,
        "Solution_reading_time":17.33,
        "Solution_score":26.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":110.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1533921190820,
        "Answerer_location":"New York, USA",
        "Answerer_reputation":26.0,
        "Answerer_views":3.0,
        "Challenge_adjusted_solved_time":7532.4821652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Challenge_closed_time":1603758115467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576591296847,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1576641179672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.9289502643,
        "Challenge_title":"How to assign users in SageMaker Studio?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1029.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1451164178848,
        "Poster_location":"Bulgaria",
        "Poster_reputation":61.0,
        "Poster_views":9.0,
        "Solution_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":12.65,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609957275000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":15,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":3.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.9279742559,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score":16.0,
        "Solution_sentence_count":77.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":608.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1512520584492,
        "Answerer_location":"Bloomington, IN, USA",
        "Answerer_reputation":868.0,
        "Answerer_views":51.0,
        "Challenge_adjusted_solved_time":7347.4097222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Challenge_closed_time":1641958092267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615507417267,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":8.9022392045,
        "Challenge_title":"Get local workspace in azureml",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1512520584492,
        "Poster_location":"Bloomington, IN, USA",
        "Poster_reputation":868.0,
        "Poster_views":51.0,
        "Solution_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":4.52,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":3,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600043448000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.8913602141,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":4.17,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1456986606312,
        "Answerer_location":null,
        "Answerer_reputation":757.0,
        "Answerer_views":80.0,
        "Challenge_adjusted_solved_time":7933.4568986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1610748471692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586792396013,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":20.5,
        "Challenge_reading_time":18.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.8031936707,
        "Challenge_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1027.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1456986606312,
        "Poster_location":null,
        "Poster_reputation":757.0,
        "Poster_views":80.0,
        "Solution_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1615352840848,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":11.02,
        "Solution_score":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":5,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645584371000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.7587590714,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1493927732160,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation":586.0,
        "Answerer_views":158.0,
        "Challenge_adjusted_solved_time":6310.4455736111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>whenever I'm running - <code>from sklearn.ensemble import RandomForestClassifier<\/code><\/p>\n\n<p>I'm getting an error - <code>ImportError: cannot import name 'parallel_helper'<\/code>\nthe stack trace is - <\/p>\n\n<pre><code>--------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-135-d80da5c856d8&gt; in &lt;module&gt;()\n      1 # feature removal using ROC-AUC score\n----&gt; 2 from sklearn.ensemble import RandomForestClassifier\n      3 roc_values = []\n      4 for feature in diabetes_MICE_X.columns:\n      5     clf = RandomForestClassifier()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/__init__.py in &lt;module&gt;()\n      5 \n      6 from .base import BaseEnsemble\n----&gt; 7 from .forest import RandomForestClassifier\n      8 from .forest import RandomForestRegressor\n      9 from .forest import RandomTreesEmbedding\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/forest.py in &lt;module&gt;()\n     59 from ..exceptions import DataConversionWarning, NotFittedError\n     60 from .base import BaseEnsemble, _partition_estimators\n---&gt; 61 from ..utils.fixes import parallel_helper, _joblib_parallel_args\n     62 from ..utils.multiclass import check_classification_targets\n     63 from ..utils.validation import check_is_fitted\n\nImportError: cannot import name 'parallel_helper'\n\n\nNote - I'm using jupyter notebook (conda_python3) in sagemaker.\nscipy version = 1.3.1\nnumpy version = 1.17.2\nscikit version = 0.21.3 \n\n\none strange thing that i'm unable to figure out is - whenever i do \n\nimport sklearn\nsklearn.__version__\n<\/code><\/pre>\n\n<p>its gives me output as 0.22<\/p>\n\n<p>can someone help me on this issue ? <\/p>",
        "Challenge_closed_time":1600076529168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577358925103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59487643",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":22.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":8.7501200218,
        "Challenge_title":"ImportError: cannot import name 'parallel_helper'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2241.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1493927732160,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation":586.0,
        "Poster_views":158.0,
        "Solution_body":"<p>The best way to overcome this problem in sagemaker is to use lifecycle configuration.\nRather than doing pip install inside the notebook, write all your requirements.txt installs inside the lifecycle configurations. The notebook will take more time to spawn but the code but the libraries will be pre-installed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.97,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1393576024047,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation":879.0,
        "Answerer_views":138.0,
        "Challenge_adjusted_solved_time":5394.9012733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i have started a sagemaker job:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server'{'enabled':False}})\n\ntraining_data_uri ='s3:\/\/path\/to\/my\/data'\nmytraining.fit(training_data_uri,run_tensorboard_locally=True)\n<\/code><\/pre>\n\n<p>using <code>run_tesorboard_locally=True<\/code> gave me<\/p>\n\n<pre><code>Tensorboard is not supported with script mode. You can run the following command: tensorboard --logdir None --host localhost --port 6006 This can be run from anywhere with access to the S3 URI used as the logdir.\n<\/code><\/pre>\n\n<p>It seems like i cant use it script mode, but I can access the logs of tensorboard in s3? But where are the logs in s3?<\/p>\n\n<pre><code>def _parse_args():\n    parser = argparse.ArgumentParser()\n\n    # Data, model, and output directories\n    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n    parser.add_argument('--model_dir', type=str)\n    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n\n    return parser.parse_known_args()\n\nif __name__ == \"__main__\":\n    args, unknown = _parse_args()\n\n    train_data, train_labels = load_training_data(args.train)\n    eval_data, eval_labels = load_testing_data(args.train)\n\n    mymodel= model(train_data, train_labels, eval_data, eval_labels)\n\n    if args.current_host == args.hosts[0]:\n        mymodel.save(os.path.join(args.sm_model_dir, '000000002\/model.h5'))\n<\/code><\/pre>\n\n<p>similiar question is here :<a href=\"https:\/\/stackoverflow.com\/questions\/53713660\/tensorboard-without-callbacks-for-keras-docker-image-in-sagemaker\">stack<\/a><\/p>\n\n<p>EDIT i tried this new config but it doesnt work.<\/p>\n\n<pre><code> tensorboard_output_config = TensorBoardOutputConfig( s3_output_path='s3:\/\/PATH\/to\/my\/bucket')\n\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server': {'enabled':False}},\n                        tensorboard_output_config=tensorboard_output_config)\n<\/code><\/pre>\n\n<p>i added the callback in my model.py script that is actually what i use without sagemaker. As logdir i defined the default dir, where the TensoboardOutputConfig writes the data.. but it doesnt work. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_TensorBoardOutputConfig.html\" rel=\"nofollow noreferrer\">docs<\/a> I also used it without the callback.<\/p>\n\n<pre><code> tensorboardCallback = tf.keras.callbacks.TensorBoard(\n        log_dir='\/opt\/ml\/output\/tensorboard',\n        histogram_freq=0,\n        # batch_size=32,ignored tf.2.0\n        write_graph=True,\n        write_grads=False,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None,\n        embeddings_data=None,\n        update_freq='batch') \n<\/code><\/pre>",
        "Challenge_closed_time":1604514854867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585083712870,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1585093210283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60839279",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":43.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":8.5938837226,
        "Challenge_title":"how can i use tensorboard with aws sagemaker tensorflow?",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1011.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1455058326760,
        "Poster_location":null,
        "Poster_reputation":1337.0,
        "Poster_views":214.0,
        "Solution_body":"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.<\/p>\n<ol>\n<li><p>Followed guide on <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook\" rel=\"noreferrer\">sagemaker debugging<\/a> to configure the <code>S3<\/code> output path for tensorboard logs.<\/p>\n<pre><code>from sagemaker.debugger import TensorBoardOutputConfig\n\ntensorboard_output_config = TensorBoardOutputConfig(\n       s3_output_path = 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n)\n\nestimator = TensorFlow(entry_point='train.py',\n               source_dir='.\/',\n               model_dir=model_dir,\n               output_path= output_dir,\n               train_instance_type=train_instance_type,\n               train_instance_count=1,\n               hyperparameters=hyperparameters,\n               role=sagemaker.get_execution_role(),\n               base_job_name='Testing-TrainingJob',\n               framework_version='2.2',\n               py_version='py37',\n               script_mode=True,\n               tensorboard_output_config=tensorboard_output_config)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<\/li>\n<li><p>Start the tensorboard with the <code>S3<\/code> location provided above via a terminal on the notebook instance.<\/p>\n<pre><code>$ tensorboard --logdir 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n<\/code><\/pre>\n<\/li>\n<li><p>Access the board via URL with <code>\/proxy\/6006\/<\/code>. You need to update the notebook instance details in the following URL.<\/p>\n<pre><code>https:\/\/myinstance.notebook.us-east-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":20.74,
        "Solution_score":5.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1539125320752,
        "Answerer_location":null,
        "Answerer_reputation":61.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":5233.5912852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Challenge_closed_time":1600204151310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581363222683,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":47.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.5630440468,
        "Challenge_title":"Tensorflow Parameter Servers on SageMaker",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":350,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1416337478872,
        "Poster_location":null,
        "Poster_reputation":101.0,
        "Poster_views":13.0,
        "Solution_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.32,
        "Solution_score":1.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1391261341596,
        "Answerer_location":null,
        "Answerer_reputation":76.0,
        "Answerer_views":4.0,
        "Challenge_adjusted_solved_time":5196.6227847222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Challenge_closed_time":1600285333648,
        "Challenge_comment_count":1,
        "Challenge_created_time":1581577491623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60202189",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.5559566433,
        "Challenge_title":"How to create azure machine learning resource using terraform resource providers?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1152.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1565633099383,
        "Poster_location":null,
        "Poster_reputation":110.0,
        "Poster_views":12.0,
        "Solution_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.1,
        "Solution_reading_time":11.31,
        "Solution_score":4.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1412713062067,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation":3603.0,
        "Answerer_views":228.0,
        "Challenge_adjusted_solved_time":5092.1609733333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to know if it's possible to get an Amazon ECR container URI for a specific image programmatically (using AWS CLI or Python). For example, if I need the URL for the latest <code>linear-learner<\/code> (built-in model) image for the <code>eu-central-1<\/code> region.<\/p>\n<p>Expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:latest\n<\/code><\/pre>\n<p>EDIT: I have found the solution with <code>get_image_uri<\/code>. It looks like this function will be depreceated and I don't know how to use <code>ImageURIProvider<\/code> instead.<\/p>",
        "Challenge_closed_time":1617813746592,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599474994217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1599481967088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63775893",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.5360341568,
        "Challenge_title":"How to get an Amazon ECR container URI for a specific model image in Sagemaker?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":3017.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1511210305768,
        "Poster_location":"Bad Orb, Germany",
        "Poster_reputation":12908.0,
        "Poster_views":1267.0,
        "Solution_body":"<p>The newer versions of SageMaker SDK have a more centralized API for getting the URIs:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker \nsagemaker.image_uris.retrieve(&quot;linear-learner&quot;, &quot;eu-central-1&quot;)\n<\/code><\/pre>\n<p>which gives the expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.7,
        "Solution_reading_time":5.24,
        "Solution_score":4.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":5056.4061419444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Challenge_closed_time":1657872107440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639669045329,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":43.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":8.5286090107,
        "Challenge_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":326,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":5.95,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":3,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.4703228499,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Kubernetes Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1550779047856,
        "Answerer_location":null,
        "Answerer_reputation":363.0,
        "Answerer_views":9.0,
        "Challenge_adjusted_solved_time":4538.5667388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Challenge_closed_time":1659424830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643025892210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1643085989932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.4242575114,
        "Challenge_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1501131989640,
        "Poster_location":null,
        "Poster_reputation":329.0,
        "Poster_views":88.0,
        "Solution_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":10.14,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":66.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1530826195963,
        "Answerer_location":null,
        "Answerer_reputation":202.0,
        "Answerer_views":30.0,
        "Challenge_adjusted_solved_time":4541.5040575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Challenge_closed_time":1660920166467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644569855830,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1644570751860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.421288485,
        "Challenge_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1446577693503,
        "Poster_location":null,
        "Poster_reputation":361.0,
        "Poster_views":32.0,
        "Solution_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.65,
        "Solution_score":2.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1393021961043,
        "Answerer_location":null,
        "Answerer_reputation":838.0,
        "Answerer_views":193.0,
        "Challenge_adjusted_solved_time":4512.6045455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following this example notebook to learn SageMaker's processing jobs API: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb<\/a><\/p>\n<p>I'm trying to modify their code to avoid using the default S3 bucket, namely: <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code><\/p>\n<p>For their data processing step with the <code>.run<\/code> method:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>I was able to modify it to use my own S3 bucket by using the <code>destination<\/code> parameter like this:<\/p>\n<pre><code>sklearn_processor.run( \n    code=output_bucket_uri + &quot;preprocessing.py&quot;, \n    inputs=[ProcessingInput( \n        source=input_bucket_uri + &quot;census-income.csv&quot;, \n        destination=path+&quot;input\/&quot;, \n    )], \n    outputs=[ \n        ProcessingOutput( \n            output_name=&quot;train_data&quot;, \n            source=path+&quot;train\/&quot;, \n            destination=output_bucket_uri + &quot;train\/&quot;, \n        ), \n        ProcessingOutput( \n            output_name=&quot;test_data&quot;, \n            source=path+&quot;test\/&quot;, \n            destination=output_bucket_uri + &quot;test\/&quot;, \n        ), \n    ], \n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;], \n)\n<\/code><\/pre>\n<p>But for the <code>.fit<\/code> method:<\/p>\n<pre><code>sklearn.fit({&quot;train&quot;: preprocessed_training_data})\n<\/code><\/pre>\n<p>I have not been able to find a parameter to pass it so that the output artifacts are saved to a S3 bucket that I specify instead of the default s3 bucket <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code>.<\/p>",
        "Challenge_closed_time":1648557908696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632276763933,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1632312532332,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69277390",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":26.2,
        "Challenge_reading_time":32.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":8.417050199,
        "Challenge_title":"Can I specify S3 bucket for sagemaker.sklearn.estimator's SKLearn?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1329161697310,
        "Poster_location":null,
        "Poster_reputation":4154.0,
        "Poster_views":314.0,
        "Solution_body":"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:<\/p>\n<pre><code>from sagemaker.session import Session    \nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role='&lt;arn-role&gt;',\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1,\n                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))\n<\/code><\/pre>\n<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.96,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Model Registry",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1418476563283,
        "Answerer_location":null,
        "Answerer_reputation":676.0,
        "Answerer_views":113.0,
        "Challenge_adjusted_solved_time":51185.3566025,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.<\/p>",
        "Challenge_closed_time":1474621313252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458446579210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459352400923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36110109",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.4104943758,
        "Challenge_title":"Has anyone any experience on implementing the R Package XGBoost within the Azure ML Studio environment?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1428820274636,
        "Poster_location":"Lexington, KY, United States",
        "Poster_reputation":65.0,
        "Poster_views":21.0,
        "Solution_body":"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.<\/p>\n<p>You can follow the instructions over <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Using-XGBoost-to-build-Graduation-Admit-Model-1\" rel=\"nofollow noreferrer\">here<\/a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/archive\/blogs\/benjguin\/how-to-upload-an-r-package-to-azure-machine-learning\" rel=\"nofollow noreferrer\">This<\/a> is for any generic packages which are not preloaded in the environment<\/p>\n<p>The following is a <a href=\"https:\/\/web.archive.org\/web\/20161020215633\/https:\/\/azure.microsoft.com\/en-in\/documentation\/articles\/machine-learning-r-csharp-web-service-examples\/\" rel=\"nofollow noreferrer\">list of experiments<\/a> to publish R models as a web service<\/p>\n<p>Hope this helps!<\/p>\n<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643619684692,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":14.54,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1411546424280,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation":58.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":4477.9070705556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1580113898207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":8.4071343382,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1531840489147,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation":425.0,
        "Poster_views":92.0,
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1558539179612,
        "Answerer_location":null,
        "Answerer_reputation":16.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":4461.4114333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Challenge_closed_time":1607117580710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591056499550,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.403444579,
        "Challenge_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1392607100776,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation":133.0,
        "Poster_views":29.0,
        "Solution_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.55,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1490025251112,
        "Answerer_location":null,
        "Answerer_reputation":349.0,
        "Answerer_views":24.0,
        "Challenge_adjusted_solved_time":4421.6864666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run linear learner on a simple dataset.  My csv of data is uploaded to a bucket.  The problem is that when I run it I get the following error:<\/p>\n\n<pre><code>UnexpectedStatusException: Error for Training job linear-learner-2020-05-23-22-31-40-894: Failed. Reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [22:34:37] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.2746.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number.\n<\/code><\/pre>\n\n<p>I did some googling and it says to change the content_type to 'text\/csv'.  My question is, how do I do this?  Or does anyone know how to get this working?  Thanks!  Here is my linear learner code:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n\nlinear = sagemaker.estimator.Estimator(container,\n                                      role,\n                                      train_instance_count = 1,\n                                      train_instance_type = 'ml.c4.xlarge',\n                                      output_path = output_location,\n                                      sagemaker_session = sess)\n\nlinear.set_hyperparameters(predictor_type = 'regressor',\n                          mini_batch_size = 200)\n<\/code><\/pre>",
        "Challenge_closed_time":1606193228063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590275156783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61979691",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":17.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":8.3945025883,
        "Challenge_title":"Changing input type for linear learner to csv",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":406.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1585954841920,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation":163.0,
        "Poster_views":31.0,
        "Solution_body":"<p>You can use SageMaker input channels:<\/p>\n<pre><code>\ntrain_data = sagemaker.inputs.TrainingInput(\n    \"s3:\/\/my-bucket\/path\/to\/train\",\n    distribution=\"FullyReplicated\",\n    content_type=\"text\/csv\",\n    s3_data_type=\"S3Prefix\",\n    record_wrapping=None,\n    compression=None\n)\n\nvalidation_data = sagemaker.inputs.TrainingInput(\n    \"s3:\/\/my-bucket\/path\/to\/validation\",\n    distribution=\"FullyReplicated\",\n    content_type=\"text\/csv\",\n    s3_data_type=\"S3Prefix\",\n    record_wrapping=None,\n    compression=None\n)\n\nlinear.fit({\"train\": train_data, \"validation\": validation_data})\n<\/pre><\/code>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/linear_learner_abalone\/Linear_Learner_Regression_csv_format.ipynb\" rel=\"nofollow noreferrer\">See this example<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":44.9,
        "Solution_reading_time":10.77,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628006476000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.3944125636,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":164,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score":0.0,
        "Solution_sentence_count":14.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":15,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":8.3936600933,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score":0.0,
        "Solution_sentence_count":227.0,
        "Solution_topic":"Kubernetes Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1626973312768,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":92.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":4407.5466925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having the error mentioned in the title when trying to upload a large file (15gb) to my s3 bucket from a Sagemaker notebook instance.<\/p>\n<p>I know that there are some similar questions here that i have already visited. I have gone through <a href=\"https:\/\/stackoverflow.com\/questions\/52541933\/accessdenied-when-calling-the-createmultipartupload-operation-in-django-using-dj\">this<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/37630635\/createmultipartupload-operation-aws-policy-items-needed\">this<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/36272286\/getting-access-denied-when-calling-the-putobject-operation-with-bucket-level-per\">this<\/a> question, but after following the steps mentioned, and applying the policies described in these questions i still have the same error.<\/p>\n<p>I have also come to <a href=\"https:\/\/aws.amazon.com\/es\/premiumsupport\/knowledge-center\/s3-access-denied-error-kms\/#:%7E:text=%22An%20error%20occurred%20(AccessDenied)%20when%20calling%20the%20CreateMultipartUpload%20operation,GenerateDataKey%20and%20kms%3ADecrypt%20actions.\" rel=\"nofollow noreferrer\">this<\/a> documentation page eventually. The problem is that when i go into my users page in the IAM section, i see no users. I can see some roles but no users and i don't know which role should i edit following the steps mentioned in the documentation page. Also, my bucket DON'T have encryption enabled so i'm not really sure that the steps in the documentation page will fix the error for me.<\/p>\n<p>This is the policy in currently using for my bucket:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Id&quot;: &quot;Policy1&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::XXXX:root&quot;\n            },\n            &quot;Action&quot;: &quot;s3:*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::bauer-bucket&quot;,\n                &quot;arn:aws:s3:::bauer-bucket\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I'm totally lost with this, i need to upload that file to my bucket. Please help.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1645238505323,
        "Challenge_comment_count":2,
        "Challenge_created_time":1629371337230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68846704",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":16.2,
        "Challenge_reading_time":29.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":8.3913003659,
        "Challenge_title":"An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1521854999168,
        "Poster_location":"Spain",
        "Poster_reputation":1338.0,
        "Poster_views":265.0,
        "Solution_body":"<p>The access is dictated by the execution role that is attached to the SageMaker notebook. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> goes through how add additional s3 permissions to a SageMaker execution role.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.3,
        "Solution_reading_time":4.72,
        "Solution_score":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.3778000171,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score":0.0,
        "Solution_sentence_count":17.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1572208191120,
        "Answerer_location":null,
        "Answerer_reputation":139.0,
        "Answerer_views":45.0,
        "Challenge_adjusted_solved_time":4337.3736888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Challenge_closed_time":1662537228663,
        "Challenge_comment_count":3,
        "Challenge_created_time":1646922683383,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.3752548308,
        "Challenge_title":"Run Sagemaker notebook instance and be able to close tab",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1154.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1572208191120,
        "Poster_location":null,
        "Poster_reputation":139.0,
        "Poster_views":45.0,
        "Solution_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":9.86,
        "Solution_score":0.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585108798000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.3360831005,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566233711000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.3217203341,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":4025.8450966667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Challenge_closed_time":1639574747348,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625081705000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":8.3007384939,
        "Challenge_title":"How to pass data to an endpoint",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":201,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.16,
        "Solution_score":0.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":143.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":8.2725972861,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score":0.0,
        "Solution_sentence_count":14.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1291793452900,
        "Answerer_location":"Berkeley, CA, United States",
        "Answerer_reputation":752.0,
        "Answerer_views":120.0,
        "Challenge_adjusted_solved_time":3886.8446766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Challenge_closed_time":1633922581063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619929277080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1619929940227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":41.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.2656575946,
        "Challenge_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":280,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1375186444008,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation":912.0,
        "Poster_views":288.0,
        "Solution_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":57.4,
        "Solution_reading_time":11.41,
        "Solution_score":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation":7916.0,
        "Answerer_views":801.0,
        "Challenge_adjusted_solved_time":3708.3794758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1533111465720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1519761299607,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":8.2186198844,
        "Challenge_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1457596845392,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation":4046.0,
        "Poster_views":825.0,
        "Solution_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.65,
        "Solution_score":3.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1646219230528,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":3670.8803794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Challenge_closed_time":1659435013616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646219844250,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":18.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":8.2084591748,
        "Challenge_title":"What are valid Azure ML Workspace connection argument options?",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1646219230528,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":1.0,
        "Solution_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":8.53,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641941774000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":8.2062214903,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score":1.0,
        "Solution_sentence_count":15.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":261.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1361339272692,
        "Answerer_location":"NYC",
        "Answerer_reputation":6281.0,
        "Answerer_views":958.0,
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.1973038133,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1370231111260,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation":5295.0,
        "Poster_views":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1443426419048,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation":842.0,
        "Answerer_views":219.0,
        "Challenge_adjusted_solved_time":3592.1438852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Challenge_closed_time":1466402236047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453470518060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.1867828323,
        "Challenge_title":"Azure: plot without labels",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1436432728608,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation":809.0,
        "Poster_views":361.0,
        "Solution_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":3.66,
        "Solution_score":-1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1389049461896,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":455.0,
        "Answerer_views":108.0,
        "Challenge_adjusted_solved_time":3574.5224019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Challenge_closed_time":1486144727963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1473271529687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1473276447316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.1822485422,
        "Challenge_title":"How to call Tensorflow in Azure ML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1743.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1435075201580,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation":546.0,
        "Poster_views":59.0,
        "Solution_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":4.47,
        "Solution_score":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1612454694036,
        "Answerer_location":"Mexico City, CDMX, Mexico",
        "Answerer_reputation":36.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":3418.0769397223,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Challenge_closed_time":1614800344830,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602495267847,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615561798207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":48.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":8.137125893,
        "Challenge_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":785.0,
        "Challenge_word_count":386,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1600260166047,
        "Poster_location":null,
        "Poster_reputation":15.0,
        "Poster_views":4.0,
        "Solution_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1614801695372,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":40.74,
        "Solution_score":1.0,
        "Solution_sentence_count":25.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":253.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation":224.0,
        "Answerer_views":40.0,
        "Challenge_adjusted_solved_time":3528.4438297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Challenge_closed_time":1627578025127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615324569760,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":38.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":8.132923153,
        "Challenge_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1575485255683,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":3.0,
        "Solution_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1628026967547,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":4.2,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1565289301123,
        "Answerer_location":null,
        "Answerer_reputation":79.0,
        "Answerer_views":13.0,
        "Challenge_adjusted_solved_time":3320.1937519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the different data sources we can import data into Azure Machine Learning Services storage or notebook. I mean from Salesforce or any ERP or any website? As of now I have seen importing data using URL or getting it from data location in storage where notebook will also be stored.<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Challenge_closed_time":1566330717407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554359325097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554378019900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55509207",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.1096419346,
        "Challenge_title":"Data sources in Azure Machine Learning Services",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1545289999703,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":10.0,
        "Solution_body":"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. \nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data<\/a><\/p>\n\n<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.\nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.7,
        "Solution_reading_time":10.46,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":8.1093494232,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score":1.0,
        "Solution_sentence_count":25.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":296.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1344510903550,
        "Answerer_location":"Hannover, Germany",
        "Answerer_reputation":33554.0,
        "Answerer_views":2182.0,
        "Challenge_adjusted_solved_time":3240.8772311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Challenge_closed_time":1510651584203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1498984762217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1498985131100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":29.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":8.0838790391,
        "Challenge_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1344510903550,
        "Poster_location":"Hannover, Germany",
        "Poster_reputation":33554.0,
        "Poster_views":2182.0,
        "Solution_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1510652289132,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":8.51,
        "Solution_score":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1406731060412,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation":139.0,
        "Answerer_views":6.0,
        "Challenge_adjusted_solved_time":3215.9254886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Challenge_closed_time":1626975605176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615398273417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":8.0761813652,
        "Challenge_title":"How to setup AWS sagemaker - Resource limit Error",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1530468231707,
        "Poster_location":null,
        "Poster_reputation":357.0,
        "Poster_views":72.0,
        "Solution_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.1,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587712154000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.0568253881,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score":0.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1363352099923,
        "Answerer_location":"Washington, DC, USA",
        "Answerer_reputation":828.0,
        "Answerer_views":530.0,
        "Challenge_adjusted_solved_time":2983.5262194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Challenge_closed_time":1545063624710,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534309681237,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1534322930320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.0024286604,
        "Challenge_title":"Error Tracking in Amazon SageMaker",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1363352099923,
        "Poster_location":"Washington, DC, USA",
        "Poster_reputation":828.0,
        "Poster_views":530.0,
        "Solution_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.57,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1384530039387,
        "Answerer_location":"Ljubljana, Slovenia",
        "Answerer_reputation":2470.0,
        "Answerer_views":285.0,
        "Challenge_adjusted_solved_time":2971.3031783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Challenge_closed_time":1638951877412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628255185970,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1639139946700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":30.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.9970924122,
        "Challenge_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":244,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1384530039387,
        "Poster_location":"Ljubljana, Slovenia",
        "Poster_reputation":2470.0,
        "Poster_views":285.0,
        "Solution_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639131288447,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":24.61,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1630695701727,
        "Answerer_location":null,
        "Answerer_reputation":101.0,
        "Answerer_views":5.0,
        "Challenge_adjusted_solved_time":2874.6210316667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a requirement to use azure machine learning to develop a pipeline. In this pipeline we don't pass data as inputs\/outputs but variables (for example a list or an int). I have looked on the Microsoft documentation but could not seem to find something fitting my case. Also tried to use the PipelineData class but could not retrieve my variables.<\/p>\n<ol>\n<li>Is this possible?<\/li>\n<li>Is this a good approach?<\/li>\n<\/ol>\n<p>Thanks for your help.<\/p>",
        "Challenge_closed_time":1658826630380,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648478111533,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71649163",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.9640126518,
        "Challenge_title":"Can azureml pass variables from one step to another?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":399.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1648477773363,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":2.0,
        "Solution_body":"<p>I know I'm a bit late to the party but here we go:<\/p>\n<p><strong>Passing variables between AzureML Pipeline Steps<\/strong><\/p>\n<p>To directly answer your question, to my knowledge it is not possible to pass variables directly between PythonScriptSteps in an AzureML Pipeline.<\/p>\n<p>The reason for that is that the steps are executed in isolation, i.e. the code is run in different processes or even computes. The only interface a PythonScriptStep has is (a) command line arguments that need to be set prior to submission of the pipeline and (b) data.<\/p>\n<p><strong>Using datasets to pass information between PythonScriptSteps<\/strong><\/p>\n<p>As a workaround you can use PipelineData to pass data between steps.\nThe previously posted blog post may help: <a href=\"https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/\" rel=\"nofollow noreferrer\">https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/<\/a><\/p>\n<p>As for your concrete problem:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pipeline.py\n\n# This will make Azure create a unique directory on the datastore everytime the pipeline is run.\nvariables_data = PipelineData(&quot;variables_data&quot;, datastore=datastore)\n\n# `variables_data` will be mounted on the target compute and a path is given as a command line argument\nwrite_variable = PythonScriptStep(\n    script_name=&quot;write_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    outputs=[variables_data],\n)\n\nread_variable = PythonScriptStep(\n    script_name=&quot;read_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    inputs=[variables_data],\n)\n\n<\/code><\/pre>\n<p>In your script you'll want to serialize the variable \/ object that you're trying to pass between steps:<\/p>\n<p>(You could of course use JSON or any other serialization method)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># write_variable.py\n\nimport argparse\nimport pickle\nfrom pathlib import Path\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\nobj = [1, 2, 3, 4]\n\nPath(args.data_path).mkdir(parents=True, exist_ok=True)\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;wb&quot;) as f:\n    pickle.dump(obj, f)\n<\/code><\/pre>\n<p>Finally, you can read the variable in the next step:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># read_variable.py\n\nimport argparse\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\n\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;rb&quot;) as f:\n    obj = pickle.load(f)\n\nprint(obj)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1658826747247,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":34.54,
        "Solution_score":1.0,
        "Solution_sentence_count":23.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":274.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621374020000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":7.9521862864,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":325,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score":1.0,
        "Solution_sentence_count":28.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":260.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1636044845360,
        "Answerer_location":null,
        "Answerer_reputation":241.0,
        "Answerer_views":9.0,
        "Challenge_adjusted_solved_time":2833.1668325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Challenge_closed_time":1661350818332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651142988007,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651151719750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.9503291465,
        "Challenge_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1648140384823,
        "Poster_location":null,
        "Poster_reputation":23.0,
        "Poster_views":2.0,
        "Solution_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661351120347,
        "Solution_link_count":3.0,
        "Solution_readability":12.3,
        "Solution_reading_time":8.08,
        "Solution_score":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":57.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1501040260887,
        "Answerer_location":null,
        "Answerer_reputation":66.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":2802.1180147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Challenge_closed_time":1625508026043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615420401190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.9384876533,
        "Challenge_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1588429621780,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation":21.0,
        "Poster_views":10.0,
        "Solution_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":7.65,
        "Solution_score":2.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1553882107003,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":294.0,
        "Answerer_views":28.0,
        "Challenge_adjusted_solved_time":2777.7798802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Challenge_closed_time":1600448991412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590448983843,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":51.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.9297672184,
        "Challenge_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":418,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1359061977540,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation":427.0,
        "Poster_views":62.0,
        "Solution_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":6.48,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1456986606312,
        "Answerer_location":null,
        "Answerer_reputation":757.0,
        "Answerer_views":80.0,
        "Challenge_adjusted_solved_time":2717.5608175,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Challenge_closed_time":1574121568172,
        "Challenge_comment_count":1,
        "Challenge_created_time":1564579246737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":10.7,
        "Challenge_reading_time":18.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.8829354219,
        "Challenge_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1429630461500,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation":938.0,
        "Poster_views":137.0,
        "Solution_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1574362465680,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":5.35,
        "Solution_score":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1626973229036,
        "Answerer_location":null,
        "Answerer_reputation":199.0,
        "Answerer_views":37.0,
        "Challenge_adjusted_solved_time":2502.7853702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing an issue while invoking the Pytorch model Endpoint. Please check the below error for detail.<\/p>\n<p>Error Message:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4.<\/p>\n<\/blockquote>\n<p>Automatically restarted the Endpoint after this error. No specific log in cloud watch.<\/p>",
        "Challenge_closed_time":1626976733376,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617966706043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1631708929472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67020040",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.8255590138,
        "Challenge_title":"Sagemaker Pytorch model - An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4):",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":407.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1582178023440,
        "Poster_location":"Palanpur, Gujarat, India",
        "Poster_reputation":3.0,
        "Poster_views":4.0,
        "Solution_body":"<p>There may be a few issues here we can explore the paths and ways to resolve.<\/p>\n<ol>\n<li>Inference Code Error\nSometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.<\/li>\n<\/ol>\n<pre><code>from sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)\nprint(rf_pred.predict(payload).decode('utf-8'))\n<\/code><\/pre>\n<p>For more information about the different serializers based off the type of data you are feeding in check the following link.\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html<\/a><\/p>\n<ol start=\"2\">\n<li>Throttling Limits Reached\nSometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/<\/a><\/p>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979329227,
        "Solution_link_count":4.0,
        "Solution_readability":12.9,
        "Solution_reading_time":22.27,
        "Solution_score":0.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":201.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1611181716003,
        "Answerer_location":null,
        "Answerer_reputation":119.0,
        "Answerer_views":5.0,
        "Challenge_adjusted_solved_time":2450.9835063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Challenge_closed_time":1630895031176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622071490553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.8046525704,
        "Challenge_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1611181716003,
        "Poster_location":null,
        "Poster_reputation":119.0,
        "Poster_views":5.0,
        "Solution_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.88,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1336700249823,
        "Answerer_location":null,
        "Answerer_reputation":885.0,
        "Answerer_views":127.0,
        "Challenge_adjusted_solved_time":2447.0429822222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Challenge_closed_time":1551699854476,
        "Challenge_comment_count":2,
        "Challenge_created_time":1542890499740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":7.8030442016,
        "Challenge_title":"Sacred - pass all parameters as one",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":735.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1443016881603,
        "Poster_location":"Israel",
        "Poster_reputation":9385.0,
        "Poster_views":1033.0,
        "Solution_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":5.35,
        "Solution_score":4.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":33.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1314097464768,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation":11056.0,
        "Answerer_views":544.0,
        "Challenge_adjusted_solved_time":0.0373211111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Challenge_closed_time":1503919410423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1495124614783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1503919276067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.8013908247,
        "Challenge_title":"Can I make Neptune talk to git?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1314097464768,
        "Poster_location":"Warsaw, Poland",
        "Poster_reputation":11056.0,
        "Poster_views":544.0,
        "Solution_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":28.7,
        "Solution_reading_time":3.2,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":14.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1403541426412,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation":191.0,
        "Answerer_views":22.0,
        "Challenge_adjusted_solved_time":2355.5938513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Challenge_closed_time":1439539745912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431059608047,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1446192965568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.7649725722,
        "Challenge_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1403541426412,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation":191.0,
        "Poster_views":22.0,
        "Solution_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.0,
        "Solution_score":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2331.6441666667,
        "Challenge_answer_count":6,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png)\r\n\r\nThis issue wasn't present until a few days ago. Issue shows up when we submit an experiment to azure ml workspace in the image build logs. We are using mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Challenge_closed_time":1623755236000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615361317000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1387",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.7547577386,
        "Challenge_title":"azure ml Python SDK 1.24.0 image build fails with the error failed to get layer was working fine before",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"I would think that is some transient issue. \r\nSide note, mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 was deprecated in 2019, please use \r\nmcr.microsoft.com\/azureml\/intelmpi2018.3-ubuntu16.04 or better default cpu image from your client version that is pinned to a versioned tag Still facing the same issue. Kind of blocked is their some way i could fix this. Updated the image thanks for that is it local build? if yes, try to remove the image Nope it is remote compute build. It is in AML Compute cluster The issue got resolved we were earlier creating an image and storing it in Azure Container Registry. Now we don't pass it to RunConfiguration() object. We create it directly in the AML Build process and that has fixed the issue though now the image is not cached anymore so that is problematic. @MAQ-Ravijit-Ramana it would be great to get some details of your scenario, like the script you running",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":11.27,
        "Solution_score":0.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633792312000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":11,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":7.7541933651,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":786,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score":1.0,
        "Solution_sentence_count":23.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":238.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1376606307612,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation":1567.0,
        "Answerer_views":159.0,
        "Challenge_adjusted_solved_time":8936.4835138889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Challenge_closed_time":1596561017227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588239469173,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.7461022535,
        "Challenge_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1884.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation":795.0,
        "Poster_views":37.0,
        "Solution_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1620410809823,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":3.98,
        "Solution_score":7.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":8,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612300739000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.7013119893,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score":4.0,
        "Solution_sentence_count":22.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":333.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1324808381143,
        "Answerer_location":null,
        "Answerer_reputation":9050.0,
        "Answerer_views":1750.0,
        "Challenge_adjusted_solved_time":2198.1168433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1623834809928,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7.6984020999,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1324808381143,
        "Poster_location":null,
        "Poster_reputation":9050.0,
        "Poster_views":1750.0,
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596320174000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.6633998365,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":9,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647978773000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.6617632626,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score":2.0,
        "Solution_sentence_count":25.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1549666221947,
        "Answerer_location":null,
        "Answerer_reputation":56.0,
        "Answerer_views":4.0,
        "Challenge_adjusted_solved_time":2121.3562466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Challenge_closed_time":1582590253856,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574954456480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.7,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.6601401561,
        "Challenge_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1574954057270,
        "Poster_location":"Carlow, Ireland",
        "Poster_reputation":13.0,
        "Poster_views":0.0,
        "Solution_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1582591338968,
        "Solution_link_count":4.0,
        "Solution_readability":21.5,
        "Solution_reading_time":16.66,
        "Solution_score":0.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1510046220943,
        "Answerer_location":null,
        "Answerer_reputation":292.0,
        "Answerer_views":43.0,
        "Challenge_adjusted_solved_time":2094.66748,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>My search didn't yield anything useful so I was wondering if there is any easy way to copy notebooks from one instance to another instance on Sagemaker? Of course other than manually downloading the notebooks on one instance and uploading to the other one!<\/p>",
        "Challenge_closed_time":1544938600760,
        "Challenge_comment_count":1,
        "Challenge_created_time":1537397018090,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1537397797832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52415136",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":9.9,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.6477307362,
        "Challenge_title":"How to copy notebooks between different Sagemaker instances?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5303.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1444524456120,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation":973.0,
        "Poster_views":82.0,
        "Solution_body":"<p>The recommended way to do this (as of 12\/16\/2018) would be to use the newly- launched Git integration for SageMaker Notebook Instances.<\/p>\n\n<ol>\n<li>Create a Git repository for your notebooks<\/li>\n<li>Commit and push changes from Notebook Instance #1 to your Git repo<\/li>\n<li>Start Notebook Instance #2 using the same Git repo<\/li>\n<\/ol>\n\n<p>This way your notebooks are persisted in the Git repo rather than on the  instance, and the Git repo can be shared by multiple instances. <\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.5,
        "Solution_reading_time":11.4,
        "Solution_score":3.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1621392911643,
        "Answerer_location":"London, UK",
        "Answerer_reputation":56.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":2073.4166358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Challenge_closed_time":1621508759232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614044459343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":49.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":7.6374352538,
        "Challenge_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":374,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1483140008952,
        "Poster_location":"San Francisco Bay Area, CA, United States",
        "Poster_reputation":604.0,
        "Poster_views":35.0,
        "Solution_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.2,
        "Solution_reading_time":15.17,
        "Solution_score":1.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1528629350990,
        "Answerer_location":"Pakistan",
        "Answerer_reputation":439.0,
        "Answerer_views":51.0,
        "Challenge_adjusted_solved_time":5616.0211319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pre-trained model which I am loading in AWS SageMaker Notebook Instance from S3 Bucket and upon providing a test image for prediction from S3 bucket it gives me the accurate results as required. I want to deploy it so that I can have an endpoint which I can further integrate with AWS Lambda Function and AWS API GateWay so that I can use the model with real time application.\nAny idea how can I deploy the model from AWS Sagemaker Notebook Instance and get its endpoint?\nCode inside the <code>.ipynb<\/code> file is given below for reference.<\/p>\n<pre><code>import boto3\nimport pandas as pd\nimport sagemaker\n#from sagemaker import get_execution_role\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport json\n#role = get_execution_role()\nrole = sagemaker.get_execution_role()\n\nbucketname = 'bucket' # bucket where the model is hosted\nfilename = 'test_model.h5' # name of the model\ns3 = boto3.resource('s3')\nimage= s3.Bucket(bucketname).download_file(filename, 'test_model_new.h5')\nmodel= 'test_model_new.h5'\n\nmodel = load_model(model)\n\nbucketname = 'bucket' # name of the bucket where the test image is hosted\nfilename = 'folder\/image.png' # prefix\ns3 = boto3.resource('s3')\nfile= s3.Bucket(bucketname).download_file(filename, 'image.png')\nfile_name='image.png'\n\ntest=np.array([resize(imread(file_name), (137, 310, 3))])\n\ntest_predict = model.predict(test)\n\nprint ((test_predict &gt; 0.5).astype(np.int))\n<\/code><\/pre>",
        "Challenge_closed_time":1616234632040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608787488573,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65434323",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":20.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.6351352552,
        "Challenge_title":"How to deploy a Pre-Trained model using AWS SageMaker Notebook Instance?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1387.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1528629350990,
        "Poster_location":"Pakistan",
        "Poster_reputation":439.0,
        "Poster_views":51.0,
        "Solution_body":"<p>Here is the solution that worked for me. Simply follow the following steps.<\/p>\n<p>1 - Load your model in the SageMaker's jupyter environment with the help of<\/p>\n<pre><code>from keras.models import load_model\n\nmodel = load_model (&lt;Your Model name goes here&gt;) #In my case it's model.h5\n<\/code><\/pre>\n<p>2 - Now that the model is loaded convert it into the <code>protobuf format<\/code> that is required by <code>AWS<\/code> with the help of<\/p>\n<pre><code>def convert_h5_to_aws(loaded_model):\n\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\n\nmodel_version = '1'\nexport_dir = 'export\/Servo\/' + model_version\n# Build the Protocol Buffer SavedModel at 'export_dir'\nbuilder = builder.SavedModelBuilder(export_dir)\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\nfrom keras import backend as K\n\nwith K.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\nimport tarfile\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add('export', recursive=True)\nimport sagemaker\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\nconvert_h5_to_aws(model):\n<\/code><\/pre>\n<p>3 - And now you can deploy your model with the help of<\/p>\n<pre><code>!touch train.py\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.15.2',\n                                  entry_point = 'train.py')\n%%timelog\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This will generate the endpoint which can be seen in the Inference section of the Amazon SageMaker and with the help of that endpoint you can now make predictions from the jupyter notebook as well as from web and mobile applications.\nThis <a href=\"https:\/\/www.youtube.com\/watch?v=RPnvfxR5DY8\" rel=\"nofollow noreferrer\">Youtube tutorial<\/a> by Liam and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">AWS blog<\/a> by Priya helped me alot.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1629005164648,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":34.23,
        "Solution_score":3.0,
        "Solution_sentence_count":28.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592335014000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.6211061502,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":180,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1327481639092,
        "Answerer_location":"Poznan, Poland",
        "Answerer_reputation":2923.0,
        "Answerer_views":838.0,
        "Challenge_adjusted_solved_time":2025.0750175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Challenge_closed_time":1448906765136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441616495073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.6138557115,
        "Challenge_title":"What would be the best ML method for this use case?",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1416688064183,
        "Poster_location":null,
        "Poster_reputation":347.0,
        "Poster_views":51.0,
        "Solution_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.8,
        "Solution_reading_time":3.39,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1480786532470,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation":111.0,
        "Answerer_views":44.0,
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.60953175,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1480786532470,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation":111.0,
        "Poster_views":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score":4.0,
        "Solution_sentence_count":29.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1614711784088,
        "Answerer_location":null,
        "Answerer_reputation":26.0,
        "Answerer_views":0.0,
        "Challenge_adjusted_solved_time":1978.3835261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console<\/a> docs to create device fleet. In this console, Role ARN is optional but it throws <code>RoleARN is required<\/code>. If I provide proper RoleArn it throws <code>Failed to create\/modify RoleAlias. Check your IAM role permission<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have no idea what is going wrong. Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1614723172687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607600991993,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65233943",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.5905407248,
        "Challenge_title":"Unable to create Device Fleet",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1426675778223,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation":10189.0,
        "Poster_views":1471.0,
        "Solution_body":"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":3.95,
        "Solution_score":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1368311783008,
        "Answerer_location":"San Diego, CA",
        "Answerer_reputation":1297.0,
        "Answerer_views":165.0,
        "Challenge_adjusted_solved_time":1972.6603797222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Challenge_closed_time":1637782762627,
        "Challenge_comment_count":5,
        "Challenge_created_time":1630681185260,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1637936310430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":16.5,
        "Challenge_reading_time":34.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":11.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":7.5876451584,
        "Challenge_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2139.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1500824148408,
        "Poster_location":"India",
        "Poster_reputation":4419.0,
        "Poster_views":962.0,
        "Solution_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1637783228436,
        "Solution_link_count":0.0,
        "Solution_readability":23.3,
        "Solution_reading_time":27.94,
        "Solution_score":17.0,
        "Solution_sentence_count":15.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576464430000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":7.5840963299,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":409,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score":1.0,
        "Solution_sentence_count":72.0,
        "Solution_topic":"GPU Training",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":675.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1221528724667,
        "Answerer_location":"West Coast, North America",
        "Answerer_reputation":11340.0,
        "Answerer_views":737.0,
        "Challenge_adjusted_solved_time":1955.6086208333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Challenge_closed_time":1539631654852,
        "Challenge_comment_count":1,
        "Challenge_created_time":1532591463817,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":14.3,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.578967958,
        "Challenge_title":"No space left on device in Sagemaker model training",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2721.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1366408339740,
        "Poster_location":null,
        "Poster_reputation":8057.0,
        "Poster_views":491.0,
        "Solution_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.7,
        "Solution_reading_time":9.56,
        "Solution_score":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655132901000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":7.5729645554,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":298,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score":0.0,
        "Solution_sentence_count":29.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":312.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation":2425.0,
        "Answerer_views":459.0,
        "Challenge_adjusted_solved_time":1833.7168480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Challenge_closed_time":1531369355823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524767975170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":7.5146454423,
        "Challenge_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2600.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1474520506390,
        "Poster_location":null,
        "Poster_reputation":832.0,
        "Poster_views":48.0,
        "Solution_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.35,
        "Solution_score":2.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Challenge_adjusted_solved_time":1803.6491758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1659992618003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653499480970,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.4981214895,
        "Challenge_title":"Remotely execute ClearML task using local-only repo",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1653498830776,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":10.74,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":162.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1425426748316,
        "Answerer_location":null,
        "Answerer_reputation":91.0,
        "Answerer_views":11.0,
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.4893729061,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2765.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1419000630800,
        "Poster_location":null,
        "Poster_reputation":159.0,
        "Poster_views":35.0,
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score":5.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1386098048127,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation":619.0,
        "Answerer_views":25.0,
        "Challenge_adjusted_solved_time":1733.7563858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Challenge_closed_time":1598820674152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592493163510,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592579151163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.4722971112,
        "Challenge_title":"How to find memory leak in Python MXNet?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1369257942212,
        "Poster_location":"London, UK",
        "Poster_reputation":70285.0,
        "Poster_views":13121.0,
        "Solution_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":5.82,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1753.1691666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\nCan one SageMaker host more than one model? Each model then share the same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\nUsing Multi-model Inference endpoints: Amazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found here. The sample code can be found here. Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\nUsing Bring your own algorithm on SageMaker You can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker here\n\nUsing Multi-model serving container by using multi-model archive file You can find a sample example here [4] for tensorflow serving\n\nIf models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint Sagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Challenge_closed_time":1593677528000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587366119000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sage-maker-with-multiple-models",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":43.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":7.4697506145,
        "Challenge_title":"SageMaker with multiple models",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":254.0,
        "Challenge_word_count":415,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Customer does not want to spin-up different containers for each model due to network adding additional latency.\n\nI am assuming this is a pipeline scenario where different models need to be chained. If so, it's important to keep in mind that all containers in pipeline run on the same EC2 instance so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":6.32,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1679.0761111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Challenge_closed_time":1632957644000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626912970000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.4265943756,
        "Challenge_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hi @Roshin29, thank you for the bug report! \r\n\r\nThe machine learning sample notebooks received substantial revisions in [Release 3.0.1](https:\/\/github.com\/aws\/graph-notebook\/releases\/tag\/v3.0.1). This release also included a number of changes under the hood to support the general availability release of Amazon Neptune ML.\r\n\r\nThe Gremlin query listed is only seen in older versions of the `Neptune-ML-01-Introduction-to-Node-Classification-Gremlin` sample notebook, and is now replaced by the one below:\r\n```\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"${endpoint}\").\r\n  with(\"Neptune#ml.limit\",3).\r\n  V().has('title', 'Apollo 13 (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\n```\r\nI am not able to reproduce the listed exception when running this query using graph-notebook v3.0.6, so the issue appears to have been resolved with the latest changes.\r\n\r\nClosing this issue out, as there are no further action items at this time. Please feel free to re-open if you have any further questions.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.78,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":123.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651909943000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.4193231272,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1611181716003,
        "Answerer_location":null,
        "Answerer_reputation":119.0,
        "Answerer_views":5.0,
        "Challenge_adjusted_solved_time":1654.0729555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Challenge_closed_time":1630892629443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624934847563,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1624937966803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.4121237468,
        "Challenge_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1611181716003,
        "Poster_location":null,
        "Poster_reputation":119.0,
        "Poster_views":5.0,
        "Solution_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":17.36,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1296746642860,
        "Answerer_location":null,
        "Answerer_reputation":524.0,
        "Answerer_views":48.0,
        "Challenge_adjusted_solved_time":1636.7548416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Challenge_closed_time":1488397699940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1482505382510,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41303697",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.4010815839,
        "Challenge_title":"Use an existing Gateway with Azure Machine Learning?",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1296746642860,
        "Poster_location":null,
        "Poster_reputation":524.0,
        "Poster_views":48.0,
        "Solution_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.12,
        "Solution_score":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1621.4025,
        "Challenge_answer_count":6,
        "Challenge_body":"When training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and wandb enabled:\r\n```\r\nFile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\n",
        "Challenge_closed_time":1591178971000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585341922000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/287",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.04,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.3916633543,
        "Challenge_title":"wandb RuntimeError",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":104,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"There does seem to be an issue with XLNet and ALBERT when using wandb. I haven't found the exact cause yet. I'll look into it again when I can. Thank you. Maybe it should be reported to wandb because it is from its `wandb.watch`?\r\nI temporarily fixed this with `wandb.watch(model, log=None)`. Does it still log the metrics when `log` is set to `None`? Yes, but without gradients. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n Was able to fix it for me with `pip install --upgrade wandb`",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":7.42,
        "Solution_score":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":108.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1286692213960,
        "Answerer_location":null,
        "Answerer_reputation":328.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":1559.6549622222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a MongoDB database (the Bitnami one) hosted on Azure. I want to import the data there to use it in my Azure Machine Learning experiment.<\/p>\n\n<p>Currently, I am exporting the data to <strong>.csv<\/strong> using <strong>mongoexport<\/strong> and then copy\/pasting it to the <strong>\"Enter Manually Data\"<\/strong> module. This is fine for small amounts of data but I would prefer to have a more robust technique for larger databases.<\/p>\n\n<p>I also thought about using the <strong>\"Import Data\"<\/strong> module from http url along with the <strong>http port (28017) of my mongodb<\/strong> instance but read this was not the recommended use of the http mongodb feature.<\/p>\n\n<p>Finally, I have installed <strong>cosmosDB<\/strong> instead of my bitnami MongoDB and it worked fine but this thing <strong>costs an arm<\/strong> when used with sitecore (it reaches around 100\u20ac per day) and we can't afford it so I switched back to by Mongo.<\/p>\n\n<p>So is there a better way to export data from Mongo to Azure ML ?<\/p>",
        "Challenge_closed_time":1504686538047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499071780183,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44881303",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":13.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.3528608597,
        "Challenge_title":"Best way to import MongoDB data in Azure Machine Learning",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":724.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1441267698016,
        "Poster_location":null,
        "Poster_reputation":781.0,
        "Poster_views":97.0,
        "Solution_body":"<p>one way is to use a Python code block in AzureML, something like this:<\/p>\n\n<pre><code>import pandas as p\nimport pymongo as m\n\ndef azureml_main():\n    c = m.MongoClient(host='host_IP')\n    a = p.DataFrame(c.database_names())\n    return a\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":3.06,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1565289301123,
        "Answerer_location":null,
        "Answerer_reputation":79.0,
        "Answerer_views":13.0,
        "Challenge_adjusted_solved_time":1547.3938463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for a working example how to access data on a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#access-datastores-during-training\" rel=\"nofollow noreferrer\">Azure Machine Learning managed data store<\/a> from within a train.py script. I followed the instructions in the link and my script is able to resolve the datastore.<\/p>\n\n<p>However, whatever I tried (<code>as_download(), as_mount()<\/code>) the only thing I always got was a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.data_reference.datareference?view=azure-ml-py\" rel=\"nofollow noreferrer\">DataReference<\/a> object. Or maybe I just don't understand how actually read data from a file with that.<\/p>\n\n<pre><code>run = Run.get_context()\nexp = run.experiment\nws = run.experiment.workspace\n\nds = Datastore.get(ws, datastore_name='mydatastore')\ndata_folder_mount = ds.path('mnist').as_mount()\n\n# So far this all works. But how to go from here?\n<\/code><\/pre>",
        "Challenge_closed_time":1565289458360,
        "Challenge_comment_count":4,
        "Challenge_created_time":1559718840513,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56455761",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":12.2,
        "Challenge_reading_time":13.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.3449734445,
        "Challenge_title":"Access data on AML datastore from training script",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1342685175156,
        "Poster_location":"Germany",
        "Poster_reputation":12103.0,
        "Poster_views":1451.0,
        "Solution_body":"<p>You can pass in the DataReference object you created as the input to your training product (scriptrun\/estimator\/hyperdrive\/pipeline). Then in your training script, you can access the mounted path via argument.\nfull tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":6.26,
        "Solution_score":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1531231343652,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation":676.0,
        "Answerer_views":70.0,
        "Challenge_adjusted_solved_time":1504.1879255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Challenge_closed_time":1630466729772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625051653240,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1630503140507,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7.3166730369,
        "Challenge_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1570620624976,
        "Poster_location":null,
        "Poster_reputation":53.0,
        "Poster_views":6.0,
        "Solution_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":6.05,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Kubernetes Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1373651649052,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation":1066.0,
        "Answerer_views":60.0,
        "Challenge_adjusted_solved_time":1490.0125933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a dataset in Azure ML where the data source are multiple files (eg images) in a Blob Storage. How do you do that correctly?<\/p>\n<h3>Here is the error I get following the documented approach in the UI<\/h3>\n<p>When I create the dataset in the UI and select the blob storage and directory with either just <code>dirname<\/code> or <code>dirname\/**<\/code> then the files can not be found in the explorer tab with the error <code>ScriptExecution.StreamAccess.NotFound: The provided path is not valid or the files could not be accessed.<\/code> When I try to download the data with the code snippet in the consume tab then I get the error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Dataset\n\n# set variables \n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='teststar')\ndataset.download(target_path='.', overwrite=False)\n<\/code><\/pre>\n<pre><code>Error Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: 'https:\/\/mystoragename.blob.core.windows.net\/data\/testdata\/**'\n\n<\/code><\/pre>\n<p>When I just select one of the files instead of <code>dirname<\/code> or <code>dirname\/**<\/code> then everything works. Does AzureML actually support Datasets consisting of multiple files?<\/p>\n<h3>Here is my setup:<\/h3>\n<p>I have a Data Storage with one container <code>data<\/code>. In there is a directory <code>testdata<\/code> containing <code>testfile1.txt<\/code> and <code>testfile2.txt<\/code>.<\/p>\n<p>In AzureML I created a datastore <code>testdatastore<\/code> and there I select the <code>data<\/code> container in my data storage.<\/p>\n<p>Then in Azure ML I create a Dataset from datastore, select file dataset and the datastore above. Then I can browse the files, select a folder and select that files in subdirectories should be included. This then creates the path <code>testdata\/**<\/code> which does not work as described above.<\/p>\n<p>I got the same issue when creating the dataset and datastore in python:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import azureml.core\nfrom azureml.core import Workspace, Datastore, Dataset\n\nws = Workspace.from_config()\n\ndatastore = Datastore(ws, &quot;mydatastore&quot;)\n\ndatastore_paths = [(datastore, 'testdata')]\ntest_ds = Dataset.File.from_files(path=datastore_paths)\ntest_ds.register(ws, &quot;testpython&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1618851944876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613487159093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1613487899540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66226685",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":33.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":7.3073486978,
        "Challenge_title":"AzureML create dataset from datastore with multiple files - path not valid",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2337.0,
        "Challenge_word_count":311,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1373651649052,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation":1066.0,
        "Poster_views":60.0,
        "Solution_body":"<p>I uploaded and registered the files with this script and everything works as expected.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore, Dataset, Workspace\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=&quot;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s&quot;,\n    datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\n)\n\ndatastore_name = &quot;mydatastore&quot;\ndataset_path_on_disk = &quot;.\/data\/images_greyscale&quot;\ndataset_path_in_datastore = &quot;images_greyscale&quot;\n\nazure_dataset_name = &quot;images_grayscale&quot;\nazure_dataset_description = &quot;dataset transformed into the coco format and into grayscale images&quot;\n\n\nworkspace = Workspace.from_config()\ndatastore = Datastore.get(workspace, datastore_name=datastore_name)\n\nlogger.info(&quot;Uploading data...&quot;)\ndatastore.upload(\n    src_dir=dataset_path_on_disk, target_path=dataset_path_in_datastore, overwrite=False\n)\nlogger.info(&quot;Uploading data done.&quot;)\n\nlogger.info(&quot;Registering dataset...&quot;)\ndatastore_path = [(datastore, dataset_path_in_datastore)]\ndataset = Dataset.File.from_files(path=datastore_path)\ndataset.register(\n    workspace=workspace,\n    name=azure_dataset_name,\n    description=azure_dataset_description,\n    create_new_version=True,\n)\nlogger.info(&quot;Registering dataset done.&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.7,
        "Solution_reading_time":19.07,
        "Solution_score":0.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1488.5780555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Challenge_closed_time":1651060598000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645701717000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":148.0,
        "Challenge_repo_fork_count":956.0,
        "Challenge_repo_issue_count":3383.0,
        "Challenge_repo_star_count":6165.0,
        "Challenge_repo_watch_count":89.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.3062481746,
        "Challenge_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"fixed by https:\/\/github.com\/deepset-ai\/haystack\/pull\/2337",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.0,
        "Solution_reading_time":0.81,
        "Solution_score":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":3.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.2974710895,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.2974710895,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score":2.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":37.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1466.8491666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'mlflow.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nThose two linting functions caused the template create WFs (and sometimes even local) to fail\r\n\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThey should pass. We should discuss why they fail and how to fix!\r\nSo currently they are outcommented!\r\n",
        "Challenge_closed_time":1613430703000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608150046000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/171",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7.2915534564,
        "Challenge_title":"subprocess.call and mlflow.log_artifact checks inconsistent in linter",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@Emiller88 the linter should for all templates just check that these methods are called in the templates. Ideally you just need to add those two lines to the linter checks.\r\n\r\nI won't explain the original issue here since I just expect it to work :) If it still doesn't I will reassign @Imipenem and me.\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":3.61,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":54.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1645792458310,
        "Answerer_location":null,
        "Answerer_reputation":36.0,
        "Answerer_views":15.0,
        "Challenge_adjusted_solved_time":1465.312585,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Challenge_closed_time":1645794046503,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640518921197,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70486162",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":12.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7.2905060828,
        "Challenge_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1331727483732,
        "Poster_location":"London, UK",
        "Poster_reputation":1060.0,
        "Poster_views":139.0,
        "Solution_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":7.54,
        "Solution_score":1.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1515259002820,
        "Answerer_location":null,
        "Answerer_reputation":153.0,
        "Answerer_views":74.0,
        "Challenge_adjusted_solved_time":250.9086069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am learning AWS SageMaker which is supposed to be a serverless compute environment for Machine Learning. In this type of serverless compute environment, who is supposed to ensure the software package consistency and update the versions?<\/p>\n\n<p>For example, I ran the demo program that came with SageMaker, deepar_synthetic. In this second cell, it executes the following: !conda install -y s3fs<\/p>\n\n<p>However, I got the following warning message:<\/p>\n\n<p>Solving environment: done\n==> WARNING: A newer version of conda exists. &lt;==\n  current version: 4.4.10\n  latest version: 4.5.4\nPlease update conda by running\n    $ conda update -n base conda<\/p>\n\n<p>Since it is serverless compute, am I still supposed to update the software packages myself?<\/p>\n\n<p>Another example is as follows. I wrote a few simple lines to find out the package versions in Jupyter notebook:<\/p>\n\n<p>import platform<\/p>\n\n<p>import tensorflow as tf<\/p>\n\n<p>print(platform.python_version())<\/p>\n\n<p>print (tf.<strong>version<\/strong>)<\/p>\n\n<p>However, I got the following warning messages:<\/p>\n\n<p>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/importlib\/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\nreturn f(*args, **kwds)\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/h5py\/<strong>init<\/strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float<\/code> to <code>np.floating<\/code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type<\/code>.\nfrom ._conv import register_converters as _register_converters<\/p>\n\n<p>The prints still worked and I got the results shown beolow:<\/p>\n\n<p>3.6.4\n1.4.0<\/p>\n\n<p>I am wondering what I have to do to get the package consistent so that I don't get the warning messages. Thanks.<\/p>",
        "Challenge_closed_time":1532115126592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526869228233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1531211855607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50441181",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":25.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":7.2849539549,
        "Challenge_title":"How to ensure software package version consistency in AWS SageMaker serverless compute?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":926.0,
        "Challenge_word_count":239,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1461112434223,
        "Poster_location":"San Jose, CA, United States",
        "Poster_reputation":1075.0,
        "Poster_views":181.0,
        "Solution_body":"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. <\/p>\n\n<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: \u201cNew\u201d -> \u201cTerminal\u201d. \nNote: By default, conda installs to the root environment. <\/p>\n\n<p>The following are instructions you can follow <a href=\"https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html<\/a> on how to install libraries in the particular conda environment. <\/p>\n\n<p>In general you will need following commands, <\/p>\n\n<pre><code>conda env list \n<\/code><\/pre>\n\n<p>which list all of your conda environments <\/p>\n\n<pre><code>source activate &lt;conda environment name&gt; \n<\/code><\/pre>\n\n<p>e.g. source activate python3 <\/p>\n\n<pre><code>conda list | grep &lt;package&gt; \n<\/code><\/pre>\n\n<p>e.g. conda list | grep numpy \nlist what are the current package versions <\/p>\n\n<pre><code>pip install numpy \n<\/code><\/pre>\n\n<p>Or <\/p>\n\n<pre><code>conda install numpy \n<\/code><\/pre>\n\n<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. <\/p>\n\n<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.4,
        "Solution_reading_time":26.31,
        "Solution_score":0.0,
        "Solution_sentence_count":16.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1610703423912,
        "Answerer_location":null,
        "Answerer_reputation":36.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":1452.9271638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Challenge_closed_time":1610703448710,
        "Challenge_comment_count":3,
        "Challenge_created_time":1605472910920,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":17.7,
        "Challenge_reading_time":22.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.2820235632,
        "Challenge_title":"SageMaker Batch Transform fails with ID Column",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1080.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1319019150600,
        "Poster_location":null,
        "Poster_reputation":3073.0,
        "Poster_views":341.0,
        "Solution_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.53,
        "Solution_score":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1434.0155555556,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using `PyTorchLightningPruningCallback` to search best hyperparams, it reports `AttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'`\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom typing import List, Optional\r\n\r\nimport optuna\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\r\nfrom torch.utils.data import random_split, DataLoader\r\n\r\n\r\nclass FashionDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str, batch_size: int):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.test_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=False, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.train_set, self.valid_set = random_split(self.train_set, [55000, 5000])\r\n\r\n    def train_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n    def test_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super(SimpleNet, self).__init__()\r\n\r\n        hidden_layers = []\r\n        d_inp = 28 * 28\r\n        for d_hid in d_hids:\r\n            hidden_layers.append(nn.Linear(d_inp, d_hid))\r\n            hidden_layers.append(nn.ReLU())\r\n            hidden_layers.append(nn.Dropout(p_drop))\r\n            d_inp = d_hid\r\n        hidden_layers.append(nn.Linear(d_inp, 10))\r\n\r\n        self.layers = nn.Sequential(*hidden_layers)\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(inputs)\r\n\r\n\r\nclass LitSimpleNet(pl.LightningModule):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super().__init__()\r\n        self.model = SimpleNet(d_hids, p_drop)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.accuracy = torchmetrics.Accuracy()\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.model(inputs.view(-1, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx) -> torch.Tensor:\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        return self.criterion(outputs, targets)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        self.accuracy(outputs, targets)\r\n        self.log(\"valid_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=3e-4, weight_decay=1e-5)\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial) -> float:\r\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n    p_drop = trial.suggest_float(\"p_drop\", 0.1, 0.5)\r\n    d_hids = [trial.suggest_int(f\"d_hid_{i}\", 16, 128, log=True) for i in range(n_layers)]\r\n\r\n    datamodule = FashionDataModule(\".\", 128)\r\n    model = LitSimpleNet(d_hids, p_drop)\r\n    trainer = pl.Trainer(\r\n        max_epochs=20,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        enable_checkpointing=False,\r\n        logger=True,\r\n        default_root_dir=\".\",\r\n        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"valid_acc\")]\r\n    )\r\n\r\n    hparams = dict(n_layers=n_layers, d_hids=d_hids, p_drop=p_drop)\r\n    trainer.logger.log_hyperparams(hparams)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    return trainer.callback_metrics[\"valid_acc\"].item()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pruner = optuna.pruners.MedianPruner()\r\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\r\n    study.optimize(objective, n_trials=100, timeout=1000)\r\n\r\n    print(\"Number of Finished Trials:\", len(study.trials))\r\n\r\n    trial = study.best_trial\r\n    print(\"Best Trial:\")\r\n    print(\"\\tValue:\", trial.value)\r\n    print(\"\\tParams:\")\r\n    for key, value in trial.params.items():\r\n        print(f\"\\t\\t{key}: {value}\")\r\n\r\n```\r\n\r\n```bash\r\n[W 2022-09-08 20:14:45,294] Trial 0 failed because of the following error: AttributeError(\"'AcceleratorConnector' object has no attribute 'distributed_backend'\")\r\nTraceback (most recent call last):\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/study\/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"optuna_examples\/optuna_lightning_example.py\", line 89, in objective\r\n    trainer = pl.Trainer(\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/argparse.py\", line 345, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 497, in __init__\r\n    self._call_callback_hooks(\"on_init_start\")\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1585, in _call_callback_hooks\r\n    fn(self, *args, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/integration\/pytorch_lightning.py\", line 61, in on_init_start\r\n    trainer._accelerator_connector.distributed_backend is not None  # type: ignore\r\nAttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'\r\n```\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/github\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.ipynb\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n\r\n### Expected behavior\r\n\r\nShould not report any errors.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n\r\n```\r\n\r\n\r\n<details>\r\n  <summary>Details<\/summary>\r\n    Paste the output here and move this toggle outside of the comment block.\r\n<\/details>\r\n\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- Lightning Component:  Trainer\r\n- PyTorch Lightning Version:  1.7.5\r\n- PyTorch Version:  1.12.1\r\n- Python version: 3.8.13\r\n- OS: Linux (Ubuntu 20.04)\r\n- CUDA\/cuDNN version: 11.3.1\r\n- How you installed PyTorch: conda\r\n\r\n\n\ncc @akihironitta",
        "Challenge_closed_time":1667802669000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662640213000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/14604",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":17.8,
        "Challenge_reading_time":88.49,
        "Challenge_repo_contributor_count":447.0,
        "Challenge_repo_fork_count":2788.0,
        "Challenge_repo_issue_count":14589.0,
        "Challenge_repo_star_count":22027.0,
        "Challenge_repo_watch_count":231.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":7.2689309682,
        "Challenge_title":"Optuna integration reports AttributeError",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":522,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hey, @RegiusQuant. \r\n\r\nSide answer, you might be interested by Lightning HPO: https:\/\/github.com\/Lightning-AI\/lightning-hpo. This enables to run Optuna with PyTorch Lightning without friction and scalable in the cloud.\r\n\r\n Hey, @RegiusQuant - Thanks for the question. Can you please point me to the version of `optuna` that you are using?  For reference: https:\/\/github.com\/optuna\/optuna\/issues\/3978 @krshrimali Optuna version\uff1a3.0.0 I'm observing the same issue with Optuna 3.0.2 @hrzn Hi, I'm from the Optuna-dev team. Optuna's pytorch-lightning (PL) integration module doesn't support PL>=1.6 because it broke backwards-compatibility as investigated in https:\/\/github.com\/optuna\/optuna\/issues\/3418. Unfortunately, Optuna team doesn't have time to fix the module soon to support recent PL; we would like to wait for a PR from optuna and PL users.\r\n\r\n@tchaton I believe you can close this issue because the issue comes from Optuna... With Optuna==3.0.2 with lightning==1.5.10, I got \r\n`ValueError: optuna.integration.PyTorchLightningPruningCallback supports only optuna.storages.RDBStorage in DDP.`\r\nAfter downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.  @mikiotada Again, the error does not relate to PL. As the error message said, Optuna's integration does not support DDP without RDBStorage. \r\n\r\n> After downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.\r\n\r\nIn my understanding, Optuna 2.x didn't officially support DDP; it does not work as you expected, I'm afraid even though there was no error. Closing this issue as there seems nothing we can address from our side. Please refer to https:\/\/github.com\/optuna\/optuna\/issues\/3418.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":8.9,
        "Solution_reading_time":21.93,
        "Solution_score":5.0,
        "Solution_sentence_count":28.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":232.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1430.2352777778,
        "Challenge_answer_count":1,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367426000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625218579000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":7.266293181,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score":0.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1436771091480,
        "Answerer_location":"Brno, \u010cesko",
        "Answerer_reputation":51.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":1411.4133319445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AutoML called via Custom Python Script module in AzureML designer.\nFor that, I need to install automl packages:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.0.85 --upgrade\")\n<\/code><\/pre>\n\n<p>It worked correctly, but now when I call automl training I received this error:<\/p>\n\n<pre><code>pkg_resources.ContextualVersionConflict: (azureml-dataprep 1.3.2 (\/azureml-envs\/azureml_8d08fe76aaa5abe0ec642fd2de335a04\/lib\/python3.6\/site-packages), Requirement.parse('azureml-dataprep&lt;1.2.0a,&gt;=1.1.37a'), {'azureml-automl-core'})\n<\/code><\/pre>\n\n<p>Looks like there was an update in azureml-dataprep to version 1.3.2 which is not compatible with azureml-sdk[automl]==1.0.85.<\/p>\n\n<ol>\n<li>Would it be possible to add AutoML packages as default package in AzureML designer?<\/li>\n<li>Would it be possible to update azureml-sdk version in AzureML designer?<\/li>\n<li>Is there any workaround right now?<\/li>\n<\/ol>",
        "Challenge_closed_time":1589523303107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584439428327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584442215112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60720060",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7.253603027,
        "Challenge_title":"ContextualVersionConflict issue with azureml-automl-core in AzureML designer",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1436771091480,
        "Poster_location":"Brno, \u010cesko",
        "Poster_reputation":51.0,
        "Poster_views":2.0,
        "Solution_body":"<p>Fixed after release new version of AzureML SDK in designer and update script to:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.4.0 --upgrade\")\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.31,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1430203014072,
        "Answerer_location":null,
        "Answerer_reputation":432.0,
        "Answerer_views":54.0,
        "Challenge_adjusted_solved_time":1392.8864805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to connect to an AzureML Web Service. I have looked into the POST Method on the Arduino Homepage and also here <a href=\"https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/\" rel=\"nofollow\">https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/<\/a><\/p>\n\n<p>Here is my Setup method:<\/p>\n\n<pre><code>    void setup()\n    {\n      Serial.begin(9600);\n      while (!Serial) {\n      ; \/\/ wait for serial port to connect.\n      }\n\n     Serial.println(\"ethernet\");\n\n     if (Ethernet.begin(mac) == 0) {\n       Serial.println(\"ethernet failed\");\n       for (;;) ;\n     }\n    \/\/ give the Ethernet shield a second to initialize:\n    delay(1000);\n }\n<\/code><\/pre>\n\n<p>The Post Method is based on this: <a href=\"http:\/\/playground.arduino.cc\/Code\/WebClient\" rel=\"nofollow\">http:\/\/playground.arduino.cc\/Code\/WebClient<\/a><\/p>\n\n<p>I just added <code>sprintf(outBuf, \"Authorization: Bearer %s\\r\\n\", api_key);<\/code> to the header, with <code>char* api_key = \"the ML Web Service API KEY\"<\/code><\/p>\n\n<p>Also, unlike specified in the WebClient I use the whole WebService URI as url and do not specify a page name.<\/p>\n\n<p>This doesn't work.<\/p>\n\n<p>The Network to which I am connecting has Internet Access.<\/p>\n\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1450337261483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1445322870153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459290344956,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33229576",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.2398511537,
        "Challenge_title":"Arduino Uno - WebService (AzureML)",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":154.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1369151239452,
        "Poster_location":"Germany",
        "Poster_reputation":516.0,
        "Poster_views":57.0,
        "Solution_body":"<p>Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.<\/p>\n\n<p>One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.<\/p>\n\n<p>Here's a sample <a href=\"https:\/\/github.com\/Azure\/connectthedots\/blob\/master\/GettingStarted.md\" rel=\"nofollow\">project<\/a> from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1450346645647,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":7.59,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1625765161928,
        "Answerer_location":null,
        "Answerer_reputation":56.0,
        "Answerer_views":6.0,
        "Challenge_adjusted_solved_time":1392.0377008334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>My scoring function needs to refer to an Azure ML Registered Dataset for which I need a reference to the AzureML Workspace object. When including this in the <code>init()<\/code> function of the scoring script it  gives the following error:<\/p>\n<pre><code> &quot;code&quot;: &quot;ScoreInitRestart&quot;,\n      &quot;message&quot;: &quot;Your scoring file's init() function restarts frequently. You can address the error by increasing the value of memory_gb in deployment_config.&quot;\n<\/code><\/pre>\n<p>On debugging the issue is:<\/p>\n<pre><code>To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code [REDACTED] to authenticate.\n<\/code><\/pre>\n<p>How can I resolve this issue without exposing Service Principal Credentials in the scoring script?<\/p>",
        "Challenge_closed_time":1626961792156,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621951561220,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67689671",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":9.6,
        "Challenge_reading_time":10.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.2390217143,
        "Challenge_title":"How to get reference to AzureML Workspace Class in scoring script?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1066.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation":887.0,
        "Poster_views":130.0,
        "Solution_body":"<p>I found a workaround to reference the workspace in the scoring script. Below is a code snippet of how one can do that -<\/p>\n<p>My deploy script looks like this :<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.model import InferenceConfig\n\n#Add python dependencies for the models\nscoringenv = Environment.from_conda_specification(\n                                   name = &quot;scoringenv&quot;,\n                                   file_path=&quot;config_files\/scoring_env.yml&quot;\n                                    )\n#Create a dictionary to set-up the env variables   \nenv_variables={'tenant_id':tenant_id,\n                        'subscription_id':subscription_id,\n                        'resource_group':resource_group,\n                        'client_id':client_id,\n                        'client_secret':client_secret\n                        }\n    \nscoringenv.environment_variables=env_variables\n            \n# Configure the scoring environment\ninference_config = InferenceConfig(\n                                   entry_script='score.py',\n                                   source_directory='scripts\/',\n                                   environment=scoringenv\n                                        )\n<\/code><\/pre>\n<p>What I am doing here is creating an image with the python dependencies(in the scoring_env.yml) and passing a dictionary of the secrets as environment variables. I have the secrets stored in the key-vault.\nYou may define and pass native python datatype variables.<\/p>\n<p>Now, In my score.py, I reference these environment variables in the init() like this -<\/p>\n<pre><code>tenant_id = os.environ.get('tenant_id')\nclient_id = os.environ.get('client_id')\nclient_secret = os.environ.get('client_secret')\nsubscription_id = os.environ.get('subscription_id')\nresource_group = os.environ.get('resource_group')\n<\/code><\/pre>\n<p>Once you have these variables, you may create a workspace object using Service Principal authentication like @Anders Swanson mentioned in his reply.<\/p>\n<p>Another way to resolve this may be by using managed identities for AKS. I did not explore that option.<\/p>\n<p>Hope this helps! Please let me know if you found a better way of solving this.<\/p>\n<p>Thanks!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626962896943,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":24.02,
        "Solution_score":2.0,
        "Solution_sentence_count":22.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":200.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1340784274407,
        "Answerer_location":null,
        "Answerer_reputation":150.0,
        "Answerer_views":12.0,
        "Challenge_adjusted_solved_time":1380.8853472222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I can't seem to get the logger to work in the online azure notesbooks workspace. I'm using python 3.6 environment.\nRunning this import:<\/p>\n\n<pre><code>from azureml.logging import get_azureml_logger\n<\/code><\/pre>\n\n<p>gives me the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'azureml.logging\n<\/code><\/pre>",
        "Challenge_closed_time":1526723551880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521752364630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438358",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.2312040394,
        "Challenge_title":"'azureml.logging' module not found",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1597.0,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1326743808803,
        "Poster_location":null,
        "Poster_reputation":635.0,
        "Poster_views":50.0,
        "Solution_body":"<p>The solution is <\/p>\n\n<p>pip install \"<a href=\"https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\" rel=\"nofollow noreferrer\">https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D<\/a>\"<\/p>\n\n<p>I found the blob url here. It's inside docker container dependencies.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":49.8,
        "Solution_reading_time":11.49,
        "Solution_score":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1533754693910,
        "Answerer_location":null,
        "Answerer_reputation":801.0,
        "Answerer_views":80.0,
        "Challenge_adjusted_solved_time":1373.19555,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Challenge_closed_time":1614007459500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609059338347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609063955520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":7.2265566573,
        "Challenge_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":355.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1517147266416,
        "Poster_location":null,
        "Poster_reputation":65.0,
        "Poster_views":18.0,
        "Solution_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.48,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1339.2047222222,
        "Challenge_answer_count":25,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330541000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615509404000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":25,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.2005776591,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":9.2,
        "Solution_reading_time":145.23,
        "Solution_score":0.0,
        "Solution_sentence_count":102.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":1555.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1392607100776,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation":133.0,
        "Answerer_views":29.0,
        "Challenge_adjusted_solved_time":1338.1595822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have images of single size 400 MB to 800 MB.<\/p>\n<p>Not sure if SageMaker GroundTruth can handle it.<\/p>",
        "Challenge_closed_time":1605061301483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600243584947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1600243926987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63915711",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":2.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.1998684645,
        "Challenge_title":"What's the largest allowed image size for SageMaker GroundTruth Labelling Job?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1392607100776,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation":133.0,
        "Poster_views":29.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/input-data-limits.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/input-data-limits.html<\/a><\/p>\n<p>In case someone is also looking for the answer of this one.<\/p>\n<p>In short:<\/p>\n<blockquote>\n<p>40 MB<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.5,
        "Solution_reading_time":4.27,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1468179475927,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation":795.0,
        "Answerer_views":210.0,
        "Challenge_adjusted_solved_time":1328.6339833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to test Azure Machine Learning Studio. <\/p>\n\n<p>I want to use TensorFlow, but it is not installed on Jupyter notebook.<\/p>\n\n<p>How can I use some machine learning libraries like TensorFlow, Theano, Keras,... on the notebook?<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>!pip install tensorflow \n<\/code><\/pre>\n\n<p>But, I got error as below:<\/p>\n\n<pre><code>Collecting tensorflow\n  Downloading tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl (43.1MB)\n    100% |################################| 43.1MB 27kB\/s \nCollecting protobuf==3.1.0 (from tensorflow)\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n    100% |################################| 348kB 3.7MB\/s \nCollecting six&gt;=1.10.0 (from tensorflow)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: wheel&gt;=0.26 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: setuptools in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg (from protobuf==3.1.0-&gt;tensorflow)\nInstalling collected packages: six, protobuf, tensorflow\n  Found existing installation: six 1.9.0\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Rolling back uninstall of six\nException:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/commands\/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_set.py\", line 784, in install\n    **kwargs\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 345, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 329, in clobber\n    os.utime(destfile, (st.st_atime, st.st_mtime))\nPermissionError: [Errno 1] Operation not permitted\n<\/code><\/pre>",
        "Challenge_closed_time":1485956468903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481173386563,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41032108",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":7.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":7.1926589828,
        "Challenge_title":"How to install TensorFlow in jupyter notebook on Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2886.0,
        "Challenge_word_count":235,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1279636903496,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation":1540.0,
        "Poster_views":404.0,
        "Solution_body":"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages<\/code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\" rel=\"nofollow noreferrer\">Azure Notebooks<\/a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml<\/code> package, you'll need to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">provide your workspace id\/authorization token<\/a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":14.6,
        "Solution_score":3.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1326.7408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi @Galileo-Galilei\r\n\r\n## Description\r\nthe KedroPipelineModel has a `initial_catalog` property which causes some problems. This `initial_catalog` can contain some Kedro Datasets but it's not necessary to log them when you train your model. because of this property I can't load my model anymore. I have to train it again.\r\n\r\nI explain : when I trained my model I used a kedro home-made plugin to load a specific dataset (which has no impact for my model). After that, I updated this plugin independently of my ML project. Today, I want to load my model but I can't because the load function uses the old Kedro Catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## Context\r\nIt would be great if we can update the kedro-catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## Possible Implementation\r\nLog in Mlflow what is only necessary.\r\n\r\nI hope my issue is clear.\r\n\r\nthank you",
        "Challenge_closed_time":1644791409000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640015142000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/273",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.57,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.1912341554,
        "Challenge_title":"KedroPipelineModel requires unnecessary pipeline input dependencies to be executed",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":168,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hi, I can reproduce the issue, thank you very much for the feedback. To clarify, what happens here is the following: \r\n\r\n- the input of your inference pipeline is persisted in Kedro because you load it from the disk (e.g., pandas.ExcelDataSet)\r\n- after you log it in mlflow, it will be converted to a ``MemoryDataSet``, and you directly pass a pandas Dataframe when you want to reuse it. Mlflow complains that you need to have ``openpyxl`` installed, while you never use it in your pipeline, and you don't need it to predict.\r\n\r\nThis extra dependency is not useful as you mention. I will remove it in a patch release soon.\r\n\r\n For anyone having the same issue, notice that you can now export a pipeline as a mlflow model with the [``kedro mlflow modelify``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/05_pipeline_serving\/03_cli_modelify.html) command.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":10.52,
        "Solution_score":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":132.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1322.0291666667,
        "Challenge_answer_count":4,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Challenge_closed_time":1664069854000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659310549000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":31.58,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":7.1876792097,
        "Challenge_title":"Neptune MalformedQueryException",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":345,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for opening your first issue here!\n The PR that solves the issue in my case https:\/\/github.com\/amundsen-io\/amundsen\/pull\/1947\/files This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n This issue has been automatically closed for inactivity. If you still wish to make these changes, please open a new pull request or reopen this one.\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.27,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":67.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1317.2311111111,
        "Challenge_answer_count":13,
        "Challenge_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Challenge_closed_time":1563269566000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558527534000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":13,
        "Challenge_readability":21.6,
        "Challenge_reading_time":18.74,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.1840460495,
        "Challenge_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hi @Jeffwan ,\r\n\r\nneed your support on this.\r\n\r\nI am using training image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" and it is throwing an error for mising values for parameters K and feature_dim. Although we are not using these parameters anywhere in pipeline.\r\n\r\nCan you please provide the solution ?\r\n\r\nRegards,\r\nVarun em. I may delete the configuration fields in clean up. Let me double check and come back to you @vackysh  I can reproduce this issue. \r\n\r\n`HyperParameters` was removed by me in this commit\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/commit\/26f2719c28a731d8925ae2ce96252be1df2562aa\r\n\r\nAdd it back will solve this problem Image has been rebuilt and it should be good now.  Hi @Jeffwan ,\r\n\r\nThanks for your response.\r\n\r\nI again executed the pipeline using image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" , but getting the same issue\r\n\r\n\"Training failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\"\r\n\r\nThe pipeline parameters are:\r\n\r\n@dsl.pipeline(\r\nname='MNIST Classification pipeline',\r\ndescription='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\nimage='382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1',\r\ndataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\ninstance_type='ml.c4.8xlarge',\r\ninstance_count='2',\r\nvolume_size='50',\r\nmodel_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\nbatch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\nbatch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\nrole_arn=''\r\n):\r\n\r\nPlease suggest how to get through it if issue has already fixed at your end.\r\n @vackysh I think the problem is your machine already has this image. could you go to the machine and do a force pull? \r\n```\r\nseedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05\r\n``` HI @Jeffwan ,\r\n\r\nI refreshed the image It is now working fine.\r\nThank you so much.\r\n\r\nRegards,\r\nVarun Hi @Jeffwan,\r\n\r\nWhere can i get the actual source code (ML code ) reading from the image seedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05 (not a docker file, but actual logic for train, predction) ?\r\n\r\nI actually working on similar automation and want to analyse the source code.\r\n\r\nRegards,\r\nVarun @vackysh This is a component example for training. \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/train.py\r\n\r\nNot sure if your work is internal or public. It would be perfect if you can make some contribution! Feel free to ping me on Slack or shoot me an email. Hi @Jeffwan  ,\r\n\r\nThanks for information. But i am looking for the main Kmean algorithms code that is used for training the model. I couldn't find that anywhere on path.\r\n\r\nI have a requirement where Scikit SVM model to get deploy on kubeflow pipeline using aws sagemaker services and S3. So i want to have a look on source ML code that has been passed through image as an input to pipeline.\r\n\r\nRegards,\r\nVarun\r\n\r\n @vackysh Now I get your point, in the example, I am using the container images from SageMaker. I think KMEANS one is first-party models and you might don't have access to it. What I suggest you to do is bring your own training image if you have customization request. This issue is resolved. Now closing > This issue is resolved. Now closing\r\n\r\nHave you figured out a way to make it? Did you try bring your own container? ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.1,
        "Solution_reading_time":43.32,
        "Solution_score":0.0,
        "Solution_sentence_count":38.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":449.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1301.8425,
        "Challenge_answer_count":6,
        "Challenge_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Challenge_closed_time":1662562977000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657876344000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":16.2,
        "Challenge_reading_time":101.65,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":7.1723036949,
        "Challenge_title":"MLflow fails to log to a tracking server",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":588,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"cc @sgugger  I'm not the one who wrote or supports the ML Flow callback :-) @noise-field wrote the integration two years ago, do you have an idea of why it doesn't seem to work anymore @noise-field? @juliensimon, I had an error message similar (I think). I found that the issue was related to values with empty string values  (https:\/\/github.com\/mlflow\/mlflow\/issues\/6253), and it looks like there is a patch in the upcoming MLFLOW version 1.28 (not yet released)\r\n\r\nIn my case, I had to set `mp_parameters` to `None` instead of leaving it as an empty string (the default value), and I see your error message has `{'key': 'mp_parameters', 'value': ''}`.\r\n\r\nWhile later MLflow version fix will address this issue, I think setting the `mp_parameters` to `None` instead of an empty string is cleaner. However, I'm not sure about the extent of this change.\r\n\r\n OK, I'll give it a try and I'll let you know. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":15.15,
        "Solution_score":0.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":195.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1446631384107,
        "Answerer_location":"Helsinki, Finland",
        "Answerer_reputation":4255.0,
        "Answerer_views":877.0,
        "Challenge_adjusted_solved_time":0.5821208334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Challenge_closed_time":1578745983587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574164584153,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1578744224352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":7.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":7.1496114158,
        "Challenge_title":"How to register model from the Azure ML Pipeline Script step",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3429.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1574162655727,
        "Poster_location":null,
        "Poster_reputation":75.0,
        "Poster_views":6.0,
        "Solution_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1578746319987,
        "Solution_link_count":2.0,
        "Solution_readability":19.6,
        "Solution_reading_time":13.4,
        "Solution_score":14.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Model Registry",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1590520808976,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":56.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":1231.9309869445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Challenge_closed_time":1608656242863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604221291310,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64630198",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.1171495299,
        "Challenge_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":686.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1324808381143,
        "Poster_location":null,
        "Poster_reputation":9050.0,
        "Poster_views":1750.0,
        "Solution_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":41.9,
        "Solution_reading_time":6.98,
        "Solution_score":4.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1334762714136,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation":6557.0,
        "Answerer_views":2005.0,
        "Challenge_adjusted_solved_time":1229.6615519445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Azure ML. I am having some doubts .Could anyone please clarify my doubts listed below.<\/p>\n\n<ol>\n<li>What is the difference between Azure ML service Azure ML experimentation service.<\/li>\n<li>What is the difference between Azure ML workbench and Azure ML Studio.<\/li>\n<li>I want to use azure ML Experimentation service for building few models and creating web API's. Is it possible to do the same with ML studio. <\/li>\n<li>And also ML Experimentation service requires me to have a docker for windows installed for creating web services.\nCan i create web services without using docker?<\/li>\n<\/ol>",
        "Challenge_closed_time":1525629753687,
        "Challenge_comment_count":1,
        "Challenge_created_time":1521202972100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554674738927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49320679",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":7.1153071509,
        "Challenge_title":"Difference between Azure ML and Azure ML experimentation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":763.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1422010576070,
        "Poster_location":"United Kingdom House, London, UK",
        "Poster_reputation":388.0,
        "Poster_views":124.0,
        "Solution_body":"<ol>\n<li><p>The AML Experimentation is one of our many new ML offerings, including data preparation, experimentation, model management, and operationalization. Workbench is a PREVIEW product that provides a GUI for some of these services. But it is just a installer\/wrapper for the CLI that is needed to run. The services are Spark and Python based. Other Python frameworks will work, and you can get a little hacky to call Java\/Scala from Python. Not really sure what you mean by an \"Azure ML Service\", perhaps you are referring to the operationalization service I mentioned above. This will quickly let you create new Python based APIs using Docker containers, and will connect with the model management account to keep track of the linage between your models and your services. All services here are still in preview and may breaking change before GA release. <\/p><\/li>\n<li><p>Azure ML Studio is an older product that is perhaps simpler for some(myself an engineer not a data scientist). It offers a drag and drop experience, but is limited in it's data size to about 10G. This product is GA. <\/p><\/li>\n<li><p>It is, but you need smaller data sizes, and the job flow is not spark based. I use this to do rapid PoC's. Also you will less control over the scalability of your scoring (batch or real time), because it is PaaS, compared to the newer service which is more IaaS. I would recommend looking at the new service instead of studio for most use cases. <\/p><\/li>\n<li><p>The web services are completely based on Docker. Needing docker for experimentation is more about running things locally, which I myself rarely do. But, for the real time service, everything you package is placed into a docker container so it can be deployed to an ACS cluster. <\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":21.61,
        "Solution_score":2.0,
        "Solution_sentence_count":18.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1589205020747,
        "Answerer_location":"Germany",
        "Answerer_reputation":163.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":479.4038066667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Challenge_closed_time":1596034370720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591616299803,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594308517016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62261222",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":220.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":139,
        "Challenge_solved_time":7.1133390937,
        "Challenge_title":"Workspace Authentication: More than one token matches the criteria",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2737.0,
        "Challenge_word_count":1188,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1589205020747,
        "Poster_location":"Germany",
        "Poster_reputation":163.0,
        "Poster_views":16.0,
        "Solution_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":11.38,
        "Solution_score":2.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1562691895952,
        "Answerer_location":"Norway",
        "Answerer_reputation":3183.0,
        "Answerer_views":981.0,
        "Challenge_adjusted_solved_time":1204.3708430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a code for ML in my local machine in jupyter notebook. I notice that AWS Sagemaker has its own notebook instances too. '<\/p>\n<ol>\n<li>Can I upload my existing local jupyter notebook in AWS Sagemaker directly?\nor<\/li>\n<li>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?<\/li>\n<\/ol>",
        "Challenge_closed_time":1598173972672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593838237637,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62725467",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.0945425522,
        "Challenge_title":"AWS Sagemaker, how to connect existing local jupyter notebook with ML algorithms to AWS",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1562691895952,
        "Poster_location":"Norway",
        "Poster_reputation":3183.0,
        "Poster_views":981.0,
        "Solution_body":"<ol>\n<li><p>Can I upload my existing local jupyter notebook in AWS Sagemaker directly? or\nYes you can upload from local disk or computer<\/p>\n<\/li>\n<li><p>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?\nNo<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.19,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1181.1,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
        "Challenge_closed_time":1650182580000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645930620000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/td-p\/397685\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7.0750477968,
        "Challenge_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":441.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":5.01,
        "Solution_score":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1546882781360,
        "Answerer_location":null,
        "Answerer_reputation":106.0,
        "Answerer_views":41.0,
        "Challenge_adjusted_solved_time":1144.8089597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The output folder of an annotation job contains the following file structure: <\/p>\n\n<ul>\n<li><p>active learning<\/p><\/li>\n<li><p>annotation-tools<\/p><\/li>\n<li><p>annotations<\/p><\/li>\n<li><p>intermediate<\/p><\/li>\n<li><p>manifests<\/p><\/li>\n<\/ul>\n\n<p>Each line of the manifests\/output\/output.manifest file is a dictionary, where the key 'jobname' contains information about the annotations, and the key 'jobname-metadata' contains confidence score and other information about each of the bounding box annotations. There is also another folder called annotations which contain json files which contain information about annotations and associated worker ids. How are the two annotation informations related to each other? Is there any blogs\/tutorials which discuss how to interpret the data received from amazon sagemaker ground-truth service? Thanks in advance. <\/p>\n\n<p>Links I referred to: \n1. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html<\/a>\n2. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb<\/a> <\/p>\n\n<p>I have displayed the annotations received using the code available in the link 2 <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, which treats consolidated annotations and worker response separately.<\/p>",
        "Challenge_closed_time":1566235546552,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562114234297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56861525",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":22.5,
        "Challenge_reading_time":25.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.0438661816,
        "Challenge_title":"Interpretation of output of bounding box annotation job",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":199.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1562111269163,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":3.0,
        "Solution_body":"<p>Thank you for your question. I\u2019m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.<\/p>\n\n<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). <\/p>\n\n<p>You can find out more about the annotation consolidation feature here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html<\/a><\/p>\n\n<p>Please let us know if you have any further questions.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":11.52,
        "Solution_score":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1536318818623,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation":1033.0,
        "Answerer_views":55.0,
        "Challenge_adjusted_solved_time":1138.2136686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Challenge_closed_time":1626743937907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622646368700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.038093539,
        "Challenge_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1599816833352,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation":329.0,
        "Poster_views":58.0,
        "Solution_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.69,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1124.7483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Challenge_closed_time":1623230636000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181542000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":7.0262032787,
        "Challenge_title":"Bad MLflow artifact folder by default",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This problem has been solved adding the variable of `$ARTIFACTS_BUCKET` between `()` like this `$(ARTIFACTS_BUCKET)` in the deployment.yaml of the project-operator.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.12,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":20.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1106.8952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Challenge_closed_time":1635553585000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631568762000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":26.53,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.0102173482,
        "Challenge_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":226,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"`log_metrics` is part of the implementation of `LightningLoggerBase` yet using the experiment property returns the MlFlowClient which can be used to access methods specific to mlflow. So simply removing the experiment property from your calls should solve your problem.\r\n\r\nThe log_metric option of the mlflow client requires different args, see [here](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/mlflow.py#L226) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":8.64,
        "Solution_score":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1655446100500,
        "Answerer_location":null,
        "Answerer_reputation":26.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":161.0086202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Challenge_closed_time":1655447541936,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651551486133,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1654867910903,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":24.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":6.9877097271,
        "Challenge_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":520.0,
        "Challenge_word_count":219,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1562750927332,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation":803.0,
        "Poster_views":73.0,
        "Solution_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.4,
        "Solution_reading_time":15.17,
        "Solution_score":1.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1290013527427,
        "Answerer_location":"Berlin",
        "Answerer_reputation":15929.0,
        "Answerer_views":1202.0,
        "Challenge_adjusted_solved_time":1080.3870858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Challenge_closed_time":1655459795692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651570402183,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":6.9859998348,
        "Challenge_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1551431163127,
        "Poster_location":"Paris, France",
        "Poster_reputation":494.0,
        "Poster_views":24.0,
        "Solution_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":2.61,
        "Solution_score":1.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1320061998252,
        "Answerer_location":null,
        "Answerer_reputation":778.0,
        "Answerer_views":89.0,
        "Challenge_adjusted_solved_time":1075.1311988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Challenge_closed_time":1468139774183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1464269301867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":28.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":6.9811276653,
        "Challenge_title":"Python in AzureML fail to pass dataframe without changes",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1320061998252,
        "Poster_location":null,
        "Poster_reputation":778.0,
        "Poster_views":89.0,
        "Solution_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":4.89,
        "Solution_score":1.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1045.5080555556,
        "Challenge_answer_count":14,
        "Challenge_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_closed_time":1582730951000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578967122000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":14,
        "Challenge_readability":15.3,
        "Challenge_reading_time":1.92,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.9532142395,
        "Challenge_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@alla15747 Hi, thanks for reaching out to us. Could you please share your environment file so that we can know the details of this issue? @alla15747  please make sure that azureml-train-automl-runtime is installed in your environment if you using sdk>=1.0.76 or azureml-train-automl if using older version I'm running the code on the compute target and not my local machine. SDK 1.0.72\r\nHow to install packages in Azure Devops environment like azureml-train-automl? (base) C:\\Users\\aabdel137>pip freeze\r\nabsl-py==0.8.1\r\nadal==1.2.2\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.7\r\nanaconda-project==0.8.3\r\nansiwrap==0.8.4\r\napplicationinsights==0.11.9\r\nasn1crypto==0.24.0\r\nastor==0.8.0\r\nastroid==2.3.1\r\nastropy==3.2.1\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nazure-common==1.1.23\r\nazure-graphrbac==0.61.1\r\nazure-mgmt-authorization==0.60.0\r\nazure-mgmt-containerregistry==2.8.0\r\nazure-mgmt-keyvault==2.0.0\r\nazure-mgmt-resource==5.1.0\r\nazure-mgmt-storage==6.0.0\r\nazureml-contrib-interpret==1.0.72\r\nazureml-contrib-notebook==1.0.72\r\nazureml-core==1.0.72\r\nazureml-dataprep==1.1.29\r\nazureml-dataprep-native==13.1.0\r\nazureml-explain-model==1.0.72\r\nazureml-interpret==1.0.72.1\r\nazureml-pipeline==1.0.72\r\nazureml-pipeline-core==1.0.72\r\nazureml-pipeline-steps==1.0.72\r\nazureml-sdk==1.0.72\r\nazureml-telemetry==1.0.72\r\nazureml-train==1.0.72\r\nazureml-train-core==1.0.72\r\nazureml-train-restclients-hyperdrive==1.0.72\r\nazureml-widgets==1.0.72\r\nBabel==2.7.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.5\r\nbackports.os==0.1.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.7.1\r\nbitarray==0.9.3\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nbokeh==1.2.0\r\nboto==2.49.0\r\nBottleneck==1.2.1\r\ncertifi==2019.6.16\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.2.1\r\nclyent==1.2.2\r\ncolorama==0.4.1\r\ncomtypes==1.1.7\r\nconda==4.7.10\r\nconda-build==3.18.8\r\nconda-package-handling==1.3.11\r\nconda-verify==3.4.2\r\ncontextlib2==0.5.5\r\ncoverage==4.5.4\r\ncryptography==2.7\r\ncycler==0.10.0\r\nCython==0.29.12\r\ncytoolz==0.10.0\r\ndask==2.1.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\ndistributed==2.1.0\r\ndistro==1.4.0\r\ndocker==4.1.0\r\ndocutils==0.14\r\ndotnetcore2==2.1.10\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nflake8-formatter-junit-xml==0.0.6\r\nFlask==1.1.1\r\nfusepy==3.0.1\r\nfuture==0.17.1\r\ngast==0.3.2\r\ngevent==1.4.0\r\nglob2==0.7\r\ngoogle-pasta==0.1.7\r\ngreenlet==0.4.15\r\ngrpcio==1.24.3\r\nh5py==2.9.0\r\nheapdict==1.0.0\r\nhtml5lib==1.0.1\r\nidna==2.8\r\nimageio==2.5.0\r\nimagesize==1.1.0\r\nimportlib-metadata==0.23\r\ninterpret-community==0.1.0.3.3\r\ninterpret-core==0.1.18\r\nipykernel==5.1.1\r\nipython==7.6.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.0\r\nisodate==0.6.0\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.13.3\r\njeepney==0.4.1\r\nJinja2==2.10.1\r\njmespath==0.9.4\r\njoblib==0.13.2\r\njson5==0.8.4\r\njsonpickle==1.2\r\njsonschema==3.0.1\r\njunit-xml==1.8\r\njupyter==1.0.0\r\njupyter-client==5.3.1\r\njupyter-console==6.0.0\r\njupyter-core==4.5.0\r\njupyterlab==1.0.2\r\njupyterlab-server==1.0.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==18.0.0\r\nkiwisolver==1.1.0\r\nkmodes==0.10.1\r\nlazy-object-proxy==1.4.2\r\nlibarchive-c==2.8\r\nllvmlite==0.29.0\r\nlocket==0.2.0\r\nlxml==4.3.4\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.0\r\nmccabe==0.6.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmkl-service==2.0.2\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmsrest==0.6.10\r\nmsrestazure==0.6.2\r\nmultipledispatch==0.6.0\r\nnavigator-updater==0.2.1\r\nnbconvert==5.5.0\r\nnbformat==4.4.0\r\nndg-httpsclient==0.5.1\r\nnetworkx==2.3\r\nnltk==3.4.4\r\nnose==1.3.7\r\nnotebook==6.0.0\r\nnumba==0.44.1\r\nnumexpr==2.6.9\r\nnumpy==1.16.4\r\nnumpydoc==0.9.1\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopenpyxl==2.6.2\r\npackaging==19.2\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\npapermill==1.2.1\r\nparso==0.5.0\r\npartd==1.0.0\r\npath.py==12.0.1\r\npathlib2==2.3.4\r\npathspec==0.6.0\r\npatsy==0.5.1\r\npep8==1.7.1\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.0\r\nply==3.11\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.10.0\r\npsutil==5.6.3\r\npy==1.8.0\r\npy4j==0.10.7\r\npyasn1==0.4.7\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.3\r\npyflakes==2.1.1\r\nPygments==2.4.2\r\nPyJWT==1.7.1\r\npylint==2.4.2\r\npyodbc==4.0.26\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npypiwin32==223\r\npyreadline==2.1\r\npyrsistent==0.14.11\r\nPySocks==1.7.0\r\npyspark==2.4.4\r\npytest==5.2.2\r\npytest-arraydiff==0.3\r\npytest-astropy==0.5.0\r\npytest-cov==2.7.1\r\npytest-doctestplus==0.3.0\r\npytest-openfiles==0.3.2\r\npytest-remotedata==0.3.1\r\npython-dateutil==2.8.0\r\npython-dotenv==0.10.3\r\npytz==2019.1\r\nPyWavelets==1.0.3\r\npywin32==223\r\npywinpty==0.5.5\r\nPyYAML==5.1.1\r\npyzmq==18.0.0\r\nQtAwesome==0.5.7\r\nqtconsole==4.5.1\r\nQtPy==1.8.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.2.0\r\nrope==0.14.0\r\nruamel-yaml==0.15.46\r\nruamel.yaml==0.15.89\r\nscikit-image==0.15.0\r\nscikit-learn==0.21.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nSecretStorage==3.1.1\r\nSend2Trash==1.5.0\r\nshap==0.29.3\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.12.0\r\nsklearn==0.0\r\nsnowballstemmer==1.9.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.8\r\nSphinx==2.1.2\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.1.2\r\nspyder==3.3.6\r\nspyder-kernels==0.5.1\r\nSQLAlchemy==1.3.5\r\nstatsmodels==0.10.0\r\nsympy==1.4\r\ntables==3.5.2\r\ntblib==1.4.0\r\ntenacity==5.1.5\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-gpu==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntextwrap3==0.9.2\r\ntf-estimator-nightly==1.14.0.dev2019031401\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.37.0\r\ntraitlets==4.3.2\r\ntyped-ast==1.4.0\r\nunicodecsv==0.14.1\r\nunittest-xml-reporting==2.5.2\r\nurllib3==1.24.2\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.15.4\r\nwidgetsnbextension==3.5.0\r\nwin-inet-pton==1.1.0\r\nwin-unicode-console==0.5\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nxlrd==1.2.0\r\nXlsxWriter==1.1.8\r\nxlwings==0.15.8\r\nxlwt==1.3.0\r\nzict==1.0.0\r\nzipp==0.6.0\r\n AutoML became a part of default distribution (azureml-sdk) since 1.0.83\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2020-01-06\r\n\r\nif your client, that I believe pins the version of azureml sdk packages for the remote environment is 1.0.83, you will have automl on remote. \r\n\r\nIf you want to stay with 1.0.72 you can either reference automl extras azureml-sdk[automl] or explicitly reference azureml-train-automl (prefered).\r\n\r\nI would recommend to do both, upgrade client to the latest version and explicitly reference packages you need for your particular scenario not relying on metapackages like azureml-sdk\r\n\r\nOur reference doc will help you to get a set of the packages needed for your scenario\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py\r\n\r\nBy design every AzureML Python SDK package will bring necessary internal dependencies (of course except some corner cases :) )\r\n Thanks Vizhur, do you mind showing an example on how to reference azureml-train-automl in Azure Devops or Portal? \r\nThank you for the links! Not sure about your particular scenario, would you mind to share your ADO scenario so I can think of how to update it? MY scenario is implementing MLOPs example with automl step. Let me try to pull some relevant folks into the thread For AutoML, all the remote dependencies will get taken care of and will match whatever local dependencies are installed, e.g. if you have azureml-train-automl==1.0.72 installed, that version will be installed remotely for the training job.\r\nWe provide 2 clients to submitting these remote jobs currently, a thin client for submitting some types of remote jobs which is included as part of azureml-sdk, and a fuller client which enables more experiences such as Pipeline runs as part of azureml-train-automl. Since it looks like you are trying to use Pipelines, you will need to install the full azureml-train-automl client.\r\n\r\nFurthermore, the namespace for AutoMLStep changed recently, if you are using <1.0.76 the namespace would be \"from azureml.train.automl import AutoMLStep\", for >=1.0.76, you'll want to use \"from azureml.train.automl.runtime import AutoMLStep\" instead. I'm have sdk 1.0.72 installed. And I'm using from azureml.train.automl import AutoMLStep. Is there anyway to check the sdk version on the compute target machine? From your pip freeze, it doesn't look like you have the AutoML SDK installed. For the pipelines experience, you will need to have the SDK installed locally, not just on the target compute. Could you run \"pip install azureml-train-automl\"? @alla15747 \r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion. @SKrupa - Are you running your own code or a particular notebook sample from this repo?\r\n\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":117.81,
        "Solution_score":0.0,
        "Solution_sentence_count":38.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":786.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1374169767267,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation":548.0,
        "Answerer_views":70.0,
        "Challenge_adjusted_solved_time":1035.8951980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Challenge_closed_time":1594137982056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590408759343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1606323198012,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":27.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.9439861405,
        "Challenge_title":"Use tensorboard with object detection API in sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":311.0,
        "Challenge_word_count":286,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1416346350292,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation":2302.0,
        "Poster_views":227.0,
        "Solution_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":20.17,
        "Solution_score":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638880026000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":11,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.9429870104,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":176,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score":2.0,
        "Solution_sentence_count":41.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":426.0,
        "Tool":"DVC"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1490674180056,
        "Answerer_location":"%Temp%",
        "Answerer_reputation":302.0,
        "Answerer_views":39.0,
        "Challenge_adjusted_solved_time":1030.1723708333,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>Get-AmlWorkspace : One or more errors occurred.\nAt line:1 char:1\n+ Get-AmlWorkspace\n+ ~~~~~~~~~~~~~~~~\n+ CategoryInfo          : NotSpecified: (:) [Get-AmlWorkspace], \nAggregateException\n+ FullyQualifiedErrorId : \nSystem.AggregateException,AzureML.PowerShell.GetWorkspace\n<\/code><\/pre>\n\n<p>I am trying to use Powershell to connect to Azure ML studio as it looks like an easier way to manage a workspace. I've downloaded the dll file from <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> and changed my config.json file, but get the error above if I try to run any AzureML commands. I've unblocked the DLL file and imported the AzureMLPS module, and I can see the module and commands I am trying to use have been imported by doing <code>Get-Module<\/code> and <code>Get-Command<\/code><\/p>\n\n<p>For info I've not used Powershell before.<\/p>\n\n<p>Any suggestions much appreciated!<\/p>",
        "Challenge_closed_time":1503389654688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499681034153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45009184",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":12.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":6.938451658,
        "Challenge_title":"Powershell AzureML Get-AmlWorkspace",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1397507727100,
        "Poster_location":null,
        "Poster_reputation":340.0,
        "Poster_views":20.0,
        "Solution_body":"<p>Have you installed Azure PowerShell Installer on your local machine?\n<strong><a href=\"https:\/\/github.com\/Azure\/azure-powershell\/releases\" rel=\"nofollow noreferrer\">Click here<\/a><\/strong> for more info.<\/p>\n\n<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)<\/strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.<\/p>\n\n<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.<\/p>\n\n<pre><code># Set local folder location\nSet-Location -Path \"C:\\Insert here the location of AzureMLPS.dll\"\n\n# Unblock and import Azure Powershell Module (leverages config.json file)\nUnblock-File .\\AzureMLPS.dll\nImport-Module .\\AzureMLPS.dll\n\n# Get Azure ML Workspace info\nGet-AmlWorkspace\n<\/code><\/pre>\n\n<p>The output on my side looks like this:\n<a href=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.0,
        "Solution_reading_time":13.04,
        "Solution_score":2.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":1017.9547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053107000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619388470000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":428.0,
        "Challenge_repo_issue_count":16.0,
        "Challenge_repo_star_count":350.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.9265325987,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":136,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1221667848150,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation":849.0,
        "Answerer_views":142.0,
        "Challenge_adjusted_solved_time":1013.8935497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Challenge_closed_time":1608159311612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604509294833,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.9225390089,
        "Challenge_title":"Does AWS Sagemaker charges you per API request?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":187.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1501710710163,
        "Poster_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Poster_reputation":404.0,
        "Poster_views":25.0,
        "Solution_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.67,
        "Solution_score":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1495636394672,
        "Answerer_location":null,
        "Answerer_reputation":95.0,
        "Answerer_views":19.0,
        "Challenge_adjusted_solved_time":1006.7836063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Challenge_closed_time":1613677698376,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610053277393,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65619881",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.9155087494,
        "Challenge_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1495636394672,
        "Poster_location":null,
        "Poster_reputation":95.0,
        "Poster_views":19.0,
        "Solution_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.67,
        "Solution_score":0.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Hyperparameter Sweep",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1511190340208,
        "Answerer_location":null,
        "Answerer_reputation":385.0,
        "Answerer_views":39.0,
        "Challenge_adjusted_solved_time":1001.86839,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I noticed my Sagemaker (Amazon aws) jupyter notebook has an outdated version of the sklearn library.<\/p>\n<p>when I run <code>! pip freeze<\/code> I get:<\/p>\n<pre><code>sklearn==0.0\n<\/code><\/pre>\n<p>and when I run (with python) <code>print(sklearn.__version__)<\/code> I get<\/p>\n<pre><code>0.24.1\n<\/code><\/pre>\n<p>I'm not sure which one is my real version but I need 1.0.0 in order to use the <code>from_predictions()<\/code> method.<\/p>\n<p>But when I am trying to run <code>! \/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/bin\/python -m pip install --upgrade sklearn<\/code> I am getting the following output:<\/p>\n<blockquote>\n<p>Requirement already satisfied: sklearn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(0.0) Requirement already satisfied: scikit-learn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from sklearn) (0.24.1) Requirement already satisfied: scipy&gt;=0.19.1\nin\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.5.3) Requirement already satisfied:\njoblib&gt;=0.11 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.0.1) Requirement already satisfied:\nthreadpoolctl&gt;=2.0.0 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (2.1.0) Requirement already satisfied:\nnumpy&gt;=1.13.3 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.19.5)<\/p>\n<\/blockquote>\n<p>This is a very pupular library so it's weird if sagemaker cant upgrade it. Anyone has an idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1641036273827,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637426688750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637429547623,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70047920",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":23.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":6.9114111097,
        "Challenge_title":"How to upgrade the sklearn library in sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":634.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1381413304940,
        "Poster_location":null,
        "Poster_reputation":593.0,
        "Poster_views":94.0,
        "Solution_body":"<p>I managed to update sklearn to version 0.24.2 via the following command:<\/p>\n<pre><code>!conda update scikit-learn --yes\n<\/code><\/pre>\n<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.<\/p>\n<p>It also looks promising to create your custom conda environment, as explained here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.26,
        "Solution_score":1.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1432655047272,
        "Answerer_location":null,
        "Answerer_reputation":463.0,
        "Answerer_views":76.0,
        "Challenge_adjusted_solved_time":989.9238102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Challenge_closed_time":1604012509390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600448783673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":8.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.8986376497,
        "Challenge_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1285379271580,
        "Poster_location":null,
        "Poster_reputation":625.0,
        "Poster_views":110.0,
        "Solution_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.3,
        "Solution_reading_time":8.27,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1605831014392,
        "Answerer_location":null,
        "Answerer_reputation":46.0,
        "Answerer_views":1.0,
        "Challenge_adjusted_solved_time":988.7083763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Challenge_closed_time":1605831050088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602271699933,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.8974103304,
        "Challenge_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1289772110723,
        "Poster_location":"Mt Kisco, NY",
        "Poster_reputation":309.0,
        "Poster_views":40.0,
        "Solution_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":38.8,
        "Solution_reading_time":4.25,
        "Solution_score":3.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1442430064503,
        "Answerer_location":null,
        "Answerer_reputation":6147.0,
        "Answerer_views":1230.0,
        "Challenge_adjusted_solved_time":970.3788427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1518941429767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1515448065933,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":6.8787165495,
        "Challenge_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1457596845392,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation":4046.0,
        "Poster_views":825.0,
        "Solution_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":11.61,
        "Solution_score":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1393524211332,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":745.0,
        "Answerer_views":71.0,
        "Challenge_adjusted_solved_time":968.2322455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1531584172427,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":6.8765042587,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1358.0,
        "Challenge_word_count":332,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1340280616088,
        "Poster_location":"Laval, France",
        "Poster_reputation":2835.0,
        "Poster_views":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score":1.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":7,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607683045000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":6.8547139665,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score":0.0,
        "Solution_sentence_count":15.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":945.8408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Challenge_closed_time":1564215404000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560810377000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":41.03,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":6.8531310044,
        "Challenge_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":266,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Note: adding tensorflow-gpu==1.11.0 and rebuilding the image solves the issue Image has been updated for this",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.38,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1557646363768,
        "Answerer_location":null,
        "Answerer_reputation":51.0,
        "Answerer_views":14.0,
        "Challenge_adjusted_solved_time":937.3051341667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Challenge_closed_time":1612085605603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608711307120,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":9.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.8440751991,
        "Challenge_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":313.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1557646363768,
        "Poster_location":null,
        "Poster_reputation":51.0,
        "Poster_views":14.0,
        "Solution_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.59,
        "Solution_score":1.0,
        "Solution_sentence_count":1.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":933.8588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Challenge_closed_time":1660575729000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657213837000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":36.15,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":6.8403955969,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":355,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@omid0001 @kanwari3, \r\n\r\nWould it be possible for you to reproduce this issue (`not enough values to unpack`) without using SageMaker, i.e. just with a Python script?\r\n```bash\r\n[1,0]: bsz, seq_len = input_ids_shape[:2]\r\n[1,0]:ValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\nIt would be a good idea to verify what data is received by the model first. Usually the batches in data (`input_ids`) should be already of the format `(batch_size, sequence_length)`, and if you see the above error, it is likely the data or its processing pipeline has some issues. Using `torch.unsqueeze` is not really a good idea, as it implies you have only `batch_size` being 1.\r\n\r\nMy suggestion:\r\n- Try to run your training without SageMaker (and without the using the fix `torch.unsqueeze`)\r\n- Check what is received by the model, and check in the data pipeline if it prepares the correct input format\r\n  - If you still get the issue and can't figure it out:\r\n    - I could try to help if you could provide the training script + data processing script + a tiny portion of your data    \r\n  - If the issue only occurs when you wrap the training in SageMaker, I don't have the competence to help in this case, sorry. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":18.61,
        "Solution_score":0.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":243.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1532464254552,
        "Answerer_location":null,
        "Answerer_reputation":81.0,
        "Answerer_views":16.0,
        "Challenge_adjusted_solved_time":310.004845,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Challenge_closed_time":1547752819392,
        "Challenge_comment_count":1,
        "Challenge_created_time":1544396689430,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1546636801950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53697587",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":19.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":6.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.8386820358,
        "Challenge_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1697.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1421238326280,
        "Poster_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Poster_reputation":1951.0,
        "Poster_views":217.0,
        "Solution_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.44,
        "Solution_score":3.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"REST Payload",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":913.5883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Challenge_closed_time":1653922922000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650634004000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":18.2,
        "Challenge_reading_time":50.05,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":6.8184740551,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":296,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"cc @philschmid  I would also suggest @kanwari3 to\r\n- try to use the same Python\/PyTorch\/transformers versions (and other libraries) on SageMaker that work locally (if possible)\r\n- if the above doesn't work, try to use on local machine the same versions as those used on SageMaker, and see if you still get errors\r\n\r\nSo we have a better idea about if this is indeed a SageMaker issue or libraries issue This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. cc @philschmid  , cc @ydshieh ,  cc @sgugger \r\nHi, \r\n\r\nThis is a follow up on this post with the same title. We are trying to fix the issue and are still getting the same error after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. We tried unsqueezing the input_ids and attention_masks but it didn\u2019t fix the error.\r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\n    return {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":20.89,
        "Solution_score":1.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"TensorFlow Model",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623592472000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.7929730489,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score":0.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1458548318740,
        "Answerer_location":"Singapore",
        "Answerer_reputation":1568.0,
        "Answerer_views":266.0,
        "Challenge_adjusted_solved_time":873.3076302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Challenge_closed_time":1571189751312,
        "Challenge_comment_count":3,
        "Challenge_created_time":1568045843843,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":6.7734322934,
        "Challenge_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":343.0,
        "Challenge_word_count":181,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1523298968403,
        "Poster_location":null,
        "Poster_reputation":1754.0,
        "Poster_views":197.0,
        "Solution_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.89,
        "Solution_score":1.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1334762714136,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation":6557.0,
        "Answerer_views":2005.0,
        "Challenge_adjusted_solved_time":873.2213111111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Error installing component: azure_cli_ml_cliextension.windows \"The action failed catastrophically with <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.AzureCliException: Unable to get list of currently installed Azure CLI extensions<\/p>\n\n<p>at <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.InstallAzureCliExtensionAction.d__23.MoveNext() in C:\\swarm\\workspace\\Installer-1.2\\Installer.Engine\\Actions\\RegisteredActions\\InstallAzureCliExtensionAction.cs:line 97<\/p>\n\n<p>from there everything is stops....<\/p>",
        "Challenge_closed_time":1525629892870,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522486296150,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49586005",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":24.8,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.77333356,
        "Challenge_title":"Azure ML workbench failed installing on Windows 10 Enterprise",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":296.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1506435894640,
        "Poster_location":"Southeast Asia",
        "Poster_reputation":1922.0,
        "Poster_views":404.0,
        "Solution_body":"<p>Workbench is a preview product and issues may occur. Please try and get a newer exe and try again. It also seems like you have azure powershell issues here which I would have expected to be taken care of by the installer, but perhaps you can try and install azure powershell first. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":3.48,
        "Solution_score":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1539831335196,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation":137.0,
        "Answerer_views":56.0,
        "Challenge_adjusted_solved_time":870.7459847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Challenge_closed_time":1583860547412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580725861867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":31.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":6.7704980795,
        "Challenge_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1517147266416,
        "Poster_location":null,
        "Poster_reputation":65.0,
        "Poster_views":18.0,
        "Solution_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":19.1,
        "Solution_score":2.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":221.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1518706063680,
        "Answerer_location":null,
        "Answerer_reputation":95.0,
        "Answerer_views":15.0,
        "Challenge_adjusted_solved_time":866.1332491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Challenge_closed_time":1573932699567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570814619870,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58345935",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.7651926549,
        "Challenge_title":"The amlignore file doesn't reduce the size of snapshot",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1518706063680,
        "Poster_location":null,
        "Poster_reputation":95.0,
        "Poster_views":15.0,
        "Solution_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":28.3,
        "Solution_reading_time":3.35,
        "Solution_score":2.0,
        "Solution_sentence_count":3.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1337759214688,
        "Answerer_location":"Pune India",
        "Answerer_reputation":1036.0,
        "Answerer_views":124.0,
        "Challenge_adjusted_solved_time":810.2160711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1519637555372,
        "Challenge_comment_count":4,
        "Challenge_created_time":1516531050743,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1516720777516,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":6.7614778019,
        "Challenge_title":"How to call Sagemaker training model endpoint API in C#",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2093.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1337759214688,
        "Poster_location":"Pune India",
        "Poster_reputation":1036.0,
        "Poster_views":124.0,
        "Solution_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.8,
        "Solution_reading_time":6.92,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":862.2688888889,
        "Challenge_answer_count":2,
        "Challenge_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.\n\nThanks!",
        "Challenge_closed_time":1579845347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576741179000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.7607262172,
        "Challenge_title":"how to increase the storage of host instance",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":308.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"The disk size is currently not configurable for SageMaker Endpoints with EBS backed volumes. As a workaround, please use instances with ephemeral storage for your SageMaker endpoint.\n\nExample instance types with ephemeral storage:\n\nm5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/m5\/\nc5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/c5\/\nr5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/r5\/\n\nThe full list of Amazon SageMaker instance types can be accessed here: https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.8,
        "Solution_reading_time":7.14,
        "Solution_score":0.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1424063473423,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":334.0,
        "Answerer_views":347.0,
        "Challenge_adjusted_solved_time":810.2807111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed the instructions <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">here<\/a> to set up an EMR cluster and a SageMaker notebook. I did not have any errors until the last step.<\/p>\n\n<p>When I open a new Notebook in Sagemaker, I get the message:<\/p>\n\n<pre><code>The kernel appears to have died. It will restart automatically.\n<\/code><\/pre>\n\n<p>And then:<\/p>\n\n<pre><code>        The kernel has died, and the automatic restart has failed.\n        It is possible the kernel cannot be restarted. \n        If you are not able to restart the kernel, you will still be able to save the \nnotebook, but running code will no longer work until the notebook is reopened.\n<\/code><\/pre>\n\n<p>This only happens when I use the pyspark\/Sparkmagic kernel. Notebooks opened with the Conda kernel or any other kernel work fine. <\/p>\n\n<p>My EMR cluster is set up exactly as in the instructions, with an added rule:<\/p>\n\n<pre><code>[\n  {\n    \"Classification\": \"spark\",\n    \"Properties\": {\n      \"maximizeResourceAllocation\": \"true\"\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd appreciate any pointers on why this is happening and how I can debug\/fix.<\/p>\n\n<p>P.S.: I've done this successfully in the past without any issues. When I tried re-doing this today, I ran into this issue. I tried re-creating the EMR clusters and Sagemaker notebooks, but that didn't help. <\/p>",
        "Challenge_closed_time":1531254842950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1528337832390,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50732094",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.6986141238,
        "Challenge_title":"Sagemaker PySpark: Kernel Dead",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1768.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1430255275880,
        "Poster_location":"Pittsburgh, PA",
        "Poster_reputation":125.0,
        "Poster_views":13.0,
        "Solution_body":"<p>Thank you for using Amazon SageMaker.<\/p>\n\n<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.<\/p>\n\n<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0<\/code>.<\/p>\n\n<p>You can get more information in this open github issue <a href=\"https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458\" rel=\"noreferrer\">https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458<\/a>.<\/p>\n\n<p>Let us know if there is any other way we can be of assistance.<\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.68,
        "Solution_score":5.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":8,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581000215000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":6.6800335555,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.3,
        "Solution_reading_time":19.93,
        "Solution_score":0.0,
        "Solution_sentence_count":20.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":275.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":788.8893794444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello experts,\n\nI would like to attach a managed disk to my machine learning compute instance. Is that possible?\n\nThere is a possible overlap to the question Attach Disk to Virtual Machine, but steps doesn't seem to apply to ML compute instances.\n\nThanks in advance,",
        "Challenge_closed_time":1604477155916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601637154150,
        "Challenge_favorite_count":6.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/115201\/how-can-i-attach-a-managed-disk-to-a-machine-learn.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.6718929096,
        "Challenge_title":"How can I attach a managed disk to a Machine Learning Compute instance?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Hello,\n\nYou can attach your managed disk by following steps in Azure portal:\n\n\nMore details and limitation please see:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#azure-machine-learning-compute-managed\n\n\n\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":3.35,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Challenge_adjusted_solved_time":766.9947927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here is a high-level picture of what I am trying to achieve: I want to train a LightGBM model with spark as a compute <a href=\"https:\/\/microsoft.github.io\/SynapseML\/docs\/features\/lightgbm\/about\/\" rel=\"nofollow noreferrer\">backend<\/a>, all in SageMaker using their Training Job api.\nTo clarify:<\/p>\n<ol>\n<li>I have to use LightGBM in general, there is no option here.<\/li>\n<li>The reason I need to use spark compute backend is because the training with the current dataset does not fit in memory anymore.<\/li>\n<li>I want to use SageMaker Training job setting so I could use SM Hyperparameter optimisation job to find the best hyperparameters for LightGBM. While LightGBM spark interface itself does offer some hyperparameter tuning capabilities, it does not offer Bayesian HP tuning.<\/li>\n<\/ol>\n<p>Now, I know the general approach to running custom training in SM: build a container in a certain way, and then just pull it from ECR and kick-off a training job\/hyperparameter tuning job through <code>sagemaker.Estimator<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">API<\/a>. Now, in this case SM would handle resource provisioning for you, would create an instance and so on. What I am confused about is that essentially, to use spark compute backend, I would need to have an EMR cluster running, so the SDK would have to handle that as well. However, I do not see how this is possible with the API above.<\/p>\n<p>Now, there is also that thing called <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">Sagemaker Pyspark SDK<\/a>. However, the provided <code>SageMakerEstimator<\/code> API from that package does not support on-the-fly cluster configuration either.<\/p>\n<p>Does anyone know a way how to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities?<\/p>\n<p>One way I see is to run an EMR cluster in the background, and then just create a regular SM estimator job that would connect to the EMR cluster and do the training, essentially running a spark driver program in SM Estimator job.<\/p>\n<p>Has anyone done anything similar in the past?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1645793823447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643032642193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70835006",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":29.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":6.6437829529,
        "Challenge_title":"How to integrate spark.ml pipeline fitting and hyperparameter optimisation in AWS Sagemaker?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":340,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1357233199987,
        "Poster_location":null,
        "Poster_reputation":2171.0,
        "Poster_views":126.0,
        "Solution_body":"<p>Thanks for your questions. Here are answers:<\/p>\n<ul>\n<li><p><strong>SageMaker PySpark SDK<\/strong> <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/<\/a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.<\/p>\n<\/li>\n<li><p><strong>Running Spark in SageMaker jobs<\/strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">SageMaker Processing has a built-in Spark Container<\/a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune\/\" rel=\"nofollow noreferrer\">Syne Tune<\/a> from AWS itself (that supports bayesian optimization)<\/p>\n<\/li>\n<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Processing container code here<\/a> to build a custom Training container<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:<\/p>\n<ul>\n<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.<\/li>\n<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)<\/li>\n<\/ul>\n<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.6,
        "Solution_reading_time":35.55,
        "Solution_score":2.0,
        "Solution_sentence_count":18.0,
        "Solution_topic":"Spark Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":376.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":766.1991666667,
        "Challenge_answer_count":7,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Expected Behavior\r\n<WARNING:elasticsearch:PUT https:\/\/my aws ES endpoint\/table_a54a9a96-c246-4bcd-b417-2d8c005c3290 [status:400 request:0.069s]\r\nINFO:databuilder.callback.call_back:No callbacks to notify\r\nTraceback (most recent call last):\r\n  File \"example\/scripts\/sample_data_loader_neptune.py\", line 403, in <module>\r\n    job_es_table.launch()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch\r\n    self.publisher.publish()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n    self.publish_impl()\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl\r\n    self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped\r\n    return func(*args, params=params, headers=headers, **kwargs)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create\r\n    \"PUT\", _make_path(index), params=params, headers=headers, body=body\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request\r\n    raise e\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request\r\n    timeout=timeout,\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request\r\n    self._raise_error(response.status_code, raw_data)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error\r\n    status_code, error_message, additional_info\r\n\r\n\r\nelasticsearch.exceptions.RequestError: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')->\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n* Amunsen version used: Databuilder: 6.7.1 Common 0.26.0 Amundsen-Gremlin 0.0.13 AWS ES : 6.8\r\n",
        "Challenge_closed_time":1649071896000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646313579000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1748",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":22.0,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":6.6427464374,
        "Challenge_title":"Bug Report elasticsearch exception for sample_neptune_loader",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for opening your first issue here!\n This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n Same problem here, does you solved? @amandeep848 could you fix the problem? @amandeep848 could you fix the problem? Hello!\r\n\r\nI have been fixed the problem by putting the version of amundsen-common to 0.24.1\r\n\r\n- My `requirements.txt` file is setup as shown below:\r\n\r\n```text\r\namundsen-databuilder==6.5.2\r\namundsen-gremlin==0.0.13\r\ngremlinpython==3.4.10\r\nrequests-aws4auth==1.1.1\r\nboto3==1.21.23\r\nbotocore==1.24.23\r\ntyping-extensions==4.1.1\r\noverrides==6.1.0\r\namundsen-common==0.24.1\r\n```\r\n\r\n- My Glue databuilder script:\r\n\r\n```python\r\nimport logging\r\nimport os\r\nimport uuid\r\nimport boto3\r\nimport textwrap\r\nimport json\r\n\r\nfrom datetime import date\r\n\r\nfrom elasticsearch import Elasticsearch\r\nfrom pyhocon import ConfigFactory\r\n\r\nfrom databuilder.clients.neptune_client import NeptuneSessionClient\r\nfrom databuilder.extractor.es_last_updated_extractor import EsLastUpdatedExtractor\r\nfrom databuilder.extractor.neptune_search_data_extractor import NeptuneSearchDataExtractor\r\n\r\nfrom databuilder.job.job import DefaultJob\r\nfrom databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader\r\nfrom databuilder.loader.file_system_neptune_csv_loader import FSNeptuneCSVLoader\r\nfrom databuilder.publisher.elasticsearch_constants import (\r\n    DASHBOARD_ELASTICSEARCH_INDEX_MAPPING, USER_ELASTICSEARCH_INDEX_MAPPING,\r\n)\r\nfrom databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher\r\nfrom databuilder.publisher.neptune_csv_publisher import NeptuneCSVPublisher\r\nfrom databuilder.task.task import DefaultTask\r\nfrom databuilder.transformer.base_transformer import ChainedTransformer, NoopTransformer\r\nfrom databuilder.transformer.dict_to_model import MODEL_CLASS, DictToModel\r\nfrom databuilder.transformer.generic_transformer import (\r\n    CALLBACK_FUNCTION, FIELD_NAME, GenericTransformer,\r\n)\r\n\r\nfrom databuilder.extractor.glue_extractor import GlueExtractor\r\nfrom databuilder.task.neptune_staleness_removal_task import NeptuneStalenessRemovalTask\r\n\r\n\r\nes_host = os.getenv('ES_HOST')\r\n\r\nneptune_host = os.getenv('NEPTUNE_HOST')\r\nneptune_port = os.getenv('NEPTUNE_PORT', 8182)\r\nneptune_iam_role_name = os.getenv('NEPTUNE_IAM_ROLE')\r\n\r\nS3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\r\ntoday = date.today()\r\nS3_DATA_PATH = f'amundsen_data\/glue_extractor\/year={today.year}\/month={today.month}\/day={today.day}'\r\n\r\nAWS_REGION = os.getenv('AWS_REGION')\r\nGLUE_DATABASE_IDENTIFIER = os.getenv('GLUE_DATABASE_IDENTIFIER')\r\n\r\nes = Elasticsearch(\r\n    '{}'.format(es_host),\r\n    scheme=\"https\",\r\n    port=443,\r\n)\r\n\r\nNEPTUNE_ENDPOINT = '{}:{}'.format(neptune_host, neptune_port)\r\n\r\nLOGGER = logging.getLogger(__name__)\r\n\r\n\r\ndef run_glue_job(job_name):\r\n    \"\"\"Run Glue metadata extraction\r\n\r\n    Args:\r\n        job_name (string): job name\r\n    \"\"\"\r\n\r\n    tmp_folder = '\/var\/tmp\/amundsen\/{job_name}'.format(job_name=job_name)\r\n    node_files_folder = '{tmp_folder}\/nodes'.format(tmp_folder=tmp_folder)\r\n    relationship_files_folder = '{tmp_folder}\/relationships'.format(tmp_folder=tmp_folder)\r\n\r\n    loader = FSNeptuneCSVLoader()\r\n    publisher = NeptuneCSVPublisher()\r\n\r\n    with open(\"databases.json\") as jsonFile:\r\n\r\n        filters = json.load(jsonFile)\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        f'extractor.glue.{GlueExtractor.CLUSTER_KEY}': GLUE_DATABASE_IDENTIFIER,\r\n        f'extractor.glue.{GlueExtractor.FILTER_KEY}': filters,\r\n        loader.get_scope(): {\r\n            FSNeptuneCSVLoader.NODE_DIR_PATH: node_files_folder,\r\n            FSNeptuneCSVLoader.RELATION_DIR_PATH: relationship_files_folder,\r\n            FSNeptuneCSVLoader.SHOULD_DELETE_CREATED_DIR: True,\r\n            FSNeptuneCSVLoader.JOB_PUBLISHER_TAG: 'unique_tag'\r\n        },\r\n        publisher.get_scope(): {\r\n            NeptuneCSVPublisher.NODE_FILES_DIR: node_files_folder,\r\n            NeptuneCSVPublisher.RELATION_FILES_DIR: relationship_files_folder,\r\n            NeptuneCSVPublisher.AWS_S3_BUCKET_NAME: S3_BUCKET_NAME,\r\n            NeptuneCSVPublisher.AWS_BASE_S3_DATA_PATH: S3_DATA_PATH,\r\n            NeptuneCSVPublisher.NEPTUNE_HOST: NEPTUNE_ENDPOINT,\r\n            NeptuneCSVPublisher.AWS_IAM_ROLE_NAME: neptune_iam_role_name,\r\n            NeptuneCSVPublisher.AWS_REGION: AWS_REGION\r\n        },\r\n    })\r\n\r\n    DefaultJob(\r\n        conf=job_config,\r\n        task=DefaultTask(\r\n            extractor=GlueExtractor(),\r\n            loader=loader,\r\n            transformer=NoopTransformer()\r\n        ),\r\n        publisher=publisher\r\n    ).launch()\r\n\r\ndef create_remove_stale_data_job():\r\n    \"\"\"Run remove stale data from Neptune\r\n\r\n    Returns:\r\n        NeptuneStalenessRemovalTask: Neptune stateleness data job\r\n    \"\"\"\r\n\r\n    target_relations = ['DESCRIPTION', 'DESCRIPTION_OF', 'COLUMN', 'COLUMN_OF', 'TABLE', 'TABLE_OF']\r\n    target_nodes = ['Table', 'Column', 'Programmatic_Description', \"Schema\"]\r\n\r\n    staleness_max_pct = 5\r\n\r\n    while True:\r\n\r\n        try:\r\n\r\n            LOGGER.info(f'Delete stale data at threshold - {staleness_max_pct}%')\r\n\r\n            job_config = ConfigFactory.from_dict({\r\n                'task.remove_stale_data': {\r\n                    NeptuneStalenessRemovalTask.TARGET_RELATIONS: target_relations,\r\n                    NeptuneStalenessRemovalTask.TARGET_NODES: target_nodes,\r\n                    NeptuneStalenessRemovalTask.STALENESS_CUT_OFF_IN_SECONDS: 86400,  # 1 day\r\n                    NeptuneStalenessRemovalTask.STALENESS_MAX_PCT: staleness_max_pct,\r\n                    'neptune.client': {\r\n                        NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                        NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                    }\r\n                }\r\n            })\r\n\r\n            job = DefaultJob(\r\n                conf=job_config,\r\n                task=NeptuneStalenessRemovalTask()\r\n            )\r\n\r\n            job.launch()\r\n\r\n            break\r\n\r\n        except Exception as ex:\r\n\r\n            LOGGER.error(ex)\r\n            LOGGER.info(f'Increase stale data threshold')\r\n\r\n            staleness_max_pct += 5\r\n\r\n            if staleness_max_pct == 105:\r\n\r\n                break\r\n\r\n\r\ndef create_es_publisher_job(elasticsearch_index_alias='table_search_index',\r\n                            elasticsearch_doc_type_key='table',\r\n                            model_name='databuilder.models.table_elasticsearch_document.TableESDocument',\r\n                            entity_type='table',\r\n                            elasticsearch_mapping=None):\r\n    \"\"\"\r\n    :param elasticsearch_index_alias:  alias for Elasticsearch used in\r\n                                       amundsensearchlibrary\/search_service\/config.py as an index\r\n    :param elasticsearch_doc_type_key: name the ElasticSearch index is prepended with. Defaults to `table` resulting in\r\n                                       `table_{uuid}`\r\n    :param model_name:                 the Databuilder model class used in transporting between Extractor and Loader\r\n    :param entity_type:                Entity type handed to the `Neo4jSearchDataExtractor` class, used to determine\r\n                                       Cypher query to extract data from Neo4j. Defaults to `table`.\r\n    :param elasticsearch_mapping:      Elasticsearch field mapping \"DDL\" handed to the `ElasticsearchPublisher` class,\r\n                                       if None is given (default) it uses the `Table` query baked into the Publisher\r\n    \"\"\"\r\n    # loader saves data to this location and publisher reads it from here\r\n    extracted_search_data_path = '\/var\/tmp\/amundsen\/search_data.json'\r\n    loader = FSElasticsearchJSONLoader()\r\n    extractor = NeptuneSearchDataExtractor()\r\n\r\n    task = DefaultTask(\r\n        loader=loader,\r\n        extractor=extractor,\r\n        transformer=NoopTransformer()\r\n    )\r\n\r\n    # elastic search client instance\r\n    elasticsearch_client = es\r\n\r\n    # unique name of new index in Elasticsearch\r\n    elasticsearch_new_index_key = '{}_'.format(elasticsearch_doc_type_key) + str(uuid.uuid4())\r\n\r\n    publisher = ElasticsearchPublisher()\r\n\r\n    session = boto3.Session(region_name=AWS_REGION)\r\n\r\n    aws_creds = session.get_credentials()\r\n    aws_access_key = aws_creds.access_key\r\n    aws_access_secret = aws_creds.secret_key\r\n    aws_token = aws_creds.token\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        extractor.get_scope(): {\r\n            NeptuneSearchDataExtractor.ENTITY_TYPE_CONFIG_KEY: entity_type,\r\n            NeptuneSearchDataExtractor.MODEL_CLASS_CONFIG_KEY: model_name,\r\n            'neptune.client': {\r\n                NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                NeptuneSessionClient.AWS_ACCESS_KEY: aws_access_key,\r\n                NeptuneSessionClient.AWS_SECRET_ACCESS_KEY: aws_access_secret,\r\n                NeptuneSessionClient.AWS_SESSION_TOKEN: aws_token\r\n            }\r\n        },\r\n        'loader.filesystem.elasticsearch.file_path': extracted_search_data_path,\r\n        'loader.filesystem.elasticsearch.mode': 'w',\r\n        publisher.get_scope(): {\r\n            'file_path': extracted_search_data_path,\r\n            'mode': 'r',\r\n            'client': elasticsearch_client,\r\n            'new_index': elasticsearch_new_index_key,\r\n            'doc_type': elasticsearch_doc_type_key,\r\n            'alias': elasticsearch_index_alias\r\n        }\r\n    })\r\n\r\n    # only optionally add these keys, so need to dynamically `put` them\r\n    if elasticsearch_mapping:\r\n        job_config.put('publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_MAPPING_CONFIG_KEY),\r\n                       elasticsearch_mapping)\r\n\r\n    job = DefaultJob(\r\n        conf=job_config,\r\n        task=task,\r\n        publisher=ElasticsearchPublisher()\r\n    )\r\n\r\n    return job\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    LOGGER.info('ES Host: ' +  es_host)\r\n    LOGGER.info('Neptune Host: ' + neptune_host)\r\n    LOGGER.info('Neptune Port: ' + str(neptune_port))\r\n    LOGGER.info('Neptune IAM Role Name: ' + neptune_iam_role_name)\r\n    LOGGER.info('S3 Bucket Name: ' + S3_BUCKET_NAME)\r\n    LOGGER.info('S3 Data Path: ' + S3_DATA_PATH)\r\n    LOGGER.info('AWS Region: ' + AWS_REGION)\r\n\r\n    logging.info('>>> Running Remove Stale Data Job <<<')\r\n\r\n    create_remove_stale_data_job()\r\n\r\n    logging.info('>>> Running Glue Extractor <<<')\r\n\r\n    run_glue_job('amundsen_glue_extractor')\r\n\r\n    logging.info('>>> Running ES Publisher <<<')\r\n\r\n    job_es_table = create_es_publisher_job(\r\n        elasticsearch_index_alias='table_search_index',\r\n        elasticsearch_doc_type_key='table',\r\n        entity_type='table',\r\n        model_name='databuilder.models.table_elasticsearch_document.TableESDocument'\r\n    )\r\n    job_es_table.launch()\r\n```\r\n\r\n- databases.json\r\n\r\n```json\r\n[]\r\n```\r\n\r\n- .env\r\n\r\n```env\r\nES_HOST=<ES_HOST>\r\nNEPTUNE_HOST=<NEPTUNE_HOST>\r\nNEPTUNE_PORT=8182\r\nNEPTUNE_IAM_ROLE=<NEPTUNE_IAM_ROLE>\r\nS3_BUCKET_NAME=<S3_BUCKET_NAME>\r\nAWS_REGION=<AWS_REGION>\r\nSECRET_NAME=<SECRET_NAME>\r\nGLUE_DATABASE_IDENTIFIER=<GLUE_DATABASE_IDENTIFIER>\r\n```\r\n\r\n\r\nHope this help!\r\n\r\nBest Regards.\r\nBill\r\n I encountered this issue, too, as I installed data builder from codebase with `python setup.py install`, and after rebase with the main branch, the previous version was not clean up when we simply rerun `python setup.py install`, the way out was to do `pip uninstall amundsen-databuilder` and `pip uninstall amundsen-common` until non of packages existed(there could be multiple versions left, more than once per each package could be required).\r\n\r\nThen the expected elastic-related code is up to date w\/o this error anymore.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":132.24,
        "Solution_score":1.0,
        "Solution_sentence_count":103.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":698.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1393509037328,
        "Answerer_location":null,
        "Answerer_reputation":768.0,
        "Answerer_views":34.0,
        "Challenge_adjusted_solved_time":765.2169341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Challenge_closed_time":1657799581543,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655044800580,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":11.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.6414653335,
        "Challenge_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1314313109232,
        "Poster_location":"Technion, Israel",
        "Poster_reputation":18777.0,
        "Poster_views":2000.0,
        "Solution_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.83,
        "Solution_score":2.0,
        "Solution_sentence_count":6.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":60.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":763.1580555556,
        "Challenge_answer_count":3,
        "Challenge_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Challenge_closed_time":1654502136000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651754767000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.6387746468,
        "Challenge_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for the issue @juliensimon, this should be fixed by https:\/\/github.com\/huggingface\/transformers\/pull\/17049. It will be in the next release which should drop early next week. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. Should be fixed now!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":6.85,
        "Solution_score":1.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":755.2722222222,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221269000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612502289000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":6.6284013937,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":21.92,
        "Solution_score":2.0,
        "Solution_sentence_count":17.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":326.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":2,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":6.6154990573,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score":0.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation":833.0,
        "Answerer_views":55.0,
        "Challenge_adjusted_solved_time":745.5753130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to implement ml ops in azure. I am running a python script through azure cli task in devops. Though I can read files from the git folder but the py script is not able to generate the output csv in git. Strangely its also not giving any error.<\/p>\n<p>I think the file is getting generated in the compute instance directory. How to instead write it to a git folder or any folder which I can see in the compute engine.<\/p>",
        "Challenge_closed_time":1642087839300,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639403768173,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335823",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":5.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.6154965001,
        "Challenge_title":"Azure ML ops task to write a file in git repo",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1482772262960,
        "Poster_location":"India",
        "Poster_reputation":349.0,
        "Poster_views":60.0,
        "Solution_body":"<p>I had a similar situation where python couldn't find the files that were supposed to exist in the root of the Azure ML project folder after deploying. After investigation, I realized that Azure ML invokes your scripting code from a different root folder.<\/p>\n<p>Here is an example of an operation that reads from the relative path where your code exists:<\/p>\n<pre><code>    SCRIPT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n    with open(SCRIPT_DIRECTORY+'filename.json', 'w') as outfile:\n        json.dump(dict_object, outfile)\n<\/code><\/pre>\n<p>You can then join to <code>SCRIPT_DIRECTORY <\/code> the relative path of your git folder, before your output.<\/p>\n<p>Alternatively, as per your comment &quot;.\/output is not getting created&quot;, you can force it with:<\/p>\n<p><code>os.makedirs(&quot;.\/outputs&quot;, exist_ok=True)<\/code><\/p>\n<p><code>exist_ok<\/code> (optional) : A default value <code>False<\/code> is used for this parameter. If the target directory already exists an <code>OSError<\/code> is raised if its value is <code>False<\/code> otherwise not. For value <code>True<\/code> leaves directory unaltered.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":14.45,
        "Solution_score":1.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":723.7180555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Challenge_closed_time":1618085440000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615480055000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.5857826903,
        "Challenge_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":323.0,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":1.66,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Permission Control",
        "Solution_topic_macro":"Identity Management",
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":720.8744444444,
        "Challenge_answer_count":16,
        "Challenge_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1671215663000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668620515000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":16,
        "Challenge_readability":14.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":6.5818512241,
        "Challenge_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":279,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Just want to verify that the autostop script in your bucket had not been updated at some point in the past unexpectedly. Is that correct? Yes-- none of the files on the s3 bucket were changed in several months. I also downloaded the autostop script from the bucket to verify manually that it matches the SWB repo version. I was not able to replicate this in v5.2.2:\r\n<img width=\"937\" alt=\"Screen Shot 2022-11-30 at 3 34 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903064-013c1899-2763-4c88-8cce-7b39128b0240.png\">\r\n\r\nIf you look at the CloudWatch log group \/aws\/sagemaker\/NotebookInstances\/BasicNotebookInstance-<id>\/LifecycleConfigOnStart do you see the following output (would be towards the end):\r\n<img width=\"960\" alt=\"Screen Shot 2022-11-30 at 3 36 08 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903303-d180144c-62d9-4775-a233-83687012715b.png\">\r\nThis is triggered on start of the instance. The screenshot you posted was from your instance, correct? Do you see the lines, \r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n``` \r\nthis line comes from this repo [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L100-L101). It should be only the one line, like below\r\n```\r\nidle = False\r\n``` \r\nwhich comes from SWB here [awslabs\/service-workbench-on-aws](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L100)\r\n\r\nThe s3 file says it's downloaded, but for whatever reason it's not using that downloaded file, and instead using the file on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples. This introduces SWB to vulnerabilities when this code is changes and a bug is introduced in that repo. SWB should instead use the autostop script that it has saved in s3, because that is locked and changes that aren't intended wouldn't be introduced. The screenshot is from my instance, yes. SWB repo contains the lines:\r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n```\r\n[here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/61200d06d1a607b9c0a209240813b261ade2c5e9\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L105). It is the lines:\r\n```\r\nidle = False\r\nprint('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\nthat I thought you said were presenting the problem (that are in the samples repo but not SWB). Is that correct?\r\n\r\nHowever, I see that my instance is not stopping even though the autostop script is the same as the SWB repo.\r\n\r\nWhere did you see the error on the cron job for the autostop? Also, to clarify, you see that the Cloudwatch logs copy from the s3 bucket to local and that the s3 bucket file is the correct file? Yet, you see the wrong file when you less the file on the instance? Oh, you're right I was looking at the wrong line. My apologies! \r\n\r\nYes- for us it's successfully copying the correct file from s3 (which I downloaded to verify), but the autostop script (`\/usr\/local\/bin\/autostop.py`) is the wrong copy. Got it. And where do you see the cron job failing? Because it was not overwriting the autostop script with the one from our s3 bucket, and instead defaulting to the script from `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples`,  when the repo owners introduced this commit: https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e, the cron job failed on lines like `print(f'Notebook idle state set as {idle} since kernel connections are ignored.')`, stating that the `print(f` part was invalid syntax.\r\n\r\nI guess the key issue is really why it didn't overwrite this default script with the s3 one, considering it successfully downloaded the one from s3. Secondly, it seems odd that this repo is somehow the default autostop script that the Sagemaker system uses. This introduces bugs if there's a failure in that script or a malicious commit. Yes, I see the problem in the other repo's commit. I am still trying to debug how the script is on your Sagemaker instance.\r\n\r\nGot ~two~ three more questions:\r\n1. Where _in you account_ did you see that error message from the cron job? CloudWatch logs? Sagemaker? etc.\r\n2. Are you working with AppStream-enabled SWB? Does Sagemaker have to go through AppStream to connect?\r\n3. What is the output from running this command in a terminal on the sagemaker instance: `\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 300 --ignore-connections`? Sure. :D Yeah I don't know how the script was there automatically. I didn't see any code in our repo that would cause it to pull from there.\r\n\r\nWe do not have appstream enabled, but we do send traffic through a proxy lambda as well as a firewall instance. However, all the environment files that get downloaded for the bootstrap process were successful, so I don't think it was a network issue.\r\n\r\nThe reason I checked the autostop script was because I saw no note in the `\/var\/log\/autostop.log` file that the script is sent to via cron job. So I ran the script by hand.\r\n\r\nFor instance, right now autostop is not working. There's no messaging that tells you it's not working. When you look at the cron logs it shows this, with no errors. The `\/var\/log\/autostop.log` script doesn't show any messages or errors.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# grep autostop \/var\/log\/cron | tail -n 1\r\nDec  1 18:14:01 ip-10-10-57-235 CROND[9860]: (root) CMD (\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600 --ignore-connections >> \/var\/log\/autostop.log)\r\n```\r\n\r\nBut if you run the autostop script exactly as the cron job has it, like below, you get an error \/right now\/ related to a boto3 import issue.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# \/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/autostop.py\", line 18, in <module>\r\n    import boto3\r\nImportError: No module named boto3\r\n```\r\n\r\nBoto3 changes were introduced in a commit here, [in the on-start script on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/13b4023c9dca45fea58b2129fe5848619284653a#diff-54051e148aa00ee3fa158cc346d6c243418d14718a6760171ef562887977748f) Yup, so I also get that problem when I try to invoke the autostop script (and my autostop script matches the SWB one). I think that is the root cause of this problem. I will add a backlog item to figure out why boto3 is not being imported correctly so that sagemaker notebooks can use them for autostop. \r\n\r\nIt still does not explain why you got the amazon-sagemaker-notebook-instance-lifecycle-config-samples in the instance. Was that only present in one instance or all instances? Is it possible someone manually changed the files when trying to debug the autostop not working?\r\n\r\nThanks so much for working through this with me! Yeah, no problem-- thanks for your patience and attention! :D\r\n\r\nI don't believe that the files have been altered. I am currently the only person on my team actively responsible for doing admin\/infrastructure activities for these systems. I've tried this on new instances to rule out individual Sagemaker systems changes by users. We have two different version of SWB deployed-- maybe there's a difference between versions.  @srpiatt please upgrade your SWB installation to the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5). Sagemaker made a change that caused all new instances to be spun up with the AL2 operating system. New Sagemaker instances will no longer be able to mount studies or autostop without the fix in v5.2.5 Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Auto stop works. Still not sure how the file got replaced with the one in that repo, but it's a non-issue at the moment. Thank you! :D",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":8.9,
        "Solution_reading_time":108.09,
        "Solution_score":5.0,
        "Solution_sentence_count":81.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":1171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1553882107003,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":294.0,
        "Answerer_views":28.0,
        "Challenge_adjusted_solved_time":715.1033155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Challenge_closed_time":1600272249596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597694559337,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597697877660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":26.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.5751108078,
        "Challenge_title":"What is called within a sagemaker custom (training) container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1484838464572,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation":3937.0,
        "Poster_views":387.0,
        "Solution_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.11,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"TensorFlow Configuration",
        "Solution_topic_macro":"Computation Management",
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580225794000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score":1.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":6.5581580069,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score":4.0,
        "Solution_sentence_count":31.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":312.0,
        "Tool":"Comet"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1548341556520,
        "Answerer_location":"Mumbai, Maharashtra, India",
        "Answerer_reputation":2907.0,
        "Answerer_views":238.0,
        "Challenge_adjusted_solved_time":701.3743433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Challenge_closed_time":1573144655863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570616046857,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1570619708227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":6.5559134802,
        "Challenge_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1548341556520,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation":2907.0,
        "Poster_views":238.0,
        "Solution_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.88,
        "Solution_score":2.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Challenge_adjusted_solved_time":688.1130463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Challenge_closed_time":1656998954310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654521747343,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.5354053307,
        "Challenge_title":"Logging and Fetching Run Parameters in AzureML",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1554497484963,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation":438.0,
        "Poster_views":120.0,
        "Solution_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.49,
        "Solution_score":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Metrics Logging",
        "Solution_topic_macro":"Performance Management",
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1412515367427,
        "Answerer_location":null,
        "Answerer_reputation":161.0,
        "Answerer_views":24.0,
        "Challenge_adjusted_solved_time":682.3236119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559693969323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557237604320,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":9.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.5269685569,
        "Challenge_title":"Beginners guide to Sagemaker",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":748.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1554397763220,
        "Poster_location":null,
        "Poster_reputation":327.0,
        "Poster_views":54.0,
        "Solution_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.6,
        "Solution_reading_time":18.79,
        "Solution_score":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Artifact Tracking",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation":493.0,
        "Answerer_views":47.0,
        "Challenge_adjusted_solved_time":8283.79234,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Challenge_closed_time":1567635751827,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565186451297,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1565186790803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":13.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.5240926476,
        "Challenge_title":"SageMaker and TensorFlow 2.0",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":4136.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1446859510543,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation":3259.0,
        "Poster_views":233.0,
        "Solution_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1595008443227,
        "Solution_link_count":7.0,
        "Solution_readability":25.0,
        "Solution_reading_time":19.1,
        "Solution_score":10.0,
        "Solution_sentence_count":13.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":7,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624292044000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":6.5150821699,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":249,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score":1.0,
        "Solution_sentence_count":17.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1432829415467,
        "Answerer_location":null,
        "Answerer_reputation":501.0,
        "Answerer_views":76.0,
        "Challenge_adjusted_solved_time":674.1436052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in AzureML which has a R module at its core. Additionally, I have some .RData files stored in Azure blob storage. The blob container is set as private (no anonymous access).<\/p>\n\n<p>Now, I am trying to make a https call from inside the R script to the azure blob storage container in order to download some files. I am using the <code>httr<\/code> package's <code>GET()<\/code> function and properly set up the url, authentication etc...The code works in R on my local machine but the same code gives me the following error when called from inside the R module in the experiment<\/p>\n\n<pre><code>error:1411809D:SSL routines:SSL_CHECK_SERVERHELLO_TLSEXT:tls invalid ecpointformat list\n<\/code><\/pre>\n\n<p>Apparently this is an error from the underlying OpenSSL library (which got fixed a while ago). Some suggested workarounds I found <a href=\"https:\/\/stackoverflow.com\/questions\/20046176\/rcurl-errors-when-fetching-ssl-endpoint\">here<\/a> were to set <code>sslversion = 3<\/code> and <code>ssl_verifypeer = 1<\/code>, or turn off verification <code>ssl_verifypeer = 0<\/code>. Both of these approaches returned the same error.<\/p>\n\n<p>I am guessing that this has something to do with the internal Azure certificate \/ validation...? Or maybe I am missing or overseeing something?<\/p>\n\n<p>Any help or ideas would be greatly appreciated. Thanks in advance.<\/p>\n\n<p>Regards<\/p>",
        "Challenge_closed_time":1450358402136,
        "Challenge_comment_count":1,
        "Challenge_created_time":1447931485157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1495540337367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33802274",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.5149254168,
        "Challenge_title":"Error:1411809D:SSL routines - When trying to make https call from inside R module in AzureML",
        "Challenge_topic":"Permission Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1432829415467,
        "Poster_location":null,
        "Poster_reputation":501.0,
        "Poster_views":76.0,
        "Solution_body":"<p>After a while, an answer came back from the support team, so I am going to post the relevant part as an answer here for anyone who lands here with the same problem. <\/p>\n\n<p>\"This is a known issue. The container (a sandbox technology known as \"drawbridge\" running on top of Azure PaaS VM) executing the Execute R module doesn't support outbound HTTPS traffic. Please try to switch to HTTP and that should work.\"<\/p>\n\n<p>As well as that a solution is on the way :<\/p>\n\n<p>\"We are actively looking at how to fix this bug. \"<\/p>\n\n<p>Here is the original <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/5866e16c-a145-481e-8764-f7c7823742b0\/https-call-from-inside-r-module-possible-?forum=MachineLearning\" rel=\"nofollow\">link<\/a> as a reference.\nhth<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.62,
        "Solution_score":0.0,
        "Solution_sentence_count":7.0,
        "Solution_topic":"Multi-Model Endpoint",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1619402503747,
        "Answerer_location":null,
        "Answerer_reputation":61.0,
        "Answerer_views":2.0,
        "Challenge_adjusted_solved_time":669.2438802778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Challenge_closed_time":1642620406232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640211128263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70455676",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":93.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":6.5076416466,
        "Challenge_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1006.0,
        "Challenge_word_count":526,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1619402503747,
        "Poster_location":null,
        "Poster_reputation":61.0,
        "Poster_views":2.0,
        "Solution_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":26.5,
        "Solution_reading_time":11.61,
        "Solution_score":1.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Model Registry",
        "Solution_topic_macro":"Model Management",
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1359732456992,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":401.0,
        "Answerer_views":55.0,
        "Challenge_adjusted_solved_time":667.9050702778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've successfully trained a LDA model with sagemaker, I've been able to set up an Inference API but it has a limit of how many records I can query at a time. <\/p>\n\n<p>I need to get predictions for a large file and have been trying to use Batch Transformation however am running against roadblock.<\/p>\n\n<p>My input date is in application\/x-recordio-protobuf content type, code is as follows:<\/p>\n\n<pre><code># Initialize the transformer object\ntransformer =sagemaker.transformer.Transformer(\n    base_transform_job_name='Batch-Transform',\n    model_name=model_name,\n    instance_count=1,\n    instance_type='ml.c4.xlarge',\n    output_path=output_location,\n    max_payload=20,\n    strategy='MultiRecord'\n    )\n# Start a transform job\ntransformer.transform(input_location, content_type='application\/x-recordio-protobuf',split_type=\"RecordIO\")\n# Then wait until the transform job has completed\ntransformer.wait()\n\n# Fetch validation result \ns3_client.download_file(bucket, 'topic_model_batch_transform\/output\/batch_tansform_part0.pbr.out', 'batch_tansform-result')\nwith open('batch_tansform-result') as f:\n    results = f.readlines()   \nprint(\"Sample transform result: {}\".format(results[0]))\n<\/code><\/pre>\n\n<p>I have chunked by input file into 10 files each around 19MB in size. I am attempting at first to run on a single chunk, therefore 19MB in total. I have tried changing strategy, trying SingleRecord. I have also tried different split_types, also trying None and \"Line\". <\/p>\n\n<p>I've read the documentation but its not clear what else I should try, also the error messages are very unclear.<\/p>\n\n<pre><code>2019-04-02T15:49:47.617:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=20, BatchStrategy=MULTI_RECORD\n#011at java.lang.Thread.run(Thread.java:748)2019-04-02T15:49:48.035:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Bad HTTP status returned from invoke: 413\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Message:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;title&gt;413 Request Entity Too Large&lt;\/title&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;h1&gt;Request Entity Too Large&lt;\/h1&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;p&gt;The data value transmitted exceeds the capacity limit.&lt;\/p&gt;\n<\/code><\/pre>\n\n<p>The above is the last one I got with the above configuration, before that I was also getting a 400 HTTP error code.<\/p>\n\n<p>Any help or pointers would be greatly appreciated! Thank you<\/p>",
        "Challenge_closed_time":1556626383163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554221924910,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55479366",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":39.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":6.5056421521,
        "Challenge_title":"Errors running Sagemaker Batch Transformation with LDA model",
        "Challenge_topic":"REST Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3622.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1359732456992,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation":401.0,
        "Poster_views":55.0,
        "Solution_body":"<p>I managed to resolve the issue, it seemed the maxpayload I was using was too high. I set  <code>MaxPayloadInMB=1<\/code> and it now runs like a dream<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.9,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1547398724312,
        "Answerer_location":null,
        "Answerer_reputation":41.0,
        "Answerer_views":6.0,
        "Challenge_adjusted_solved_time":667.1184413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1549801330896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547399704507,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1551005141750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":102.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":6.5044654653,
        "Challenge_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1312.0,
        "Challenge_word_count":542,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1547398724312,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":6.0,
        "Solution_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.7,
        "Solution_reading_time":21.42,
        "Solution_score":1.0,
        "Solution_sentence_count":9.0,
        "Solution_topic":"Data Labeling",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":659.0783333333,
        "Challenge_answer_count":4,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117002000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649744320000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":6.4923585148,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score":0.0,
        "Solution_sentence_count":10.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1465222092252,
        "Answerer_location":"Z\u00fcrich, Switzerland",
        "Answerer_reputation":1414.0,
        "Answerer_views":478.0,
        "Challenge_adjusted_solved_time":649.8259375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Challenge_closed_time":1637263566352,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634924192977,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":6.4782422293,
        "Challenge_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1465222092252,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation":1414.0,
        "Poster_views":478.0,
        "Solution_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.25,
        "Solution_score":1.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":25.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1431258605807,
        "Answerer_location":"Melbourne, Victoria, Australia",
        "Answerer_reputation":331.0,
        "Answerer_views":25.0,
        "Challenge_adjusted_solved_time":617.4599341667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Challenge_closed_time":1461479422230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459161991533,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459256566467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":8.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.4688327865,
        "Challenge_title":"Equivalent of Subset in Azure machine learning studio",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":243.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation":1677.0,
        "Poster_views":221.0,
        "Solution_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.54,
        "Solution_score":0.0,
        "Solution_sentence_count":4.0,
        "Solution_topic":"Columnar Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1280505139752,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation":4265.0,
        "Answerer_views":403.0,
        "Challenge_adjusted_solved_time":957.2375636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Challenge_closed_time":1646484376340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644217302490,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645197572643,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":6.4468980712,
        "Challenge_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Challenge_topic":"DataFrame Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":356.0,
        "Challenge_word_count":166,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1280505139752,
        "Poster_location":"Bangalore, India",
        "Poster_reputation":4265.0,
        "Poster_views":403.0,
        "Solution_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1648643627872,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":12.09,
        "Solution_score":1.0,
        "Solution_sentence_count":8.0,
        "Solution_topic":"DataFrame Manipulation",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":97.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1490275561927,
        "Answerer_location":"Tel Aviv",
        "Answerer_reputation":83.0,
        "Answerer_views":56.0,
        "Challenge_adjusted_solved_time":5100.4654275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am building a data transformation and training pipeline on Azure Machine Leaning Service. I'd like to save my fitted transformer (e.g. tf-idf) to the blob, so my prediction pipeline can later access it. <\/p>\n\n<pre><code>transformed_data = PipelineData(\"transformed_data\", \n                               datastore = default_datastore,\n                               output_path_on_compute=\"my_project\/tfidf\")\n\nstep_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data],\n                              inputs = [blob_train_data],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n\n<\/code><\/pre>\n\n<p>The above code saves the transformer to a current run's folder, which is dynamically generated during each run. <\/p>\n\n<p>I want to save the transformer to a fixed location on blob, so I can access it later, when calling a prediction pipeline.<\/p>\n\n<p>I tried to use an instance of <code>DataReference<\/code> class as <code>PythonScriptStep<\/code> output, but it results in an error: \n<code>ValueError: Unexpected output type: &lt;class 'azureml.data.data_reference.DataReference'&gt;<\/code> <\/p>\n\n<p>It's because <code>PythonScriptStep<\/code> only accepts <code>PipelineData<\/code> or <code>OutputPortBinding<\/code> objects as outputs.<\/p>\n\n<p>How could I save my fitted transformer so it's later accessible by any aribitraly process (e.g. my prediction pipeline)?<\/p>",
        "Challenge_closed_time":1562586040652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560330676657,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56558552",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":14.3,
        "Challenge_reading_time":20.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":6.4417277324,
        "Challenge_title":"How to save your fitted transformer into blob, so your prediction pipeline can use it in AML Service?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1056.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1490275561927,
        "Poster_location":"Tel Aviv",
        "Poster_reputation":83.0,
        "Poster_views":56.0,
        "Solution_body":"<p>Another solution is to pass <code>DataReference<\/code> as an input to your <code>PythonScriptStep<\/code>. <\/p>\n\n<p>Then inside <code>transform.py<\/code> you're able to read this <code>DataReference<\/code> as a command line argument. <\/p>\n\n<p>You can parse it and use it just as any regular path to save your vectorizer to.<\/p>\n\n<p>E.g. you can:<\/p>\n\n<pre><code>step_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data,\n                                           '--transformer_path', trained_transformer_path],\n                              inputs = [blob_train_data, trained_transformer_path],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n<\/code><\/pre>\n\n<p>Then inside your script (<code>transform.py<\/code> in the example above) you can e.g.:<\/p>\n\n<pre><code>import argparse\nimport joblib as jbl\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--transformer_path', dest=\"transformer_path\", required=True)\nargs = parser.parse_args()\n\ntfidf = ### HERE CREATE AND TRAIN YOUR VECTORIZER ###\n\nvect_filename = os.path.join(args.transformer_path, 'my_vectorizer.jbl')\n\n<\/code><\/pre>\n\n<hr>\n\n<p>EXTRA: The third way would be to just register the vectorizer as another model in your workspace. You can then use it exactly as any other registered model. (Though this option does not involve explicit writing to blob - as specified in the question above)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1578692352196,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":20.11,
        "Solution_score":1.0,
        "Solution_sentence_count":14.0,
        "Solution_topic":"Pipeline Automation",
        "Solution_topic_macro":"Lifecycle Management",
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1626973229036,
        "Answerer_location":null,
        "Answerer_reputation":199.0,
        "Answerer_views":37.0,
        "Challenge_adjusted_solved_time":626.8732219444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1626977403510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624722520977,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.4415145757,
        "Challenge_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":393.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1497621837832,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation":230.0,
        "Poster_views":27.0,
        "Solution_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979264576,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":21.93,
        "Solution_score":0.0,
        "Solution_sentence_count":11.0,
        "Solution_topic":"Docker Deployment",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":192.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":625.9599527778,
        "Challenge_answer_count":1,
        "Challenge_body":"How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. \"RunConfigurationException:\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=<class 'azureml.core.runconfig.OutputData'>...} with exception init() missing 2 required positional arguments: 'datastore_name' and 'relative_path'\"\n\nrun = Run.get(ws, 'run-id-123456789')\n\nfor child in run.get_children():\nprint(child.get_details())\ntry:\nchild.cancel()\nexcept Exception as e:\nprint(e)\ncontinue\n\nThe datasets and runs were configured properly because they run just fine.",
        "Challenge_closed_time":1651506755547,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649253299717,
        "Challenge_favorite_count":11.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_self_solved":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":6.4408826674,
        "Challenge_title":"Cancel all child runs in Azure ML",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"You should cancel all the children run by canceling the parent.\n\nAny benefit to cancel child once a time? Just curious",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.42,
        "Solution_score":0.0,
        "Solution_sentence_count":2.0,
        "Solution_topic":"Resource Quota",
        "Solution_topic_macro":"Service Management",
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Challenge_adjusted_solved_time":622.6719444444,
        "Challenge_answer_count":6,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248052000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630006433000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score":0.0,
        "Challenge_self_solved":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.4356245001,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Remote Storage",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":204,
        "Platform":"Github",
        "Poster_age":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Solution_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":15.53,
        "Solution_score":0.0,
        "Solution_sentence_count":12.0,
        "Solution_topic":"Remote Storage",
        "Solution_topic_macro":"Data Management",
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_age":null,
        "Answerer_created_time":1353317055072,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation":773.0,
        "Answerer_views":51.0,
        "Challenge_adjusted_solved_time":620.5295952778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to share a Azure notebook across multiple users who use different notebook VMs? It seems the VMs itself is not shareable across users. <\/p>",
        "Challenge_closed_time":1562565689110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560331782567,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56558892",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":5.6,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_self_solved":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.4321835289,
        "Challenge_title":"Share notebooks across Azure Machine learning service notebook VM",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1224.0,
        "Challenge_word_count":36,
        "Platform":"Stack Overflow",
        "Poster_age":null,
        "Poster_created_time":1353317055072,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation":773.0,
        "Poster_views":51.0,
        "Solution_body":"<p>Notebook VMs has own Jupyter environment and we don't need to use notebooks.azure.com. The former can be used in enterprise scenarios within the team to share the resources, and the latter is open, similar to google colab. When each user login to his notebook VM, there is a top level folder with his\/her alias and under that all notebooks are stored. this is stored in an Azure storage and each user's notebook VM will mount same storage. Hence If I want to view other person \\'s notebook, I need to navigate to his alias in the Jupyter nb in my nbvm<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":6.73,
        "Solution_score":2.0,
        "Solution_sentence_count":5.0,
        "Solution_topic":"Environment Configuration",
        "Solution_topic_macro":"Deployment Management",
        "Solution_word_count":101.0,
        "Tool":"Azure Machine Learning"
    }
]